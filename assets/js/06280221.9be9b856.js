"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[2057],{85162:function(n,e,t){t.d(e,{Z:function(){return i}});var a=t(67294),s=t(34334),o="tabItem_Ymn6";function i(n){let{children:e,hidden:t,className:i}=n;return a.createElement("div",{role:"tabpanel",className:(0,s.Z)(o,i),hidden:t},e)}},65488:function(n,e,t){t.d(e,{Z:function(){return _}});var a=t(83117),s=t(67294),o=t(34334),i=t(72389),l=t(67392),r=t(7094),u=t(12466),d="tabList__CuJ",c="tabItem_LNqP";function p(n){var e,t;const{lazy:i,block:p,defaultValue:_,values:f,groupId:m,className:h}=n,w=s.Children.map(n.children,(n=>{if((0,s.isValidElement)(n)&&"value"in n.props)return n;throw new Error("Docusaurus error: Bad <Tabs> child <"+("string"==typeof n.type?n.type:n.type.name)+'>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.')})),E=null!=f?f:w.map((n=>{let{props:{value:e,label:t,attributes:a}}=n;return{value:e,label:t,attributes:a}})),I=(0,l.l)(E,((n,e)=>n.value===e.value));if(I.length>0)throw new Error('Docusaurus error: Duplicate values "'+I.map((n=>n.value)).join(", ")+'" found in <Tabs>. Every value needs to be unique.');const k=null===_?_:null!=(e=null!=_?_:null==(t=w.find((n=>n.props.default)))?void 0:t.props.value)?e:w[0].props.value;if(null!==k&&!E.some((n=>n.value===k)))throw new Error('Docusaurus error: The <Tabs> has a defaultValue "'+k+'" but none of its children has the corresponding value. Available values are: '+E.map((n=>n.value)).join(", ")+". If you intend to show no default tab, use defaultValue={null} instead.");const{tabGroupChoices:v,setTabGroupChoices:b}=(0,r.U)(),[g,O]=(0,s.useState)(k),D=[],{blockElementScrollPositionUntilNextRender:R}=(0,u.o5)();if(null!=m){const n=v[m];null!=n&&n!==g&&E.some((e=>e.value===n))&&O(n)}const T=n=>{const e=n.currentTarget,t=D.indexOf(e),a=E[t].value;a!==g&&(R(e),O(a),null!=m&&b(m,String(a)))},A=n=>{var e;let t=null;switch(n.key){case"ArrowRight":{var a;const e=D.indexOf(n.currentTarget)+1;t=null!=(a=D[e])?a:D[0];break}case"ArrowLeft":{var s;const e=D.indexOf(n.currentTarget)-1;t=null!=(s=D[e])?s:D[D.length-1];break}}null==(e=t)||e.focus()};return s.createElement("div",{className:(0,o.Z)("tabs-container",d)},s.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,o.Z)("tabs",{"tabs--block":p},h)},E.map((n=>{let{value:e,label:t,attributes:i}=n;return s.createElement("li",(0,a.Z)({role:"tab",tabIndex:g===e?0:-1,"aria-selected":g===e,key:e,ref:n=>D.push(n),onKeyDown:A,onFocus:T,onClick:T},i,{className:(0,o.Z)("tabs__item",c,null==i?void 0:i.className,{"tabs__item--active":g===e})}),null!=t?t:e)}))),i?(0,s.cloneElement)(w.filter((n=>n.props.value===g))[0],{className:"margin-top--md"}):s.createElement("div",{className:"margin-top--md"},w.map(((n,e)=>(0,s.cloneElement)(n,{key:e,hidden:n.props.value!==g})))))}function _(n){const e=(0,i.Z)();return s.createElement(p,(0,a.Z)({key:String(e)},n))}},69065:function(n,e,t){t.r(e),t.d(e,{assets:function(){return c},contentTitle:function(){return u},default:function(){return f},frontMatter:function(){return r},metadata:function(){return d},toc:function(){return p}});var a=t(83117),s=(t(67294),t(3905)),o=t(65488),i=t(85162),l=t(66066);const r={description:"Make predictions on passages of text",sidebar_position:4},u="Audio",d={unversionedId:"api-guide/predict/audio",id:"api-guide/predict/audio",title:"Audio",description:"Make predictions on passages of text",source:"@site/docs/api-guide/predict/audio.md",sourceDirName:"api-guide/predict",slug:"/api-guide/predict/audio",permalink:"/api-guide/predict/audio",draft:!1,tags:[],version:"current",sidebarPosition:4,frontMatter:{description:"Make predictions on passages of text",sidebar_position:4},sidebar:"tutorialSidebar",previous:{title:"Text",permalink:"/api-guide/predict/text"},next:{title:"Prediction Parameters",permalink:"/api-guide/predict/prediction-parameters"}},c={},p=[{value:"Create a Workflow",id:"create-a-workflow",level:2},{value:"Predict via URL",id:"predict-via-url",level:2},{value:"Predict via Bytes",id:"predict-via-bytes",level:2}],_={toc:p};function f(n){let{components:e,...t}=n;return(0,s.kt)("wrapper",(0,a.Z)({},_,t,{components:e,mdxType:"MDXLayout"}),(0,s.kt)("h1",{id:"audio"},"Audio"),(0,s.kt)("p",null,(0,s.kt)("strong",{parentName:"p"},"Make predictions on audio inputs")),(0,s.kt)("hr",null),(0,s.kt)("p",null,"To make predictions on audio inputs, you need to use a ",(0,s.kt)("a",{parentName:"p",href:"https://docs.clarifai.com/api-guide/workflows/create-get-update-delete"},"workflow"),". Workflows is a useful Clarifai's feature that allows you to combine multiple models and carry out different operations. You can use Clarifai's built-in models or your own custom models."),(0,s.kt)("p",null,"In this case, we'll create a workflow that first does audio-to-text transcription and then makes predictions from the transcribed texts. "),(0,s.kt)("p",null,"The file size of each audio input should be less than 20MB."),(0,s.kt)("admonition",{type:"info"},(0,s.kt)("p",{parentName:"admonition"},"The initialization code used in the following examples is outlined in detail on the ",(0,s.kt)("a",{parentName:"p",href:"https://docs.clarifai.com/api-guide/api-overview/api-clients/#client-installation-instructions"},"client installation page."))),(0,s.kt)("h2",{id:"create-a-workflow"},"Create a Workflow"),(0,s.kt)("p",null,"In this example, we'll create a simple workflow that first converts an audio file to text and then analyses the sentiment of the transcribed text. "),(0,s.kt)("p",null,"We'll connect the following two models to achieve our objective:"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},"The ",(0,s.kt)("a",{parentName:"li",href:"https://clarifai.com/facebook/asr/models/asr-wav2vec2-base-960h-english"},"asr-wav2vec2-base-960h-english"),", which converts English audio to English text. "),(0,s.kt)("li",{parentName:"ul"},"The ",(0,s.kt)("a",{parentName:"li",href:"https://clarifai.com/erfan/text-classification/models/sentiment-analysis-distilbert-english"},"sentiment-analysis-distilbert-english"),", which predicts whether the sentiment of an English text is positive or negative. ")),(0,s.kt)("p",null,"We'll specify the IDs of the models and their versions\u2014since a model can have several versions."),(0,s.kt)(o.Z,{mdxType:"Tabs"},(0,s.kt)(i.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,s.kt)(l.Z,{className:"language-python",mdxType:"CodeBlock"},"########################################################################################\n# In this section, we set the user authentication, app ID, and the details of the new\n# custom workflow we want to create. Change these strings to run your own example.\n########################################################################################\n\nUSER_ID = 'YOUR_USER_ID_HERE'\n# Your PAT (Personal Access Token) can be found in the portal under Authentification\nPAT = 'YOUR_PAT_HERE'\nAPP_ID = 'YOUR_APP_ID_HERE'\n# Change these to create your own custom workflow\nWORKFLOW_ID = 'my-custom-workflow'\nNODE_ID_1 = 'audio-to-text'\nMODEL_ID_1 = 'asr-wav2vec2-base-960h-english'\nMODEL_VERSION_ID_1 = 'f4deae70a473492a8e2f9b7bb1dbee85'\n\nNODE_ID_2 = 'sentiment-analysis'\nMODEL_ID_2 = 'sentiment-analysis-distilbert-english'\nMODEL_VERSION_ID_2 = 'c0b09e606db94d9bae7eb40c626192fc'\n\n##########################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n##########################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (('authorization', 'Key ' + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID) # The userDataObject is required when using a PAT\n\npost_workflows_response = stub.PostWorkflows(\n    service_pb2.PostWorkflowsRequest(\n      user_app_id=userDataObject,  \n      workflows=[\n        resources_pb2.Workflow(\n          id=WORKFLOW_ID,\n          nodes=[\n            resources_pb2.WorkflowNode(\n              id=NODE_ID_1,\n              model=resources_pb2.Model(\n                id=MODEL_ID_1,\n                model_version=resources_pb2.ModelVersion(\n                  id=MODEL_VERSION_ID_1\n                )\n              )\n            ),\n            resources_pb2.WorkflowNode(\n              id=NODE_ID_2,\n              model=resources_pb2.Model(\n                id=MODEL_ID_2,\n                model_version=resources_pb2.ModelVersion(\n                  id=MODEL_VERSION_ID_2\n                )\n              ),\n              node_inputs=[\n                resources_pb2.NodeInput(node_id=NODE_ID_1)\n                ]\n            ),\n          ]\n        )\n      ]\n    ),\n    metadata=metadata\n)               \n\nif post_workflows_response.status.code != status_code_pb2.SUCCESS:\n    print(post_workflows_response.status)\n    raise Exception(\"Post workflows failed, status: \" + post_workflows_response.status.description) \n\n")),(0,s.kt)(i.Z,{value:"js_rest",label:"JavaScript (REST)",mdxType:"TabItem"},(0,s.kt)(l.Z,{className:"language-javascript",mdxType:"CodeBlock"},'\x3c!--index.html file--\x3e\n\n<script>\n    //////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, app ID, and the details of the new\n    // custom workflow we want to create. Change these strings to run your own example.\n    /////////////////////////////////////////////////////////////////////////////////////////\n\n    const USER_ID = \'YOUR_USER_ID_HERE\';\n    // Your PAT (Personal Access Token) can be found in the portal under Authentification\n    const PAT = \'YOUR_PAT_HERE\';\n    const APP_ID = \'YOUR_APP_ID_HERE\';\n    // Change these to create your own custom workflow\n    const WORKFLOW_ID = \'my-custom-workflow\';\n    const NODE_ID_1 = \'audio-to-text\';\n    const MODEL_ID_1 = \'asr-wav2vec2-base-960h-english\';\n    const MODEL_VERSION_ID_1 = \'f4deae70a473492a8e2f9b7bb1dbee85\';\n\n    const NODE_ID_2 = \'sentiment-analysis\';\n    const MODEL_ID_2 = \'sentiment-analysis-distilbert-english\';\n    const MODEL_VERSION_ID_2 = \'c0b09e606db94d9bae7eb40c626192fc\';\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////   \n\n    const raw = JSON.stringify({\n        "user_app_id": {\n            "user_id": USER_ID,\n            "app_id": APP_ID\n        },\n        "workflows": [{\n            "id": WORKFLOW_ID,\n            "nodes": [\n                {\n                    "id": NODE_ID_1,\n                    "model": {\n                        "id": MODEL_ID_1,\n                        "model_version": {\n                            "id": MODEL_VERSION_ID_1\n                        }\n                    }\n                },\n                {\n                    "id": NODE_ID_2,\n                    "model": {\n                        "id": MODEL_ID_2,\n                        "model_version": {\n                            "id": MODEL_VERSION_ID_2\n                        }\n                    },\n                        "node_inputs": [\n                            {\n                                "node_id": NODE_ID_1\n                            }\n                        ]\n                }\n            ]\n        }]\n    });\n\n    const requestOptions = {\n        method: \'POST\',\n        headers: {\n            \'Accept\': \'application/json\',\n            \'Authorization\': \'Key \' + PAT\n        },\n        body: raw\n    };\n\n    fetch(`https://api.clarifai.com/v2/workflows`, requestOptions)\n        .then(response => response.text())\n        .then(result => console.log(result))\n        .catch(error => console.log(\'error\', error));\n\n<\/script>')),(0,s.kt)(i.Z,{value:"nodejs",label:"NodeJS",mdxType:"TabItem"},(0,s.kt)(l.Z,{className:"language-javascript",mdxType:"CodeBlock"},"//index.js file\n\n//////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, app ID, and the details of the new\n// custom workflow we want to create. Change these strings to run your own example.\n/////////////////////////////////////////////////////////////////////////////////////////\n\nconst USER_ID = 'YOUR_USER_ID_HERE';\n// Your PAT (Personal Access Token) can be found in the portal under Authentification\nconst PAT = 'YOUR_PAT_HERE';\nconst APP_ID = 'YOUR_APP_ID_HERE';\n// Change these to create your own custom workflow\nconst WORKFLOW_ID = 'my-custom-workflow';\nconst NODE_ID_1 = 'audio-to-text';\nconst MODEL_ID_1 = 'asr-wav2vec2-base-960h-english';\nconst MODEL_VERSION_ID_1 = 'f4deae70a473492a8e2f9b7bb1dbee85';\n\nconst NODE_ID_2 = 'sentiment-analysis';\nconst MODEL_ID_2 = 'sentiment-analysis-distilbert-english';\nconst MODEL_VERSION_ID_2 = 'c0b09e606db94d9bae7eb40c626192fc';\n\n/////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n/////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require(\"clarifai-nodejs-grpc\");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set(\"authorization\", \"Key \" + PAT);\n\nstub.PostWorkflows(\n    {\n        user_app_id: {\n            \"user_id\": USER_ID,\n            \"app_id\": APP_ID\n        },\n        workflows: [\n            {\n                id: WORKFLOW_ID,\n                nodes: [\n                    {\n                        id: NODE_ID_1,\n                        model: {\n                            id: MODEL_ID_1,\n                            model_version: {\n                                id: MODEL_VERSION_ID_1\n                            }\n                        }\n                    },\n                    {\n                        id: NODE_ID_2,\n                        model: {\n                            id: MODEL_ID_2,\n                            model_version: {\n                                id: MODEL_VERSION_ID_2\n                            }\n                        },\n                        node_inputs: [\n                            {\n                                node_id: NODE_ID_1 \n                            }\n                        ]\n                    }\n                ]\n            }\n        ]\n    },\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            throw new Error(\"Post workflows failed, status: \" + response.status.description);\n        }\n    }\n);")),(0,s.kt)(i.Z,{value:"java",label:"Java",mdxType:"TabItem"},(0,s.kt)(l.Z,{className:"language-java",mdxType:"CodeBlock"},'package com.clarifai.example;\n\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.grpc.api.status.StatusCode;\n\npublic class ClarifaiExample {\n\n    //////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, app ID, and the details of the new\n    // custom workflow we want to create. Change these strings to run your own example.\n    /////////////////////////////////////////////////////////////////////////////////////////\n\n    static final String USER_ID = "YOUR_USER_ID_HERE";\n    //Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    static final String APP_ID = "YOUR_APP_ID_HERE";\n    // Change these to create your own custom workflow\n    static final String WORKFLOW_ID = "my-custom-workflow";\n    static final String NODE_ID_1 = "audio-to-text";\n    static final String MODEL_ID_1 = "asr-wav2vec2-base-960h-english";\n    static final String MODEL_VERSION_ID_1 = "f4deae70a473492a8e2f9b7bb1dbee85";\n\n    static final String NODE_ID_2 = "sentiment-analysis";\n    static final String MODEL_ID_2 = "sentiment-analysis-distilbert-english";\n    static final String MODEL_VERSION_ID_2 = "c0b09e606db94d9bae7eb40c626192fc";\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    public static void main(String[] args) {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n            .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        MultiWorkflowResponse postWorkflowsResponse = stub.postWorkflows(\n            PostWorkflowsRequest.newBuilder()\n            .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n            .addWorkflows(\n                Workflow.newBuilder()\n                .setId(WORKFLOW_ID)\n                .addNodes(\n                    WorkflowNode.newBuilder()\n                    .setId(NODE_ID_1)\n                    .setModel(\n                        Model.newBuilder()\n                        .setId(MODEL_ID_1)\n                        .setModelVersion(ModelVersion.newBuilder().setId(MODEL_VERSION_ID_1))\n                    )\n                )\n                .addNodes(\n                    WorkflowNode.newBuilder()\n                    .setId(NODE_ID_2)\n                    .setModel(\n                        Model.newBuilder()\n                        .setId(MODEL_ID_2)\n                        .setModelVersion(ModelVersion.newBuilder().setId(MODEL_VERSION_ID_2))\n                    )\n                    .addNodeInputs(NodeInput.newBuilder().setNodeId(NODE_ID_1))\n                )\n            ).build()\n        );\n\n        if (postWorkflowsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("Post workflows failed, status: " + postWorkflowsResponse.getStatus());\n        }\n\n    }\n\n}')),(0,s.kt)(i.Z,{value:"php",label:"PHP",mdxType:"TabItem"},(0,s.kt)(l.Z,{className:"language-php",mdxType:"CodeBlock"},'<?php\n\nrequire __DIR__ . "/vendor/autoload.php";\n\n//////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, app ID, and the details of the new\n// custom workflow we want to create. Change these strings to run your own example.\n/////////////////////////////////////////////////////////////////////////////////////////\n\n$USER_ID = "YOUR_USER_ID_HERE";\n// Your PAT (Personal Access Token) can be found in the portal under Authentification\n$PAT = "YOUR_PAT_HERE";\n$APP_ID = "YOUR_APP_ID_HERE";\n// Change these to create your own custom workflow\n$WORKFLOW_ID = "my-custom-workflow";\n$NODE_ID_1 = "audio-to-text";\n$MODEL_ID_1 = "asr-wav2vec2-base-960h-english";\n$MODEL_VERSION_ID_1 = "f4deae70a473492a8e2f9b7bb1dbee85";\n\n$NODE_ID_2 = "sentiment-analysis";\n$MODEL_ID_2 = "sentiment-analysis-distilbert-english";\n$MODEL_VERSION_ID_2 = "c0b09e606db94d9bae7eb40c626192fc";\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\PostWorkflowsRequest;\nuse Clarifai\\Api\\Workflow;\nuse Clarifai\\Api\\WorkflowNode;\nuse Clarifai\\Api\\NodeInput;\nuse Clarifai\\Api\\Model;\nuse Clarifai\\Api\\ModelVersion;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ["Authorization" => ["Key " . $PAT]]; \n\n$userDataObject = new UserAppIDSet([\n    "user_id" => $USER_ID,\n    "app_id" => $APP_ID\n]);\n\n// Let\'s make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client->PostWorkflows(\n        // The request object carries the request along with the request status and other metadata related to the request itself\n        new PostWorkflowsRequest([\n            "user_app_id" => $userDataObject,\n            "workflows" => [\n                new Workflow([\n                    "id"=> $WORKFLOW_ID,\n                    "nodes" => [\n                        new WorkflowNode([\n                            "id" => $NODE_ID_1,\n                            "model" => new Model([\n                                "id" => $MODEL_ID_1,\n                                "model_version" => new ModelVersion([\n                                    "id" => $MODEL_VERSION_ID_1\n                                ])\n                            ])\n\n                        ]),\n                        new WorkflowNode([\n                            "id" => $NODE_ID_2,\n                            "model"=> new Model([\n                                "id" => $MODEL_ID_2,\n                                "model_version" => new ModelVersion([\n                                    "id" => $MODEL_VERSION_ID_2\n                                ])\n                            ]),\n                            "node_inputs" => [\n                                new NodeInput([\n                                    "node_id"=> $NODE_ID_1\n                                ])\n                            ]\n                        ])                       \n                    ]\n                ])\n            ]\n        ]),\n        $metadata\n    )->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception("Error: {$status->details}");\n}\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure \n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    print $response->getStatus()->getDetails();\n    throw new Exception("Failure response: " . $response->getStatus()->getDescription());\n}\n\n?>\n')),(0,s.kt)(i.Z,{value:"curl",label:"cURL",mdxType:"TabItem"},(0,s.kt)(l.Z,{className:"language-bash",mdxType:"CodeBlock"},'curl -X POST "https://api.clarifai.com/v2/users/YOUR_USER_ID_HERE/apps/YOUR_APP_ID_HERE/workflows" \\\n    -H "Content-Type: application/json" \\\n    -H "Authorization: Key YOUR_PAT_HERE" \\\n    --data-raw \'{\n      "workflows": [{\n        "id": "my-custom-workflow",\n        "nodes": [\n          {\n            "id": "audio-to-text",\n            "model": {\n              "id": "asr-wav2vec2-base-960h-english",\n              "model_version": {\n                "id": "f4deae70a473492a8e2f9b7bb1dbee85"\n              }\n            }\n          },\n          {\n            "id": "sentiment-analysis",\n            "model": {\n              "id": "sentiment-analysis-distilbert-english",\n              "model_version": {\n                "id": "c0b09e606db94d9bae7eb40c626192fc"\n              }\n            },\n              "node_inputs": [\n                {\n                  "node_id": "audio-to-text"\n                }\n              ]\n          }\n        ]\n      }]\n    }\''))),(0,s.kt)("h2",{id:"predict-via-url"},"Predict via URL"),(0,s.kt)("p",null,"After creating the workflow, let's now use it to convert this ",(0,s.kt)("a",{parentName:"p",href:"https://samples.clarifai.com/negative_sentence_1.wav"},"audio file")," to text and then get the sentiment of the generated text. "),(0,s.kt)("p",null,"The response will contain the predictions each model in the workflow returns for the audio input."),(0,s.kt)(o.Z,{mdxType:"Tabs"},(0,s.kt)(i.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,s.kt)(l.Z,{className:"language-python",mdxType:"CodeBlock"},"#############################################################################\n# In this section, we set the user authentication, app ID, workflow ID, and  \n# audio URL. Change these strings to run your own example.\n##############################################################################\n\nUSER_ID = 'YOUR_USER_ID_HERE'\n# Your PAT (Personal Access Token) can be found in the portal under Authentification\nPAT = 'YOUR_PAT_HERE'\nAPP_ID = 'YOUR_APP_ID_HERE'\n# Change these to make your own predictions\nWORKFLOW_ID = 'my-custom-workflow'\nAUDIO_URL = 'https://samples.clarifai.com/negative_sentence_1.wav'\n\n##########################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n##########################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (('authorization', 'Key ' + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID) # The userDataObject is required when using a PAT\n\npost_workflow_results_response = stub.PostWorkflowResults(\n    service_pb2.PostWorkflowResultsRequest(\n        user_app_id=userDataObject,  \n        workflow_id=WORKFLOW_ID,\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    audio=resources_pb2.Audio(\n                        url=AUDIO_URL\n                    )\n                )\n            )\n        ]\n    ),\n    metadata=metadata\n)\nif post_workflow_results_response.status.code != status_code_pb2.SUCCESS:\n    print(post_workflow_results_response.status)\n    raise Exception(\"Post workflow results failed, status: \" + post_workflow_results_response.status.description)\n\n# We'll get one WorkflowResult for each input we used above. Because of one input, we have here one WorkflowResult\nresults = post_workflow_results_response.results[0]\n\n# Each model we have in the workflow will produce its output\nfor output in results.outputs:    \n    model = output.model    \n    print(\"Output for the model: `%s`\" % model.id)   \n    for concept in output.data.concepts:        \n        print(\"\\t%s %.2f\" % (concept.name, concept.value)) \n    print(output.data.text.raw)     \n\n# Uncomment this line to print the full Response JSON\n#print(results) \n")),(0,s.kt)(i.Z,{value:"js_rest",label:"JavaScript (REST)",mdxType:"TabItem"},(0,s.kt)(l.Z,{className:"language-javascript",mdxType:"CodeBlock"},'\x3c!--index.html file--\x3e\n\n<script>\n    ///////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, app ID, workflow ID, and\n    // audio URL. Change these strings to run your own example.\n    ///////////////////////////////////////////////////////////////////////////////////\n  \n    const USER_ID = "YOUR_USER_ID_HERE";\n    // Your PAT (Personal Access Token) can be found in the portal under Authentification\n    const PAT = "YOUR_PAT_HERE";\n    const APP_ID = "YOUR_APP_ID_HERE";\n    // Change these to make your own predictions\n    const WORKFLOW_ID = "my-custom-workflow";\n    const AUDIO_URL = "https://samples.clarifai.com/negative_sentence_1.wav";\n  \n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    /////////////////////////////////////////////////////////////////////////////////// \n  \n    const raw = JSON.stringify({\n      "user_app_id": {\n        "user_id": USER_ID,\n        "app_id": APP_ID\n      },\n      "inputs": [\n        {\n          "data": {\n            "audio": {\n              "url": AUDIO_URL\n            }\n          }\n        }\n      ]\n    });\n  \n    const requestOptions = {\n      method: \'POST\',\n      headers: {\n        \'Accept\': \'application/json\',\n        \'Authorization\': \'Key \' + PAT\n      },\n      body: raw\n    };\n  \n    fetch(`https://api.clarifai.com/v2/workflows/${WORKFLOW_ID}/results`, requestOptions)\n      .then(response => response.text())\n      .then(result => console.log(result))\n      .catch(error => console.log(\'error\', error));\n  <\/script>')),(0,s.kt)(i.Z,{value:"nodejs",label:"NodeJS",mdxType:"TabItem"},(0,s.kt)(l.Z,{className:"language-javascript",mdxType:"CodeBlock"},'//index.js file\n\n///////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, app ID, workflow ID, and\n// audio URL. Change these strings to run your own example.\n///////////////////////////////////////////////////////////////////////////////////\n\nconst USER_ID = \'YOUR_USER_ID_HERE\';\n// Your PAT (Personal Access Token) can be found in the portal under Authentification\nconst PAT = \'YOUR_PAT_HERE\';\nconst APP_ID = \'YOUR_APP_ID_HERE\';\n// Change these to make your own predictions\nconst WORKFLOW_ID = "my-custom-workflow";\nconst AUDIO_URL = "https://samples.clarifai.com/negative_sentence_1.wav";\n\n/////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n/////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\nstub.PostWorkflowResults(\n  {\n    user_app_id: {\n      "user_id": USER_ID,\n      "app_id": APP_ID,\n    },\n    workflow_id: WORKFLOW_ID,\n    inputs: [{ data: { audio: { url: AUDIO_URL } } }],\n  },\n  metadata,\n  (err, response) => {\n    if (err) {\n      throw new Error(err);\n    }\n\n    if (response.status.code !== 10000) {\n      throw new Error(\n        "Post workflow results failed, status: " + response.status.description\n      );\n    }\n\n    // We\'ll get one WorkflowResult for each input we used above. Because of one input, we have here \n    // one WorkflowResult\n    const results = response.results[0];\n\n    // Each model we have in the workflow will produce its output   \n    for (const output of results.outputs) {\n      const model = output.model;\n      console.log("Output for the model: `" + model.id + "`");        \n      for (const concept of output.data.concepts){    \n        console.log("\\t" + concept.name + " " + concept.value);        \n      } \n      if(output.data.text){\n      console.log(output.data.text.raw);        \n      }               \n    }\n  }\n);\n')),(0,s.kt)(i.Z,{value:"java",label:"Java",mdxType:"TabItem"},(0,s.kt)(l.Z,{className:"language-java",mdxType:"CodeBlock"},'package com.clarifai.example;\n\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.grpc.api.status.StatusCode;\n\npublic class ClarifaiExample {\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, app ID, workflow ID, and\n    // audio URL. Change these strings to run your own example.\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    static final String USER_ID = "YOUR_USER_ID_HERE";\n    //Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    static final String APP_ID = "YOUR_APP_ID_HERE";\n    // Change these to make your own predictions\n    static final String WORKFLOW_ID = "my-custom-workflow";\n    static final String AUDIO_URL = "https://samples.clarifai.com/negative_sentence_1.wav";\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    public static void main(String[] args) {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n            .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        PostWorkflowResultsResponse postWorkflowResultsResponse = stub.postWorkflowResults(\n            PostWorkflowResultsRequest.newBuilder()\n            .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n            .setWorkflowId(WORKFLOW_ID)\n            .addInputs(\n                Input.newBuilder().setData(\n                    Data.newBuilder().setAudio(\n                        Audio.newBuilder().setUrl(AUDIO_URL)\n                    )\n                )\n            )\n            .build()\n        );\n\n        if (postWorkflowResultsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("Post workflow results failed, status: " + postWorkflowResultsResponse.getStatus());\n        }\n\n        // We\'ll get one WorkflowResult for each input we used above. Because of one input, we have here\n        // one WorkflowResult\n        WorkflowResult results = postWorkflowResultsResponse.getResults(0);\n\n        // Each model we have in the workflow will produce its output\n        for (Output output: results.getOutputsList()) {\n            Model model = output.getModel();\n            System.out.println("Output for the model: `" + model.getId() + "`");   \n            for (Concept concept: output.getData().getConceptsList()) {       \n            \tSystem.out.printf("\\t%s %.2f%n",concept.getName(), concept.getValue());                 \n            }\n            System.out.println(output.getData().getText().getRaw());\n        }\n\n    }\n\n}')),(0,s.kt)(i.Z,{value:"php",label:"PHP",mdxType:"TabItem"},(0,s.kt)(l.Z,{className:"language-php",mdxType:"CodeBlock"},'<?php\n\nrequire __DIR__ . "/vendor/autoload.php";\n\n///////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, app ID, workflow ID, and\n// audio URL. Change these strings to run your own example.\n///////////////////////////////////////////////////////////////////////////////////\n\n$USER_ID = "YOUR_USER_ID_HERE";\n// Your PAT (Personal Access Token) can be found in the portal under Authentification\n$PAT = "YOUR_PAT_HERE";\n$APP_ID = "YOUR_APP_ID_HERE";\n// Change these to make your own predictions\n$WORKFLOW_ID = "my-custom-workflow";\n$AUDIO_URL = "https://samples.clarifai.com/negative_sentence_1.wav";\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\Api\\Audio;\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\PostWorkflowResultsRequest;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ["Authorization" => ["Key " . $PAT]]; \n\n$userDataObject = new UserAppIDSet([\n    "user_id" => $USER_ID,\n    "app_id" => $APP_ID\n]);\n\n// Let\'s make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client->PostWorkflowResults(\n        // The request object carries the request along with the request status and other metadata related to the request itself\n        new PostWorkflowResultsRequest([\n            "user_app_id" => $userDataObject,\n            "workflow_id" => $WORKFLOW_ID,\n            "inputs" => [\n                new Input([\n                    "data" => new Data([\n                        "audio" => new Audio([\n                            "url" => $AUDIO_URL\n                        ])\n                    ])\n                ])\n            ]          \n        ]),\n        $metadata\n    )->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception("Error: {$status->details}");\n}\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure \n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    print $response->getStatus()->getDetails();\n    throw new Exception("Failure response: " . $response->getStatus()->getDescription());\n}\n\n// We\'ll get one WorkflowResult for each input we used above. Because of one input, we have here one WorkflowResult\n$results = $response->getResults()[0];\n\n// Each model we have in the workflow will produce its output\nforeach ($results->getOutputs() as $output){\n    $model = $output->getModel();\n    print "Output for the model: `" . $model->getId() . "`" . "<br>";\n    foreach ($output->getData()->getConcepts() as $concept){\n        print $concept->getName() . " " . number_format($concept->getValue(),2) . "<br>";\n    }\n    print $output->getData()->getText()->getRaw() . "<br>";\n}\n\n?>\n')),(0,s.kt)(i.Z,{value:"curl",label:"cURL",mdxType:"TabItem"},(0,s.kt)(l.Z,{className:"language-bash",mdxType:"CodeBlock"},'curl -X POST "https://api.clarifai.com/v2/users/YOUR_USER_ID_HERE/apps/YOUR_APP_ID_HERE/workflows/YOUR_WORKFLOW_ID_HERE/results" \\\n  -H "authorization: Key YOUR_PAT_HERE" \\\n  -H "content-type: application/json" \\\n  -d \'{\n    "inputs": [\n        {\n          "data": {\n            "audio": {\n              "url": "https://samples.clarifai.com/negative_sentence_1.wav"\n          }\n        }\n      }\n    ]\n}\''))),(0,s.kt)("details",null,(0,s.kt)("summary",null,"Code Output Example"),(0,s.kt)(l.Z,{className:"language-text",mdxType:"CodeBlock"},"Output for the model: `asr-wav2vec2-base-960h-english`\nI AM NOT FLYING TO ENGLAND\nOutput for the model: `sentiment-analysis-distilbert-english`\n\tNEGATIVE 1.00\n\tPOSITIVE 0.00")),(0,s.kt)("details",null,(0,s.kt)("summary",null,"JSON Output Example"),(0,s.kt)(l.Z,{className:"language-javascript",mdxType:"CodeBlock"},'status {\n  code: SUCCESS\n  description: "Ok"\n}\ninput {\n  id: "9c39fec1ac9d4d58b7c1353c20cf8538"\n  data {\n    audio {\n      url: "https://samples.clarifai.com/negative_sentence_1.wav"\n    }\n  }\n}\noutputs {\n  id: "e04117c61f0b46fdb537b6663f235300"\n  status {\n    code: SUCCESS\n    description: "Ok"\n  }\n  created_at {\n    seconds: 1664555464\n    nanos: 495787635\n  }\n  model {\n    id: "asr-wav2vec2-base-960h-english"\n    name: "facebook/wav2vec2-base-960h"\n    created_at {\n      seconds: 1634677145\n      nanos: 661061000\n    }\n    app_id: "asr"\n    output_info {\n      output_config {\n      }\n      message: "Show output_info with: GET /models/{model_id}/output_info"\n      fields_map {\n        fields {\n          key: "text"\n          value {\n            string_value: "text"\n          }\n        }\n      }\n    }\n    model_version {\n      id: "f4deae70a473492a8e2f9b7bb1dbee85"\n      created_at {\n        seconds: 1634677145\n        nanos: 668993000\n      }\n      status {\n        code: MODEL_TRAINED\n        description: "Model is trained and ready"\n      }\n      visibility {\n        gettable: PUBLIC\n      }\n      app_id: "asr"\n      user_id: "facebook"\n      metadata {\n      }\n      license: "Apache-2.0"\n    }\n    user_id: "facebook"\n    input_info {\n      fields_map {\n        fields {\n          key: "audio"\n          value {\n            string_value: "audio"\n          }\n        }\n      }\n    }\n    train_info {\n    }\n    model_type_id: "audio-to-text"\n    visibility {\n      gettable: PUBLIC\n    }\n    modified_at {\n      seconds: 1658884251\n      nanos: 743015000\n    }\n    import_info {\n    }\n  }\n  data {\n    text {\n      raw: "I AM NOT FLYING TO ENGLAND"\n      text_info {\n        encoding: "UnknownTextEnc"\n      }\n    }\n  }\n}\noutputs {\n  id: "2f37022e94e74ac7a7a5f5e927bbcf66"\n  status {\n    code: SUCCESS\n    description: "Ok"\n  }\n  created_at {\n    seconds: 1664555464\n    nanos: 504551013\n  }\n  model {\n    id: "sentiment-analysis-distilbert-english"\n    name: "sentiment-analysis-distilbert-english"\n    created_at {\n      seconds: 1656524917\n      nanos: 269700000\n    }\n    app_id: "text-classification"\n    output_info {\n      output_config {\n      }\n      message: "Show output_info with: GET /models/{model_id}/output_info"\n      fields_map {\n        fields {\n          key: "concepts"\n          value {\n            string_value: "softmax"\n          }\n        }\n      }\n      params {\n        fields {\n          key: "max_concepts"\n          value {\n            number_value: 20.0\n          }\n        }\n        fields {\n          key: "min_value"\n          value {\n            number_value: 0.0\n          }\n        }\n        fields {\n          key: "select_concepts"\n          value {\n            list_value {\n            }\n          }\n        }\n      }\n    }\n    model_version {\n      id: "c0b09e606db94d9bae7eb40c626192fc"\n      created_at {\n        seconds: 1656524917\n        nanos: 276685000\n      }\n      status {\n        code: MODEL_TRAINED\n        description: "Model is trained and ready"\n      }\n      visibility {\n        gettable: PUBLIC\n      }\n      app_id: "text-classification"\n      user_id: "erfan"\n      metadata {\n        fields {\n          key: "Model version logs zipped"\n          value {\n            string_value: "https://s3.amazonaws.com/clarifai-temp/prod/c0b09e606db94d9bae7eb40c626192fc.zip"\n          }\n        }\n      }\n    }\n    user_id: "erfan"\n    input_info {\n      fields_map {\n        fields {\n          key: "text"\n          value {\n            string_value: "text"\n          }\n        }\n      }\n    }\n    train_info {\n      params {\n        fields {\n          key: "model_config"\n          value {\n            struct_value {\n              fields {\n                key: "_name_or_path"\n                value {\n                  string_value: "distilbert-base-uncased-finetuned-sst-2-english"\n                }\n              }\n              fields {\n                key: "activation"\n                value {\n                  string_value: "gelu"\n                }\n              }\n              fields {\n                key: "architectures"\n                value {\n                  list_value {\n                    values {\n                      string_value: "DistilBertForSequenceClassification"\n                    }\n                  }\n                }\n              }\n              fields {\n                key: "attention_dropout"\n                value {\n                  number_value: 0.1\n                }\n              }\n              fields {\n                key: "dim"\n                value {\n                  number_value: 768.0\n                }\n              }\n              fields {\n                key: "dropout"\n                value {\n                  number_value: 0.1\n                }\n              }\n              fields {\n                key: "finetuning_task"\n                value {\n                  string_value: "sst-2"\n                }\n              }\n              fields {\n                key: "hidden_dim"\n                value {\n                  number_value: 3072.0\n                }\n              }\n              fields {\n                key: "id2label"\n                value {\n                  struct_value {\n                    fields {\n                      key: "0"\n                      value {\n                        string_value: "NEGATIVE"\n                      }\n                    }\n                    fields {\n                      key: "1"\n                      value {\n                        string_value: "POSITIVE"\n                      }\n                    }\n                  }\n                }\n              }\n              fields {\n                key: "initializer_range"\n                value {\n                  number_value: 0.02\n                }\n              }\n              fields {\n                key: "label2id"\n                value {\n                  struct_value {\n                    fields {\n                      key: "NEGATIVE"\n                      value {\n                        number_value: 0.0\n                      }\n                    }\n                    fields {\n                      key: "POSITIVE"\n                      value {\n                        number_value: 1.0\n                      }\n                    }\n                  }\n                }\n              }\n              fields {\n                key: "max_position_embeddings"\n                value {\n                  number_value: 512.0\n                }\n              }\n              fields {\n                key: "model_type"\n                value {\n                  string_value: "distilbert"\n                }\n              }\n              fields {\n                key: "n_heads"\n                value {\n                  number_value: 12.0\n                }\n              }\n              fields {\n                key: "n_layers"\n                value {\n                  number_value: 6.0\n                }\n              }\n              fields {\n                key: "output_past"\n                value {\n                  bool_value: true\n                }\n              }\n              fields {\n                key: "pad_token_id"\n                value {\n                  number_value: 0.0\n                }\n              }\n              fields {\n                key: "qa_dropout"\n                value {\n                  number_value: 0.1\n                }\n              }\n              fields {\n                key: "seq_classif_dropout"\n                value {\n                  number_value: 0.2\n                }\n              }\n              fields {\n                key: "sinusoidal_pos_embds"\n                value {\n                  bool_value: false\n                }\n              }\n              fields {\n                key: "tie_weights_"\n                value {\n                  bool_value: true\n                }\n              }\n              fields {\n                key: "torch_dtype"\n                value {\n                  string_value: "float32"\n                }\n              }\n              fields {\n                key: "transformers_version"\n                value {\n                  string_value: "4.20.0"\n                }\n              }\n              fields {\n                key: "vocab_size"\n                value {\n                  number_value: 30522.0\n                }\n              }\n            }\n          }\n        }\n        fields {\n          key: "tokenizer_config"\n          value {\n            struct_value {\n              fields {\n                key: "cls_token"\n                value {\n                  string_value: "[CLS]"\n                }\n              }\n              fields {\n                key: "do_basic_tokenize"\n                value {\n                  bool_value: true\n                }\n              }\n              fields {\n                key: "do_lower_case"\n                value {\n                  bool_value: true\n                }\n              }\n              fields {\n                key: "mask_token"\n                value {\n                  string_value: "[MASK]"\n                }\n              }\n              fields {\n                key: "model_max_length"\n                value {\n                  number_value: 512.0\n                }\n              }\n              fields {\n                key: "name_or_path"\n                value {\n                  string_value: "distilbert-base-uncased-finetuned-sst-2-english"\n                }\n              }\n              fields {\n                key: "never_split"\n                value {\n                  null_value: NULL_VALUE\n                }\n              }\n              fields {\n                key: "pad_token"\n                value {\n                  string_value: "[PAD]"\n                }\n              }\n              fields {\n                key: "sep_token"\n                value {\n                  string_value: "[SEP]"\n                }\n              }\n              fields {\n                key: "special_tokens_map_file"\n                value {\n                  null_value: NULL_VALUE\n                }\n              }\n              fields {\n                key: "strip_accents"\n                value {\n                  null_value: NULL_VALUE\n                }\n              }\n              fields {\n                key: "tokenize_chinese_chars"\n                value {\n                  bool_value: true\n                }\n              }\n              fields {\n                key: "tokenizer_class"\n                value {\n                  string_value: "DistilBertTokenizer"\n                }\n              }\n              fields {\n                key: "unk_token"\n                value {\n                  string_value: "[UNK]"\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    model_type_id: "text-classifier"\n    visibility {\n      gettable: PUBLIC\n    }\n    modified_at {\n      seconds: 1656525047\n      nanos: 842099000\n    }\n    import_info {\n      params {\n        fields {\n          key: "model_name"\n          value {\n            string_value: "distilbert-base-uncased-finetuned-sst-2-english"\n          }\n        }\n        fields {\n          key: "pipeline_name"\n          value {\n            string_value: "text-classification"\n          }\n        }\n        fields {\n          key: "tokenizer_name"\n          value {\n            string_value: "distilbert-base-uncased-finetuned-sst-2-english"\n          }\n        }\n        fields {\n          key: "toolkit"\n          value {\n            string_value: "HuggingFace"\n          }\n        }\n      }\n    }\n  }\n  data {\n    concepts {\n      id: "NEGATIVE"\n      name: "NEGATIVE"\n      value: 0.9991872906684875\n      app_id: "c4660162651d490285bcbfc5aff50bf0"\n    }\n    concepts {\n      id: "POSITIVE"\n      name: "POSITIVE"\n      value: 0.0008126832544803619\n      app_id: "c4660162651d490285bcbfc5aff50bf0"\n    }\n  }\n}')),(0,s.kt)("h2",{id:"predict-via-bytes"},"Predict via Bytes"),(0,s.kt)("p",null,"Below is an example of how you would send the bytes of an audio and receive predictions from the above-mentioned workflow."),(0,s.kt)(o.Z,{mdxType:"Tabs"},(0,s.kt)(i.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,s.kt)(l.Z,{className:"language-python",mdxType:"CodeBlock"},"##############################################################################\n# In this section, we set the user authentication, app ID, workflow ID, and  \n# audio file location. Change these strings to run your own example.\n##############################################################################\n\nUSER_ID = 'YOUR_USER_ID_HERE'\n# Your PAT (Personal Access Token) can be found in the portal under Authentification\nPAT = 'YOUR_PAT_HERE'\nAPP_ID = 'YOUR_APP_ID_HERE'\n# Change these to make your own predictions\nWORKFLOW_ID = 'my-custom-workflow'\nAUDIO_FILE_LOCATION = 'YOUR_AUDIO_FILE_LOCATION_HERE' \n\n##########################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n##########################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (('authorization', 'Key ' + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID) # The userDataObject is required when using a PAT\n\nwith open(AUDIO_FILE_LOCATION, \"rb\") as f:\n    file_bytes = f.read()\n\npost_workflow_results_response = stub.PostWorkflowResults(\n    service_pb2.PostWorkflowResultsRequest(\n        user_app_id=userDataObject,  \n        workflow_id=WORKFLOW_ID,\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    audio=resources_pb2.Audio(\n                        base64=file_bytes\n                    )\n                )\n            )\n        ]\n    ),\n    metadata=metadata\n)\nif post_workflow_results_response.status.code != status_code_pb2.SUCCESS:\n    print(post_workflow_results_response.status)\n    raise Exception(\"Post workflow results failed, status: \" + post_workflow_results_response.status.description)\n\n# We'll get one WorkflowResult for each input we used above. Because of one input, we have here one WorkflowResult\nresults = post_workflow_results_response.results[0]\n\n# Each model we have in the workflow will produce its output\nfor output in results.outputs:    \n    model = output.model    \n    print(\"Output for the model: `%s`\" % model.id)   \n    for concept in output.data.concepts:        \n        print(\"\\t%s %.2f\" % (concept.name, concept.value)) \n    print(output.data.text.raw)     \n\n# Uncomment this line to print the full Response JSON\n#print(results) \n")),(0,s.kt)(i.Z,{value:"js_rest",label:"JavaScript (REST)",mdxType:"TabItem"},(0,s.kt)(l.Z,{className:"language-javascript",mdxType:"CodeBlock"},'\x3c!--index.html file--\x3e\n\n<script>\n    //////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, app ID, workflow ID, and bytes of\n    // the audio we want as an input. Change these strings to run your own example.\n    /////////////////////////////////////////////////////////////////////////////////////////\n  \n    const USER_ID = "YOUR_USER_ID_HERE";\n    // Your PAT (Personal Access Token) can be found in the portal under Authentification\n    const PAT = "YOUR_PAT_HERE";\n    const APP_ID = "YOUR_APP_ID_HERE";\n    // Change these to make your own predictions\n    const WORKFLOW_ID = "my-custom-workflow";\n    const AUDIO_BYTES_STRING = "YOUR_BYTES_STRING_HERE";\n  \n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    /////////////////////////////////////////////////////////////////////////////////// \n  \n    const raw = JSON.stringify({\n      "user_app_id": {\n        "user_id": USER_ID,\n        "app_id": APP_ID\n      },\n      "inputs": [\n        {\n          "data": {\n            "audio": {\n              "base64": AUDIO_BYTES_STRING\n            }\n          }\n        }\n      ]\n    });\n  \n    const requestOptions = {\n      method: \'POST\',\n      headers: {\n        \'Accept\': \'application/json\',\n        \'Authorization\': \'Key \' + PAT\n      },\n      body: raw\n    };\n  \n    fetch(`https://api.clarifai.com/v2/workflows/${WORKFLOW_ID}/results`, requestOptions)\n      .then(response => response.text())\n      .then(result => console.log(result))\n      .catch(error => console.log(\'error\', error));\n  <\/script>')),(0,s.kt)(i.Z,{value:"nodejs",label:"NodeJS",mdxType:"TabItem"},(0,s.kt)(l.Z,{className:"language-javascript",mdxType:"CodeBlock"},'//index.js file\n\n///////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, app ID, workflow ID, and\n// audio file location. Change these strings to run your own example.\n///////////////////////////////////////////////////////////////////////////////////\n\nconst USER_ID = \'YOUR_USER_ID_HERE\';\n// Your PAT (Personal Access Token) can be found in the portal under Authentification\nconst PAT = \'YOUR_PAT_HERE\';\nconst APP_ID = \'YOUR_APP_ID_HERE\';\n// Change these to make your own predictions\nconst WORKFLOW_ID = "my-custom-workflow";\nconst AUDIO_FILE_LOCATION = "YOUR_AUDIO_FILE_LOCATION_HERE";\n\n/////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n/////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\nconst fs = require("fs");\nconst audioBytes = fs.readFileSync(AUDIO_FILE_LOCATION);\n\nstub.PostWorkflowResults(\n  {\n    user_app_id: {\n      "user_id": USER_ID,\n      "app_id": APP_ID,\n    },\n    workflow_id: WORKFLOW_ID,\n    inputs: [{ data: { audio: { base64: audioBytes } } }],\n  },\n  metadata,\n  (err, response) => {\n    if (err) {\n      throw new Error(err);\n    }\n\n    if (response.status.code !== 10000) {\n      throw new Error(\n        "Post workflow results failed, status: " + response.status.description\n      );\n    }\n\n    // We\'ll get one WorkflowResult for each input we used above. Because of one input, we have here \n    // one WorkflowResult\n    const results = response.results[0];\n\n    // Each model we have in the workflow will produce its output   \n    for (const output of results.outputs) {\n      const model = output.model;\n      console.log("Output for the model: `" + model.id + "`");        \n      for (const concept of output.data.concepts){    \n        console.log("\\t" + concept.name + " " + concept.value);        \n      } \n      if(output.data.text){\n      console.log(output.data.text.raw);        \n      }               \n    }\n  }\n);\n')),(0,s.kt)(i.Z,{value:"java",label:"Java",mdxType:"TabItem"},(0,s.kt)(l.Z,{className:"language-java",mdxType:"CodeBlock"},'package com.clarifai.example;\n\nimport java.io.File;\nimport java.io.IOException;\nimport java.nio.file.Files;\n\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.grpc.api.status.StatusCode;\nimport com.google.protobuf.ByteString;\n\npublic class ClarifaiExample {\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, app ID, workflow ID, and\n    // audio file location. Change these strings to run your own example.\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    static final String USER_ID = "YOUR_USER_ID_HERE";\n    //Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    static final String APP_ID = "YOUR_APP_ID_HERE";\n    // Change these to make your own predictions\n    static final String WORKFLOW_ID = "my-custom-workflow";\n    static final String AUDIO_FILE_LOCATION = "YOUR_AUDIO_FILE_LOCATION_HERE";\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    public static void main(String[] args) throws IOException {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n            .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        PostWorkflowResultsResponse postWorkflowResultsResponse = stub.postWorkflowResults(\n            PostWorkflowResultsRequest.newBuilder()\n            .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n            .setWorkflowId(WORKFLOW_ID)\n            .addInputs(\n                Input.newBuilder().setData(\n                    Data.newBuilder().setAudio(\n                        Audio.newBuilder().setBase64(ByteString.copyFrom(Files.readAllBytes(\n                                new File(AUDIO_FILE_LOCATION).toPath()\n                                )))\n                    )\n                )\n            )\n            .build()\n        );\n\n        if (postWorkflowResultsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("Post workflow results failed, status: " + postWorkflowResultsResponse.getStatus());\n        }\n\n        // We\'ll get one WorkflowResult for each input we used above. Because of one input, we have here\n        // one WorkflowResult\n        WorkflowResult results = postWorkflowResultsResponse.getResults(0);\n\n        // Each model we have in the workflow will produce its output\n        for (Output output: results.getOutputsList()) {\n            Model model = output.getModel();\n            System.out.println("Output for the model: `" + model.getId() + "`");   \n            for (Concept concept: output.getData().getConceptsList()) {       \n            \tSystem.out.printf("\\t%s %.2f%n",concept.getName(), concept.getValue());                 \n            }\n            System.out.println(output.getData().getText().getRaw());\n        }\n\n    }\n\n}')),(0,s.kt)(i.Z,{value:"php",label:"PHP",mdxType:"TabItem"},(0,s.kt)(l.Z,{className:"language-php",mdxType:"CodeBlock"},'<?php\n\nrequire __DIR__ . "/vendor/autoload.php";\n\n//////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, app ID, workflow ID, and bytes of\n// the audio we want as an input. Change these strings to run your own example.\n/////////////////////////////////////////////////////////////////////////////////////////\n\n$USER_ID = "YOUR_USER_ID_HERE";\n// Your PAT (Personal Access Token) can be found in the portal under Authentification\n$PAT = "YOUR_PAT_HERE";\n$APP_ID = "YOUR_APP_ID_HERE";\n// Change these to make your own predictions\n$WORKFLOW_ID = "my-custom-workflow";\n$AUDIO_BYTES_STRING = "YOUR_AUDIO_FILE_LOCATION_HERE";\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\Api\\Audio;\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\PostWorkflowResultsRequest;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ["Authorization" => ["Key " . $PAT]]; \n\n$userDataObject = new UserAppIDSet([\n    "user_id" => $USER_ID,\n    "app_id" => $APP_ID\n]);\n\n$audioData = file_get_contents($AUDIO_BYTES_STRING); // Get the audio bytes data from the location\n\n// Let\'s make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client->PostWorkflowResults(\n        // The request object carries the request along with the request status and other metadata related to the request itself\n        new PostWorkflowResultsRequest([\n            "user_app_id" => $userDataObject,\n            "workflow_id" => $WORKFLOW_ID,\n            "inputs" => [\n                new Input([\n                    "data" => new Data([\n                        "audio" => new Audio([\n                            "base64" => $audioData\n                        ])\n                    ])\n                ])\n            ]          \n        ]),\n        $metadata\n    )->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception("Error: {$status->details}");\n}\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure \n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    print $response->getStatus()->getDetails();\n    throw new Exception("Failure response: " . $response->getStatus()->getDescription());\n}\n\n// We\'ll get one WorkflowResult for each input we used above. Because of one input, we have here one WorkflowResult\n$results = $response->getResults()[0];\n\n// Each model we have in the workflow will produce its output\nforeach ($results->getOutputs() as $output){\n    $model = $output->getModel();\n    print "Output for the model: `" . $model->getId() . "`" . "<br>";\n    foreach ($output->getData()->getConcepts() as $concept){\n        print $concept->getName() . " " . number_format($concept->getValue(),2) . "<br>";\n    }\n    print $output->getData()->getText()->getRaw() . "<br>";\n}\n\n?>\n')),(0,s.kt)(i.Z,{value:"curl",label:"cURL",mdxType:"TabItem"},(0,s.kt)(l.Z,{className:"language-bash",mdxType:"CodeBlock"},'curl -X POST "https://api.clarifai.com/v2/users/YOUR_USER_ID_HERE/apps/YOUR_APP_ID_HERE/workflows/YOUR_WORKFLOW_ID_HERE/results" \\\n  -H "authorization: Key YOUR_PAT_HERE" \\\n  -H "content-type: application/json" \\\n  -d \'{\n    "inputs": [\n        {\n          "data": {\n            "audio": {\n              "base64": "YOUR_BYTES_STRING_HERE"\n          }\n        }\n      }\n    ]\n}\''))),(0,s.kt)("details",null,(0,s.kt)("summary",null,"Code Output Example"),(0,s.kt)(l.Z,{className:"language-text",mdxType:"CodeBlock"},"Output for the model: `asr-wav2vec2-base-960h-english`\nI AM NOT FLYING TO ENGLAND\nOutput for the model: `sentiment-analysis-distilbert-english`\n\tNEGATIVE 1.00\n\tPOSITIVE 0.00")),(0,s.kt)("details",null,(0,s.kt)("summary",null,"JSON Output Example"),(0,s.kt)(l.Z,{className:"language-javascript",mdxType:"CodeBlock"},'status {\n    code: SUCCESS\n    description: "Ok"\n  }\n  input {\n    id: "c4e45044f96a4fc9b1a4a1ddbd24ee26"\n    data {\n      audio {\n        url: "https://samples.clarifai.com/placeholder.gif"\n        base64: "true"\n      }\n    }\n  }\n  outputs {\n    id: "557b489f3031436bbe5501f92d03461f"\n    status {\n      code: SUCCESS\n      description: "Ok"\n    }\n    created_at {\n      seconds: 1664805906\n      nanos: 332458633\n    }\n    model {\n      id: "asr-wav2vec2-base-960h-english"\n      name: "facebook/wav2vec2-base-960h"\n      created_at {\n        seconds: 1634677145\n        nanos: 661061000\n      }\n      app_id: "asr"\n      output_info {\n        output_config {\n        }\n        message: "Show output_info with: GET /models/{model_id}/output_info"\n        fields_map {\n          fields {\n            key: "text"\n            value {\n              string_value: "text"\n            }\n          }\n        }\n      }\n      model_version {\n        id: "f4deae70a473492a8e2f9b7bb1dbee85"\n        created_at {\n          seconds: 1634677145\n          nanos: 668993000\n        }\n        status {\n          code: MODEL_TRAINED\n          description: "Model is trained and ready"\n        }\n        visibility {\n          gettable: PUBLIC\n        }\n        app_id: "asr"\n        user_id: "facebook"\n        metadata {\n        }\n        license: "Apache-2.0"\n      }\n      user_id: "facebook"\n      input_info {\n        fields_map {\n          fields {\n            key: "audio"\n            value {\n              string_value: "audio"\n            }\n          }\n        }\n      }\n      train_info {\n      }\n      model_type_id: "audio-to-text"\n      visibility {\n        gettable: PUBLIC\n      }\n      modified_at {\n        seconds: 1658884251\n        nanos: 743015000\n      }\n      import_info {\n      }\n    }\n    data {\n      text {\n        raw: "I AM NOT FLYING TO ENGLAND"\n        text_info {\n          encoding: "UnknownTextEnc"\n        }\n      }\n    }\n  }\n  outputs {\n    id: "e7d1bf12c487435bb005504dd402642e"\n    status {\n      code: SUCCESS\n      description: "Ok"\n    }\n    created_at {\n      seconds: 1664805906\n      nanos: 336852253\n    }\n    model {\n      id: "sentiment-analysis-distilbert-english"\n      name: "sentiment-analysis-distilbert-english"\n      created_at {\n        seconds: 1656524917\n        nanos: 269700000\n      }\n      app_id: "text-classification"\n      output_info {\n        output_config {\n        }\n        message: "Show output_info with: GET /models/{model_id}/output_info"\n        fields_map {\n          fields {\n            key: "concepts"\n            value {\n              string_value: "softmax"\n            }\n          }\n        }\n        params {\n          fields {\n            key: "max_concepts"\n            value {\n              number_value: 20.0\n            }\n          }\n          fields {\n            key: "min_value"\n            value {\n              number_value: 0.0\n            }\n          }\n          fields {\n            key: "select_concepts"\n            value {\n              list_value {\n              }\n            }\n          }\n        }\n      }\n      model_version {\n        id: "c0b09e606db94d9bae7eb40c626192fc"\n        created_at {\n          seconds: 1656524917\n          nanos: 276685000\n        }\n        status {\n          code: MODEL_TRAINED\n          description: "Model is trained and ready"\n        }\n        visibility {\n          gettable: PUBLIC\n        }\n        app_id: "text-classification"\n        user_id: "erfan"\n        metadata {\n          fields {\n            key: "Model version logs zipped"\n            value {\n              string_value: "https://s3.amazonaws.com/clarifai-temp/prod/c0b09e606db94d9bae7eb40c626192fc.zip"\n            }\n          }\n        }\n      }\n      user_id: "erfan"\n      input_info {\n        fields_map {\n          fields {\n            key: "text"\n            value {\n              string_value: "text"\n            }\n          }\n        }\n      }\n      train_info {\n        params {\n          fields {\n            key: "model_config"\n            value {\n              struct_value {\n                fields {\n                  key: "_name_or_path"\n                  value {\n                    string_value: "distilbert-base-uncased-finetuned-sst-2-english"\n                  }\n                }\n                fields {\n                  key: "activation"\n                  value {\n                    string_value: "gelu"\n                  }\n                }\n                fields {\n                  key: "architectures"\n                  value {\n                    list_value {\n                      values {\n                        string_value: "DistilBertForSequenceClassification"\n                      }\n                    }\n                  }\n                }\n                fields {\n                  key: "attention_dropout"\n                  value {\n                    number_value: 0.1\n                  }\n                }\n                fields {\n                  key: "dim"\n                  value {\n                    number_value: 768.0\n                  }\n                }\n                fields {\n                  key: "dropout"\n                  value {\n                    number_value: 0.1\n                  }\n                }\n                fields {\n                  key: "finetuning_task"\n                  value {\n                    string_value: "sst-2"\n                  }\n                }\n                fields {\n                  key: "hidden_dim"\n                  value {\n                    number_value: 3072.0\n                  }\n                }\n                fields {\n                  key: "id2label"\n                  value {\n                    struct_value {\n                      fields {\n                        key: "0"\n                        value {\n                          string_value: "NEGATIVE"\n                        }\n                      }\n                      fields {\n                        key: "1"\n                        value {\n                          string_value: "POSITIVE"\n                        }\n                      }\n                    }\n                  }\n                }\n                fields {\n                  key: "initializer_range"\n                  value {\n                    number_value: 0.02\n                  }\n                }\n                fields {\n                  key: "label2id"\n                  value {\n                    struct_value {\n                      fields {\n                        key: "NEGATIVE"\n                        value {\n                          number_value: 0.0\n                        }\n                      }\n                      fields {\n                        key: "POSITIVE"\n                        value {\n                          number_value: 1.0\n                        }\n                      }\n                    }\n                  }\n                }\n                fields {\n                  key: "max_position_embeddings"\n                  value {\n                    number_value: 512.0\n                  }\n                }\n                fields {\n                  key: "model_type"\n                  value {\n                    string_value: "distilbert"\n                  }\n                }\n                fields {\n                  key: "n_heads"\n                  value {\n                    number_value: 12.0\n                  }\n                }\n                fields {\n                  key: "n_layers"\n                  value {\n                    number_value: 6.0\n                  }\n                }\n                fields {\n                  key: "output_past"\n                  value {\n                    bool_value: true\n                  }\n                }\n                fields {\n                  key: "pad_token_id"\n                  value {\n                    number_value: 0.0\n                  }\n                }\n                fields {\n                  key: "qa_dropout"\n                  value {\n                    number_value: 0.1\n                  }\n                }\n                fields {\n                  key: "seq_classif_dropout"\n                  value {\n                    number_value: 0.2\n                  }\n                }\n                fields {\n                  key: "sinusoidal_pos_embds"\n                  value {\n                    bool_value: false\n                  }\n                }\n                fields {\n                  key: "tie_weights_"\n                  value {\n                    bool_value: true\n                  }\n                }\n                fields {\n                  key: "torch_dtype"\n                  value {\n                    string_value: "float32"\n                  }\n                }\n                fields {\n                  key: "transformers_version"\n                  value {\n                    string_value: "4.20.0"\n                  }\n                }\n                fields {\n                  key: "vocab_size"\n                  value {\n                    number_value: 30522.0\n                  }\n                }\n              }\n            }\n          }\n          fields {\n            key: "tokenizer_config"\n            value {\n              struct_value {\n                fields {\n                  key: "cls_token"\n                  value {\n                    string_value: "[CLS]"\n                  }\n                }\n                fields {\n                  key: "do_basic_tokenize"\n                  value {\n                    bool_value: true\n                  }\n                }\n                fields {\n                  key: "do_lower_case"\n                  value {\n                    bool_value: true\n                  }\n                }\n                fields {\n                  key: "mask_token"\n                  value {\n                    string_value: "[MASK]"\n                  }\n                }\n                fields {\n                  key: "model_max_length"\n                  value {\n                    number_value: 512.0\n                  }\n                }\n                fields {\n                  key: "name_or_path"\n                  value {\n                    string_value: "distilbert-base-uncased-finetuned-sst-2-english"\n                  }\n                }\n                fields {\n                  key: "never_split"\n                  value {\n                    null_value: NULL_VALUE\n                  }\n                }\n                fields {\n                  key: "pad_token"\n                  value {\n                    string_value: "[PAD]"\n                  }\n                }\n                fields {\n                  key: "sep_token"\n                  value {\n                    string_value: "[SEP]"\n                  }\n                }\n                fields {\n                  key: "special_tokens_map_file"\n                  value {\n                    null_value: NULL_VALUE\n                  }\n                }\n                fields {\n                  key: "strip_accents"\n                  value {\n                    null_value: NULL_VALUE\n                  }\n                }\n                fields {\n                  key: "tokenize_chinese_chars"\n                  value {\n                    bool_value: true\n                  }\n                }\n                fields {\n                  key: "tokenizer_class"\n                  value {\n                    string_value: "DistilBertTokenizer"\n                  }\n                }\n                fields {\n                  key: "unk_token"\n                  value {\n                    string_value: "[UNK]"\n                  }\n                }\n              }\n            }\n          }\n        }\n      }\n      model_type_id: "text-classifier"\n      visibility {\n        gettable: PUBLIC\n      }\n      modified_at {\n        seconds: 1656525047\n        nanos: 842099000\n      }\n      import_info {\n        params {\n          fields {\n            key: "model_name"\n            value {\n              string_value: "distilbert-base-uncased-finetuned-sst-2-english"\n            }\n          }\n          fields {\n            key: "pipeline_name"\n            value {\n              string_value: "text-classification"\n            }\n          }\n          fields {\n            key: "tokenizer_name"\n            value {\n              string_value: "distilbert-base-uncased-finetuned-sst-2-english"\n            }\n          }\n          fields {\n            key: "toolkit"\n            value {\n              string_value: "HuggingFace"\n            }\n          }\n        }\n      }\n    }\n    data {\n      concepts {\n        id: "NEGATIVE"\n        name: "NEGATIVE"\n        value: 0.9991872906684875\n        app_id: "c4660162651d490285bcbfc5aff50bf0"\n      }\n      concepts {\n        id: "POSITIVE"\n        name: "POSITIVE"\n        value: 0.0008126832544803619\n        app_id: "c4660162651d490285bcbfc5aff50bf0"\n      }\n    }\n  }')))}f.isMDXComponent=!0}}]);
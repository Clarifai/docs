"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[7541],{36134:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>h,contentTitle:()=>p,default:()=>m,frontMatter:()=>f,metadata:()=>o,toc:()=>w});const o=JSON.parse('{"id":"api-guide/workflows/common-workflows/visual-text-recognition-walkthrough","title":"Visual Text Recognition","description":"Work with text in images, just like you work with encoded text.","source":"@site/docs/api-guide/workflows/common-workflows/visual-text-recognition-walkthrough.md","sourceDirName":"api-guide/workflows/common-workflows","slug":"/api-guide/workflows/common-workflows/visual-text-recognition-walkthrough","permalink":"/api-guide/workflows/common-workflows/visual-text-recognition-walkthrough","draft":false,"unlisted":false,"editUrl":"https://github.com/Clarifai/docs/blob/main/docs/api-guide/workflows/common-workflows/visual-text-recognition-walkthrough.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"description":"Work with text in images, just like you work with encoded text.","sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Custom KNN Face Classifier Workflow","permalink":"/api-guide/workflows/common-workflows/knn-face-classifier-workflow-walkthrough"},"next":{"title":"Custom Prompter Model","permalink":"/api-guide/workflows/common-workflows/prompter-model"}}');var a=t(74848),i=t(28453),r=t(65537),s=t(79329),l=t(58069);const d="###################################################################################\n# In this section, we set the user authentication, app ID, and the details of the \n# VTR Workflow we want to build. Change these strings to run your own example.\n##################################################################################\n\nUSER_ID = 'YOUR_USER_ID_HERE'\n# Your PAT (Personal Access Token) can be found in the Account's Security section\nPAT = 'YOUR_PAT_HERE'\nAPP_ID = 'YOUR_APP_ID_HERE'\n# Change these to build your own VTR Workflow\nWORKFLOW_ID = 'visual-text-recognition-id'\n\nWORKFLOWNODE_ID_1 = 'detect-concept'\nMODEL_ID_1 = '2419e2eae04d04f820e5cf3aba42d0c7'\nMODEL_VERSION_ID_1 = '75a5b92a0dec436a891b5ad224ac9170'\n\nWORKFLOWNODE_ID_2 = 'image-crop'\nMODEL_ID_2 = 'ce3f5832af7a4e56ae310d696cbbefd8'\nMODEL_VERSION_ID_2 = 'a78efb13f7774433aa2fd4864f41f0e6'\n\nWORKFLOWNODE_ID_3 = 'image-to-text'\nMODEL_ID_3 = '9fe78b4150a52794f86f237770141b33'\nMODEL_VERSION_ID_3 = 'd94413e582f341f68884cac72dbd2c7b'\n\n##########################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n##########################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (('authorization', 'Key ' + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID) # The userDataObject is required when using a PAT\n\npost_workflows_response = stub.PostWorkflows(\n    service_pb2.PostWorkflowsRequest(\n        user_app_id=userDataObject,  \n        workflows=[\n            resources_pb2.Workflow(\n                id=WORKFLOW_ID,\n                nodes=[\n                    resources_pb2.WorkflowNode(\n                        id=WORKFLOWNODE_ID_1,\n                        model=resources_pb2.Model(\n                            id=MODEL_ID_1,\n                            model_version=resources_pb2.ModelVersion(\n                                id=MODEL_VERSION_ID_1\n                            )\n                        )\n                    ),\n                    resources_pb2.WorkflowNode(\n                        id=WORKFLOWNODE_ID_2,\n                        model=resources_pb2.Model(\n                            id=MODEL_ID_2,\n                            model_version=resources_pb2.ModelVersion(\n                                id=MODEL_VERSION_ID_2\n                                )\n                            ),\n                            node_inputs=[\n                                resources_pb2.NodeInput(node_id=WORKFLOWNODE_ID_1)\n                            ]\n                        ),\n                    resources_pb2.WorkflowNode(\n                        id=WORKFLOWNODE_ID_3,\n                        model=resources_pb2.Model(\n                            id=MODEL_ID_3,\n                            model_version=resources_pb2.ModelVersion(\n                                id=MODEL_VERSION_ID_3\n                                )\n                            ),\n                            node_inputs=[\n                                resources_pb2.NodeInput(node_id=WORKFLOWNODE_ID_2)\n                            ]\n                        ),\n                ]\n            )\n        ]\n    ),\n    metadata=metadata\n)\n\nif post_workflows_response.status.code != status_code_pb2.SUCCESS:\n    raise Exception(\"Post workflows failed, status: \" + post_workflows_response.status.description)\n",c="//index.js file\n\n/////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, app ID, and the details of the \n// VTR Workflow we want to build. Change these strings to run your own example.\n/////////////////////////////////////////////////////////////////////////////////////\n\nconst USER_ID = 'YOUR_USER_ID_HERE';\n// Your PAT (Personal Access Token) can be found in the Account's Security section\nconst PAT = 'YOUR_PAT_HERE';\nconst APP_ID = 'YOUR_APP_ID_HERE';\n// Change these to build your own VTR Workflow\nconst WORKFLOW_ID = 'visual-text-recognition-id';\n\nconst WORKFLOWNODE_ID_1 = 'detect-concept';\nconst MODEL_ID_1 = '2419e2eae04d04f820e5cf3aba42d0c7';\nconst MODEL_VERSION_ID_1 = '75a5b92a0dec436a891b5ad224ac9170';\n\nconst WORKFLOWNODE_ID_2 = 'image-crop';\nconst MODEL_ID_2 = 'ce3f5832af7a4e56ae310d696cbbefd8';\nconst MODEL_VERSION_ID_2 = 'a78efb13f7774433aa2fd4864f41f0e6';\n\nconst WORKFLOWNODE_ID_3 = 'image-to-text';\nconst MODEL_ID_3 = '9fe78b4150a52794f86f237770141b33';\nconst MODEL_VERSION_ID_3 = 'd94413e582f341f68884cac72dbd2c7b';\n\n/////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n/////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require(\"clarifai-nodejs-grpc\");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set(\"authorization\", \"Key \" + PAT);\n\nstub.PostWorkflows(\n    {\n        user_app_id: {\n            user_id: USER_ID,\n            app_id: APP_ID\n        },\n        workflows: [\n            {\n                id: WORKFLOW_ID,\n                nodes: [\n                    {\n                        id: WORKFLOWNODE_ID_1,\n                        model: {\n                            id: MODEL_ID_1,\n                            model_version: {\n                                id: MODEL_VERSION_ID_1\n                            }\n                        }\n                    },\n                    {\n                        id: WORKFLOWNODE_ID_2,\n                        model: {\n                            id: MODEL_ID_2,\n                            model_version: {\n                                id: MODEL_VERSION_ID_2\n                            }\n                        },\n                        node_inputs: [\n                            {\n                                node_id: WORKFLOWNODE_ID_1\n                            }\n                        ]\n                    },\n                    {\n                        id: WORKFLOWNODE_ID_3,\n                        model: {\n                            id: MODEL_ID_3,\n                            model_version: {\n                                id: MODEL_VERSION_ID_3\n                            }\n                        },\n                        node_inputs: [\n                            {\n                                node_id: WORKFLOWNODE_ID_2\n                            }\n                        ]\n                    }\n                ]\n            }\n        ]\n    },\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            console.log(response.status);\n            throw new Error(\"Post workflows failed, status: \" + response.status.description);\n        }\n    }\n);",u='package com.clarifai.example;\n\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.grpc.api.status.StatusCode;\n\npublic class ClarifaiExample {\n\n    /////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, app ID, and the details of the \n    // VTR Workflow we want to build. Change these strings to run your own example.\n    /////////////////////////////////////////////////////////////////////////////////////\n\n    static final String USER_ID = "YOUR_USER_ID_HERE";\n    //Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    static final String APP_ID = "YOUR_APP_ID_HERE";\n    // Change these to build your own VTR Workflow\n    static final String WORKFLOW_ID = "visual-text-recognition-id";\n\n    static final String WORKFLOWNODE_ID_1 = "detect-concept";\n    static final String MODEL_ID_1 = "2419e2eae04d04f820e5cf3aba42d0c7";\n    static final String MODEL_VERSION_ID_1 = "75a5b92a0dec436a891b5ad224ac9170";\n\n    static final String WORKFLOWNODE_ID_2 = "image-crop";\n    static final String MODEL_ID_2 = "ce3f5832af7a4e56ae310d696cbbefd8";\n    static final String MODEL_VERSION_ID_2 = "a78efb13f7774433aa2fd4864f41f0e6";\n\n    static final String WORKFLOWNODE_ID_3 = "image-to-text";\n    static final String MODEL_ID_3 = "9fe78b4150a52794f86f237770141b33";\n    static final String MODEL_VERSION_ID_3 = "d94413e582f341f68884cac72dbd2c7b";\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    public static void main(String[] args) {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n            .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        MultiWorkflowResponse postWorkflowsResponse = stub.postWorkflows(\n            PostWorkflowsRequest.newBuilder()\n            .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n            .addWorkflows(\n                Workflow.newBuilder()\n                .setId(WORKFLOW_ID)\n                .addNodes(\n                    WorkflowNode.newBuilder()\n                    .setId(WORKFLOWNODE_ID_1)\n                    .setModel(\n                        Model.newBuilder()\n                        .setId(MODEL_ID_1)\n                        .setModelVersion(\n                            ModelVersion.newBuilder()\n                            .setId(MODEL_VERSION_ID_1)\n                        )\n                    )\n                )\n                .addNodes(\n                    WorkflowNode.newBuilder()\n                    .setId(WORKFLOWNODE_ID_2)\n                    .setModel(\n                        Model.newBuilder()\n                        .setId(MODEL_ID_2)\n                        .setModelVersion(\n                            ModelVersion.newBuilder()\n                            .setId(MODEL_VERSION_ID_2)\n                        )\n                    )\n                    .addNodeInputs(NodeInput.newBuilder().setNodeId(WORKFLOWNODE_ID_1))\n                )\n                .addNodes(\n                    WorkflowNode.newBuilder()\n                    .setId(WORKFLOWNODE_ID_3)\n                    .setModel(\n                        Model.newBuilder()\n                        .setId(MODEL_ID_3)\n                        .setModelVersion(\n                            ModelVersion.newBuilder()\n                            .setId(MODEL_VERSION_ID_3)\n                        )\n                    )\n                    .addNodeInputs(NodeInput.newBuilder().setNodeId(WORKFLOWNODE_ID_2))\n                )\n            )\n            .build()\n        );\n\n        if (postWorkflowsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("Post workflows failed, status: " + postWorkflowsResponse.getStatus());\n        }\n\n    }\n\n}',_='curl -X POST "https://api.clarifai.com/v2/users/YOUR_USER_ID_HERE/apps/YOUR_APP_ID_HERE/workflows" \\\n    -H "Authorization: Key YOUR_PAT_HERE" \\\n    -H "Content-Type: application/json" \\\n    --data-raw \'{\n        "workflows": [\n            {\n                "id": "visual-text-recognition-id",\n                "nodes": [\n                    {\n                        "id": "detect-concept",\n                        "model": {\n                            "id": "2419e2eae04d04f820e5cf3aba42d0c7",\n                            "model_version": {\n                                "id": "75a5b92a0dec436a891b5ad224ac9170"\n                            }\n                        }\n                    },\n                    {\n                        "id": "image-crop",\n                        "model": {\n                            "id": "ce3f5832af7a4e56ae310d696cbbefd8",\n                            "model_version": {\n                                "id": "a78efb13f7774433aa2fd4864f41f0e6"\n                            }\n                        },\n                        "node_inputs": [\n                            {\n                                "node_id": "general-concept"\n                            }\n                        ]\n                    },\n                    {\n                        "id": "image-to-text",\n                        "model": {\n                            "id": "9fe78b4150a52794f86f237770141b33",\n                            "model_version": {\n                                "id": "d94413e582f341f68884cac72dbd2c7b"\n                            }\n                        },\n                        "node_inputs": [\n                            {\n                                "node_id": "image-crop"\n                            }\n                        ]\n                    }\n                ]\n            }\n        ]\n    }\'',f={description:"Work with text in images, just like you work with encoded text.",sidebar_position:4},p="Visual Text Recognition",h={},w=[{value:"How VTR Works",id:"how-vtr-works",level:2},{value:"Building a VTR Workflow",id:"building-a-vtr-workflow",level:2}];function O(e){const n={a:"a",admonition:"admonition",h1:"h1",h2:"h2",header:"header",img:"img",li:"li",p:"p",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"visual-text-recognition",children:"Visual Text Recognition"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Work with text in images, just like you work with encoded text"})}),"\n",(0,a.jsx)("hr",{}),"\n",(0,a.jsx)(n.p,{children:"Visual text recognition (VTR) helps you convert printed text in images and videos into machine-encoded text. You can input a scanned document, a photo of a document, a scene-photo (such as the text on signs and billboards), or text superimposed on an image (such as in a meme), and output the words and individual characters present in the images."}),"\n",(0,a.jsx)(n.p,{children:'VTR lets you "digitize" text so that it can be edited, searched, stored, displayed, and analyzed.'}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{src:t(83073).A+"",width:"1000",height:"562"})}),"\n",(0,a.jsx)(n.admonition,{title:"Note",type:"tip",children:(0,a.jsx)(n.p,{children:"The current version of our VTR model is not designed for use with handwritten text or documents with tightly-packed text\u2014like you might see on the page of a novel, for example."})}),"\n",(0,a.jsx)(n.h2,{id:"how-vtr-works",children:"How VTR Works"}),"\n",(0,a.jsx)(n.p,{children:"VTR works by first detecting the location of text in your photos or video frames, then cropping the region where the text is present, and then finally running a specialized classification model that will extract text from the cropped image. To accomplish these different tasks, you will need to configure a workflow."}),"\n",(0,a.jsx)(n.p,{children:"You will then add these three models to your workflow:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.strong,{children:"Visual Text Detection"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.strong,{children:"1.0 Cropper"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.strong,{children:"Visual Text Recognition"})}),"\n"]}),"\n",(0,a.jsx)(n.admonition,{type:"info",children:(0,a.jsxs)(n.p,{children:["The initialization code used in the following example is outlined in detail on the ",(0,a.jsx)(n.a,{href:"https://docs.clarifai.com/api-guide/api-overview/api-clients/#client-installation-instructions",children:"client installation page."})]})}),"\n","\n","\n",(0,a.jsx)(n.h2,{id:"building-a-vtr-workflow",children:"Building a VTR Workflow"}),"\n",(0,a.jsxs)(r.A,{children:[(0,a.jsx)(s.A,{value:"grpc_python",label:"gRPC Python",children:(0,a.jsx)(l.A,{className:"language-python",children:d})}),(0,a.jsx)(s.A,{value:"grpc_nodejs",label:"gRPC NodeJS",children:(0,a.jsx)(l.A,{className:"language-javascript",children:c})}),(0,a.jsx)(s.A,{value:"grpc_java",label:"gRPC Java",children:(0,a.jsx)(l.A,{className:"language-java",children:u})}),(0,a.jsx)(s.A,{value:"curl",label:"cURL",children:(0,a.jsx)(l.A,{className:"language-bash",children:_})})]})]})}function m(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(O,{...e})}):O(e)}},65537:(e,n,t)=>{t.d(n,{A:()=>I});var o=t(96540),a=t(18215),i=t(65627),r=t(56347),s=t(50372),l=t(30604),d=t(11861),c=t(78749);function u(e){return o.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,o.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function _(e){const{values:n,children:t}=e;return(0,o.useMemo)((()=>{const e=n??function(e){return u(e).map((e=>{let{props:{value:n,label:t,attributes:o,default:a}}=e;return{value:n,label:t,attributes:o,default:a}}))}(t);return function(e){const n=(0,d.XI)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,t])}function f(e){let{value:n,tabValues:t}=e;return t.some((e=>e.value===n))}function p(e){let{queryString:n=!1,groupId:t}=e;const a=(0,r.W6)(),i=function(e){let{queryString:n=!1,groupId:t}=e;if("string"==typeof n)return n;if(!1===n)return null;if(!0===n&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:n,groupId:t});return[(0,l.aZ)(i),(0,o.useCallback)((e=>{if(!i)return;const n=new URLSearchParams(a.location.search);n.set(i,e),a.replace({...a.location,search:n.toString()})}),[i,a])]}function h(e){const{defaultValue:n,queryString:t=!1,groupId:a}=e,i=_(e),[r,l]=(0,o.useState)((()=>function(e){let{defaultValue:n,tabValues:t}=e;if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(n){if(!f({value:n,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${n}" but none of its children has the corresponding value. Available values are: ${t.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return n}const o=t.find((e=>e.default))??t[0];if(!o)throw new Error("Unexpected error: 0 tabValues");return o.value}({defaultValue:n,tabValues:i}))),[d,u]=p({queryString:t,groupId:a}),[h,w]=function(e){let{groupId:n}=e;const t=function(e){return e?`docusaurus.tab.${e}`:null}(n),[a,i]=(0,c.Dv)(t);return[a,(0,o.useCallback)((e=>{t&&i.set(e)}),[t,i])]}({groupId:a}),O=(()=>{const e=d??h;return f({value:e,tabValues:i})?e:null})();(0,s.A)((()=>{O&&l(O)}),[O]);return{selectedValue:r,selectValue:(0,o.useCallback)((e=>{if(!f({value:e,tabValues:i}))throw new Error(`Can't select invalid tab value=${e}`);l(e),u(e),w(e)}),[u,w,i]),tabValues:i}}var w=t(9136);const O={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var m=t(74848);function g(e){let{className:n,block:t,selectedValue:o,selectValue:r,tabValues:s}=e;const l=[],{blockElementScrollPositionUntilNextRender:d}=(0,i.a_)(),c=e=>{const n=e.currentTarget,t=l.indexOf(n),a=s[t].value;a!==o&&(d(n),r(a))},u=e=>{let n=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const t=l.indexOf(e.currentTarget)+1;n=l[t]??l[0];break}case"ArrowLeft":{const t=l.indexOf(e.currentTarget)-1;n=l[t]??l[l.length-1];break}}n?.focus()};return(0,m.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,a.A)("tabs",{"tabs--block":t},n),children:s.map((e=>{let{value:n,label:t,attributes:i}=e;return(0,m.jsx)("li",{role:"tab",tabIndex:o===n?0:-1,"aria-selected":o===n,ref:e=>{l.push(e)},onKeyDown:u,onClick:c,...i,className:(0,a.A)("tabs__item",O.tabItem,i?.className,{"tabs__item--active":o===n}),children:t??n},n)}))})}function b(e){let{lazy:n,children:t,selectedValue:i}=e;const r=(Array.isArray(t)?t:[t]).filter(Boolean);if(n){const e=r.find((e=>e.props.value===i));return e?(0,o.cloneElement)(e,{className:(0,a.A)("margin-top--md",e.props.className)}):null}return(0,m.jsx)("div",{className:"margin-top--md",children:r.map(((e,n)=>(0,o.cloneElement)(e,{key:n,hidden:e.props.value!==i})))})}function D(e){const n=h(e);return(0,m.jsxs)("div",{className:(0,a.A)("tabs-container",O.tabList),children:[(0,m.jsx)(g,{...n,...e}),(0,m.jsx)(b,{...n,...e})]})}function I(e){const n=(0,w.A)();return(0,m.jsx)(D,{...e,children:u(e.children)},String(n))}},79329:(e,n,t)=>{t.d(n,{A:()=>r});t(96540);var o=t(18215);const a={tabItem:"tabItem_Ymn6"};var i=t(74848);function r(e){let{children:n,hidden:t,className:r}=e;return(0,i.jsx)("div",{role:"tabpanel",className:(0,o.A)(a.tabItem,r),hidden:t,children:n})}},83073:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/vtr-7d53fc1ef292e02c58f04d0f9bed3df1.jpg"}}]);
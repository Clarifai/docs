"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[9463],{1464:(e,t,i)=>{i.d(t,{A:()=>n});const n=i.p+"assets/images/Kalman Reid Setup-52a159f3c0d6fd039c2ad651bf3aba3c.png"},2312:(e,t,i)=>{i.d(t,{A:()=>n});const n=i.p+"assets/images/byte_tracker_1-8d0c656fb858f1bde8c776fa6b8a9d39.png"},24545:(e,t,i)=>{i.d(t,{A:()=>n});const n=i.p+"assets/images/tracker-1-4b9eb361354a463a852fadcc3fca3d2e.png"},28453:(e,t,i)=>{i.d(t,{R:()=>o,x:()=>d});var n=i(96540);const r={},s=n.createContext(r);function o(e){const t=n.useContext(s);return n.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function d(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),n.createElement(s.Provider,{value:t},e.children)}},42832:(e,t,i)=>{i.r(t),i.d(t,{assets:()=>a,contentTitle:()=>d,default:()=>l,frontMatter:()=>o,metadata:()=>n,toc:()=>c});const n=JSON.parse('{"id":"portal-guide/agent-system-operators/tracker","title":"Tracker","description":"Learn about our tracker operators","source":"@site/docs/portal-guide/agent-system-operators/tracker.md","sourceDirName":"portal-guide/agent-system-operators","slug":"/portal-guide/agent-system-operators/tracker","permalink":"/portal-guide/agent-system-operators/tracker","draft":false,"unlisted":false,"editUrl":"https://github.com/Clarifai/docs/blob/main/docs/portal-guide/agent-system-operators/tracker.md","tags":[],"version":"current","sidebarPosition":8,"frontMatter":{"description":"Learn about our tracker operators","sidebar_position":8},"sidebar":"tutorialSidebar","previous":{"title":"Algorithmic Predict","permalink":"/portal-guide/agent-system-operators/algorithmic-predict"},"next":{"title":"Embed","permalink":"/portal-guide/agent-system-operators/embed"}}');var r=i(74848),s=i(28453);const o={description:"Learn about our tracker operators",sidebar_position:8},d="Tracker",a={},c=[{value:"BYTE Tracker",id:"byte-tracker",level:2},{value:"Centroid Tracker",id:"centroid-tracker",level:2},{value:"Kalman Filter Hungarian Tracker",id:"kalman-filter-hungarian-tracker",level:2},{value:"Kalman Reid Tracker",id:"kalman-reid-tracker",level:2},{value:"Tracker Operators Parameters",id:"tracker-operators-parameters",level:2}];function h(e){const t={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",header:"header",img:"img",li:"li",ol:"ol",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(t.header,{children:(0,r.jsx)(t.h1,{id:"tracker",children:"Tracker"})}),"\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.strong,{children:"Learn about our tracker operators"})}),"\n",(0,r.jsx)("hr",{}),"\n",(0,r.jsx)(t.p,{children:"Tracker operators are a specific type of agent system operators designed for object tracking in computer vision. Object tracking involves following the movement of objects in a sequence of images or frames in a video. Tracker models use detection-based tracking algorithms that don't require training and help them identify and track objects over time."}),"\n",(0,r.jsx)(t.p,{children:"The goal of object tracking is to maintain the identity of the object(s) over time, despite changes in position, scale, orientation, and lighting conditions."}),"\n",(0,r.jsx)(t.admonition,{type:"tip",children:(0,r.jsxs)(t.p,{children:['Since the tracker operators can be "chained" together with models to automate tasks in a workflow, you can learn how to create workflows ',(0,r.jsx)(t.a,{href:"https://docs.clarifai.com/portal-guide/workflows/input-nodes#create-your-workflow",children:"here"}),"."]})}),"\n",(0,r.jsx)(t.h2,{id:"byte-tracker",children:"BYTE Tracker"}),"\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"Input"}),": ",(0,r.jsx)(t.code,{children:"frames[\u2026].data.regions[\u2026].data.concepts"}),", ",(0,r.jsx)(t.code,{children:"frames[\u2026].data.regions[\u2026].region_info.bounding_box"})]}),"\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"Output"}),": ",(0,r.jsx)(t.code,{children:"frames[\u2026].data.regions[\u2026].track_id"})]}),"\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.a,{href:"https://arxiv.org/abs/2110.06864",children:"BYTE Tracker"})," is a multi-object tracking by-detection model built upon the ",(0,r.jsx)(t.a,{href:"https://arxiv.org/abs/1602.00763",children:"Simple Online and Real-time Tracking"})," (SORT) principles. Multi-object tracking aims to predict the bounding boxes and identities of objects within video sequences. BYTE tracker can also be seen as an enhanced version of the Kalman Filter Hungarian Tracker."]}),"\n",(0,r.jsx)(t.p,{children:"Most tracking techniques retrieve identities by associating detection boxes whose scores are higher than a threshold. Unlike simpler trackers that ditch detections with low confidence scores, BYTE Tracker considers them, too, making it better at handling situations like temporary occlusions or lighting changes."}),"\n",(0,r.jsx)(t.p,{children:"Typically, it works in two stages:"}),"\n",(0,r.jsxs)(t.ol,{children:["\n",(0,r.jsxs)(t.li,{children:["\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"High Confidence Matches"}),": First, BYTE Tracker focuses on high-scoring detections (bounding boxes around objects). It uses a combination of motion similarity (how much the object moved between frames) and appearance similarity (features extracted from the object) to match these detections with existing tracks (tracklets). A motion prediction technique is then used to predict the position of these tracks in the next frame."]}),"\n"]}),"\n",(0,r.jsxs)(t.li,{children:["\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"Low Confidence Recovery"}),": Here's where BYTE Tracker differs. It revisits the low confidence detections (discarded by simpler trackers) and unmatched tracklets from the previous stage. Using the same motion similarity metric, BYTE Tracker tries to re-associate these with each other, potentially recovering tracks that were lost due to occlusions or low initial confidence."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(t.p,{children:["With this powerful operator, you can seamlessly integrate object tracking into your detect-track workflows and unlock advanced capabilities. Let's demonstrate how you can use the BYTE Tracker, alongside ",(0,r.jsx)(t.a,{href:"https://docs.clarifai.com/portal-guide/model/model-types/visual-detector",children:"a detection model"}),", to efficiently track objects in videos."]}),"\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"1."})," Go to the workflow builder page. Search for the ",(0,r.jsx)(t.strong,{children:"visual-detector"})," option in the left-hand sidebar and drag it onto the empty workspace. Then, use the pop-up that appears on the right-hand sidebar to search for a detection model, such as ",(0,r.jsx)(t.a,{href:"https://clarifai.com/clarifai/main/models/general-image-detection",children:"general-image-detection"}),", and select its version. You can also set the other configuration options \u2014 including selecting the concepts you want to filter."]}),"\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"2."})," Search for the ",(0,r.jsx)(t.strong,{children:"byte-tracker"})," option in the left-hand sidebar and drag it onto the workspace. You can set up its output configuration parameters, which are outlined below."]}),"\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"3."})," Connect the ",(0,r.jsx)(t.strong,{children:"visual-detector"})," model with the ",(0,r.jsx)(t.strong,{children:"byte-tracker"})," operator and save your workflow."]}),"\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.img,{src:i(2312).A+"",width:"1920",height:"838"})}),"\n",(0,r.jsxs)(t.p,{children:["To observe it in action, navigate to the workflow's individual page and click the ",(0,r.jsx)(t.strong,{children:"+"})," button to input your video. For this example, let's provide ",(0,r.jsx)(t.a,{href:"https://samples.clarifai.com/kep_CLIP_20180509-114253.mp4",children:"this video"}),"."]}),"\n",(0,r.jsx)(t.p,{children:"The workflow will analyze the video and identify objects consistently throughout its duration."}),"\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.img,{alt:"alt text",src:i(53874).A+"",width:"3024",height:"1487"})}),"\n",(0,r.jsxs)(t.p,{children:["You can try this workflow ",(0,r.jsx)(t.a,{href:"https://clarifai.com/clarifai/Sample-Workflows-for-Docs/workflows/BYTE-Tracker?version=29cfec1d5dbd456c82a54e9302fd37ee",children:"here"}),"."]}),"\n",(0,r.jsx)(t.admonition,{type:"note",children:(0,r.jsxs)(t.p,{children:["Before trying to access the workflow, please make sure that you have a Clarifai account and are logged in to the Clarifai platform to access the example workflow. If you do not have clarifai account you can\xa0signup\xa0",(0,r.jsx)(t.a,{href:"https://clarifai.com/explore",children:"here"}),"."]})}),"\n",(0,r.jsx)(t.h2,{id:"centroid-tracker",children:"Centroid Tracker"}),"\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"Input"}),": ",(0,r.jsx)(t.code,{children:"frames[\u2026].data.regions[\u2026].data.concepts"}),", ",(0,r.jsx)(t.code,{children:"frames[\u2026].data.regions[\u2026].region_info.bounding_box"})]}),"\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"Output"}),": ",(0,r.jsx)(t.code,{children:"frames[\u2026].data.regions[\u2026].track_id"})]}),"\n",(0,r.jsx)(t.p,{children:"Centroid trackers rely on the Euclidean distance between centroids of regions in different video frames to assign the same track ID to detections of the same object."}),"\n",(0,r.jsx)(t.p,{children:"Here's a breakdown of how they operate:"}),"\n",(0,r.jsxs)(t.ol,{children:["\n",(0,r.jsxs)(t.li,{children:["\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"Object Detection"}),": In the first step, an object detector or a segmentation model (not part of the centroid tracker itself) identifies objects in each frame of a video. The detector outputs bounding boxes around the identified objects."]}),"\n"]}),"\n",(0,r.jsxs)(t.li,{children:["\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"Centroid Calculation"}),":  For each bounding box, the centroid tracker calculates its centroid. The centroid is simply the center point of the box, typically represented by its X and Y coordinates."]}),"\n"]}),"\n",(0,r.jsxs)(t.li,{children:["\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"Distance Comparison"}),": The tracker then compares the centroids of objects detected in the current frame with the centroids of objects from the previous frame. It calculates the Euclidean distance, which is a straight-line distance between two points in space."]}),"\n"]}),"\n",(0,r.jsxs)(t.li,{children:["\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"Track Assignment"}),": Based on a predefined threshold value, the tracker assigns track IDs. Objects in the current frame whose centroids are within a certain distance of a centroid in the previous frame are considered to be the same object and are assigned the same track ID. Objects with centroids exceeding the threshold distance are assumed to be new objects and assigned new track IDs."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(t.p,{children:["Let's demonstrate how you can use the centroid tracker, alongside ",(0,r.jsx)(t.a,{href:"https://docs.clarifai.com/portal-guide/model/model-types/visual-detector",children:"a detection model"}),", to efficiently track objects in videos."]}),"\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"1."})," Go to the workflow builder page. Search for the ",(0,r.jsx)(t.strong,{children:"visual-detector"})," option in the left-hand sidebar and drag it onto the empty workspace. Then, use the pop-up that appears on the right-hand sidebar to search for a detection model, such as ",(0,r.jsx)(t.a,{href:"https://clarifai.com/clarifai/main/models/general-image-detection",children:"general-image-detection"}),", and select its version. You can also set the other configuration options \u2014 including selecting the concepts you want to filter."]}),"\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"2."})," Search for the ",(0,r.jsx)(t.strong,{children:"centroid-tracker"})," option in the left-hand sidebar and drag it onto the workspace. You can set up its output configuration parameters, which are outlined below."]}),"\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"3."})," Connect the ",(0,r.jsx)(t.strong,{children:"visual-detector"})," model with the ",(0,r.jsx)(t.strong,{children:"centroid-tracker"})," operator and save your workflow."]}),"\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.img,{src:i(24545).A+"",width:"1911",height:"836"})}),"\n",(0,r.jsxs)(t.p,{children:["To observe it in action, navigate to the workflow's individual page and click the ",(0,r.jsx)(t.strong,{children:"+"})," button to input your video. For this example, let's provide ",(0,r.jsx)(t.a,{href:"https://samples.clarifai.com/kep_CLIP_20180509-114253.mp4",children:"this video"}),"."]}),"\n",(0,r.jsx)(t.p,{children:"The workflow will analyze the video and identify objects consistently throughout its duration."}),"\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.img,{src:i(75562).A+"",width:"1890",height:"866"})}),"\n",(0,r.jsxs)(t.p,{children:["You can try this workflow ",(0,r.jsx)(t.a,{href:"https://clarifai.com/clarifai/Sample-Workflows-for-Docs/workflows/Centroid-Tracker?version=ae5e933187e84428bf316caf79b8b3eb",children:"here"}),"."]}),"\n",(0,r.jsx)(t.admonition,{type:"note",children:(0,r.jsxs)(t.p,{children:["Before trying to access the workflow, please make sure that you have a Clarifai account and are logged in to the Clarifai platform to access the example workflow. If you do not have clarifai account you can\xa0signup\xa0",(0,r.jsx)(t.a,{href:"https://clarifai.com/explore",children:"here"}),"."]})}),"\n",(0,r.jsx)(t.h2,{id:"kalman-filter-hungarian-tracker",children:"Kalman Filter Hungarian Tracker"}),"\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"Output:"})," ",(0,r.jsx)(t.code,{children:"frames|...].data.regions...].data.concepts, frames[...].data.regions[...].region_info.bounding_box"})]}),"\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"Input:"})," ",(0,r.jsx)(t.code,{children:"frames|...].data.regions...] track_id"})]}),"\n",(0,r.jsx)(t.p,{children:"Kalman filter tracker rely on the Kalman filter algorithm to estimate an object's next position based on its position and velocity in previous frames. Then, detections are matched to predictions using the Hungarian algorithm. This sophisticated model excels in environments where objects move predictably, such as controlled traffic scenes or automated industrial processes."}),"\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.strong,{children:"Key Features:"})}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Kalman Filter"}),": Utilizes a state prediction model that accounts for the linear dynamics of moving objects, calculating their future states based on velocities and directional movements."]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Hungarian Algorithm"}),": Assigns new detections to predicted states by solving an optimization problem that minimizes the total cost of assigning predictions to observations, thus ensuring the best possible tracking accuracy."]}),"\n"]}),"\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.strong,{children:"Operational Details:"})}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsxs)(t.li,{children:["\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"State Prediction"}),": The Kalman filter predicts the future location of each tracked object based on its current state and motion prediction. This is crucial for maintaining track continuity in environments with predictable movement patterns."]}),"\n"]}),"\n",(0,r.jsxs)(t.li,{children:["\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"Detection Matching"}),": After prediction, detections from the current frame are matched to these predictions using the Hungarian algorithm. This method effectively handles assignments even under conditions where objects may occlude each other or momentarily disappear from view."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(t.p,{children:["Let's demonstrate how you can use the Kalman filter tracker, alongside ",(0,r.jsx)(t.a,{href:"https://docs.clarifai.com/portal-guide/model/model-types/visual-detector",children:"a detection model"}),", to efficiently track objects in videos."]}),"\n",(0,r.jsxs)(t.ol,{children:["\n",(0,r.jsxs)(t.li,{children:["\n",(0,r.jsxs)(t.p,{children:["Go to the workflow builder page. Search for the visual-detector option in the left-hand sidebar and drag it onto the empty workspace. Then, use the pop-up that appears on the right-hand sidebar to search for a detection model, such as ",(0,r.jsx)(t.a,{href:"https://clarifai.com/clarifai/main/models/general-image-detection",children:"general-image-detection"}),", and select its version. You can also set the other configuration options \u2014 including selecting the concepts you want to filter."]}),"\n"]}),"\n",(0,r.jsxs)(t.li,{children:["\n",(0,r.jsx)(t.p,{children:"Search for the Kalman filter tracker option in the left-hand sidebar and drag it onto the workspace. You can set up its output configuration parameters, which are outlined below."}),"\n"]}),"\n",(0,r.jsxs)(t.li,{children:["\n",(0,r.jsx)(t.p,{children:"Connect the visual-detector model with the centroid-tracker operator and save your workflow."}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.img,{alt:"alt text",src:i(47458).A+"",width:"3024",height:"1510"})}),"\n",(0,r.jsxs)(t.p,{children:["To observe it in action, navigate to the workflow's individual page and click the + button to input your video. For this example, let's provide ",(0,r.jsx)(t.a,{href:"https://samples.clarifai.com/JUFNERPILCQEHCZXSAZVCKZBYVUAQUXF.mp4",children:"this video"}),"."]}),"\n",(0,r.jsx)(t.p,{children:"The workflow will analyze the video and identify objects consistently throughout its duration."}),"\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.img,{alt:"alt text",src:i(59846).A+"",width:"3024",height:"1560"})}),"\n",(0,r.jsx)(t.h2,{id:"kalman-reid-tracker",children:"Kalman Reid Tracker"}),"\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"Output:"})," ",(0,r.jsx)(t.code,{children:"frames|...].data.regions|...].data.concepts"})]}),"\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"Input:"})," ",(0,r.jsx)(t.code,{children:"frames|...].data.regions...] track_id"})]}),"\n",(0,r.jsx)(t.p,{children:"The Kalman Reid Tracker is an advanced version of the Kalman filter tracking system, enhanced with Re-Identification (ReID) capabilities using appearance embeddings. This tracker is particularly effective in environments where objects frequently occlude each other or experience significant appearance changes, such as in crowded urban areas or complex indoor environments."}),"\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.strong,{children:"Key Features:"})}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Embedding Utilization"}),": Leverages appearance embeddings to provide a secondary layer of identity verification, which helps in accurately re-identifying objects even after they have been occluded or altered in appearance."]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Adaptive Tracking"}),": Integrates dynamic adjustments to tracking strategies based on the embedding distances, allowing for more flexible and robust tracking capabilities."]}),"\n"]}),"\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.strong,{children:"Operational Details"})}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsxs)(t.li,{children:["\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"Kalman Prediction"}),": Continuously predicts the next positions of objects based on their measured positions and velocities, using a state estimation technique that considers the physics of motion."]}),"\n"]}),"\n",(0,r.jsxs)(t.li,{children:["\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"ReID Application"}),": When there are discrepancies between predicted and observed positions, the system uses appearance embeddings to reassess and realign track IDs. This is particularly useful for maintaining tracking accuracy in scenarios where objects undergo significant appearance changes."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(t.p,{children:["Let's demonstrate how you can use the Kalman Reid tracker, alongside ",(0,r.jsx)(t.a,{href:"https://docs.clarifai.com/portal-guide/model/model-types/visual-detector",children:"a detection model"}),", to efficiently track objects in videos."]}),"\n",(0,r.jsxs)(t.ol,{children:["\n",(0,r.jsxs)(t.li,{children:["\n",(0,r.jsxs)(t.p,{children:["Go to the workflow builder page. Search for the visual-detector option in the left-hand sidebar and drag it onto the empty workspace. Then, use the pop-up that appears on the right-hand sidebar to search for a detection model, such as ",(0,r.jsx)(t.a,{href:"https://clarifai.com/clarifai/main/models/general-image-detection",children:"general-image-detection"}),", and select its version. You can also set the other configuration options \u2014 including selecting the concepts you want to filter."]}),"\n"]}),"\n",(0,r.jsxs)(t.li,{children:["\n",(0,r.jsx)(t.p,{children:"Search for the Kalman Reid tracker option in the left-hand sidebar and drag it onto the workspace. Then, you can set its output configuration parameters, which are outlined below."}),"\n"]}),"\n",(0,r.jsxs)(t.li,{children:["\n",(0,r.jsx)(t.p,{children:"Connect the visual-detector model with the centroid-tracker operator and save your workflow."}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.img,{alt:"alt text",src:i(1464).A+"",width:"3024",height:"1496"})}),"\n",(0,r.jsxs)(t.p,{children:["To observe it in action, navigate to the workflow's individual page and click the + button to input your video. For this example, let's provide ",(0,r.jsx)(t.a,{href:"https://samples.clarifai.com/JUFNERPILCQEHCZXSAZVCKZBYVUAQUXF.mp4",children:"this video"}),"."]}),"\n",(0,r.jsx)(t.p,{children:"The workflow will analyze the video and identify objects consistently throughout its duration."}),"\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.img,{alt:"alt text",src:i(75816).A+"",width:"3024",height:"1560"})}),"\n",(0,r.jsx)(t.h2,{id:"tracker-operators-parameters",children:"Tracker Operators Parameters"}),"\n",(0,r.jsx)(t.p,{children:"Here is a table outlining the various output configuration parameters you can configure for each operator (the \u2713 symbol represents the operator that supports the parameter)."}),"\n",(0,r.jsxs)(t.table,{children:[(0,r.jsx)(t.thead,{children:(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.th,{children:"Parameter"}),(0,r.jsx)(t.th,{children:"Description"}),(0,r.jsx)(t.th,{children:"BYTE Tracker"}),(0,r.jsx)(t.th,{children:"Centroid Tracker"}),(0,r.jsx)(t.th,{children:"Kalman Filter Hungarian Tracker"}),(0,r.jsx)(t.th,{children:"Kalman Reid Tracker"})]})}),(0,r.jsxs)(t.tbody,{children:[(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"min_confidence"})}),(0,r.jsx)(t.td,{children:"This is the minimum confidence score for detections to be considered for tracking"}),(0,r.jsx)(t.td,{children:"\u2713"}),(0,r.jsx)(t.td,{children:"\u2713"}),(0,r.jsx)(t.td,{children:"\u2713"}),(0,r.jsx)(t.td,{children:"\u2713"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"min_visible_frames"})}),(0,r.jsx)(t.td,{children:"Only return tracks with minimum visible frames > min_visible_frames"}),(0,r.jsx)(t.td,{children:"\u2713"}),(0,r.jsx)(t.td,{children:"\u2713"}),(0,r.jsx)(t.td,{children:"\u2713"}),(0,r.jsx)(t.td,{children:"\u2713"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"track_id_prefix"})}),(0,r.jsx)(t.td,{children:"Prefix to add on to track and eliminate conflicts"}),(0,r.jsx)(t.td,{children:"\u2713"}),(0,r.jsx)(t.td,{children:"\u2713"}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{children:"\u2713"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"max_disappeared"})}),(0,r.jsx)(t.td,{children:"This is the number of maximum consecutive frames a given object is allowed to be marked as \u201cdisappeared\u201d until we need to deregister the object from tracking"}),(0,r.jsx)(t.td,{children:"\u2713"}),(0,r.jsx)(t.td,{children:"\u2713"}),(0,r.jsx)(t.td,{children:"\u2713"}),(0,r.jsx)(t.td,{children:"\u2713"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"new_track_confidence_thresh"})}),(0,r.jsx)(t.td,{children:"Initialize a new track if the confidence score of the new detection is greater than the setting"}),(0,r.jsx)(t.td,{children:"\u2713"}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"confidence_thresh"})}),(0,r.jsx)(t.td,{children:"This is used to categorize high score detections for the first association if their scores are greater, and the second association if not"}),(0,r.jsx)(t.td,{children:"\u2713"}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"high_confidence_match_thresh"})}),(0,r.jsx)(t.td,{children:"The distance threshold for high-score detection"}),(0,r.jsx)(t.td,{children:"\u2713"}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"low_confidence_match_thresh"})}),(0,r.jsx)(t.td,{children:"The distance threshold for low-score detection"}),(0,r.jsx)(t.td,{children:"\u2713"}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"unconfirmed_match_thresh"})}),(0,r.jsxs)(t.td,{children:["The distance threshold for unconfirmed tracks, usually tracks with only one beginning frame. ",(0,r.jsx)(t.code,{children:"{\u201cmin\u201d: 0, \u201cmax\u201d: 1}"})]}),(0,r.jsx)(t.td,{children:"\u2713"}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"max_distance"})}),(0,r.jsx)(t.td,{children:"Associate tracks with detections only when their distance is below max_distance"}),(0,r.jsx)(t.td,{children:"\u2713"}),(0,r.jsx)(t.td,{children:"\u2713"}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{children:"\u2713"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"filtered_probability"})}),(0,r.jsx)(t.td,{children:"If false, return original detection probability; if true, return processed probability from the tracker"}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{children:"\u2713"}),(0,r.jsx)(t.td,{})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"max_detection"})}),(0,r.jsx)(t.td,{children:"Maximum detection per frame"}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{children:"\u2713"}),(0,r.jsx)(t.td,{})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"has_probability"})}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{children:"\u2713"}),(0,r.jsx)(t.td,{})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"has_embedding"})}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{children:"\u2713"}),(0,r.jsx)(t.td,{})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"association_confidence"})}),(0,r.jsx)(t.td,{children:"The list of association confidences to perform for each round"}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{children:"\u2713"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"covariance_error"})}),(0,r.jsx)(t.td,{children:"Magnitude of the uncertainty on the initial state"}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{children:"\u2713"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"observation_error"})}),(0,r.jsx)(t.td,{children:"Magnitude of the uncertainty on detection coordinates"}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{children:"\u2713"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"distance_metric"})}),(0,r.jsx)(t.td,{children:"Distance metric for Hungarian matching"}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{children:"\u2713"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"initialization_confidence"})}),(0,r.jsx)(t.td,{children:"Confidence for starting a new track. Must be > min_confidence to have an effect"}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{children:"\u2713"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"project_track"})}),(0,r.jsx)(t.td,{children:"How many frames in total to the project box when detection isn\u2019t recorded for track"}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{children:"\u2713"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"use_detect_box"})}),(0,r.jsx)(t.td,{children:"How many frames to project the last detection box, should be less than project_track_frames (1 is the current frame)"}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{children:"\u2713"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"project_without_detect"})}),(0,r.jsx)(t.td,{children:"Whether to keep projecting the box forward if no detect is matched"}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{children:"\u2713"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"project_fix_box_size"})}),(0,r.jsx)(t.td,{children:"Whether to fix the box size when the track is in a project state"}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{children:"\u2713"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"detect_box_fall_back"})}),(0,r.jsx)(t.td,{children:"Rely on the detect box if the association error is above this value"}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{children:"\u2713"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"keep_track_in_image"})}),(0,r.jsx)(t.td,{children:"If this is 1, then push the tracker predict to stay inside image boundaries"}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{children:"\u2713"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"match_limit_ratio"})}),(0,r.jsx)(t.td,{children:"Multiplier to constrain association (< 1 is ignored) based on other associations"}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{children:"\u2713"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"match_limit_min_matches"})}),(0,r.jsx)(t.td,{children:"Minimum number of matched tracks needed to invoke match limit"}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{children:"\u2713"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"optimal_assignment"})}),(0,r.jsx)(t.td,{children:"If True, rule out pairs with distance > max_distance before assignment"}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{children:"\u2713"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"max_emb_distance"})}),(0,r.jsx)(t.td,{children:"Maximum embedding distance to be considered a re-identification"}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{children:"\u2713"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"max_dead"})}),(0,r.jsx)(t.td,{children:"Maximum number of frames for track to be dead before we re-assign the ID"}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{children:"\u2713"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"var_tracker"})}),(0,r.jsx)(t.td,{children:"String that determines how embeddings from multiple timestamps are aggregated, defaults to \u201cna\u201d (most recent embedding overwrites past embeddings)"}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{children:"\u2713"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"reid_model_path"})}),(0,r.jsx)(t.td,{children:"The path to the linker"}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{children:"\u2713"})]})]})]})]})}function l(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,r.jsx)(t,{...e,children:(0,r.jsx)(h,{...e})}):h(e)}},47458:(e,t,i)=>{i.d(t,{A:()=>n});const n=i.p+"assets/images/Kalman Filter Setup-60094cdfe207ee9f86b5e8687b919339.png"},53874:(e,t,i)=>{i.d(t,{A:()=>n});const n=i.p+"assets/images/Byte Tracker Output-73bef8361da175af8b6670195cdfe898.png"},59846:(e,t,i)=>{i.d(t,{A:()=>n});const n=i.p+"assets/images/Kalman Filter Output-3f7ab967d907e58c3956c6508fa41a3e.png"},75562:(e,t,i)=>{i.d(t,{A:()=>n});const n=i.p+"assets/images/tracker-2-4264b84c1f76369edccfcf9b313dac8f.png"},75816:(e,t,i)=>{i.d(t,{A:()=>n});const n=i.p+"assets/images/Kalman Reid Output-34c780d3e79d61366bdfec03c5cc79e9.png"}}]);
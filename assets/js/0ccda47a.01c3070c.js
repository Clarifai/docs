"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[9919],{85162:(e,t,n)=>{n.d(t,{Z:()=>o});var a=n(67294),s=n(86010);const i={tabItem:"tabItem_Ymn6"};function o(e){let{children:t,hidden:n,className:o}=e;return a.createElement("div",{role:"tabpanel",className:(0,s.Z)(i.tabItem,o),hidden:n},t)}},74866:(e,t,n)=>{n.d(t,{Z:()=>O});var a=n(87462),s=n(67294),i=n(86010),o=n(12466),r=n(16550),l=n(91980),u=n(67392),c=n(50012);function p(e){return function(e){return s.Children.map(e,(e=>{if(!e||(0,s.isValidElement)(e)&&function(e){const{props:t}=e;return!!t&&"object"==typeof t&&"value"in t}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}(e).map((e=>{let{props:{value:t,label:n,attributes:a,default:s}}=e;return{value:t,label:n,attributes:a,default:s}}))}function d(e){const{values:t,children:n}=e;return(0,s.useMemo)((()=>{const e=t??p(n);return function(e){const t=(0,u.l)(e,((e,t)=>e.value===t.value));if(t.length>0)throw new Error(`Docusaurus error: Duplicate values "${t.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[t,n])}function h(e){let{value:t,tabValues:n}=e;return n.some((e=>e.value===t))}function m(e){let{queryString:t=!1,groupId:n}=e;const a=(0,r.k6)(),i=function(e){let{queryString:t=!1,groupId:n}=e;if("string"==typeof t)return t;if(!1===t)return null;if(!0===t&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:t,groupId:n});return[(0,l._X)(i),(0,s.useCallback)((e=>{if(!i)return;const t=new URLSearchParams(a.location.search);t.set(i,e),a.replace({...a.location,search:t.toString()})}),[i,a])]}function f(e){const{defaultValue:t,queryString:n=!1,groupId:a}=e,i=d(e),[o,r]=(0,s.useState)((()=>function(e){let{defaultValue:t,tabValues:n}=e;if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(t){if(!h({value:t,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${t}" but none of its children has the corresponding value. Available values are: ${n.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return t}const a=n.find((e=>e.default))??n[0];if(!a)throw new Error("Unexpected error: 0 tabValues");return a.value}({defaultValue:t,tabValues:i}))),[l,u]=m({queryString:n,groupId:a}),[p,f]=function(e){let{groupId:t}=e;const n=function(e){return e?`docusaurus.tab.${e}`:null}(t),[a,i]=(0,c.Nk)(n);return[a,(0,s.useCallback)((e=>{n&&i.set(e)}),[n,i])]}({groupId:a}),_=(()=>{const e=l??p;return h({value:e,tabValues:i})?e:null})();(0,s.useLayoutEffect)((()=>{_&&r(_)}),[_]);return{selectedValue:o,selectValue:(0,s.useCallback)((e=>{if(!h({value:e,tabValues:i}))throw new Error(`Can't select invalid tab value=${e}`);r(e),u(e),f(e)}),[u,f,i]),tabValues:i}}var _=n(72389);const g={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};function T(e){let{className:t,block:n,selectedValue:r,selectValue:l,tabValues:u}=e;const c=[],{blockElementScrollPositionUntilNextRender:p}=(0,o.o5)(),d=e=>{const t=e.currentTarget,n=c.indexOf(t),a=u[n].value;a!==r&&(p(t),l(a))},h=e=>{let t=null;switch(e.key){case"Enter":d(e);break;case"ArrowRight":{const n=c.indexOf(e.currentTarget)+1;t=c[n]??c[0];break}case"ArrowLeft":{const n=c.indexOf(e.currentTarget)-1;t=c[n]??c[c.length-1];break}}t?.focus()};return s.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,i.Z)("tabs",{"tabs--block":n},t)},u.map((e=>{let{value:t,label:n,attributes:o}=e;return s.createElement("li",(0,a.Z)({role:"tab",tabIndex:r===t?0:-1,"aria-selected":r===t,key:t,ref:e=>c.push(e),onKeyDown:h,onClick:d},o,{className:(0,i.Z)("tabs__item",g.tabItem,o?.className,{"tabs__item--active":r===t})}),n??t)})))}function E(e){let{lazy:t,children:n,selectedValue:a}=e;const i=(Array.isArray(n)?n:[n]).filter(Boolean);if(t){const e=i.find((e=>e.props.value===a));return e?(0,s.cloneElement)(e,{className:"margin-top--md"}):null}return s.createElement("div",{className:"margin-top--md"},i.map(((e,t)=>(0,s.cloneElement)(e,{key:t,hidden:e.props.value!==a}))))}function I(e){const t=f(e);return s.createElement("div",{className:(0,i.Z)("tabs-container",g.tabList)},s.createElement(T,(0,a.Z)({},e,t)),s.createElement(E,(0,a.Z)({},e,t)))}function O(e){const t=(0,_.Z)();return s.createElement(I,(0,a.Z)({key:String(t)},e))}},24121:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>$,contentTitle:()=>U,default:()=>Y,frontMatter:()=>k,metadata:()=>M,toc:()=>H});var a=n(87462),s=(n(67294),n(3905)),i=n(74866),o=n(85162),r=n(90814);const l="######################################################################################################\n# In this section, we set the user authentication, user and app ID, model details, and the URL of \n# the text we want as an input. Change these strings to run your own example.\n######################################################################################################\n\n# Your PAT (Personal Access Token) can be found in the portal under Authentification\nPAT = 'YOUR_PAT_HERE'\n# Specify the correct user_id/app_id pairings\n# Since you're making inferences outside your app's scope\nUSER_ID = 'nlptownres'\nAPP_ID = 'text-classification'\n# Change these to whatever model and text URL you want to use\nMODEL_ID = 'multilingual-uncased-sentiment'\nMODEL_VERSION_ID = '29d5fef0229a4936a607380d7ef775dd'\nTEXT_FILE_URL = 'https://samples.clarifai.com/negative_sentence_12.txt'\n\n############################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n############################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (('authorization', 'Key ' + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\npost_model_outputs_response = stub.PostModelOutputs(\n    service_pb2.PostModelOutputsRequest(\n        user_app_id=userDataObject,  # The userDataObject is created in the overview and is required when using a PAT\n        model_id=MODEL_ID,\n        version_id=MODEL_VERSION_ID,  # This is optional. Defaults to the latest model version\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    text=resources_pb2.Text(\n                        url=TEXT_FILE_URL\n                    )\n                )\n            )\n        ]\n    ),\n    metadata=metadata\n)\nif post_model_outputs_response.status.code != status_code_pb2.SUCCESS:\n    print(post_model_outputs_response.status)\n    raise Exception(\"Post model outputs failed, status: \" + post_model_outputs_response.status.description)\n\n# Since we have one input, one output will exist here\noutput = post_model_outputs_response.outputs[0]\n\nprint(\"Predicted concepts:\")\nfor concept in output.data.concepts:\n    print(\"%s %.2f\" % (concept.name, concept.value))\n\n# Uncomment this line to print the full Response JSON\n#print(output)",u="#######################################################################################################\n# In this section, we set the user authentication, user and app ID, model details, and the location\n# of the text we want as an input. Change these strings to run your own example.    \n#######################################################################################################\n\n# Your PAT (Personal Access Token) can be found in the portal under Authentification\nPAT = 'YOUR_PAT_HERE'\n# Specify the correct user_id/app_id pairings\n# Since you're making inferences outside your app's scope\nUSER_ID = 'nlptownres'\nAPP_ID = 'text-classification'\n# Change these to whatever model and text input you want to use\nMODEL_ID = 'multilingual-uncased-sentiment'\nMODEL_VERSION_ID = '29d5fef0229a4936a607380d7ef775dd'\nTEXT_FILE_LOCATION = 'YOUR_TEXT_FILE_LOCATION_HERE'\n\n############################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n############################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (('authorization', 'Key ' + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\nwith open(TEXT_FILE_LOCATION, \"rb\") as f:\n    file_bytes = f.read()\n\npost_model_outputs_response = stub.PostModelOutputs(\n    service_pb2.PostModelOutputsRequest(\n        user_app_id=userDataObject,  # The userDataObject is created in the overview and is required when using a PAT\n        model_id=MODEL_ID,\n        version_id=MODEL_VERSION_ID,  # This is optional. Defaults to the latest model version\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    text=resources_pb2.Text(\n                        raw=file_bytes\n                    )\n                )\n            )\n        ]\n    ),\n    metadata=metadata\n)\nif post_model_outputs_response.status.code != status_code_pb2.SUCCESS:\n    print(post_model_outputs_response.status)\n    raise Exception(\"Post model outputs failed, status: \" + post_model_outputs_response.status.description)\n\n# Since we have one input, one output will exist here.\noutput = post_model_outputs_response.outputs[0]\n\nprint(\"Predicted concepts:\")\nfor concept in output.data.concepts:\n    print(\"%s %.2f\" % (concept.name, concept.value))\n\n# Uncomment this line to print the full Response JSON\n#print(output)",c="#########################################################################################################\n# In this section, we set the user authentication, user and app ID, model details, and the raw text\n# we want as an input. Change these strings to run your own example.\n########################################################################################################\n\n# Your PAT (Personal Access Token) can be found in the portal under Authentification\nPAT = 'YOUR_PAT_HERE'\n# Specify the correct user_id/app_id pairings\n# Since you're making inferences outside your app's scope\nUSER_ID = 'nlptownres'\nAPP_ID = 'text-classification'\n# Change these to whatever model and raw text you want to use\nMODEL_ID = 'multilingual-uncased-sentiment'\nMODEL_VERSION_ID = '29d5fef0229a4936a607380d7ef775dd'\nRAW_TEXT = 'I love your product very much'\n\n############################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n############################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (('authorization', 'Key ' + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\npost_model_outputs_response = stub.PostModelOutputs(\n    service_pb2.PostModelOutputsRequest(\n        user_app_id=userDataObject,  # The userDataObject is created in the overview and is required when using a PAT\n        model_id=MODEL_ID,\n        version_id=MODEL_VERSION_ID,  # This is optional. Defaults to the latest model version\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    text=resources_pb2.Text(\n                        raw=RAW_TEXT\n                    )\n                )\n            )\n        ]\n    ),\n    metadata=metadata\n)\nif post_model_outputs_response.status.code != status_code_pb2.SUCCESS:\n    print(post_model_outputs_response.status)\n    raise Exception(\"Post model outputs failed, status: \" + post_model_outputs_response.status.description)\n\n# Since we have one input, one output will exist here\noutput = post_model_outputs_response.outputs[0]\n\nprint(\"Predicted concepts:\")\nfor concept in output.data.concepts:\n    print(\"%s %.2f\" % (concept.name, concept.value))\n\n# Uncomment this line to print the full Response JSON\n#print(output)",p="#################################################################################################################\n# In this section, we set the user authentication, user and app ID, model details, and the prompt text we want\n# to provide as an input. Change these strings to run your own example.\n#################################################################################################################\n\n# Your PAT (Personal Access Token) can be found in the portal under Authentification\nPAT = 'YOUR_PAT_HERE'\n# Specify the correct user_id/app_id pairings\n# Since you're making inferences outside your app's scope\nUSER_ID = 'stability-ai'\nAPP_ID = 'stable-diffusion-2'\n# Change these to whatever model and text URL you want to use\nMODEL_ID = 'stable-diffusion-xl'\nMODEL_VERSION_ID = '0c919cc1edfc455dbc96207753f178d7'\nRAW_TEXT = 'A penguin watching the sunset.'\n# To use a hosted text file, assign the URL variable\n# TEXT_FILE_URL = 'https://samples.clarifai.com/negative_sentence_12.txt'\n# Or, to use a local text file, assign the location variable\n# TEXT_FILE_LOCATION = 'YOUR_TEXT_FILE_LOCATION_HERE'\n\n############################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n############################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (('authorization', 'Key ' + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\n# To use a local text file, uncomment the following lines\n# with open(TEXT_FILE_LOCATION, \"rb\") as f:\n#    file_bytes = f.read()\n\npost_model_outputs_response = stub.PostModelOutputs(\n    service_pb2.PostModelOutputsRequest(\n        user_app_id=userDataObject,  # The userDataObject is created in the overview and is required when using a PAT\n        model_id=MODEL_ID,\n        version_id=MODEL_VERSION_ID,  # This is optional. Defaults to the latest model version\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    text=resources_pb2.Text(\n                        raw=RAW_TEXT\n                        # url=TEXT_FILE_URL\n                        # raw=file_bytes\n                    )\n                )\n            )\n        ]\n    ),\n    metadata=metadata\n)\nif post_model_outputs_response.status.code != status_code_pb2.SUCCESS:\n    print(post_model_outputs_response.status)\n    raise Exception(\"Post model outputs failed, status: \" + post_model_outputs_response.status.description)\n\n# Since we have one input, one output will exist here\noutput = post_model_outputs_response.outputs[0].data.image.base64\n\nimage_filename = f\"gen-image.jpg\"\nwith open(image_filename, 'wb') as f:\n      f.write(output)\n",d="\x3c!--index.html file--\x3e\n\n<script>\n    ////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, and the URL\n    // of the text we want as an input. Change these strings to run your own example.\n    ///////////////////////////////////////////////////////////////////////////////////////////////////\n\n    // Your PAT (Personal Access Token) can be found in the portal under Authentification\n    const PAT = 'YOUR_PAT_HERE';\n    // Specify the correct user_id/app_id pairings\n    // Since you're making inferences outside your app's scope\n    const USER_ID = 'nlptownres';    \n    const APP_ID = 'text-classification';\n    // Change these to whatever model and text URL you want to use\n    const MODEL_ID = 'multilingual-uncased-sentiment';\n    const MODEL_VERSION_ID = '29d5fef0229a4936a607380d7ef775dd';    \n    const TEXT_FILE_URL = 'https://samples.clarifai.com/negative_sentence_12.txt';\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    const raw = JSON.stringify({\n        \"user_app_id\": {\n            \"user_id\": USER_ID,\n            \"app_id\": APP_ID\n        },\n        \"inputs\": [\n            {\n                \"data\": {\n                    \"text\": {\n                        \"url\": TEXT_FILE_URL\n                    }\n                }\n            }\n        ]\n    });\n\n    const requestOptions = {\n        method: 'POST',\n        headers: {\n            'Accept': 'application/json',\n            'Authorization': 'Key ' + PAT\n        },\n        body: raw\n    };\n\n    // NOTE: MODEL_VERSION_ID is optional, you can also call prediction with the MODEL_ID only\n    // https://api.clarifai.com/v2/models/{YOUR_MODEL_ID}/outputs\n    // this will default to the latest version_id\n\n    fetch(\"https://api.clarifai.com/v2/models/\" + MODEL_ID + \"/versions/\" + MODEL_VERSION_ID + \"/outputs\", requestOptions)\n        .then(response => response.text())\n        .then(result => console.log(result))\n        .catch(error => console.log('error', error));\n<\/script>",h="\x3c!--index.html file--\x3e\n\n<script>\n    /////////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, and the location\n    // of the text we want as an input. Change these strings to run your own example.\n    /////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n    // Your PAT (Personal Access Token) can be found in the portal under Authentification\n    const PAT = 'YOUR_PAT_HERE';\n    // Specify the correct user_id/app_id pairings\n    // Since you're making inferences outside your app's scope\n    const USER_ID = 'nlptownres';    \n    const APP_ID = 'text-classification';\n    // Change these to whatever model and text input you want to use\n    const MODEL_ID = 'multilingual-uncased-sentiment';\n    const MODEL_VERSION_ID = '29d5fef0229a4936a607380d7ef775dd';    \n    const TEXT_FILE_BYTES = 'YOUR_TEXT_FILE_BYTES_HERE';\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    const raw = JSON.stringify({\n        \"user_app_id\": {\n            \"user_id\": USER_ID,\n            \"app_id\": APP_ID\n        },\n        \"inputs\": [\n            {\n                \"data\": {\n                    \"text\": {\n                        \"raw\": TEXT_FILE_BYTES\n                    }\n                }\n            }\n        ]\n    });\n\n    const requestOptions = {\n        method: 'POST',\n        headers: {\n            'Accept': 'application/json',\n            'Authorization': 'Key ' + PAT\n        },\n        body: raw\n    };\n\n    // NOTE: MODEL_VERSION_ID is optional, you can also call prediction with the MODEL_ID only\n    // https://api.clarifai.com/v2/models/{YOUR_MODEL_ID}/outputs\n    // this will default to the latest version_id\n\n    fetch(\"https://api.clarifai.com/v2/models/\" + MODEL_ID + \"/versions/\" + MODEL_VERSION_ID + \"/outputs\", requestOptions)\n        .then(response => response.text())\n        .then(result => console.log(result))\n        .catch(error => console.log('error', error));\n<\/script>",m="\x3c!--index.html file--\x3e\n\n<script>\n    //////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, and the raw\n    // text we want as an input. Change these strings to run your own example.\n    /////////////////////////////////////////////////////////////////////////////////////////////////////\n\n    // Your PAT (Personal Access Token) can be found in the portal under Authentification\n    const PAT = 'YOUR_PAT_HERE';\n    // Specify the correct user_id/app_id pairings\n    // Since you're making inferences outside your app's scope\n    const USER_ID = 'nlptownres';   \n    const APP_ID = 'text-classification';\n    // Change these to whatever model and raw text you want to use\n    const MODEL_ID = 'multilingual-uncased-sentiment';\n    const MODEL_VERSION_ID = '29d5fef0229a4936a607380d7ef775dd';    \n    const RAW_TEXT = 'I love your product very much';\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    const raw = JSON.stringify({\n        \"user_app_id\": {\n            \"user_id\": USER_ID,\n            \"app_id\": APP_ID\n        },\n        \"inputs\": [\n            {\n                \"data\": {\n                    \"text\": {\n                        \"raw\": RAW_TEXT\n                    }\n                }\n            }\n        ]\n    });\n\n    const requestOptions = {\n        method: 'POST',\n        headers: {\n            'Accept': 'application/json',\n            'Authorization': 'Key ' + PAT\n        },\n        body: raw\n    };\n\n    // NOTE: MODEL_VERSION_ID is optional, you can also call prediction with the MODEL_ID only\n    // https://api.clarifai.com/v2/models/{YOUR_MODEL_ID}/outputs\n    // this will default to the latest version_id\n\n    fetch(\"https://api.clarifai.com/v2/models/\" + MODEL_ID + \"/versions/\" + MODEL_VERSION_ID + \"/outputs\", requestOptions)\n        .then(response => response.text())\n        .then(result => console.log(result))\n        .catch(error => console.log('error', error));\n<\/script>",f="\x3c!--index.html file--\x3e\n\n<script>\n    /////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, and the prompt text we want\n    // to provide as an input. Change these strings to run your own example.\n    /////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n    // Your PAT (Personal Access Token) can be found in the portal under Authentification\n    const PAT = 'YOUR_PAT_HERE';    \n    // Specify the correct user_id/app_id pairings\n    // Since you're making inferences outside your app's scope\n    const USER_ID = 'stability-ai';\n    const APP_ID = 'stable-diffusion-2';\n    // Change these to whatever model and text you want to use\n    const MODEL_ID = 'stable-diffusion-xl';\n    const MODEL_VERSION_ID = '0c919cc1edfc455dbc96207753f178d7';\n    const RAW_TEXT = 'A penguin watching the sunset.';\n    // To use a hosted text file, assign the URL variable\n    // const TEXT_FILE_URL = 'https://samples.clarifai.com/negative_sentence_12.txt'\n    \n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    const raw = JSON.stringify({  \n        \"inputs\": [\n        {\n            \"data\": {\n              \"text\": {\n                   \"raw\": RAW_TEXT\n                  // \"url\": TEXT_FILE_URL\n               }\n            }\n        }\n    ],\n         \n    });\n\n    const requestOptions = {\n        method: 'POST',\n        headers: {\n            'Accept': 'application/json',\n            'Authorization': 'Key ' + PAT\n        },\n        body: raw\n    };\n\n    fetch(`https://api.clarifai.com/v2/users/${USER_ID}/apps/${APP_ID}/models/${MODEL_ID}/versions/${MODEL_VERSION_ID}/outputs`, requestOptions)\n        .then(response => response.json())\n        .then(result => {\n                const imageBase64 = result.outputs[0].data.image.base64;\n                // Create an anchor element for downloading the image\n                const downloadLink = document.createElement('a');\n                downloadLink.href = `data:image/jpeg;base64,${imageBase64}`;\n                downloadLink.download = 'gen-image.jpg';\n                // Trigger a click event on the link to prompt the download\n                downloadLink.click();\n        })      \n        .catch(error => console.log('error', error));\n\n<\/script>",_='//index.js file\n\n////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, and the URL\n// of the text we want as an input. Change these strings to run your own example.\n/////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the portal under Authentification\nconst PAT = \'YOUR_PAT_HERE\';\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\nconst USER_ID = \'nlptownres\';\nconst APP_ID = \'text-classification\';\n// Change these to whatever model and text URL you want to use\nconst MODEL_ID = \'multilingual-uncased-sentiment\';\nconst MODEL_VERSION_ID = \'29d5fef0229a4936a607380d7ef775dd\';\nconst TEXT_URL = \'https://samples.clarifai.com/negative_sentence_12.txt\';\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\nstub.PostModelOutputs(\n    {\n        user_app_id: {\n            "user_id": USER_ID,\n            "app_id": APP_ID\n        },\n        model_id: MODEL_ID,\n        version_id: MODEL_VERSION_ID, // This is optional. Defaults to the latest model version.\n        inputs: [\n            { data: { text: { url: TEXT_URL, allow_duplicate_url: true } } }\n        ]\n    },\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            throw new Error("Post model outputs failed, status: " + response.status.description);\n        }\n\n        // Since we have one input, one output will exist here.\n        const output = response.outputs[0];\n\n        console.log("Predicted concepts:");\n        for (const concept of output.data.concepts) {\n            console.log(concept.name + " " + concept.value);\n        }\n    }\n\n);',g='//index.js file\n\n/////////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, and the location\n// of the text we want as an input. Change these strings to run your own example.\n/////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the portal under Authentification\nconst PAT = \'YOUR_PAT_HERE\';\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\nconst USER_ID = \'nlptownres\';\nconst APP_ID = \'text-classification\';\n// Change these to whatever model and text input you want to use\nconst MODEL_ID = \'multilingual-uncased-sentiment\';\nconst MODEL_VERSION_ID = \'29d5fef0229a4936a607380d7ef775dd\';\nconst TEXT_FILE_LOCATION = \'YOUR_TEXT_FILE_LOCATION_HERE\';\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\nconst fs = require("fs");\nconst fileBytes = fs.readFileSync(TEXT_FILE_LOCATION);\n\nstub.PostModelOutputs(\n    {\n        user_app_id: {\n            "user_id": USER_ID,\n            "app_id": APP_ID\n        },\n        model_id: MODEL_ID,\n        version_id: MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n        inputs: [\n            { data: { text: { raw: fileBytes } } }\n        ]\n    },\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            throw new Error("Post model outputs failed, status: " + response.status.description);\n        }\n\n        // Since we have one input, one output will exist here\n        const output = response.outputs[0];\n\n        console.log("Predicted concepts:");\n        for (const concept of output.data.concepts) {\n            console.log(concept.name + " " + concept.value);\n        }\n    }\n\n);',T='//index.js file\n\n////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, and the raw\n// text we want as an input. Change these strings to run your own example.\n////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the portal under Authentification\nconst PAT = \'YOUR_PAT_HERE\';\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\nconst USER_ID = \'nlptownres\';\nconst APP_ID = \'text-classification\';\n// Change these to whatever model and raw text you want to use\nconst MODEL_ID = \'multilingual-uncased-sentiment\';\nconst MODEL_VERSION_ID = \'29d5fef0229a4936a607380d7ef775dd\';\nconst RAW_TEXT = \'I love your product very much\';\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\nstub.PostModelOutputs(\n    {\n        user_app_id: {\n            "user_id": USER_ID,\n            "app_id": APP_ID\n        },\n        model_id: MODEL_ID,\n        version_id: MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n        inputs: [\n            { data: { text: { raw: RAW_TEXT } } }\n        ]\n    },\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            throw new Error("Post model outputs failed, status: " + response.status.description);\n        }\n\n        // Since we have one input, one output will exist here\n        const output = response.outputs[0];\n\n        console.log("Predicted concepts:");\n        for (const concept of output.data.concepts) {\n            console.log(concept.name + " " + concept.value);\n        }\n    }\n\n);',E="//index.js file\n\n//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, and the prompt text we want\n// to provide as an input. Change these strings to run your own example.\n//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the portal under Authentification\nconst PAT = 'YOUR_PAT_HERE';    \n// Specify the correct user_id/app_id pairings\n// Since you're making inferences outside your app's scope\nconst USER_ID = 'stability-ai';\nconst APP_ID = 'stable-diffusion-2';\n// Change these to whatever model and text you want to use\nconst MODEL_ID = 'stable-diffusion-xl';\nconst MODEL_VERSION_ID = '0c919cc1edfc455dbc96207753f178d7';\nconst RAW_TEXT = 'A penguin watching the sunset.';\n// To use a hosted text file, assign the URL variable\n// const TEXT_FILE_URL = 'https://samples.clarifai.com/negative_sentence_12.txt'\n// Or, to use a local text file, assign the location variable\n// TEXT_FILE_LOCATION = 'YOUR_TEXT_FILE_LOCATION_HERE'\n\n/////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n/////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require(\"clarifai-nodejs-grpc\");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set(\"authorization\", \"Key \" + PAT);\n\n// To use a local text file, uncomment the following lines\n// const fs = require(\"fs\");\n// const fileBytes = fs.readFileSync(TEXT_FILE_LOCATION);\n\nstub.PostModelOutputs(\n    {\n        user_app_id: {\n            \"user_id\": USER_ID,\n            \"app_id\": APP_ID\n        },\n        model_id: MODEL_ID,\n        version_id: MODEL_VERSION_ID,  // This is optional. Defaults to the latest model version\n        inputs: [\n            {\n                \"data\": {\n                    \"text\": {\n                        \"raw\": RAW_TEXT\n                        // url: TEXT_FILE_URL\n                        // raw: fileBytes\n                    }\n                }\n            }\n        ],\n\n    },\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            throw new Error(\"Post models failed, status: \" + response.status.description);\n        }\n        // Since we have one input, one output will exist here.\n        const output = response.outputs[0].data.image.base64;\n\n        const fs = require('fs');\n        const imageFilename = 'gen-image.jpg';\n        fs.writeFileSync(imageFilename, Buffer.from(output, 'base64'));\n        console.log(`Image saved as ${imageFilename}`);\n\n    }\n);",I='package com.clarifai.example;\n\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.status.StatusCode;\n\npublic class ClarifaiExample {\n\n    ///////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, and the URL\n    // of the text we want as an input. Change these strings to run your own example.\n    //////////////////////////////////////////////////////////////////////////////////////////////////\n\n    //Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    static final String USER_ID = "nlptownres";    \n    static final String APP_ID = "text-classification";\n    // Change these to whatever model and text URL you want to use\n    static final String MODEL_ID = "multilingual-uncased-sentiment"; \n    static final String MODEL_VERSION_ID = "29d5fef0229a4936a607380d7ef775dd";   \n    static final String TEXT_URL = "https://samples.clarifai.com/negative_sentence_12.txt";     \n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\t\n\n    public static void main(String[] args) {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n            .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        MultiOutputResponse postModelOutputsResponse = stub.postModelOutputs(\n            PostModelOutputsRequest.newBuilder()\n            .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n            .setModelId(MODEL_ID)\n            .setVersionId(MODEL_VERSION_ID) // This is optional. Defaults to the latest model version\n            .addInputs(\n                Input.newBuilder().setData(\n                    Data.newBuilder().setText(\n                        Text.newBuilder().setUrl(TEXT_URL)\n                    )\n                )\n            )\n            .build()\n        );\n\n        if (postModelOutputsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("Post model outputs failed, status: " + postModelOutputsResponse.getStatus());\n        }\n\n        // Since we have one input, one output will exist here\n        Output output = postModelOutputsResponse.getOutputs(0);\n\n        System.out.println("Predicted concepts:");\n        for (Concept concept: output.getData().getConceptsList()) {\n            System.out.printf("%s %.2f%n", concept.getName(), concept.getValue());\n        }\n\n    }\n\n}',O='package com.clarifai.example;\n\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.status.StatusCode;\nimport com.google.protobuf.ByteString;\nimport java.io.File;\nimport java.io.IOException;\nimport java.nio.file.Files;\n\npublic class ClarifaiExample {\n\n    /////////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, and the location\n    // of the text we want as an input. Change these strings to run your own example.\n    /////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n    //Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    static final String USER_ID = "nlptownres";    \n    static final String APP_ID = "text-classification";\n    // Change these to whatever model and text input you want to use\n    static final String MODEL_ID = "multilingual-uncased-sentiment"; \n     static final String MODEL_VERSION_ID = "29d5fef0229a4936a607380d7ef775dd";   \n    static final String TEXT_FILE_LOCATION = "YOUR_TEXT_FILE_LOCATION_HERE";\n   \n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\t\n\n    public static void main(String[] args) throws IOException {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n            .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        MultiOutputResponse postModelOutputsResponse = stub.postModelOutputs(\n            PostModelOutputsRequest.newBuilder()\n            .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n            .setModelId(MODEL_ID)\n            .setVersionId(MODEL_VERSION_ID) // This is optional. Defaults to the latest model version\n            .addInputs(\n                Input.newBuilder().setData(\n                    Data.newBuilder().setText(\n                        Text.newBuilder()\n                        .setRawBytes(ByteString.copyFrom(Files.readAllBytes(\n                            new File(TEXT_FILE_LOCATION).toPath()\n                        )))\n                    )\n                )\n            )\n            .build()\n        );\n\n        if (postModelOutputsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("Post model outputs failed, status: " + postModelOutputsResponse.getStatus());\n        }\n\n        // Since we have one input, one output will exist here\n        Output output = postModelOutputsResponse.getOutputs(0);\n\n        System.out.println("Predicted concepts:");\n        for (Concept concept: output.getData().getConceptsList()) {\n            System.out.printf("%s %.2f%n", concept.getName(), concept.getValue());\n        }\n\n    }\n\n}',w='package com.clarifai.example;\n\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.status.StatusCode;\n\npublic class ClarifaiExample {\n\n    ////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, and the raw\n    // text we want as an input. Change these strings to run your own example.\n    ///////////////////////////////////////////////////////////////////////////////////////////////////\n\n    //Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    static final String USER_ID = "nlptownres";   \n    static final String APP_ID = "text-classification";\n    // Change these to whatever model and raw text you want to use\n    static final String MODEL_ID = "multilingual-uncased-sentiment";\n    static final String MODEL_VERSION_ID = "29d5fef0229a4936a607380d7ef775dd";    \n    static final String RAW_TEXT = "I love your product very much";    \n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\t\n\n    public static void main(String[] args) {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n            .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        MultiOutputResponse postModelOutputsResponse = stub.postModelOutputs(\n            PostModelOutputsRequest.newBuilder()\n            .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n            .setModelId(MODEL_ID)\n            .setVersionId(MODEL_VERSION_ID) // This is optional. Defaults to the latest model version\n            .addInputs(\n                Input.newBuilder().setData(\n                    Data.newBuilder().setText(\n                        Text.newBuilder().setRaw(RAW_TEXT)\n                    )\n                )\n            )\n            .build()\n        );\n\n        if (postModelOutputsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("Post model outputs failed, status: " + postModelOutputsResponse.getStatus());\n        }\n\n        // Since we have one input, one output will exist here\n        Output output = postModelOutputsResponse.getOutputs(0);\n\n        System.out.println("Predicted concepts:");\n        for (Concept concept: output.getData().getConceptsList()) {\n            System.out.printf("%s %.2f%n", concept.getName(), concept.getValue());\n        }\n\n    }\n\n}',D='package com.clarifai.example;\n\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.status.StatusCode;\nimport com.google.protobuf.ByteString;\nimport java.io.FileOutputStream;\nimport java.io.IOException;\n// import java.io.File; //  Uncomment to use a local text file\n// import java.nio.file.Files; //  Uncomment to use a local text file\n\npublic class ClarifaiExample {\n\n    ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, and the prompt text we want\n    // to provide as an input. Change these strings to run your own example.\n    //////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n    // Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    static final String USER_ID = "stability-ai";\n    static final String APP_ID = "stable-diffusion-2";\n    // Change these to whatever model you want to use\n    static final String MODEL_ID = "stable-diffusion-xl";\n    static final String MODEL_VERSION_ID = "0c919cc1edfc455dbc96207753f178d7";\n    static final String RAW_TEXT = "A penguin watching the sunset.";\n    // To use a hosted text file, assign the URL variable\n    // static final String TEXT_FILE_URL = "https://samples.clarifai.com/negative_sentence_12.txt";\n    // Or, to use a local text file, assign the location variable\n    // static final String TEXT_FILE_LOCATION = "YOUR_TEXT_FILE_LOCATION_HERE";\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\t\n    public static void main(String[] args) {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n                .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        MultiOutputResponse postModelOutputsResponse = stub.postModelOutputs(\n                PostModelOutputsRequest.newBuilder()\n                        .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n                        .setModelId(MODEL_ID)\n                        .setVersionId(MODEL_VERSION_ID) // This is optional. Defaults to the latest model version.\n                        .addInputs(\n                                Input.newBuilder().setData(\n                                        Data.newBuilder().setText(\n                                                Text.newBuilder().setRaw(RAW_TEXT)\n                                        // Text.newBuilder().setUrl(TEXT_FILE_URL)\n                                        // Text.newBuilder().setRawBytes(ByteString.copyFrom(Files.readAllBytes(\n                                        // new File(TEXT_FILE_LOCATION).toPath()\n                                        // )))\n                                        )\n                                )\n                        )\n                        .build()\n        );\n\n        if (postModelOutputsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("Post model outputs failed, status: " + postModelOutputsResponse.getStatus());\n        }\n\n        // Extract the base64-encoded image data\n        ByteString imageData = postModelOutputsResponse.getOutputs(0).getData().getImage().getBase64();\n\n        // Save the image to a file\n        try {\n            FileOutputStream outputStream = new FileOutputStream("gen-image.jpg");\n            outputStream.write(imageData.toByteArray());\n            outputStream.close();\n        } catch (IOException e) {\n            System.err.println("Error writing image to file: " + e.getMessage());\n            System.exit(1);\n        }\n\n    }\n}\n',b="<?php\n\nrequire __DIR__ . '/vendor/autoload.php';\n\n////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, and the URL\n// of the text we want as an input. Change these strings to run your own example.\n////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the portal under Authentification\n$PAT = 'YOUR_PAT_HERE';\n// Specify the correct user_id/app_id pairings\n// Since you're making inferences outside your app's scope\n$USER_ID = 'nlptownres';\n$APP_ID = 'text-classification';\n// Change these to whatever model and text URL you want to use\n$MODEL_ID = 'multilingual-uncased-sentiment';\n$MODEL_VERSION_ID = '29d5fef0229a4936a607380d7ef775dd';\n$TEXT_URL = 'https://samples.clarifai.com/negative_sentence_12.txt';\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Text;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\PostModelOutputsRequest;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ['Authorization' => ['Key ' . $PAT ]];\n\n$userDataObject = new UserAppIDSet([\n    'user_id' => $USER_ID, \n    'app_id' => $APP_ID \n]);\n\n// Let's make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client->PostModelOutputs(\n    // The request object carries the request along with the request status and other metadata related to the request itself\n    new PostModelOutputsRequest([\n        'user_app_id' => $userDataObject,\n        'model_id' => $MODEL_ID,  \n        'version_id' => $MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n        'inputs' => [\n            new Input([ // The Input object wraps the Data object in order to meet the API specification                \n                'data' => new Data([ // The Data object is constructed around the Text object. It offers a container that has additional text independent\n                                    // metadata. In this particular use case, no other metadata is needed to be specified\n                    'text' => new Text([ // In the Clarifai platform, a text is defined by a special Text object\n                        'url' => $TEXT_URL \n                    ])\n                ])\n            ])\n        ]        \n    ]),\n    $metadata\n)->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception(\"Error: {$status->details}\");\n}\n\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure \n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    throw new Exception(\"Failure response: \" . $response->getStatus()->getDescription() . \" \" .\n        $response->getStatus()->getDetails());\n}\n\n// The output of a successful call can be used in many ways. In this example, we loop through all of the predicted concepts and print them out along with\n// their numerical prediction value (confidence)\necho \"Predicted concepts: </br>\";\nforeach ($response->getOutputs()[0]->getData()->getConcepts() as $concept) {\n    echo $concept->getName() . \": \" . number_format($concept->getValue(), 2) . \"</br>\";\n}\n\n?>",S="<?php\n\nrequire __DIR__ . '/vendor/autoload.php';\n\n/////////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, and the location\n// of the text we want as an input. Change these strings to run your own example.\n////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the portal under Authentification\n$PAT = 'YOUR_PAT_HERE';\n// Specify the correct user_id/app_id pairings\n// Since you're making inferences outside your app's scope\n$USER_ID = 'nlptownres';\n$APP_ID = 'text-classification';\n// Change these to whatever model and text URL you want to use\n$MODEL_ID = 'multilingual-uncased-sentiment';\n$MODEL_VERSION_ID = '29d5fef0229a4936a607380d7ef775dd';\n$TEXT_FILE_LOCATION = 'YOUR_TEXT_FILE_LOCATION_HERE';\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Text;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\PostModelOutputsRequest;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ['Authorization' => ['Key ' . $PAT ]];\n\n$userDataObject = new UserAppIDSet([\n    'user_id' => $USER_ID, \n    'app_id' => $APP_ID \n]);\n\n$textData = file_get_contents($TEXT_FILE_LOCATION); // Get the text bytes data from the location\n\n// Let's make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client->PostModelOutputs(\n    // The request object carries the request along with the request status and other metadata related to the request itself\n    new PostModelOutputsRequest([\n        'user_app_id' => $userDataObject,\n        'model_id' => $MODEL_ID,  \n        'version_id' => $MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n        'inputs' => [\n            new Input([ // The Input object wraps the Data object in order to meet the API specification                \n                'data' => new Data([ // The Data object is constructed around the Text object. It offers a container that has additional text independent\n                                    // metadata. In this particular use case, no other metadata is needed to be specified\n                    'text' => new Text([ // In the Clarifai platform, a text is defined by a special Text object\n                        'raw' => $textData \n                    ])\n                ])\n            ])\n        ]        \n    ]),\n    $metadata\n)->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception(\"Error: {$status->details}\");\n}\n\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure \n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    throw new Exception(\"Failure response: \" . $response->getStatus()->getDescription() . \" \" .\n        $response->getStatus()->getDetails());\n}\n\n// The output of a successful call can be used in many ways. In this example, we loop through all of the predicted concepts and print them out along with\n// their numerical prediction value (confidence)\necho \"Predicted concepts: </br>\";\nforeach ($response->getOutputs()[0]->getData()->getConcepts() as $concept) {\n    echo $concept->getName() . \": \" . number_format($concept->getValue(), 2) . \"</br>\";\n}\n\n?>",x="<?php\n\nrequire __DIR__ . '/vendor/autoload.php';\n\n/////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, and the raw\n// text we want as an input. Change these strings to run your own example.\n////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the portal under Authentification\n$PAT = 'YOUR_PAT_HERE';\n// Specify the correct user_id/app_id pairings\n// Since you're making inferences outside your app's scope\n$USER_ID = 'nlptownres';\n$APP_ID = 'text-classification';\n// Change these to whatever model and raw text you want to use\n$MODEL_ID = 'multilingual-uncased-sentiment';\n$MODEL_VERSION_ID = '29d5fef0229a4936a607380d7ef775dd';\n$RAW_TEXT = 'I love your product very much';\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Text;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\PostModelOutputsRequest;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ['Authorization' => ['Key ' . $PAT ]];\n\n$userDataObject = new UserAppIDSet([\n    'user_id' => $USER_ID, \n    'app_id' => $APP_ID \n]);\n\n// Let's make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client->PostModelOutputs(\n    // The request object carries the request along with the request status and other metadata related to the request itself\n    new PostModelOutputsRequest([\n        'user_app_id' => $userDataObject,\n        'model_id' => $MODEL_ID,  \n        'version_id' => $MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n        'inputs' => [\n            new Input([ // The Input object wraps the Data object in order to meet the API specification                \n                'data' => new Data([ // The Data object is constructed around the Text object. It offers a container that has additional text independent\n                                    // metadata. In this particular use case, no other metadata is needed to be specified\n                    'text' => new Text([ // In the Clarifai platform, a text is defined by a special Text object\n                        'raw' => $RAW_TEXT \n                    ])\n                ])\n            ])\n        ]        \n    ]),\n    $metadata\n)->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception(\"Error: {$status->details}\");\n}\n\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure \n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    throw new Exception(\"Failure response: \" . $response->getStatus()->getDescription() . \" \" .\n        $response->getStatus()->getDetails());\n}\n\n// The output of a successful call can be used in many ways. In this example, we loop through all of the predicted concepts and print them out along with\n// their numerical prediction value (confidence)\necho \"Predicted concepts: </br>\";\nforeach ($response->getOutputs()[0]->getData()->getConcepts() as $concept) {\n    echo $concept->getName() . \": \" . number_format($concept->getValue(), 2) . \"</br>\";\n}\n\n?>",A="<?php\n\nrequire __DIR__ . '/vendor/autoload.php';\n\n////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, and the prompt text we want\n// to provide as an input. Change these strings to run your own example.\n//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the portal under Authentification\n$PAT = 'YOUR_PAT_HERE';\n// Specify the correct user_id/app_id pairings\n// Since you're making inferences outside your app's scope\n$USER_ID = 'stability-ai';\n$APP_ID = 'stable-diffusion-2';\n// Change these to whatever model and image URL you want to use\n$MODEL_ID = 'stable-diffusion-xl';\n$MODEL_VERSION_ID = '0c919cc1edfc455dbc96207753f178d7';\n$RAW_TEXT = 'A penguin watching the sunset.';\n# To use a hosted text file, assign the URL variable\n# $TEXT_FILE_URL = 'https://samples.clarifai.com/negative_sentence_12.txt';\n# Or, to use a local text file, assign the location variable\n# $TEXT_FILE_LOCATION = 'YOUR_TEXT_FILE_LOCATION_HERE';\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Text;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\PostModelOutputsRequest;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ['Authorization' => ['Key ' . $PAT]];\n\n$userDataObject = new UserAppIDSet([\n    'user_id' => $USER_ID,\n    'app_id' => $APP_ID\n]);\n\n//$textData = file_get_contents($TEXT_FILE_LOCATION); // Get the text bytes data from the location\n\n// Let's make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client->PostModelOutputs(\n    // The request object carries the request along with the request status and other metadata related to the request itself\n    new PostModelOutputsRequest([\n        'user_app_id' => $userDataObject,\n        'model_id' => $MODEL_ID,\n        'version_id' => $MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n        'inputs' => [\n            new Input([ // The Input object wraps the Data object in order to meet the API specification                \n                'data' => new Data([ // The Data object is constructed around the Text object. It offers a container that has additional text independent\n                    // metadata. In this particular use case, no other metadata is needed to be specified\n                    'text' => new Text([ // In the Clarifai platform, a text is defined by a special Text object\n                        'raw' => $RAW_TEXT\n                        // 'url' => $TEXT_FILE_URL \n                        // 'raw' => $textData \n                    ])\n                ])\n            ])\n        ]\n    ]),\n    $metadata\n)->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception(\"Error: {$status->details}\");\n}\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure \n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    throw new Exception(\"Failure response: \" . $response->getStatus()->getDescription() . \" \" .\n        $response->getStatus()->getDetails());\n}\n\n// Save the output image\n$output = $response->getOutputs()[0]->getData()->getImage()->getBase64();\n\n$imageFilename = \"gen-image.jpg\";\nfile_put_contents($imageFilename, base64_decode($output));\n",C='curl -X POST "https://api.clarifai.com/v2/users/nlptownres/apps/text-classification/models/multilingual-uncased-sentiment/versions/29d5fef0229a4936a607380d7ef775dd/outputs" \\\n    -H "Authorization: Key YOUR_PAT_HERE" \\\n    -H "Content-Type: application/json" \\\n    -d \'{\n      "inputs": [\n        {\n          "data": {\n            "text": {\n              "url": "https://samples.clarifai.com/negative_sentence_12.txt"\n            }\n          }\n        }\n      ]\n    }\'\n   ',y='# Smaller files (195 KB or less)\n\ncurl -X POST "https://api.clarifai.com/v2/users/nlptownres/apps/text-classification/models/multilingual-uncased-sentiment/versions/29d5fef0229a4936a607380d7ef775dd/outputs" \\\n    -H "Authorization: Key YOUR_PAT_HERE" \\\n    -H "Content-Type: application/json" \\\n    -d \'{\n      "inputs": [\n        {\n          "data": {\n            "text": {\n              "raw": "YOUR_TEXT_FILE_BYTES_HERE"\n            }\n          }\n        }\n      ]\n    }\'\n   ',v='curl -X POST "https://api.clarifai.com/v2/users/nlptownres/apps/text-classification/models/multilingual-uncased-sentiment/versions/29d5fef0229a4936a607380d7ef775dd/outputs" \\\n    -H "Authorization: Key YOUR_PAT_HERE" \\\n    -H "Content-Type: application/json" \\\n    -d \'{\n      "inputs": [\n        {\n          "data": {\n            "text": {\n              "raw": "I love your product very much"\n            }\n          }\n        }\n      ]\n    }\'\n   ',P="Predicted concepts:\n3-stars 0.25\n2-stars 0.23\n1-star 0.20\n4-stars 0.17\n5-stars 0.14",R='Predicted concepts:\nid: "f021a57a5dd24038ae3453ad1a5baa67"\nstatus {\n  code: SUCCESS\n  description: "Ok"\n}\ncreated_at {\n  seconds: 1676381113\n  nanos: 815547169\n}\nmodel {\n  id: "multilingual-uncased-sentiment"\n  name: "multilingual-uncased-sentiment"\n  created_at {\n    seconds: 1656469244\n    nanos: 44961000\n  }\n  app_id: "text-classification"\n  model_version {\n    id: "29d5fef0229a4936a607380d7ef775dd"\n    created_at {\n      seconds: 1656469244\n      nanos: 60443000\n    }\n    status {\n      code: MODEL_TRAINED\n      description: "Model is trained and ready"\n    }\n    visibility {\n      gettable: PUBLIC\n    }\n    app_id: "text-classification"\n    user_id: "nlptownres"\n    metadata {\n      fields {\n        key: "Model version logs zipped"\n        value {\n          string_value: "https://s3.amazonaws.com/clarifai-temp/prod/29d5fef0229a4936a607380d7ef775dd.zip"\n        }\n      }\n    }\n  }\n  user_id: "nlptownres"\n  model_type_id: "text-classifier"\n  visibility {\n    gettable: PUBLIC\n  }\n  modified_at {\n    seconds: 1661364520\n    nanos: 417454000\n  }\n  task: "text-classification"\n  presets {\n  }\n  workflow_recommended {\n  }\n}\ninput {\n  id: "709cca36ea0b4545b8d1dd53a2e97e5f"\n  data {\n    text {\n      url: "https://samples.clarifai.com/negative_sentence_12.txt"\n    }\n  }\n}\ndata {\n  concepts {\n    id: "3-stars"\n    name: "3-stars"\n    value: 0.25399050116539\n    app_id: "text-classification"\n  }\n  concepts {\n    id: "2-stars"\n    name: "2-stars"\n    value: 0.23382413387298584\n    app_id: "text-classification"\n  }\n  concepts {\n    id: "1-star"\n    name: "1-star"\n    value: 0.20093096792697906\n    app_id: "text-classification"\n  }\n  concepts {\n    id: "4-stars"\n    name: "4-stars"\n    value: 0.17351143062114716\n    app_id: "text-classification"\n  }\n  concepts {\n    id: "5-stars"\n    name: "5-stars"\n    value: 0.1377429962158203\n    app_id: "text-classification"\n  }\n}',N="Predicted concepts:\n5-stars 0.87\n4-stars 0.12\n3-stars 0.01\n1-star 0.00\n2-stars 0.00",L='Predicted concepts:\nid: "9b7efc5d5e3d4d8da20faad4dc35dd8a"\nstatus {\n  code: SUCCESS\n  description: "Ok"\n}\ncreated_at {\n  seconds: 1676382425\n  nanos: 572796576\n}\nmodel {\n  id: "multilingual-uncased-sentiment"\n  name: "multilingual-uncased-sentiment"\n  created_at {\n    seconds: 1656469244\n    nanos: 44961000\n  }\n  app_id: "text-classification"\n  model_version {\n    id: "29d5fef0229a4936a607380d7ef775dd"\n    created_at {\n      seconds: 1656469244\n      nanos: 60443000\n    }\n    status {\n      code: MODEL_TRAINED\n      description: "Model is trained and ready"\n    }\n    visibility {\n      gettable: PUBLIC\n    }\n    app_id: "text-classification"\n    user_id: "nlptownres"\n    metadata {\n      fields {\n        key: "Model version logs zipped"\n        value {\n          string_value: "https://s3.amazonaws.com/clarifai-temp/prod/29d5fef0229a4936a607380d7ef775dd.zip"\n        }\n      }\n    }\n  }\n  user_id: "nlptownres"\n  model_type_id: "text-classifier"\n  visibility {\n    gettable: PUBLIC\n  }\n  modified_at {\n    seconds: 1661364520\n    nanos: 417454000\n  }\n  task: "text-classification"\n  presets {\n  }\n  workflow_recommended {\n  }\n}\ninput {\n  id: "09633b75b0864a2792e635c5aa098614"\n  data {\n    text {\n      raw: "I love your product very much"\n      url: "https://samples.clarifai.com/placeholder.gif"\n    }\n  }\n}\ndata {\n  concepts {\n    id: "5-stars"\n    name: "5-stars"\n    value: 0.866517961025238\n    app_id: "text-classification"\n  }\n  concepts {\n    id: "4-stars"\n    name: "4-stars"\n    value: 0.11985278874635696\n    app_id: "text-classification"\n  }\n  concepts {\n    id: "3-stars"\n    name: "3-stars"\n    value: 0.009703087620437145\n    app_id: "text-classification"\n  }\n  concepts {\n    id: "1-star"\n    name: "1-star"\n    value: 0.0020059270318597555\n    app_id: "text-classification"\n  }\n  concepts {\n    id: "2-stars"\n    name: "2-stars"\n    value: 0.001920199254527688\n    app_id: "text-classification"\n  }\n}\n',k={description:"Make predictions on passages of texts",sidebar_position:3},U="Text",M={unversionedId:"api-guide/predict/text",id:"api-guide/predict/text",title:"Text",description:"Make predictions on passages of texts",source:"@site/docs/api-guide/predict/text.md",sourceDirName:"api-guide/predict",slug:"/api-guide/predict/text",permalink:"/api-guide/predict/text",draft:!1,editUrl:"https://github.com/Clarifai/docs/blob/main/docs/api-guide/predict/text.md",tags:[],version:"current",sidebarPosition:3,frontMatter:{description:"Make predictions on passages of texts",sidebar_position:3},sidebar:"tutorialSidebar",previous:{title:"Video",permalink:"/api-guide/predict/video"},next:{title:"Large Language Models (LLMs)",permalink:"/api-guide/predict/llms"}},$={},H=[{value:"Text Classification",id:"text-classification",level:2},{value:"Predict via URL",id:"predict-via-url",level:3},{value:"Predict via Local Files",id:"predict-via-local-files",level:3},{value:"Predict via Raw Text",id:"predict-via-raw-text",level:3},{value:"Text-to-Image Generation",id:"text-to-image-generation",level:2}],B={toc:H},j="wrapper";function Y(e){let{components:t,...k}=e;return(0,s.kt)(j,(0,a.Z)({},B,k,{components:t,mdxType:"MDXLayout"}),(0,s.kt)("h1",{id:"text"},"Text"),(0,s.kt)("p",null,(0,s.kt)("strong",{parentName:"p"},"Make predictions on text inputs")),(0,s.kt)("hr",null),(0,s.kt)("p",null,"To get predictions for a given text input, you need to supply the text along with the specific model from which you wish to receive predictions. You can supply the text via a publicly accessible URL, a local text file, or in its raw format. "),(0,s.kt)("p",null,"The file size of each text input should be less than 20MB."),(0,s.kt)("p",null,"You need to specify your choice of model for prediction by utilizing the ",(0,s.kt)("inlineCode",{parentName:"p"},"MODEL_ID")," parameter."),(0,s.kt)("h2",{id:"text-classification"},"Text Classification"),(0,s.kt)("p",null,(0,s.kt)("strong",{parentName:"p"},"Input"),": Text"),(0,s.kt)("p",null,(0,s.kt)("strong",{parentName:"p"},"Output"),": ",(0,s.kt)("a",{parentName:"p",href:"https://docs.clarifai.com/portal-guide/concepts/create-get-update-delete"},"Concepts")),(0,s.kt)("p",null,"Text classification is the process of categorizing text documents into predefined categories based on their content. This task is typically accomplished using machine learning models trained on labeled datasets, where each document is associated with a specific category. "),(0,s.kt)("p",null,"These models learn patterns and features in the text data during training, enabling them to classify new, unseen documents into the relevant categories effectively."),(0,s.kt)("h3",{id:"predict-via-url"},"Predict via URL"),(0,s.kt)("p",null,"Below is an example of how you would make predictions on passages of text hosted on the web from the ",(0,s.kt)("a",{parentName:"p",href:"https://clarifai.com/nlptownres/text-classification/models/multilingual-uncased-sentiment"},(0,s.kt)("inlineCode",{parentName:"a"},"multilingual-uncased-sentiment"))," model. "),(0,s.kt)("admonition",{type:"info"},(0,s.kt)("p",{parentName:"admonition"},"The initialization code used in the following examples is outlined in detail on the ",(0,s.kt)("a",{parentName:"p",href:"https://docs.clarifai.com/api-guide/api-overview/api-clients/#client-installation-instructions"},"client installation page."))),(0,s.kt)(i.Z,{mdxType:"Tabs"},(0,s.kt)(o.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,s.kt)(r.Z,{className:"language-python",mdxType:"CodeBlock"},l)),(0,s.kt)(o.Z,{value:"js_rest",label:"JavaScript (REST)",mdxType:"TabItem"},(0,s.kt)(r.Z,{className:"language-javascript",mdxType:"CodeBlock"},d)),(0,s.kt)(o.Z,{value:"nodejs",label:"NodeJS",mdxType:"TabItem"},(0,s.kt)(r.Z,{className:"language-javascript",mdxType:"CodeBlock"},_)),(0,s.kt)(o.Z,{value:"java",label:"Java",mdxType:"TabItem"},(0,s.kt)(r.Z,{className:"language-java",mdxType:"CodeBlock"},I)),(0,s.kt)(o.Z,{value:"php",label:"PHP",mdxType:"TabItem"},(0,s.kt)(r.Z,{className:"language-php",mdxType:"CodeBlock"},b)),(0,s.kt)(o.Z,{value:"curl",label:"cURL",mdxType:"TabItem"},(0,s.kt)(r.Z,{className:"language-bash",mdxType:"CodeBlock"},C))),(0,s.kt)("details",null,(0,s.kt)("summary",null,"Text Output Example"),(0,s.kt)(r.Z,{className:"language-text",mdxType:"CodeBlock"},P)),(0,s.kt)("details",null,(0,s.kt)("summary",null,"JSON Output Example"),(0,s.kt)(r.Z,{className:"language-javascript",mdxType:"CodeBlock"},R)),(0,s.kt)("h3",{id:"predict-via-local-files"},"Predict via Local Files"),(0,s.kt)("p",null,"Below is an example of how you would provide text inputs via local text files and receive predictions from the ",(0,s.kt)("a",{parentName:"p",href:"https://clarifai.com/nlptownres/text-classification/models/multilingual-uncased-sentiment"},(0,s.kt)("inlineCode",{parentName:"a"},"multilingual-uncased-sentiment"))," model. "),(0,s.kt)(i.Z,{mdxType:"Tabs"},(0,s.kt)(o.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,s.kt)(r.Z,{className:"language-python",mdxType:"CodeBlock"},u)),(0,s.kt)(o.Z,{value:"js_rest",label:"JavaScript (REST)",mdxType:"TabItem"},(0,s.kt)(r.Z,{className:"language-javascript",mdxType:"CodeBlock"},h)),(0,s.kt)(o.Z,{value:"nodejs",label:"NodeJS",mdxType:"TabItem"},(0,s.kt)(r.Z,{className:"language-javascript",mdxType:"CodeBlock"},g)),(0,s.kt)(o.Z,{value:"java",label:"Java",mdxType:"TabItem"},(0,s.kt)(r.Z,{className:"language-java",mdxType:"CodeBlock"},O)),(0,s.kt)(o.Z,{value:"php",label:"PHP",mdxType:"TabItem"},(0,s.kt)(r.Z,{className:"language-php",mdxType:"CodeBlock"},S)),(0,s.kt)(o.Z,{value:"curl",label:"cURL",mdxType:"TabItem"},(0,s.kt)(r.Z,{className:"language-bash",mdxType:"CodeBlock"},y))),(0,s.kt)("h3",{id:"predict-via-raw-text"},"Predict via Raw Text"),(0,s.kt)("p",null,"Below is an example of how you would provide raw text inputs and receive predictions from the ",(0,s.kt)("a",{parentName:"p",href:"https://clarifai.com/nlptownres/text-classification/models/multilingual-uncased-sentiment"},(0,s.kt)("inlineCode",{parentName:"a"},"multilingual-uncased-sentiment"))," model. "),(0,s.kt)(i.Z,{mdxType:"Tabs"},(0,s.kt)(o.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,s.kt)(r.Z,{className:"language-python",mdxType:"CodeBlock"},c)),(0,s.kt)(o.Z,{value:"js_rest",label:"JavaScript (REST)",mdxType:"TabItem"},(0,s.kt)(r.Z,{className:"language-javascript",mdxType:"CodeBlock"},m)),(0,s.kt)(o.Z,{value:"nodejs",label:"NodeJS",mdxType:"TabItem"},(0,s.kt)(r.Z,{className:"language-javascript",mdxType:"CodeBlock"},T)),(0,s.kt)(o.Z,{value:"java",label:"Java",mdxType:"TabItem"},(0,s.kt)(r.Z,{className:"language-java",mdxType:"CodeBlock"},w)),(0,s.kt)(o.Z,{value:"php",label:"PHP",mdxType:"TabItem"},(0,s.kt)(r.Z,{className:"language-php",mdxType:"CodeBlock"},x)),(0,s.kt)(o.Z,{value:"curl",label:"cURL",mdxType:"TabItem"},(0,s.kt)(r.Z,{className:"language-bash",mdxType:"CodeBlock"},v))),(0,s.kt)("details",null,(0,s.kt)("summary",null,"Text Output Example"),(0,s.kt)(r.Z,{className:"language-text",mdxType:"CodeBlock"},N)),(0,s.kt)("details",null,(0,s.kt)("summary",null,"JSON Output Example"),(0,s.kt)(r.Z,{className:"language-javascript",mdxType:"CodeBlock"},L)),(0,s.kt)("h2",{id:"text-to-image-generation"},"Text-to-Image Generation"),(0,s.kt)("p",null,(0,s.kt)("strong",{parentName:"p"},"Input"),": Text"),(0,s.kt)("p",null,(0,s.kt)("strong",{parentName:"p"},"Output"),": Images"),(0,s.kt)("p",null,"Text-to-image generation involves creating visual images based on textual descriptions. In this field, machine learning models are trained to establish a meaningful connection between textual descriptions and their corresponding visual representations. "),(0,s.kt)("p",null,"Then, when given a textual input, these models can generate images that accurately reflect the content described in the text."),(0,s.kt)("p",null,"Below is an example of how you would perform text-to-image generation using the ",(0,s.kt)("a",{parentName:"p",href:"https://clarifai.com/stability-ai/stable-diffusion-2/models/stable-diffusion-xl"},"Stable Diffusion XL")," model."),(0,s.kt)("admonition",{type:"tip"},(0,s.kt)("p",{parentName:"admonition"},(0,s.kt)("a",{parentName:"p",href:"https://docs.clarifai.com/api-guide/predict/llms/#use-hyperparameters-to-customize-llms"},"Click here")," to learn how to configure the inference parameters such as temperature, max tokens, and more, for text-to-image generative tasks. ")),(0,s.kt)(i.Z,{mdxType:"Tabs"},(0,s.kt)(o.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,s.kt)(r.Z,{className:"language-python",mdxType:"CodeBlock"},p)),(0,s.kt)(o.Z,{value:"js_rest",label:"JavaScript (REST)",mdxType:"TabItem"},(0,s.kt)(r.Z,{className:"language-javascript",mdxType:"CodeBlock"},f)),(0,s.kt)(o.Z,{value:"nodejs",label:"NodeJS",mdxType:"TabItem"},(0,s.kt)(r.Z,{className:"language-javascript",mdxType:"CodeBlock"},E)),(0,s.kt)(o.Z,{value:"java",label:"Java",mdxType:"TabItem"},(0,s.kt)(r.Z,{className:"language-java",mdxType:"CodeBlock"},D)),(0,s.kt)(o.Z,{value:"php",label:"PHP",mdxType:"TabItem"},(0,s.kt)(r.Z,{className:"language-php",mdxType:"CodeBlock"},A))),(0,s.kt)("p",null,"Here is a generated output example:"),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"generated image output example",src:n(4340).Z,width:"200",height:"200"})))}Y.isMDXComponent=!0},4340:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/gen-image_200-0f1bae82129ba7cbc01241e82c572dc8.jpg"}}]);
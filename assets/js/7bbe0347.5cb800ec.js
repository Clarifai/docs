"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[2953],{3905:(e,t,a)=>{a.d(t,{Zo:()=>p,kt:()=>f});var i=a(67294);function n(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function r(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);t&&(i=i.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,i)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?r(Object(a),!0).forEach((function(t){n(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,i,n=function(e,t){if(null==e)return{};var a,i,n={},r=Object.keys(e);for(i=0;i<r.length;i++)a=r[i],t.indexOf(a)>=0||(n[a]=e[a]);return n}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(i=0;i<r.length;i++)a=r[i],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(n[a]=e[a])}return n}var l=i.createContext({}),c=function(e){var t=i.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},p=function(e){var t=c(e.components);return i.createElement(l.Provider,{value:t},e.children)},d="mdxType",h={inlineCode:"code",wrapper:function(e){var t=e.children;return i.createElement(i.Fragment,{},t)}},u=i.forwardRef((function(e,t){var a=e.components,n=e.mdxType,r=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),d=c(a),u=n,f=d["".concat(l,".").concat(u)]||d[u]||h[u]||r;return a?i.createElement(f,o(o({ref:t},p),{},{components:a})):i.createElement(f,o({ref:t},p))}));function f(e,t){var a=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var r=a.length,o=new Array(r);o[0]=u;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[d]="string"==typeof e?e:n,o[1]=s;for(var c=2;c<r;c++)o[c]=a[c];return i.createElement.apply(null,o)}return i.createElement.apply(null,a)}u.displayName="MDXCreateElement"},15887:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>s,toc:()=>c});var i=a(87462),n=(a(67294),a(3905));const r={description:"Learn about our visual classification templates",sidebar_position:1},o="Visual Classification Templates",s={unversionedId:"portal-guide/model/deep-training/visual-classification-templates",id:"portal-guide/model/deep-training/visual-classification-templates",title:"Visual Classification Templates",description:"Learn about our visual classification templates",source:"@site/docs/portal-guide/model/deep-training/visual-classification-templates.md",sourceDirName:"portal-guide/model/deep-training",slug:"/portal-guide/model/deep-training/visual-classification-templates",permalink:"/portal-guide/model/deep-training/visual-classification-templates",draft:!1,tags:[],version:"current",sidebarPosition:1,frontMatter:{description:"Learn about our visual classification templates",sidebar_position:1},sidebar:"tutorialSidebar",previous:{title:"Deep Training",permalink:"/portal-guide/model/deep-training/"},next:{title:"Visual Detection Templates",permalink:"/portal-guide/model/deep-training/visual-detection-templates"}},l={},c=[{value:"MMClassification_ResNet_50_RSB_A1",id:"mmclassification_resnet_50_rsb_a1",level:2},{value:"Clarifai_InceptionBatchNorm",id:"clarifai_inceptionbatchnorm",level:2},{value:"Clarifai_InceptionV2",id:"clarifai_inceptionv2",level:2},{value:"Clarifai_ResNext",id:"clarifai_resnext",level:2},{value:"Clarifai_InceptionTransferEmbedNorm",id:"clarifai_inceptiontransferembednorm",level:2}],p={toc:c},d="wrapper";function h(e){let{components:t,...a}=e;return(0,n.kt)(d,(0,i.Z)({},p,a,{components:t,mdxType:"MDXLayout"}),(0,n.kt)("h1",{id:"visual-classification-templates"},"Visual Classification Templates"),(0,n.kt)("p",null,(0,n.kt)("strong",{parentName:"p"},"Learn about our visual classification templates")),(0,n.kt)("hr",null),(0,n.kt)("p",null,"Classification templates let you classify what is in your inputs."),(0,n.kt)("h2",{id:"mmclassification_resnet_50_rsb_a1"},"MMClassification_ResNet_50_RSB_A1"),(0,n.kt)("p",null,"This template is a customized variant of the ResNet-50 architecture for multimodal classification tasks. "),(0,n.kt)("p",null,"Let\u2019s break down the components in the naming of the deep learning model architecture:"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("p",{parentName:"li"},(0,n.kt)("strong",{parentName:"p"},"MMClassification:"),' This refers to an abbreviation for "Multimodal Classification." It suggests that the model is designed for the task of classifying or categorizing inputs that contain multiple modalities of data, such as images, text, and audio.')),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("p",{parentName:"li"},(0,n.kt)("strong",{parentName:"p"},"ResNet_50:"),' This refers to a specific variant of the Residual Network (ResNet) architecture. ResNet is a popular deep neural network architecture known for its skip connections that help alleviate the vanishing gradient problem. The number "50" typically denotes the depth or number of layers in the ResNet model.')),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("p",{parentName:"li"},(0,n.kt)("strong",{parentName:"p"},"RSB_A1:")," This refers to a particular modification, adaptation, or variant of the ResNet architecture."))),(0,n.kt)("h2",{id:"clarifai_inceptionbatchnorm"},"Clarifai_InceptionBatchNorm"),(0,n.kt)("p",null,"This is an image classifier template based on the Inception architecture, which has been pre-trained on a combination of the ImageNet-21K dataset and additional image classification data."),(0,n.kt)("p",null,"The Inception architecture, initially introduced by Google, is known for its effectiveness in image classification tasks. It utilizes various convolutional layers and pooling operations to extract hierarchical features from images, enabling accurate classification."),(0,n.kt)("p",null,"By leveraging transfer learning, the pretrained Inception model can be used as a starting point for training an image classifier on a specific dataset or task."),(0,n.kt)("p",null,"In this case, the model has been pre-trained on the ImageNet-21K dataset, which consists of millions of labeled images from a wide range of categories. This dataset serves as a general-purpose pretraining source, providing the model with a foundation of knowledge about various visual concepts and features."),(0,n.kt)("p",null,"Additionally, the model has been further trained or fine-tuned on additional image classification data. This suggests that specific image datasets related to the intended classification task or domain have been utilized to enhance the model's performance and adapt it to the specific context."),(0,n.kt)("p",null,"The template is implemented using the Batch Normalization technique. The Batch Normalization method is a normalization technique that helps accelerate training and improve model performance by reducing internal covariate shift."),(0,n.kt)("p",null,"By incorporating Batch Normalization into the Inception architecture, the Clarifai_InceptionBatchNorm classifier achieves better generalization and stability during training. It allows for efficient and accurate classification of images, leveraging the rich pretraining on the ImageNet-21K dataset and the additional image classification data."),(0,n.kt)("h2",{id:"clarifai_inceptionv2"},"Clarifai_InceptionV2"),(0,n.kt)("p",null,"This template is an implementation of the InceptionV2 architecture without any modifications, starting with randomly initialized weights. This means that the model does not utilize any pretraining on large-scale datasets like ImageNet or any other specific initialization method."),(0,n.kt)("p",null,"Instead, it begins with random parameter values for all the layers in the InceptionV2 network. This allows for training the model from scratch or adapting it to a specific task or dataset by optimizing the weights based on the provided training data."),(0,n.kt)("p",null,"The InceptionV2 architecture is a variant of the Inception architecture, which was introduced by researchers at Google as a deep convolutional neural network (CNN) for image classification tasks. InceptionV2 is an improvement upon the original Inception architecture, also known as InceptionV1."),(0,n.kt)("p",null,"The main goal of the InceptionV2 architecture, like its predecessor, is to efficiently capture multi-scale information from images by utilizing various convolutional layers with different receptive field sizes. This allows the network to handle objects of different scales and capture both fine-grained and high-level features simultaneously."),(0,n.kt)("h2",{id:"clarifai_resnext"},"Clarifai_ResNext"),(0,n.kt)("p",null,"This template combines the power of the ResNeXt architecture, pre-trained on ImageNet-21K, with fine-tuning on domain-specific image classification data, and tailored modifications to meet Clarifai's unique requirements."),(0,n.kt)("p",null,'ResNeXt, short for "Residual Next," is a deep convolutional neural network (CNN) architecture that extends the ResNet (Residual Network) architecture. It was introduced by researchers at Facebook AI Research (FAIR) as an advancement in the field of computer vision.'),(0,n.kt)("p",null,'ResNeXt introduces the concept of "cardinality" to enhance the representational power of the network. The cardinality represents the number of parallel paths within each network block, and it captures different types of feature interactions. Unlike the original ResNet architecture, which focuses on increasing depth or width,\nResNeXt achieves higher model capacity by increasing the number of parallel branches, thus allowing for richer and more diverse feature representations.'),(0,n.kt)("p",null,"The main idea behind ResNeXt is to provide a flexible and scalable architecture that can be easily adjusted based on available computational resources and requirements. By varying the cardinality parameter, ResNeXt can be customized to balance model complexity and performance."),(0,n.kt)("p",null,"ResNeXt architectures have demonstrated superior performance on various computer vision tasks, particularly image classification, by leveraging the power of deep residual connections, which enable efficient training of very deep networks. These networks have achieved state-of-the-art results on benchmark datasets, such as ImageNet."),(0,n.kt)("p",null,"This implementation is pre-trained on the ImageNet-21K dataset, which encompasses millions of labeled images across a diverse range of categories. By leveraging this large-scale pretraining, Clarifai_ResNext benefits from learning rich and generalizable visual representations from the vast and diverse ImageNet-21K dataset."),(0,n.kt)("p",null,"Additionally, Clarifai_ResNext has been further trained or fine-tuned on additional image classification data specific to the target domain or task. This additional training ensures that the model is adapted to the nuances and characteristics of the specific image classification problem, further improving its performance and accuracy within the desired context."),(0,n.kt)("h2",{id:"clarifai_inceptiontransferembednorm"},"Clarifai_InceptionTransferEmbedNorm"),(0,n.kt)("p",null,"This template is an advanced image classifier that leverages the power of the Inception architecture as its foundation. It has been pre-trained on the vast and diverse ImageNet-21K dataset, which provides a comprehensive understanding of various visual concepts. Additionally, to enhance its capabilities further, the model has been exposed to additional image classification data, enabling it to handle a broader range of tasks."),(0,n.kt)("p",null,"To adapt the pretrained model for transfer learning, the classification head and hyperparameters have undergone careful modifications and tuning. The classification head refers to the top layers of the network responsible for mapping the learned representations to specific classes or categories. By customizing this component, Clarifai_InceptionTransferEmbedNorm can effectively transfer its knowledge from the source domain (ImageNet-21K) to new, target domains with different sets of classes."),(0,n.kt)("p",null,"Furthermore, the hyperparameters of the model have been fine-tuned to optimize its performance for transfer learning tasks. Hyperparameters are adjustable settings that govern the learning process, such as learning rate, batch size, and regularization parameters. Through meticulous experimentation and validation, the hyperparameters of Clarifai_InceptionTransferEmbedNorm have been carefully chosen to strike a balance between preserving the general knowledge from the source domain and adapting to the unique characteristics of the target domain."),(0,n.kt)("p",null,"By combining the powerful Inception architecture, pre-trained on ImageNet-21K, with the tailored modifications and hyperparameter tuning for transfer learning, Clarifai_InceptionTransferEmbedNorm offers an effective and efficient solution for various image classification tasks, providing accurate predictions and insights."))}h.isMDXComponent=!0}}]);
"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[8770],{11470:(e,n,t)=>{t.d(n,{A:()=>w});var r=t(96540),s=t(18215),o=t(17559),i=t(23104),a=t(56347),l=t(205),d=t(57485),c=t(31682),m=t(70679);function p(e){return r.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,r.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function u(e){const{values:n,children:t}=e;return(0,r.useMemo)(()=>{const e=n??function(e){return p(e).map(({props:{value:e,label:n,attributes:t,default:r}})=>({value:e,label:n,attributes:t,default:r}))}(t);return function(e){const n=(0,c.XI)(e,(e,n)=>e.value===n.value);if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[n,t])}function f({value:e,tabValues:n}){return n.some(n=>n.value===e)}function _({queryString:e=!1,groupId:n}){const t=(0,a.W6)(),s=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,d.aZ)(s),(0,r.useCallback)(e=>{if(!s)return;const n=new URLSearchParams(t.location.search);n.set(s,e),t.replace({...t.location,search:n.toString()})},[s,t])]}function h(e){const{defaultValue:n,queryString:t=!1,groupId:s}=e,o=u(e),[i,a]=(0,r.useState)(()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!f({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const t=n.find(e=>e.default)??n[0];if(!t)throw new Error("Unexpected error: 0 tabValues");return t.value}({defaultValue:n,tabValues:o})),[d,c]=_({queryString:t,groupId:s}),[p,h]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[t,s]=(0,m.Dv)(n);return[t,(0,r.useCallback)(e=>{n&&s.set(e)},[n,s])]}({groupId:s}),g=(()=>{const e=d??p;return f({value:e,tabValues:o})?e:null})();(0,l.A)(()=>{g&&a(g)},[g]);return{selectedValue:i,selectValue:(0,r.useCallback)(e=>{if(!f({value:e,tabValues:o}))throw new Error(`Can't select invalid tab value=${e}`);a(e),c(e),h(e)},[c,h,o]),tabValues:o}}var g=t(92303);const x={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var y=t(74848);function v({className:e,block:n,selectedValue:t,selectValue:r,tabValues:o}){const a=[],{blockElementScrollPositionUntilNextRender:l}=(0,i.a_)(),d=e=>{const n=e.currentTarget,s=a.indexOf(n),i=o[s].value;i!==t&&(l(n),r(i))},c=e=>{let n=null;switch(e.key){case"Enter":d(e);break;case"ArrowRight":{const t=a.indexOf(e.currentTarget)+1;n=a[t]??a[0];break}case"ArrowLeft":{const t=a.indexOf(e.currentTarget)-1;n=a[t]??a[a.length-1];break}}n?.focus()};return(0,y.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,s.A)("tabs",{"tabs--block":n},e),children:o.map(({value:e,label:n,attributes:r})=>(0,y.jsx)("li",{role:"tab",tabIndex:t===e?0:-1,"aria-selected":t===e,ref:e=>{a.push(e)},onKeyDown:c,onClick:d,...r,className:(0,s.A)("tabs__item",x.tabItem,r?.className,{"tabs__item--active":t===e}),children:n??e},e))})}function k({lazy:e,children:n,selectedValue:t}){const o=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=o.find(e=>e.props.value===t);return e?(0,r.cloneElement)(e,{className:(0,s.A)("margin-top--md",e.props.className)}):null}return(0,y.jsx)("div",{className:"margin-top--md",children:o.map((e,n)=>(0,r.cloneElement)(e,{key:n,hidden:e.props.value!==t}))})}function b(e){const n=h(e);return(0,y.jsxs)("div",{className:(0,s.A)(o.G.tabs.container,"tabs-container",x.tabList),children:[(0,y.jsx)(v,{...n,...e}),(0,y.jsx)(k,{...n,...e})]})}function w(e){const n=(0,g.A)();return(0,y.jsx)(b,{...e,children:p(e.children)},String(n))}},19365:(e,n,t)=>{t.d(n,{A:()=>i});t(96540);var r=t(18215);const s={tabItem:"tabItem_Ymn6"};var o=t(74848);function i({children:e,hidden:n,className:t}){return(0,o.jsx)("div",{role:"tabpanel",className:(0,r.A)(s.tabItem,t),hidden:n,children:e})}},30220:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>Ee,contentTitle:()=>Ie,default:()=>Ne,frontMatter:()=>Se,metadata:()=>r,toc:()=>Pe});const r=JSON.parse('{"id":"compute/upload/examples","title":"Model Upload Examples","description":"Learn how to upload and customize your own models on the Clarifai platform","source":"@site/docs/compute/upload/examples.md","sourceDirName":"compute/upload","slug":"/compute/upload/examples","permalink":"/compute/upload/examples","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"description":"Learn how to upload and customize your own models on the Clarifai platform","sidebar_position":5},"sidebar":"tutorialSidebar","previous":{"title":"Input and Output Data Types","permalink":"/compute/upload/data-types"},"next":{"title":"Local Runners","permalink":"/compute/local-runners/"}}');var s=t(74848),o=t(28453),i=(t(11470),t(19365),t(88149));const a='from clarifai.runners.models.model_class import ModelClass\nfrom clarifai.runners.utils.data_utils import Param\nfrom typing import Iterator\nimport random\nimport string\n\nclass MyModel(ModelClass):\n  """This is a model that does some string manipulation."""\n\n  def load_model(self):\n    """Nothing to load for this model."""\n\n  @ModelClass.method\n  def predict(self, prompt: str, number_of_letters: int = Param(default=3, description="number of letters to add")) -> str:\n    """Function to append some string information"""\n    return new_str(prompt, number_of_letters)\n\n  @ModelClass.method\n  def generate(self, prompt: str = "", number_of_letters: int = Param(default=3, description="number of letters to add")) -> Iterator[str]:\n    """Example yielding a whole batch of streamed stuff back."""\n    for i in range(10):  # fake something iterating generating 10 times.\n      yield new_str(str(i) + "-" + prompt, number_of_letters)\n\n  @ModelClass.method\n  def s(self, input_iterator: Iterator[str], number_of_letters: int = Param(default=3, description="number of letters to add")) -> Iterator[str]:\n    """Example yielding getting an iterator and yielding back results."""\n    for i, inp in enumerate(input_iterator):\n      yield new_str(inp, number_of_letters)\n\n\ndef new_str(input_str: str, number_of_letters: int = 3) -> str:\n    """Append a dash and random letters to the input string."""\n    random_letters = \'\'.join(random.choices(string.ascii_letters, k=number_of_letters))\n    return f"{input_str}-{random_letters}\\n"\n\n\ndef test_predict() -> None:\n    """Test the predict method of MyModel by printing its output."""\n    model = MyModel()\n    model.load_model()\n    print("Testing predict method:")\n    output = model.predict("TestPredict", number_of_letters=5)\n    print(output, end="\\n")\n\ndef test_generate() -> None:\n    """Test the generate method of MyModel by printing its outputs."""\n    model = MyModel()\n    model.load_model()\n    print("Testing generate method:")\n    for output in model.generate("Test", number_of_letters=5):\n        print(output, end="\\n")\n\nif __name__ == "__main__":\n    test_predict()\n    test_generate()',l="clarifai",d='model:\n  id: "my-model-id"\n  user_id: "my-user-id"\n  app_id: "my-app-id"\n  model_type_id: "text-to-text"\n\nbuild_info:\n  python_version: "3.12"\n\ninference_compute_info:\n  cpu_limit: 50m\n  cpu_memory: 250Mi\n  num_accelerators: 0',c='import os\nfrom typing import List, Iterator\n\n# Third-party imports\nimport cv2\nimport torch\nfrom PIL import Image as PILImage\nfrom transformers import AutoModelForImageClassification, ViTImageProcessor\n\n# Clarifai imports\nfrom clarifai.runners.models.visual_classifier_class import VisualClassifierClass\nfrom clarifai.runners.utils.data_types import Concept, Image, Video\nfrom clarifai.runners.models.model_builder import ModelBuilder\nfrom clarifai.utils.logging import logger\n\n\nclass ImageClassifierModel(VisualClassifierClass):\n    """A custom runner that classifies images and outputs concepts."""\n\n    def load_model(self):\n        """Load the model and processor."""\n\n        model_path = os.path.dirname(os.path.dirname(__file__))\n        builder = ModelBuilder(model_path, download_validation_only=True)\n        checkpoints = builder.download_checkpoints(stage="runtime")\n\n        self.device = \'cuda\' if torch.cuda.is_available() else \'cpu\'\n        logger.info(f"Running on device: {self.device}")\n\n        self.model = AutoModelForImageClassification.from_pretrained(checkpoints,).to(self.device)\n        self.model_labels = self.model.config.id2label\n        self.processor = ViTImageProcessor.from_pretrained(checkpoints)\n\n        logger.info("Done loading!")\n\n    @VisualClassifierClass.method\n    def predict(self, image: Image) -> List[Concept]:\n        """Predict concepts for a list of images."""\n        pil_image = VisualClassifierClass.preprocess_image(image.bytes)\n        inputs = self.processor(images=pil_image, return_tensors="pt")\n        inputs = {name: tensor.to(self.device) for name, tensor in inputs.items()}\n        with torch.no_grad():\n            logits = self.model(**inputs).logits\n        outputs = VisualClassifierClass.process_concepts(logits, self.model_labels)\n        return outputs[0]\n\n    @VisualClassifierClass.method\n    def generate(self, video: Video) -> Iterator[List[Concept]]:\n        """Generate concepts for frames extracted from a video."""\n        video_bytes = video.bytes\n        frame_generator = VisualClassifierClass.video_to_frames(video_bytes)\n        for frame in frame_generator:\n            image = VisualClassifierClass.preprocess_image(frame.image.bytes)\n            inputs = self.processor(images=image, return_tensors="pt")\n            inputs = {name: tensor.to(self.device) for name, tensor in inputs.items()}\n            with torch.no_grad():\n                logits = self.model(**inputs).logits\n                outputs = VisualClassifierClass.process_concepts(logits, self.model_labels)  # Yield concepts for each frame\n                yield outputs[0]\n\n    @VisualClassifierClass.method\n    def stream_image(self, image_stream: Iterator[Image]) -> Iterator[List[Concept]]:\n        """Stream process image inputs."""\n        for image in image_stream:\n            result = self.predict(image)\n            yield result\n\n    @VisualClassifierClass.method\n    def stream_video(self, video_stream: Iterator[Video]) -> Iterator[List[Concept]]:\n        """Stream process video inputs."""\n        for video in video_stream:\n            for frame_result in self.generate(video):\n                yield frame_result\n\n    def test(self):\n        """Test the model functionality."""\n        import requests  # Import moved here as it\'s only used for testing\n\n        # Test configuration\n        TEST_URLS = {\n            "images": [\n                "https://samples.clarifai.com/metro-north.jpg",\n                "https://samples.clarifai.com/dog.tiff"\n            ],\n            "video": "https://samples.clarifai.com/beer.mp4"\n        }\n\n        def get_test_data(url):\n            return Image(bytes=requests.get(url).content)\n\n        def get_test_video():\n            return Video(bytes=requests.get(TEST_URLS["video"]).content)\n\n        def run_test(name, test_fn):\n            logger.info(f"\\nTesting {name}...")\n            try:\n                test_fn()\n                logger.info(f"{name} test completed successfully")\n            except Exception as e:\n                logger.error(f"Error in {name} test: {e}")\n\n        # Test predict\n        def test_predict():\n            result = self.predict(get_test_data(TEST_URLS["images"][0]))\n            logger.info(f"Predict result: {result}")\n\n        # Test generate\n        def test_generate():\n            for classifications in self.generate(get_test_video()):\n                logger.info(f"First frame classifications: {classifications}")\n                break\n\n        # Test stream\n        def test_stream():\n            # Split into two separate test functions for clarity\n            def test_stream_image():\n                images = [get_test_data(url) for url in TEST_URLS["images"]]\n                for result in self.stream_image(iter(images)):\n                    logger.info(f"Image stream result: {result}")\n\n            def test_stream_video():\n                for result in self.stream_video(iter([get_test_video()])):\n                    logger.info(f"Video stream result: {result}")\n                    break  # Just test first frame\n\n            logger.info("\\nTesting image streaming...")\n            test_stream_image()\n            logger.info("\\nTesting video streaming...")\n            test_stream_video()\n\n        # Run all tests\n        for test_name, test_fn in [\n            ("predict", test_predict),\n            ("generate", test_generate),\n            ("stream", test_stream)\n        ]:\n            run_test(test_name, test_fn)',m="torch==2.6.0\ntransformers>=4.51.1\npillow==10.4.0\nrequests==2.32.3\ntimm==1.0.12\nopencv-python-headless==4.10.0.84\nclarifai>=11.5.0,<12.0.0",p='# This is the sample config file for the image-classification model.\n\nmodel:\n  id: "nsfw_image_detection"\n  user_id: "user_id"\n  app_id: "app_id"\n  model_type_id: "visual-detector"\n\nbuild_info:\n  python_version: "3.11"\n\ninference_compute_info:\n  cpu_limit: "2"\n  cpu_memory: "2Gi"\n  num_accelerators: 1\n  accelerator_type: ["NVIDIA-*"]\n  accelerator_memory: "3Gi"\n\n\ncheckpoints:\n  type: "huggingface"\n  repo_id: "Falconsai/nsfw_image_detection"',u='# Standard library imports\nimport os\nfrom typing import List, Dict, Any, Iterator\n\n# Third-party imports\nimport cv2\nimport torch\nfrom PIL import Image as PILImage\nfrom transformers import DetrForObjectDetection, DetrImageProcessor\n\n# Clarifai imports\nfrom clarifai.runners.models.model_builder import ModelBuilder\nfrom clarifai.runners.models.visual_detector_class import VisualDetectorClass\nfrom clarifai.runners.utils.data_types import Image, Video, Region, Frame\nfrom clarifai.utils.logging import logger\n\n\ndef detect_objects(\n    images: List[PILImage],\n    model: DetrForObjectDetection,\n    processor: DetrImageProcessor,\n    device: str\n) -> Dict[str, Any]:\n    """Process images through the DETR model to detect objects.\n\n    Args:\n        images: List of preprocessed images\n        model: DETR model instance\n        processor: Image processor for DETR\n        device: Computation device (CPU/GPU)\n\n    Returns:\n        Detection results from the model\n    """\n    model_inputs = processor(images=images, return_tensors="pt").to(device)\n    model_inputs = {name: tensor.to(device) for name, tensor in model_inputs.items()}\n    model_output = model(**model_inputs)\n    results = processor.post_process_object_detection(model_output)\n    return results\n\n\nclass MyRunner(VisualDetectorClass):\n    """A custom runner for DETR object detection model that processes images and videos"""\n\n    def load_model(self):\n        """Load the model here."""\n        model_path = os.path.dirname(os.path.dirname(__file__))\n        builder = ModelBuilder(model_path, download_validation_only=True)\n        checkpoint_path = builder.download_checkpoints(stage="runtime")\n        \n        self.device = \'cuda\' if torch.cuda.is_available() else \'cpu\'\n        logger.info(f"Running on device: {self.device}")\n\n        self.model = DetrForObjectDetection.from_pretrained(checkpoint_path).to(self.device)\n        self.processor = DetrImageProcessor.from_pretrained(checkpoint_path)\n        self.model.eval()\n        self.threshold = 0.9\n        self.model_labels = self.model.config.id2label\n\n        logger.info("Done loading!")\n\n    @VisualDetectorClass.method\n    def predict(self, image: Image) -> List[Region]:\n        """Process a single image and return detected objects."""\n        image_bytes = image.bytes\n        image = VisualDetectorClass.preprocess_image(image_bytes)\n        \n        with torch.no_grad():\n            results = detect_objects([image], self.model, self.processor, self.device)\n            outputs = VisualDetectorClass.process_detections(results, self.threshold, self.model_labels)\n            return outputs[0]  # Return detections for single image\n\n    @VisualDetectorClass.method\n    def generate(self, video: Video) -> Iterator[Frame]:\n        """Process video frames and yield detected objects for each frame."""\n        frame_generator = VisualDetectorClass.video_to_frames(video.bytes)\n        for frame in frame_generator:\n            with torch.no_grad():\n                image = VisualDetectorClass.preprocess_image(frame.image.bytes)\n                results = detect_objects([image], self.model, self.processor, self.device)\n                outputs = VisualDetectorClass.process_detections(results, self.threshold, self.model_labels)\n                frame.regions = outputs[0]  # Assign detections to the frame\n                yield frame  # Yield the frame with detections\n\n    @VisualDetectorClass.method\n    def stream_image(self, image_stream: Iterator[Image]) -> Iterator[List[Region]]:\n        """Stream process image inputs."""\n        for image in image_stream:\n            result = self.predict(image)\n            yield result\n\n    @VisualDetectorClass.method\n    def stream_video(self, video_stream: Iterator[Video]) -> Iterator[Frame]:\n        """Stream process video inputs."""\n        for video in video_stream:\n            for frame_result in self.generate(video):\n                yield frame_result\n        \n    def test(self):\n        """Test the model functionality."""\n        import requests  # Import moved here as it\'s only used for testing\n        \n        # Test configuration\n        TEST_URLS = {\n            "images": [\n                "https://samples.clarifai.com/metro-north.jpg",\n                "https://samples.clarifai.com/dog.tiff"\n            ],\n            "video": "https://samples.clarifai.com/beer.mp4"\n        }\n\n        def get_test_data(url):\n            return Image(bytes=requests.get(url).content)\n\n        def get_test_video():\n            return Video(bytes=requests.get(TEST_URLS["video"]).content)\n\n        def run_test(name, test_fn):\n            logger.info(f"\\nTesting {name}...")\n            try:\n                test_fn()\n                logger.info(f"{name} test completed successfully")\n            except Exception as e:\n                logger.error(f"Error in {name} test: {e}")\n\n        # Test predict\n        def test_predict():\n            result = self.predict(get_test_data(TEST_URLS["images"][0]))\n            logger.info(f"Predict result: {result}")\n\n        # Test generate\n        def test_generate():\n            for detections in self.generate(get_test_video()):\n                logger.info(f"First frame detections: {detections}")\n                break\n\n        # Test stream\n        def test_stream():\n            # Split into two separate test functions for clarity\n            def test_stream_image():\n                images = [get_test_data(url) for url in TEST_URLS["images"]]\n                for result in self.stream_image(iter(images)):\n                    logger.info(f"Image stream result: {result}")\n\n            def test_stream_video():\n                for result in self.stream_video(iter([get_test_video()])):\n                    logger.info(f"Video stream result: {result}")\n                    break  # Just test first frame\n\n            logger.info("\\nTesting image streaming...")\n            test_stream_image()\n            logger.info("\\nTesting video streaming...")\n            test_stream_video()\n\n        # Run all tests\n        for test_name, test_fn in [\n            ("predict", test_predict),\n            ("generate", test_generate),\n            ("stream", test_stream)\n        ]:\n            run_test(test_name, test_fn)',f="torch==2.6.0\ntransformers>=4.51.1\npillow==10.4.0\nrequests==2.32.3\ntimm==1.0.12\nopencv-python-headless==4.10.0.84\nclarifai>=11.4.10,<12.0.0",_='# This is the sample config file for the image-detection model.\n\nmodel:\n  id: "detr-resnet-50"\n  user_id: "user_id"\n  app_id: "app_id"\n  model_type_id: "visual-detector"\n\nbuild_info:\n  python_version: "3.11"\n\ninference_compute_info:\n  cpu_limit: "4"\n  cpu_memory: "2Gi"\n  num_accelerators: 1\n  accelerator_type: ["NVIDIA-*"]\n  accelerator_memory: "5Gi"\n\n\ncheckpoints:\n  type: "huggingface"\n  repo_id: "facebook/detr-resnet-50"\n  hf_token: "hf_token"',h='import os\n\nfrom typing import Iterator, List, Tuple\n\nfrom clarifai.runners.models.visual_detector_class import VisualDetectorClass\nfrom clarifai.runners.utils import data_types as dt\nfrom clarifai.utils.logging import logger\n\nimport yaml\nimport torch\nfrom PIL import Image as PILImage\nfrom transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation \n\n\nROOT = os.path.dirname(__file__)\n\n\nclass MyRunner(VisualDetectorClass):\n  \n  def _load_concepts(self, config_path, name, model_path):\n    \n    with open(config_path, "r") as f:\n      data = yaml.safe_load(f)\n    \n    # Map Clarifai concept name to id and reverse\n    self.conceptid2name = {each["id"] : each["name"] for each in data.get("concepts", [])}\n    self.conceptname2id = {each["name"] : each["id"] for each in data.get("concepts", [])}\n  \n  def load_model(self):\n    """Load the model here."""\n    checkpoint_path = "facebook/mask2former-swin-tiny-ade-semantic"\n    self.device = \'cuda\' #if torch.cuda.is_available() else \'cpu\'\n    logger.info(f"Running on device: {self.device}")\n    self.model = Mask2FormerForUniversalSegmentation.from_pretrained(\n        checkpoint_path, trust_remote_code=True).to(self.device)\n    self.processor = AutoImageProcessor.from_pretrained(checkpoint_path, trust_remote_code=True)\n    self.model.eval()\n    # Load clarifai concept\n    config_path = os.path.join(ROOT, "../config.yaml")\n    self._load_concepts(config_path, "mask2former-ade", checkpoint_path)\n        \n    logger.info("Done loading!")\n  \n  \n  @VisualDetectorClass.method\n  def model_predict(\n    self, \n    images: List[PILImage.Image]\n  ) -> List[List[dt.Region]]:\n    \n    inputs = self.processor(images=images, return_tensors="pt").to(self.device)\n    with torch.no_grad():\n      outputs = self.model(**inputs)\n    target_sizes = [image.size[::-1] for image in images]\n    results = self.processor.post_process_semantic_segmentation(outputs, target_sizes=target_sizes)\n    outputs = []\n    for i, all_masks_tensor in enumerate(results):\n      masks = []\n      for clss_id in all_masks_tensor.unique().tolist():\n        label = self.model.config.id2label[clss_id]\n        mask = torch.zeros_like(all_masks_tensor)\n        mask[all_masks_tensor == clss_id] = 255\n        mask = mask.cpu().numpy() if self.device == "cuda" else mask.numpy()\n        mask = PILImage.fromarray(mask.astype("uint8"))\n        region = dt.Region(\n          mask=mask,\n          concepts=[dt.Concept(id=self.conceptname2id[label], name=label)]\n        )\n        masks.append(region)\n      outputs.append(masks)\n    \n    return outputs\n    \n  @VisualDetectorClass.method\n  def predict(self, image: dt.Image) -> List[dt.Region]:\n    return self.model_predict([image.to_pil()])[0]\n\n  @VisualDetectorClass.method\n  def stream(self, images: Iterator[dt.Image]) -> Iterator[dt.Region]:\n    for each in images:\n      yield self.predict(image=each)\n      \n  @VisualDetectorClass.method\n  def generate(self, video: dt.Video) -> Iterator[dt.Region]:\n    for frame in self.video_to_frames(video.bytes):\n      yield self.predict(image=frame.image)\n      \n  def test(self):\n    import requests\n    \n    image = dt.Image(bytes=requests.get("https://samples.clarifai.com/metro-north.jpg").content)\n    video = dt.Video(bytes=requests.get("https://samples.clarifai.com/beer.mp4").content)   \n\n    logger.info("# -------- Test predict/detect -------------")\n    logger.info(f"{self.predict(image=image)}")\n    logger.info("# -------- Test generate -------------")\n    n=5\n    for i, each in enumerate(self.generate(video=video)):\n      print(each)\n      if i > n:\n        break\n    \n    logger.info("# -------- Test stream -------------")\n    \n    def iteration():\n      for each in [image]*10:\n        yield each\n    \n    for i, each in enumerate(self.stream(images=iteration())):\n      print(each)\n      if i > n:\n        break',g="torch==2.6.0\ntokenizers>=0.19.1\ntransformers>=4.44.2\npillow>=10.4.0\nrequests>=2.32.3\ntimm\nopencv-python-headless==4.10.0.84\nnumpy\naiohttp\nscipy>=1.10.1",x="model:\n  id: facebook_mask2former-swin-tiny-ade-semantic\n  user_id: a\n  app_id: b\n  model_type_id: visual-segmenter\nbuild_info:\n  python_version: '3.12'\ninference_compute_info:\n  cpu_limit: '2'\n  cpu_memory: 8Gi\n  num_accelerators: 1\n  accelerator_type:\n  - NVIDIA-*\n  accelerator_memory: 21Gi\n\nconcepts:\n- id: id-mask2former-ade-0\n  name: wall\n- id: id-mask2former-ade-1\n  name: building\n- id: id-mask2former-ade-2\n  name: sky\n- id: id-mask2former-ade-3\n  name: floor\n- id: id-mask2former-ade-4\n  name: tree\n- id: id-mask2former-ade-5\n  name: ceiling\n- id: id-mask2former-ade-6\n  name: road\n- id: id-mask2former-ade-7\n  name: 'bed '\n- id: id-mask2former-ade-8\n  name: windowpane\n- id: id-mask2former-ade-9\n  name: grass\n- id: id-mask2former-ade-10\n  name: cabinet\n- id: id-mask2former-ade-11\n  name: sidewalk\n- id: id-mask2former-ade-12\n  name: person\n- id: id-mask2former-ade-13\n  name: earth\n- id: id-mask2former-ade-14\n  name: door\n- id: id-mask2former-ade-15\n  name: table\n- id: id-mask2former-ade-16\n  name: mountain\n- id: id-mask2former-ade-17\n  name: plant\n- id: id-mask2former-ade-18\n  name: curtain\n- id: id-mask2former-ade-19\n  name: chair\n- id: id-mask2former-ade-20\n  name: car\n- id: id-mask2former-ade-21\n  name: water\n- id: id-mask2former-ade-22\n  name: painting\n- id: id-mask2former-ade-23\n  name: sofa\n- id: id-mask2former-ade-24\n  name: shelf\n- id: id-mask2former-ade-25\n  name: house\n- id: id-mask2former-ade-26\n  name: sea\n- id: id-mask2former-ade-27\n  name: mirror\n- id: id-mask2former-ade-28\n  name: rug\n- id: id-mask2former-ade-29\n  name: field\n- id: id-mask2former-ade-30\n  name: armchair\n- id: id-mask2former-ade-31\n  name: seat\n- id: id-mask2former-ade-32\n  name: fence\n- id: id-mask2former-ade-33\n  name: desk\n- id: id-mask2former-ade-34\n  name: rock\n- id: id-mask2former-ade-35\n  name: wardrobe\n- id: id-mask2former-ade-36\n  name: lamp\n- id: id-mask2former-ade-37\n  name: bathtub\n- id: id-mask2former-ade-38\n  name: railing\n- id: id-mask2former-ade-39\n  name: cushion\n- id: id-mask2former-ade-40\n  name: base\n- id: id-mask2former-ade-41\n  name: box\n- id: id-mask2former-ade-42\n  name: column\n- id: id-mask2former-ade-43\n  name: signboard\n- id: id-mask2former-ade-44\n  name: chest of drawers\n- id: id-mask2former-ade-45\n  name: counter\n- id: id-mask2former-ade-46\n  name: sand\n- id: id-mask2former-ade-47\n  name: sink\n- id: id-mask2former-ade-48\n  name: skyscraper\n- id: id-mask2former-ade-49\n  name: fireplace\n- id: id-mask2former-ade-50\n  name: refrigerator\n- id: id-mask2former-ade-51\n  name: grandstand\n- id: id-mask2former-ade-52\n  name: path\n- id: id-mask2former-ade-53\n  name: stairs\n- id: id-mask2former-ade-54\n  name: runway\n- id: id-mask2former-ade-55\n  name: case\n- id: id-mask2former-ade-56\n  name: pool table\n- id: id-mask2former-ade-57\n  name: pillow\n- id: id-mask2former-ade-58\n  name: screen door\n- id: id-mask2former-ade-59\n  name: stairway\n- id: id-mask2former-ade-60\n  name: river\n- id: id-mask2former-ade-61\n  name: bridge\n- id: id-mask2former-ade-62\n  name: bookcase\n- id: id-mask2former-ade-63\n  name: blind\n- id: id-mask2former-ade-64\n  name: coffee table\n- id: id-mask2former-ade-65\n  name: toilet\n- id: id-mask2former-ade-66\n  name: flower\n- id: id-mask2former-ade-67\n  name: book\n- id: id-mask2former-ade-68\n  name: hill\n- id: id-mask2former-ade-69\n  name: bench\n- id: id-mask2former-ade-70\n  name: countertop\n- id: id-mask2former-ade-71\n  name: stove\n- id: id-mask2former-ade-72\n  name: palm\n- id: id-mask2former-ade-73\n  name: kitchen island\n- id: id-mask2former-ade-74\n  name: computer\n- id: id-mask2former-ade-75\n  name: swivel chair\n- id: id-mask2former-ade-76\n  name: boat\n- id: id-mask2former-ade-77\n  name: bar\n- id: id-mask2former-ade-78\n  name: arcade machine\n- id: id-mask2former-ade-79\n  name: hovel\n- id: id-mask2former-ade-80\n  name: bus\n- id: id-mask2former-ade-81\n  name: towel\n- id: id-mask2former-ade-82\n  name: light\n- id: id-mask2former-ade-83\n  name: truck\n- id: id-mask2former-ade-84\n  name: tower\n- id: id-mask2former-ade-85\n  name: chandelier\n- id: id-mask2former-ade-86\n  name: awning\n- id: id-mask2former-ade-87\n  name: streetlight\n- id: id-mask2former-ade-88\n  name: booth\n- id: id-mask2former-ade-89\n  name: television receiver\n- id: id-mask2former-ade-90\n  name: airplane\n- id: id-mask2former-ade-91\n  name: dirt track\n- id: id-mask2former-ade-92\n  name: apparel\n- id: id-mask2former-ade-93\n  name: pole\n- id: id-mask2former-ade-94\n  name: land\n- id: id-mask2former-ade-95\n  name: bannister\n- id: id-mask2former-ade-96\n  name: escalator\n- id: id-mask2former-ade-97\n  name: ottoman\n- id: id-mask2former-ade-98\n  name: bottle\n- id: id-mask2former-ade-99\n  name: buffet\n- id: id-mask2former-ade-100\n  name: poster\n- id: id-mask2former-ade-101\n  name: stage\n- id: id-mask2former-ade-102\n  name: van\n- id: id-mask2former-ade-103\n  name: ship\n- id: id-mask2former-ade-104\n  name: fountain\n- id: id-mask2former-ade-105\n  name: conveyer belt\n- id: id-mask2former-ade-106\n  name: canopy\n- id: id-mask2former-ade-107\n  name: washer\n- id: id-mask2former-ade-108\n  name: plaything\n- id: id-mask2former-ade-109\n  name: swimming pool\n- id: id-mask2former-ade-110\n  name: stool\n- id: id-mask2former-ade-111\n  name: barrel\n- id: id-mask2former-ade-112\n  name: basket\n- id: id-mask2former-ade-113\n  name: waterfall\n- id: id-mask2former-ade-114\n  name: tent\n- id: id-mask2former-ade-115\n  name: bag\n- id: id-mask2former-ade-116\n  name: minibike\n- id: id-mask2former-ade-117\n  name: cradle\n- id: id-mask2former-ade-118\n  name: oven\n- id: id-mask2former-ade-119\n  name: ball\n- id: id-mask2former-ade-120\n  name: food\n- id: id-mask2former-ade-121\n  name: step\n- id: id-mask2former-ade-122\n  name: tank\n- id: id-mask2former-ade-123\n  name: trade name\n- id: id-mask2former-ade-124\n  name: microwave\n- id: id-mask2former-ade-125\n  name: pot\n- id: id-mask2former-ade-126\n  name: animal\n- id: id-mask2former-ade-127\n  name: bicycle\n- id: id-mask2former-ade-128\n  name: lake\n- id: id-mask2former-ade-129\n  name: dishwasher\n- id: id-mask2former-ade-130\n  name: screen\n- id: id-mask2former-ade-131\n  name: blanket\n- id: id-mask2former-ade-132\n  name: sculpture\n- id: id-mask2former-ade-133\n  name: hood\n- id: id-mask2former-ade-134\n  name: sconce\n- id: id-mask2former-ade-135\n  name: vase\n- id: id-mask2former-ade-136\n  name: traffic light\n- id: id-mask2former-ade-137\n  name: tray\n- id: id-mask2former-ade-138\n  name: ashcan\n- id: id-mask2former-ade-139\n  name: fan\n- id: id-mask2former-ade-140\n  name: pier\n- id: id-mask2former-ade-141\n  name: crt screen\n- id: id-mask2former-ade-142\n  name: plate\n- id: id-mask2former-ade-143\n  name: monitor\n- id: id-mask2former-ade-144\n  name: bulletin board\n- id: id-mask2former-ade-145\n  name: shower\n- id: id-mask2former-ade-146\n  name: radiator\n- id: id-mask2former-ade-147\n  name: glass\n- id: id-mask2former-ade-148\n  name: clock\n- id: id-mask2former-ade-149\n  name: flag",y='from typing import List\nimport os\n\nfrom clarifai.runners.models.model_builder import ModelBuilder\nfrom clarifai.runners.models.openai_class import OpenAIModelClass\nfrom clarifai.runners.utils.data_types import Image\n\nfrom clarifai.runners.utils.data_utils import Param\nfrom clarifai.utils.logging import logger\n\nfrom diffusers import StableDiffusionDepth2ImgPipeline\nfrom diffusers.utils import load_image\n\nimport torch\n\n\n\nclass StableDiffusion(OpenAIModelClass):\n    """\n    A Model that integrates with the Clarifai platform and uses the FluxFillPipeline for image inpainting.\n    """\n\n    client = True  # This will be set in load_model method\n    model = True  # This will be set in load_model method\n\n    def load_model(self):\n        """Load the model here."""\n        model_path = os.path.dirname(os.path.dirname(__file__))\n        builder = ModelBuilder(model_path, download_validation_only=True)\n        hf_token = builder.config["checkpoints"]["hf_token"]\n        \n        self.client = StableDiffusionDepth2ImgPipeline.from_pretrained("stabilityai/stable-diffusion-2-depth",\n                                                                       torch_dtype=torch.float16,\n                                                                       use_safetensors=True,\n                                                                       token=hf_token)\n        self.client.to("cuda" if torch.cuda.is_available() else "cpu")\n        logger.info("stable-diffusion model loaded successfully.")\n    \n    \n    @OpenAIModelClass.method\n    def predict(self,\n                prompt: str,\n                image: Image = None,\n                negative_prompt: str = None,\n                mask: Image = None,\n                strength: float = 0.8\n                ) -> Image:\n        """\n        Predict method that uses the FluxFillPipeline to inpaint images based on the provided prompt.\n\n        """\n        if image:\n            if image.url:\n                image = load_image(image.url)\n            elif image.bytes:\n                image=image.to_pil()\n        \n        response = self.client(prompt=prompt,\n                            image=image,\n                            negative_prompt=negative_prompt,\n                            strength=strength).images[0]\n        \n        return Image.from_pil(pil_image = response)',v="tokenizers==0.21.0\ntransformers>=4.48\ndiffusers==0.32.2\naccelerate==1.2.0\noptimum==1.23.3\nxformers\neinops==0.8.0\nrequests==2.32.3\nnumpy>2.0\ntorch==2.5.1\nclarifai\nclarifai-protocol",k='model:\n  id: "stable-diffusion-2-depth"\n  user_id: "user_id"\n  app_id: "app_id"\n  model_type_id: "multimodal-to-text"\n\nbuild_info:\n  python_version: \'3.11\'\n\ninference_compute_info:\n  cpu_limit: \'3\' \n  cpu_memory: 15Gi\n  num_accelerators: 1\n  accelerator_type: ["NVIDIA-*"]\n  accelerator_memory: 6Gi\n\ncheckpoints:\n  type: huggingface\n  repo_id: "stabilityai/stable-diffusion-2-depth"\n  hf_token: "hf_token"\n  when: runtime',b='import os\nimport sys\n\nsys.path.append(os.path.dirname(__file__))\nfrom typing import Iterator, List\n\nfrom clarifai.runners.models.model_builder import ModelBuilder\nfrom clarifai.runners.models.openai_class import OpenAIModelClass\nfrom clarifai.runners.utils.data_utils import Param\nfrom clarifai.runners.utils.openai_convertor import build_openai_messages\nfrom clarifai.utils.logging import logger\nfrom openai import OpenAI\nfrom openai_server_starter import OpenAI_APIServer\n\n##################\n\nclass SglangModel(OpenAIModelClass):\n    """\n    A custom runner that integrates with the Clarifai platform and uses Server inference\n    to process inputs, including text.\n    """\n\n    client = True  # This will be set in load_model method\n    model = True  # This will be set in load_model method\n\n    def load_model(self):\n        """Load the model here and start the  server."""\n        os.path.join(os.path.dirname(__file__))\n        # Use downloaded checkpoints.\n        # Or if you intend to download checkpoint at runtime, set hf id instead. For example:\n        # checkpoints = "Qwen/Qwen2-7B-Instruct"\n\n        # server args were generated by `upload` module\n        server_args = {\n                    \'dtype\': \'auto\',\n                    \'kv_cache_dtype\': \'auto\',\n                    \'tp_size\': 1,\n                    \'load_format\': \'auto\',\n                    \'context_length\': None,\n                    \'device\': \'cuda\',\n                    \'port\': 23333,\n                    \'host\': \'0.0.0.0\',\n                    \'mem_fraction_static\': 0.9,\n                    \'max_total_tokens\': \'8192\',\n                    \'max_prefill_tokens\': None,\n                    \'schedule_policy\': \'fcfs\',\n                    \'schedule_conservativeness\': 1.0,\n                    \'checkpoints\': \'runtime\'}\n\n        # if checkpoints == "checkpoints" => assign to checkpoints var aka local checkpoints path\n        stage = server_args.get("checkpoints")\n        if stage in ["build", "runtime"]:\n            #checkpoints = os.path.join(os.path.dirname(__file__), "checkpoints")\n            config_path = os.path.dirname(os.path.dirname(__file__))\n            builder = ModelBuilder(config_path, download_validation_only=True)\n            checkpoints = builder.download_checkpoints(stage=stage)\n            server_args.update({"checkpoints": checkpoints})\n\n        if server_args.get("additional_list_args") == [\'\']:\n            server_args.pop("additional_list_args")\n\n        # Start server\n        # This line were generated by `upload` module\n        self.server = OpenAI_APIServer.from_sglang_backend(**server_args)\n\n        # Create client\n        self.client = OpenAI(\n                api_key="notset",\n                base_url=SglangModel.make_api_url(self.server.host, self.server.port))\n        self.model = self._get_model()\n\n        logger.info(f"OpenAI {self.model} model loaded successfully!")\n\n    def _get_model(self):\n        try:\n            return self.client.models.list().data[0].id\n        except Exception as e:\n            raise ConnectionError("Failed to retrieve model ID from API") from e\n\n    @staticmethod\n    def make_api_url(host: str, port: int, version: str = "v1") -> str:\n        return f"http://{host}:{port}/{version}"\n\n    @OpenAIModelClass.method\n    def predict(self,\n                prompt: str,\n                chat_history: List[dict] = None,\n                max_tokens: int = Param(default=512, description="The maximum number of tokens to generate. Shorter token lengths will provide faster performance.", ),\n                temperature: float = Param(default=0.7, description="A decimal number that determines the degree of randomness in the response", ),\n                top_p: float = Param(default=0.8, description="An alternative to sampling with temperature, where the model considers the results of the tokens with top_p probability mass.", )\n                ) -> str:\n        """This is the method that will be called when the runner is run. It takes in an input and\n        returns an output.\n        """\n        openai_messages = build_openai_messages(prompt=prompt, messages=chat_history)\n        response = self.client.chat.completions.create(\n            model=self.model,\n            messages=openai_messages,\n            max_completion_tokens=max_tokens,\n            temperature=temperature,\n            top_p=top_p)\n        if response.usage and response.usage.prompt_tokens and response.usage.completion_tokens:\n            self.set_output_context(prompt_tokens=response.usage.prompt_tokens, completion_tokens=response.usage.completion_tokens)\n        return response.choices[0].message.content\n\n    @OpenAIModelClass.method\n    def generate(self,\n                prompt: str,\n                chat_history: List[dict] = None,\n                max_tokens: int = Param(default=512, description="The maximum number of tokens to generate. Shorter token lengths will provide faster performance.", ),\n                temperature: float = Param(default=0.7, description="A decimal number that determines the degree of randomness in the response", ),\n                top_p: float = Param(default=0.8, description="An alternative to sampling with temperature, where the model considers the results of the tokens with top_p probability mass.", )\n                ) -> Iterator[str]:\n        """Example yielding a whole batch of streamed stuff back."""\n        openai_messages = build_openai_messages(prompt=prompt, messages=chat_history)\n        for chunk in self.client.chat.completions.create(\n            model=self.model,\n            messages=openai_messages,\n            max_completion_tokens=max_tokens,\n            temperature=temperature,\n            top_p=top_p,\n            stream=True):\n            if chunk.choices:\n                text = (chunk.choices[0].delta.content\n                        if (chunk and chunk.choices[0].delta.content) is not None else \'\')\n                yield text\n\n    # This method is needed to test the model with the test-locally CLI command.\n    def test(self):\n        """Test the model here."""\n        try:\n            print("Testing predict...")\n            # Test predict\n            print(self.predict(prompt="Hello, how are you?",))\n        except Exception as e:\n            print("Error in predict", e)\n\n        try:\n            print("Testing generate...")\n            # Test generate\n            for each in self.generate(prompt="Hello, how are you?",):\n                print(each, end=" ")\n        except Exception as e:\n            print("Error in generate", e)',w='import os\nimport signal\nimport subprocess\nimport sys\nimport threading\nfrom typing import List\n\nimport psutil\nfrom clarifai.utils.logging import logger\n\nPYTHON_EXEC = sys.executable\n\n\ndef kill_process_tree(parent_pid, include_parent: bool = True, skip_pid: int = None):\n  """Kill the process and all its child processes."""\n  if parent_pid is None:\n    parent_pid = os.getpid()\n    include_parent = False\n\n  try:\n    itself = psutil.Process(parent_pid)\n  except psutil.NoSuchProcess:\n    return\n\n  children = itself.children(recursive=True)\n  for child in children:\n    if child.pid == skip_pid:\n      continue\n    try:\n      child.kill()\n    except psutil.NoSuchProcess:\n      pass\n\n  if include_parent:\n    try:\n      itself.kill()\n\n      # Sometime processes cannot be killed with SIGKILL (e.g, PID=1 launched by kubernetes),\n      # so we send an additional signal to kill them.\n      itself.send_signal(signal.SIGQUIT)\n    except psutil.NoSuchProcess:\n      pass\n\n\nclass OpenAI_APIServer:\n\n  def __init__(self, **kwargs):\n    self.server_started_event = threading.Event()\n    self.process = None\n    self.backend = None\n    self.server_thread = None\n\n  def __del__(self, *exc):\n    # This is important\n    # close the server when exit the program\n    self.close()\n\n  def close(self):\n    if self.process:\n      try:\n        kill_process_tree(self.process.pid)\n      except:\n        self.process.terminate()\n    if self.server_thread:\n      self.server_thread.join()\n\n  def wait_for_startup(self):\n    self.server_started_event.wait()\n\n  def validate_if_server_start(self, line: str):\n    line_lower = line.lower()\n    if self.backend in ["vllm", "sglang", "lmdeploy"]:\n      if self.backend == "vllm":\n        return "application startup complete" in line_lower or "vllm api server on" in line_lower\n      else:\n        return f" running on http://{self.host}:" in line.strip()\n    elif self.backend == "llamacpp":\n      return "waiting for new tasks" in line_lower\n    elif self.backend == "tgi":\n      return "Connected" in line.strip()\n\n  def _start_server(self, cmds):\n    try:\n      env = os.environ.copy()\n      env["VLLM_USAGE_SOURCE"] = "production-docker-image"\n      self.process = subprocess.Popen(\n          cmds,\n          stdout=subprocess.PIPE,\n          stderr=subprocess.STDOUT,\n          text=True,\n      )\n      for line in self.process.stdout:\n        logger.info("Server Log:  " + line.strip())\n        if self.validate_if_server_start(line):\n          self.server_started_event.set()\n          # break\n    except Exception as e:\n      if self.process:\n        self.process.terminate()\n      raise RuntimeError(f"Failed to start Server server: {e}")\n\n  def start_server_thread(self, cmds: str):\n    try:\n      # Start the  server in a separate thread\n      self.server_thread = threading.Thread(target=self._start_server, args=(cmds,), daemon=None)\n      self.server_thread.start()\n\n      # Wait for the server to start\n      self.wait_for_startup()\n    except Exception as e:\n      raise Exception(e)\n\n  @classmethod\n  def from_sglang_backend(\n      cls,\n      checkpoints,\n      dtype: str = "auto",\n      kv_cache_dtype: str = "auto",\n      tp_size: int = 1,\n      quantization: str = None,\n      load_format: str = "auto",\n      context_length: str = None,\n      device: str = "cuda",\n      port=23333,\n      host="0.0.0.0",\n      chat_template: str = None,\n      mem_fraction_static: float = 0.8,\n      max_running_requests: int = None,\n      max_total_tokens: int = None,\n      max_prefill_tokens: int = None,\n      schedule_policy: str = "fcfs",\n      schedule_conservativeness: float = 1.0,\n      cpu_offload_gb: int = 0,\n      additional_list_args: List[str] = [],\n  ):\n    """Start SGlang OpenAI compatible server.\n\n    Args:\n        checkpoints (str): model id or path.\n        dtype (str, optional): Dtype used for the model {"auto", "half", "float16", "bfloat16", "float", "float32"}. Defaults to "auto".\n        kv_cache_dtype (str, optional): Dtype of the kv cache, defaults to the dtype. Defaults to "auto".\n        tp_size (int, optional): The number of GPUs the model weights get sharded over. Mainly for saving memory rather than for high throughput. Defaults to 1.\n        quantization (str, optional): Quantization format {"awq","fp8","gptq","marlin","gptq_marlin","awq_marlin","bitsandbytes","gguf","modelopt","w8a8_int8"}. Defaults to None.\n        load_format (str, optional): The format of the model weights to load:\\n* `auto`: will try to load the weights in the safetensors format and fall back to the pytorch bin format if safetensors format is not available.\\n* `pt`: will load the weights in the pytorch bin format. \\n* `safetensors`: will load the weights in the safetensors format. \\n* `npcache`: will load the weights in pytorch format and store a numpy cache to speed up the loading. \\n* `dummy`: will initialize the weights with random values, which is mainly for profiling.\\n* `gguf`: will load the weights in the gguf format. \\n* `bitsandbytes`: will load the weights using bitsandbytes quantization."\\n* `layered`: loads weights layer by layer so that one can quantize a layer before loading another to make the peak memory envelope smaller.\\n. Defaults to "auto".\\n\n        context_length (str, optional): The model\'s maximum context length. Defaults to None (will use the value from the model\'s config.json instead). Defaults to None.\n        device (str, optional): The device type {"cuda", "xpu", "hpu", "cpu"}. Defaults to "cuda".\n        port (int, optional): Port number. Defaults to 23333.\n        host (str, optional): Host name. Defaults to "0.0.0.0".\n        chat_template (str, optional): The buliltin chat template name or the path of the chat template file. This is only used for OpenAI-compatible API server.. Defaults to None.\n        mem_fraction_static (float, optional): The fraction of the memory used for static allocation (model weights and KV cache memory pool). Use a smaller value if you see out-of-memory errors. Defaults to 0.8.\n        max_running_requests (int, optional): The maximum number of running requests.. Defaults to None.\n        max_total_tokens (int, optional): The maximum number of tokens in the memory pool. If not specified, it will be automatically calculated based on the memory usage fraction. This option is typically used for development and debugging purposes.. Defaults to None.\n        max_prefill_tokens (int, optional): The maximum number of tokens in a prefill batch. The real bound will be the maximum of this value and the model\'s maximum context length. Defaults to None.\n        schedule_policy (str, optional): The scheduling policy of the requests {"lpm", "random", "fcfs", "dfs-weight"}. Defaults to "fcfs".\n        schedule_conservativeness (float, optional): How conservative the schedule policy is. A larger value means more conservative scheduling. Use a larger value if you see requests being retracted frequently. Defaults to 1.0.\n        cpu_offload_gb (int, optional): How many GBs of RAM to reserve for CPU offloading. Defaults to 0.\n        additional_list_args (List[str], optional): additional args to run subprocess cmd e.g. ["--arg-name", "arg value"]. See more at [github](https://github.com/sgl-project/sglang/blob/1baa9e6cf90b30aaa7dae51c01baa25229e8f7d5/python/sglang/srt/server_args.py#L298). Defaults to [].\n\n    Returns:\n        _type_: _description_\n    """\n\n    from sglang.utils import execute_shell_command, wait_for_server\n\n    cmds = [\n        PYTHON_EXEC, \'-m\', \'sglang.launch_server\', \'--model-path\', checkpoints, \'--dtype\',\n        str(dtype), \'--device\',\n        str(device), \'--kv-cache-dtype\',\n        str(kv_cache_dtype), \'--tp-size\',\n        str(tp_size), \'--load-format\',\n        str(load_format), \'--mem-fraction-static\',\n        str(mem_fraction_static), \'--schedule-policy\',\n        str(schedule_policy), \'--schedule-conservativeness\',\n        str(schedule_conservativeness), \'--port\',\n        str(port), \'--host\',\n        host, "--trust-remote-code"\n    ]\n    if chat_template:\n      cmds += ["--chat-template", chat_template]\n    if quantization:\n      cmds += [\n          \'--quantization\',\n          quantization,\n      ]\n    if context_length:\n      cmds += [\n          \'--context-length\',\n          context_length,\n      ]\n    if max_running_requests:\n      cmds += [\n          \'--max-running-requests\',\n          max_running_requests,\n      ]\n    if max_total_tokens:\n      cmds += [\n          \'--max-total-tokens\',\n          max_total_tokens,\n      ]\n    if max_prefill_tokens:\n      cmds += [\n          \'--max-prefill-tokens\',\n          max_prefill_tokens,\n      ]\n\n    if additional_list_args:\n      cmds += additional_list_args\n\n    print("CMDS to run `sglang` server: ", " ".join(cmds), "\\n")\n    _self = cls()\n\n    _self.host = host\n    _self.port = port\n    _self.backend = "sglang"\n    # _self.start_server_thread(cmds)\n    # new_path = os.environ["PATH"] + ":/sbin"\n    # _self.process = subprocess.Popen(cmds, text=True, stderr=subprocess.STDOUT, env={**os.environ, "PATH": new_path})\n    _self.process = execute_shell_command(" ".join(cmds))\n\n    logger.info("Waiting for " + f"http://{_self.host}:{_self.port}")\n    wait_for_server(f"http://{_self.host}:{_self.port}")\n    logger.info("Done")\n\n    return _self\n',T="torch==2.6.0\ntokenizers==0.21.1\naccelerate==1.2.0\noptimum==1.23.3\nxformers\neinops==0.8.0\npackaging\nninja\n\nqwen-vl-utils==0.0.8\ntimm==1.0.12\nopenai\nclarifai>=11.5.0,<12.0.0\npsutil\n--extra-index-url https://flashinfer.ai/whl/cu124/torch2.4/\nflashinfer\nsglang[all]==0.4.6\ntransformers==4.51.1",A='# Config file for the Sglang runner\n\nmodel:\n  id: "SmolLM2-1_7B-Instruct"\n  user_id: "USER_ID"\n  app_id: "APP_ID"\n  model_type_id: "text-to-text"\n\nbuild_info:\n  python_version: "3.11"\n\ninference_compute_info:\n  cpu_limit: "1"\n  cpu_memory: "6Gi"\n  num_accelerators: 1\n  accelerator_type: ["NVIDIA-*"]\n  accelerator_memory: "44Gi"\n\ncheckpoints:\n  type: "huggingface"\n  repo_id: "HuggingFaceTB/SmolLM2-1.7B-Instruct"\n  hf_token: "hf_token"',j='from typing import List, Iterator\nfrom threading import Thread\nimport os\nimport torch\n\nfrom clarifai.runners.models.model_class import ModelClass\nfrom clarifai.utils.logging import logger\nfrom clarifai.runners.models.model_builder import ModelBuilder\nfrom clarifai.runners.utils.openai_convertor import openai_response\nfrom clarifai.runners.utils.data_utils import Param\nfrom transformers import (AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer)\n\n\nclass MyModel(ModelClass):\n  """A custom runner for llama-3.2-1b-instruct llm that integrates with the Clarifai platform"""\n\n  def load_model(self):\n    """Load the model here."""\n    self.device = \'cuda\' if torch.cuda.is_available() else \'cpu\'\n    logger.info(f"Running on device: {self.device}")\n\n    # Load checkpoints\n    model_path = os.path.dirname(os.path.dirname(__file__))\n    builder = ModelBuilder(model_path, download_validation_only=True)\n    self.checkpoints = builder.download_checkpoints(stage="runtime")\n    \n    # Load model and tokenizer\n    self.tokenizer = AutoTokenizer.from_pretrained(self.checkpoints,)\n    self.tokenizer.pad_token = self.tokenizer.eos_token  # Set pad token to eos token\n    self.model = AutoModelForCausalLM.from_pretrained(\n        self.checkpoints,\n        low_cpu_mem_usage=True,\n        device_map=self.device,\n        torch_dtype=torch.bfloat16,\n    )\n    self.streamer = TextIteratorStreamer(tokenizer=self.tokenizer, skip_prompt=True, skip_special_tokens=True)\n    self.chat_template = None\n    logger.info("Done loading!")\n\n  @ModelClass.method\n  def predict(self,\n              prompt: str ="",\n              chat_history: List[dict] = None,\n              max_tokens: int = Param(default=512, description="The maximum number of tokens to generate. Shorter token lengths will provide faster performance.", ),\n              temperature: float = Param(default=0.7, description="A decimal number that determines the degree of randomness in the response", ),\n              top_p: float = Param(default=0.8, description="An alternative to sampling with temperature, where the model considers the results of the tokens with top_p probability mass.", )) -> str:\n    """\n    Predict the response for the given prompt and chat history using the model.\n    """\n    # Construct chat-style messages\n    messages = chat_history if chat_history else []\n    if prompt:\n        messages.append({\n            "role": "user",\n            "content": [{"type": "text", "text": prompt}]\n        })\n\n    inputs = self.tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors="pt").to(self.model.device)\n\n    generation_kwargs = {\n        "input_ids": inputs,\n        "do_sample": True,\n        "max_new_tokens": max_tokens,\n        "temperature": temperature,\n        "top_p": top_p,\n        "eos_token_id": self.tokenizer.eos_token_id,\n    }\n\n    output = self.model.generate(**generation_kwargs)\n    generated_tokens = output[0][inputs.shape[-1]:]\n    return self.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n\n  @ModelClass.method\n  def generate(self,\n              prompt: str="",\n              chat_history: List[dict] = None,\n              max_tokens: int = Param(default=512, description="The maximum number of tokens to generate. Shorter token lengths will provide faster performance.", ),\n              temperature: float = Param(default=0.7, description="A decimal number that determines the degree of randomness in the response", ),\n              top_p: float = Param(default=0.8, description="An alternative to sampling with temperature, where the model considers the results of the tokens with top_p probability mass.", )) -> Iterator[str]:\n      """Stream generated text tokens from a prompt + optional chat history."""\n\n\n      # Construct chat-style messages\n      messages = chat_history if chat_history else []\n      if prompt:\n          messages.append({\n              "role": "user",\n              "content": [{"type": "text", "text": prompt}]\n          })\n      \n      response = self.chat(\n          messages=messages,\n          max_tokens=max_tokens,\n          temperature=temperature,\n          top_p=top_p\n      )\n      \n      for each in response:\n          if \'choices\' in each and \'delta\' in each[\'choices\'][0] and \'content\' in each[\'choices\'][0][\'delta\']:\n                  yield each[\'choices\'][0][\'delta\'][\'content\']\n\n\n  @ModelClass.method\n  def chat(self,\n          messages: List[dict],\n          max_tokens: int = Param(default=512, description="The maximum number of tokens to generate. Shorter token lengths will provide faster performance.", ),\n          temperature: float = Param(default=0.7, description="A decimal number that determines the degree of randomness in the response", ),\n          top_p: float = Param(default=0.8, description="An alternative to sampling with temperature, where the model considers the results of the tokens with top_p probability mass.", )\n          ) -> Iterator[dict]:\n      """\n      Stream back JSON dicts for assistant messages.\n      Example return format:\n      {"role": "assistant", "content": [{"type": "text", "text": "response here"}]}\n      """\n      # Tokenize using chat template\n      inputs = self.tokenizer.apply_chat_template(\n          messages,\n          tokenize=True,\n          add_generation_prompt=True,\n          return_tensors="pt"\n      ).to(self.model.device)\n\n      generation_kwargs = {\n          "input_ids": inputs,\n          "do_sample": True,\n          "max_new_tokens": max_tokens,\n          "temperature": temperature,\n          "top_p": top_p,\n          "eos_token_id": self.tokenizer.eos_token_id,\n          "streamer": self.streamer\n      }\n\n      thread = Thread(target=self.model.generate, kwargs=generation_kwargs)\n      thread.start()\n\n      # Accumulate response text\n      for chunk in openai_response(self.streamer):\n          yield chunk\n\n      thread.join()\n\n\n  def test(self):\n    """Test the model here."""\n    try:\n      print("Testing predict...")\n      # Test predict\n      print(self.predict(prompt="What is the capital of India?",))\n    except Exception as e:\n      print("Error in predict", e)\n\n    try:\n      print("Testing generate...")\n      # Test generate\n      for each in self.generate(prompt="What is the capital of India?",):\n        print(each, end="")\n      print()\n    except Exception as e:\n      print("Error in generate", e)\n\n    try:\n      print("Testing chat...")\n      messages = [\n        {"role": "system", "content": "You are an helpful assistant."},\n        {"role": "user", "content": "What is the capital of India?"},\n      ]\n      for each in self.chat(messages=messages,):\n        print(each, end="")\n      print()\n    except Exception as e:\n      print("Error in generate", e)',L="torch==2.5.1\ntokenizers>=0.21.0\ntransformers>=4.47.0\naccelerate>=1.2.0\nscipy==1.10.1\noptimum>=1.23.3\nprotobuf==5.27.3\neinops>=0.8.0\nrequests==2.32.3\nclarifai>=11.4.1",S='model:\n  id: "Llama-3_2-1B-Instruct"\n  user_id: "user_id"\n  app_id: "app_id"\n  model_type_id: "text-to-text"\n\nbuild_info:\n  python_version: "3.11"\n\ninference_compute_info:\n  cpu_limit: "1"\n  cpu_memory: "13Gi"\n  num_accelerators: 1\n  accelerator_type: ["NVIDIA-*"]\n  accelerator_memory: "44Gi"\n\ncheckpoints:\n  type: "huggingface"\n  repo_id: "unsloth/Llama-3.2-1B-Instruct"\n  hf_token: "hf_token"\n  when: "runtime"',I='import os\nimport sys\n\nfrom typing import Iterator, List\nimport json\n\nfrom clarifai.runners.models.model_builder import ModelBuilder\nfrom clarifai.runners.models.openai_class import OpenAIModelClass\nfrom clarifai.utils.logging import logger\nfrom clarifai.runners.utils.data_utils import Param\nfrom clarifai.runners.utils.openai_convertor import build_openai_messages\n\nfrom openai import OpenAI\n\n\nPYTHON_EXEC = sys.executable\n\ndef lmdeploy_openai_server(checkpoints, **kwargs):\n    """Start lmdeploy OpenAI compatible server."""\n    \n    from clarifai.runners.utils.model_utils import execute_shell_command, wait_for_server, terminate_process\n    # Start building the command\n    cmds = [\n        PYTHON_EXEC, \'-m\', \'lmdeploy\', \'serve\', \'api_server\', checkpoints,\n    ]\n    # Add all parameters from kwargs to the command\n    for key, value in kwargs.items():\n        if value is None:  # Skip None values\n            continue\n        param_name = key.replace(\'_\', \'-\')\n        if isinstance(value, bool):\n            if value:  # Only add the flag if True\n                cmds.append(f\'--{param_name}\')\n        else:\n            cmds.extend([f\'--{param_name}\', str(value)])\n    # Create server instance\n    server = type(\'Server\', (), {\n        \'host\': kwargs.get(\'server_name\', \'0.0.0.0\'),\n        \'port\': kwargs.get(\'server_port\', 23333),\n        \'backend\': "lmdeploy",\n        \'process\': None\n    })()\n    \n    try:\n        server.process = execute_shell_command(" ".join(cmds))\n        logger.info("Waiting for " + f"http://{server.host}:{server.port}")\n        wait_for_server(f"http://{server.host}:{server.port}")\n        logger.info("Server started successfully at " + f"http://{server.host}:{server.port}")\n    except Exception as e:\n        logger.error(f"Failed to start lmdeploy server: {str(e)}")\n        if server.process:\n            terminate_process(server.process)\n        raise RuntimeError(f"Failed to start lmdeploy server: {str(e)}")\n\n    return server\n\n\nclass LMDeployModel(OpenAIModelClass):\n    """\n    A custom runner that integrates with the Clarifai platform and uses lmdeploy framework for inference\n    """\n    client = True\n    model = True\n    def load_model(self):\n        """Load the model here and start the  server."""\n\n        # server args were generated by `upload` module\n        server_args = {\'backend\': \'turbomind\', \'cache_max_entry_count\': 0.95, \n                      \'tp\': 1, \'max_prefill_token_num\': 8192, \'dtype\': \'auto\', \n                      \'model_format\': None, \'quant_policy\': 0, \'chat_template\': \'llama3_2\', \'max_batch_size\': 16, \n                      \'device\': \'cuda\', \'server_name\': \'0.0.0.0\', \'server_port\': 23333, \n                      \'tool_call_parser\': \'llama3\',\n                      }\n        model_path = os.path.dirname(os.path.dirname(__file__))\n        builder = ModelBuilder(model_path, download_validation_only=True)\n        model_config = builder.config\n        \n        stage = model_config["checkpoints"][\'when\']\n        checkpoints = builder.config["checkpoints"][\'repo_id\']\n        if stage in ["build", "runtime"]:\n          checkpoints = builder.download_checkpoints(stage=stage)\n\n        # Start server\n        self.server = lmdeploy_openai_server(checkpoints, **server_args)\n        # Create client\n        self.client = OpenAI(\n                api_key="notset",\n                base_url= f"http://{self.server.host}:{self.server.port}/v1")\n        self.model = self.client.models.list().data[0].id\n    \n  \n    @OpenAIModelClass.method\n    def predict(self,\n                prompt: str,\n                chat_history: List[dict] = None,\n                tools: List[dict] = None,\n                tool_choice: str = None,\n                max_tokens: int = Param(default=512, description="The maximum number of tokens to generate. Shorter token lengths will provide faster performance.", ),\n                temperature: float = Param(default=0.7, description="A decimal number that determines the degree of randomness in the response", ),\n                top_p: float = Param(default=0.95, description="An alternative to sampling with temperature, where the model considers the results of the tokens with top_p probability mass."), \n                ) -> str:\n      """\n      This method is used to predict the response for the given prompt and chat history using the model and tools.\n      """\n\n      if tools is not None and tool_choice is None:\n          tool_choice = "auto"\n              \n      messages = build_openai_messages(prompt=prompt, messages=chat_history)\n      response = self.client.chat.completions.create(\n          model=self.model,\n          messages=messages,\n          tools=tools,\n          tool_choice=tool_choice,\n          max_completion_tokens=max_tokens,\n          temperature=temperature,\n          top_p=top_p)\n        \n      if response.choices[0] and response.choices[0].message.tool_calls:\n        # If the response contains tool calls, return as a string\n        tool_calls = response.choices[0].message.tool_calls\n        tool_calls_json = json.dumps([tc.to_dict() for tc in tool_calls], indent=2)\n        return tool_calls_json\n      else:\n        # Otherwise, return the content of the first choice\n        return response.choices[0].message.content\n      \n\n    @OpenAIModelClass.method\n    def generate(self,\n                prompt: str,\n                chat_history: List[dict] = None,\n                tools: List[dict] = None,\n                tool_choice: str = None,\n                max_tokens: int = Param(default=512, description="The maximum number of tokens to generate. Shorter token lengths will provide faster performance.", ),\n                temperature: float = Param(default=0.7, description="A decimal number that determines the degree of randomness in the response", ),\n                top_p: float = Param(default=0.95, description="An alternative to sampling with temperature, where the model considers the results of the tokens with top_p probability mass.")) -> Iterator[str]:\n      """\n      This method is used to stream generated text tokens from a prompt + optional chat history and tools.\n      """\n      messages = build_openai_messages(prompt=prompt, messages=chat_history)\n      response = self.client.chat.completions.create(\n          model=self.model,\n          messages=messages,\n          tools=tools,\n          tool_choice=tool_choice,\n          max_completion_tokens=max_tokens,\n          temperature=temperature,\n          top_p=top_p,\n          stream=True)\n      \n      for chunk in response:\n        if chunk.choices:\n          if chunk.choices[0].delta.tool_calls:\n            # If the response contains tool calls, return the first one as a string\n            tool_calls = chunk.choices[0].delta.tool_calls\n            tool_calls_json = [tc.to_dict() for tc in tool_calls]\n            # Convert to JSON string\n            json_string = json.dumps(tool_calls_json, indent=2)\n            # Yield the JSON string\n            yield json_string\n          else:\n            # Otherwise, return the content of the first choice\n            text = (chunk.choices[0].delta.content\n                    if (chunk and chunk.choices[0].delta.content) is not None else \'\')\n            yield text\n\n    # This method is needed to test the model with the test-locally CLI command.\n    def test(self):\n        """Test the model here."""\n        try:\n            print("Testing predict...")\n            # Test predict\n            print(self.predict(prompt="Hello, how are you?",))\n        except Exception as e:\n            print("Error in predict", e)\n\n        try:\n            print("Testing generate...")\n            # Test generate\n            for each in self.generate(prompt="Hello, how are you?",):\n                print(each, end=" ")\n        except Exception as e:\n            print("Error in generate", e)',E="tokenizers>=0.21.0\naccelerate>=1.2.0\noptimum>=1.23.3\neinops>=0.8.0\npackaging\nninja\n\ntimm\nopenai\nclarifai\nclarifai-protocol\npsutil\n\ntorch==2.6.0\nlmdeploy==0.8.0\ntransformers>=4.51.2\npartial-json-parser",P="model:\n  id: Llama-3_2-3B-Instruct\n  user_id: \n  app_id: \n  model_type_id: text-to-text\nbuild_info:\n  python_version: '3.11'\ninference_compute_info:\n  cpu_limit: '1'\n  cpu_memory: 6Gi\n  num_accelerators: 1\n  accelerator_type:\n  - NVIDIA-*\n  accelerator_memory: 20Gi\nnum_threads: 64\ncheckpoints:\n  type: huggingface\n  repo_id: meta-llama/Llama-3.2-3B-Instruct\n  hf_token: \n  when: runtime",C='import os\nimport sys\n\nfrom typing import List, Iterator\n\nfrom clarifai.runners.models.model_builder import ModelBuilder\nfrom clarifai.runners.models.openai_class import OpenAIModelClass\nfrom openai import OpenAI\nfrom clarifai.runners.utils.openai_convertor import build_openai_messages\nfrom clarifai.runners.utils.data_utils import Param\nfrom clarifai.utils.logging import logger\n\nPYTHON_EXEC = sys.executable\n\ndef vllm_openai_server(checkpoints, **kwargs):\n    """Start vLLM OpenAI compatible server."""\n    \n    from clarifai.runners.utils.model_utils import execute_shell_command, wait_for_server, terminate_process\n    # Start building the command\n    cmds = [\n        PYTHON_EXEC, \'-m\', \'vllm.entrypoints.openai.api_server\', \'--model\', checkpoints,\n    ]\n    # Add all parameters from kwargs to the command\n    for key, value in kwargs.items():\n        if value is None:  # Skip None values\n            continue\n        param_name = key.replace(\'_\', \'-\')\n        if isinstance(value, bool):\n            if value:  # Only add the flag if True\n                cmds.append(f\'--{param_name}\')\n        else:\n            cmds.extend([f\'--{param_name}\', str(value)])\n    # Create server instance\n    server = type(\'Server\', (), {\n        \'host\': kwargs.get(\'host\', \'0.0.0.0\'),\n        \'port\': kwargs.get(\'port\', 23333),\n        \'backend\': "vllm",\n        \'process\': None\n    })()\n    \n    try:\n        server.process = execute_shell_command(" ".join(cmds))\n        logger.info("Waiting for " + f"http://{server.host}:{server.port}")\n        wait_for_server(f"http://{server.host}:{server.port}")\n        logger.info("Server started successfully at " + f"http://{server.host}:{server.port}")\n    except Exception as e:\n        logger.error(f"Failed to start vllm server: {str(e)}")\n        if server.process:\n            terminate_process(server.process)\n        raise RuntimeError(f"Failed to start vllm server: {str(e)}")\n\n    return server\n\nclass VLLMLlamaModel(OpenAIModelClass):\n  """\n  A Model that integrates with the Clarifai platform and uses vLLM framework for inference to run the Llama 3.1 8B model with tool calling capabilities.\n  """\n  client = True  # This will be set in load_model method\n  model = True  # This will be set in load_model method\n\n  def load_model(self):\n    """Load the model here and start the  server."""\n    os.path.join(os.path.dirname(__file__))\n    # This is the path to the chat template file and you can get this chat template from vLLM repo(https://github.com/vllm-project/vllm/blob/main/examples/tool_chat_template_llama3.1_json.jinja)\n\n    server_args = {\n        \'max_model_len\': 2048,\n        #\'gpu_memory_utilization\': 0.8,\n        \'dtype\': \'auto\',\n        \'task\': \'auto\',\n        \'kv_cache_dtype\': \'auto\',\n        \'tensor_parallel_size\': 1,\n        \'quantization\': None,\n        \'cpu_offload_gb\': 5.0,\n        \'chat_template\': None,\n        \'port\': 23333,\n        \n        \'host\': \'localhost\',\n    }\n\n    model_path = os.path.dirname(os.path.dirname(__file__))\n    builder = ModelBuilder(model_path, download_validation_only=True)\n    model_config = builder.config\n    \n    stage = model_config["checkpoints"][\'when\']\n    checkpoints = builder.config["checkpoints"][\'repo_id\']\n    if stage in ["build", "runtime"]:\n      checkpoints = builder.download_checkpoints(stage=stage)\n\n    # Start server\n    self.server = vllm_openai_server(checkpoints, **server_args)\n    # CLIent initialization\n    self.client = OpenAI(\n            api_key="notset",\n            base_url=f\'http://{self.server.host}:{self.server.port}/v1\')\n    self.model = self.client.models.list().data[0].id\n\n  @OpenAIModelClass.method\n  def predict(self,\n              prompt: str,\n              chat_history: List[dict] = None,\n              tools: List[dict] = None,\n              tool_choice: str = None,\n              max_tokens: int = Param(default=512, description="The maximum number of tokens to generate. Shorter token lengths will provide faster performance.", ),\n              temperature: float = Param(default=0.7, description="A decimal number that determines the degree of randomness in the response", ),\n              top_p: float = Param(default=0.95, description="An alternative to sampling with temperature, where the model considers the results of the tokens with top_p probability mass."), \n              ) -> str:\n    """\n    This method is used to predict the response for the given prompt and chat history using the model and tools.\n    """\n    if tools is not None and tool_choice is None:\n        tool_choice = "auto"\n            \n    messages = build_openai_messages(prompt=prompt, messages=chat_history)\n    response = self.client.chat.completions.create(\n        model=self.model,\n        messages=messages,\n        tools=tools,\n        tool_choice=tool_choice,\n        max_completion_tokens=max_tokens,\n        temperature=temperature,\n        top_p=top_p)\n      \n    if response.choices[0] and response.choices[0].message.tool_calls:\n      import json\n      # If the response contains tool calls, return as a string\n\n      tool_calls = response.choices[0].message.tool_calls\n      tool_calls_json = json.dumps([tc.to_dict() for tc in tool_calls], indent=2)\n      return tool_calls_json\n    else:\n      # Otherwise, return the content of the first choice\n      return response.choices[0].message.content\n    \n\n  @OpenAIModelClass.method\n  def generate(self,\n               prompt: str,\n               chat_history: List[dict] = None,\n               tools: List[dict] = None,\n               tool_choice: str = None,\n               max_tokens: int = Param(default=512, description="The maximum number of tokens to generate. Shorter token lengths will provide faster performance.", ),\n               temperature: float = Param(default=0.7, description="A decimal number that determines the degree of randomness in the response", ),\n               top_p: float = Param(default=0.95, description="An alternative to sampling with temperature, where the model considers the results of the tokens with top_p probability mass.")) -> Iterator[str]:\n    """\n    This method is used to stream generated text tokens from a prompt + optional chat history and tools.\n    """\n    messages = build_openai_messages(prompt=prompt, messages=chat_history)\n    response = self.client.chat.completions.create(\n        model=self.model,\n        messages=messages,\n        tools=tools,\n        tool_choice=tool_choice,\n        max_completion_tokens=max_tokens,\n        temperature=temperature,\n        top_p=top_p,\n        stream=True)\n    \n    for chunk in response:\n      if chunk.choices:\n        if chunk.choices[0].delta.tool_calls:\n          # If the response contains tool calls, return the first one as a string\n          import json\n          tool_calls = chunk.choices[0].delta.tool_calls\n          tool_calls_json = [tc.to_dict() for tc in tool_calls]\n          # Convert to JSON string\n          json_string = json.dumps(tool_calls_json, indent=2)\n          # Yield the JSON string\n          yield json_string\n        else:\n          # Otherwise, return the content of the first choice\n          text = (chunk.choices[0].delta.content\n                  if (chunk and chunk.choices[0].delta.content) is not None else \'\')\n          yield text',N="tokenizers==0.21.0\naccelerate==1.2.0\noptimum==1.23.3\n# xformers\neinops==0.8.0\npackaging\nninja\n\nqwen-vl-utils==0.0.8\ntimm\nopenai\nclarifai\npsutil\n\ntorch==2.6.0\nvllm==0.8.0\ntransformers==4.50.1\nbackoff==2.2.1\npeft>=0.13.2\nsoundfile>=0.13.1\nscipy==1.15.2\nlibrosa\ndecord",M="build_info:\n  python_version: '3.12'\ncheckpoints:\n  hf_token: \"<your_hf_token>\"\n  repo_id: google/gemma-3-1b-it\n  type: huggingface\n  when: runtime\ninference_compute_info:\n  accelerator_memory: 5Gi\n  accelerator_type:\n  - NVIDIA-*\n  cpu_limit: '1'\n  cpu_memory: 5Gi\n  num_accelerators: 1\nmodel:\n  app_id: APP_ID\n  id: MODEL_ID\n  model_type_id: text-to-text\n  user_id: USER_ID",D='import os\nimport sys\n\nfrom typing import List, Iterator\n\nfrom clarifai.runners.models.model_builder import ModelBuilder\nfrom clarifai.runners.models.openai_class import OpenAIModelClass\nfrom openai import OpenAI\nfrom clarifai.runners.utils.openai_convertor import build_openai_messages\nfrom clarifai.runners.utils.data_utils import Param\nfrom clarifai.utils.logging import logger\n\nPYTHON_EXEC = sys.executable\n\ndef vllm_openai_server(checkpoints, **kwargs):\n    """Start vLLM OpenAI compatible server."""\n    \n    from clarifai.runners.utils.model_utils import execute_shell_command, wait_for_server, terminate_process\n    # Start building the command\n    cmds = [\n        PYTHON_EXEC, \'-m\', \'vllm.entrypoints.openai.api_server\', \'--model\', checkpoints,\n    ]\n    # Add all parameters from kwargs to the command\n    for key, value in kwargs.items():\n        if value is None:  # Skip None values\n            continue\n        param_name = key.replace(\'_\', \'-\')\n        if isinstance(value, bool):\n            if value:  # Only add the flag if True\n                cmds.append(f\'--{param_name}\')\n        else:\n            cmds.extend([f\'--{param_name}\', str(value)])\n    # Create server instance\n    server = type(\'Server\', (), {\n        \'host\': kwargs.get(\'host\', \'0.0.0.0\'),\n        \'port\': kwargs.get(\'port\', 23333),\n        \'backend\': "vllm",\n        \'process\': None\n    })()\n    \n    try:\n        server.process = execute_shell_command(" ".join(cmds))\n        logger.info("Waiting for " + f"http://{server.host}:{server.port}")\n        wait_for_server(f"http://{server.host}:{server.port}")\n        logger.info("Server started successfully at " + f"http://{server.host}:{server.port}")\n    except Exception as e:\n        logger.error(f"Failed to start vllm server: {str(e)}")\n        if server.process:\n            terminate_process(server.process)\n        raise RuntimeError(f"Failed to start vllm server: {str(e)}")\n\n    return server\n\nclass VLLMLlamaModel(OpenAIModelClass):\n  """\n  A Model that integrates with the Clarifai platform and uses vLLM framework for inference to run the Llama 3.1 8B model with tool calling capabilities.\n  """\n  client = True  # This will be set in load_model method\n  model = True  # This will be set in load_model method\n\n  def load_model(self):\n    """Load the model here and start the  server."""\n    os.path.join(os.path.dirname(__file__))\n    # This is the path to the chat template file and you can get this chat template from vLLM repo(https://github.com/vllm-project/vllm/blob/main/examples/tool_chat_template_llama3.1_json.jinja)\n    chat_template = \'examples/tool_chat_template_llama3.1_json.jinja\'\n\n    server_args = {\n        \'max_model_len\': 2048,\n        \'gpu_memory_utilization\': 0.8,\n        \'dtype\': \'auto\',\n        \'task\': \'auto\',\n        \'kv_cache_dtype\': \'auto\',\n        \'tensor_parallel_size\': 1,\n        \'quantization\': None,\n        \'chat_template\': chat_template,\n        \'cpu_offload_gb\': 0.0,\n        \'port\': 23333,\n        \'host\': \'localhost\',\n        "enable_auto_tool_choice": True,\n        \'tool_call_parser\': "llama3_json",\n    }\n\n    model_path = os.path.dirname(os.path.dirname(__file__))\n    builder = ModelBuilder(model_path, download_validation_only=True)\n    model_config = builder.config\n    \n    stage = model_config["checkpoints"][\'when\']\n    checkpoints = builder.config["checkpoints"][\'repo_id\']\n    if stage in ["build", "runtime"]:\n      checkpoints = builder.download_checkpoints(stage=stage)\n\n    # Start server\n    self.server = vllm_openai_server(checkpoints, **server_args)\n    # CLIent initialization\n    self.client = OpenAI(\n            api_key="notset",\n            base_url=f\'http://{self.server.host}:{self.server.port}/v1\')\n    self.model = self.client.models.list().data[0].id\n\n  @OpenAIModelClass.method\n  def predict(self,\n              prompt: str,\n              chat_history: List[dict] = None,\n              tools: List[dict] = None,\n              tool_choice: str = None,\n              max_tokens: int = Param(default=512, description="The maximum number of tokens to generate. Shorter token lengths will provide faster performance.", ),\n              temperature: float = Param(default=0.7, description="A decimal number that determines the degree of randomness in the response", ),\n              top_p: float = Param(default=0.95, description="An alternative to sampling with temperature, where the model considers the results of the tokens with top_p probability mass."), \n              ) -> str:\n    """\n    This method is used to predict the response for the given prompt and chat history using the model and tools.\n    """\n    if tools is not None and tool_choice is None:\n        tool_choice = "auto"\n            \n    messages = build_openai_messages(prompt=prompt, messages=chat_history)\n    response = self.client.chat.completions.create(\n        model=self.model,\n        messages=messages,\n        tools=tools,\n        tool_choice=tool_choice,\n        max_completion_tokens=max_tokens,\n        temperature=temperature,\n        top_p=top_p)\n      \n    if response.choices[0] and response.choices[0].message.tool_calls:\n      import json\n      # If the response contains tool calls, return as a string\n\n      tool_calls = response.choices[0].message.tool_calls\n      tool_calls_json = json.dumps([tc.to_dict() for tc in tool_calls], indent=2)\n      return tool_calls_json\n    else:\n      # Otherwise, return the content of the first choice\n      return response.choices[0].message.content\n    \n\n  @OpenAIModelClass.method\n  def generate(self,\n               prompt: str,\n               chat_history: List[dict] = None,\n               tools: List[dict] = None,\n               tool_choice: str = None,\n               max_tokens: int = Param(default=512, description="The maximum number of tokens to generate. Shorter token lengths will provide faster performance.", ),\n               temperature: float = Param(default=0.7, description="A decimal number that determines the degree of randomness in the response", ),\n               top_p: float = Param(default=0.95, description="An alternative to sampling with temperature, where the model considers the results of the tokens with top_p probability mass.")) -> Iterator[str]:\n    """\n    This method is used to stream generated text tokens from a prompt + optional chat history and tools.\n    """\n    messages = build_openai_messages(prompt=prompt, messages=chat_history)\n    response = self.client.chat.completions.create(\n        model=self.model,\n        messages=messages,\n        tools=tools,\n        tool_choice=tool_choice,\n        max_completion_tokens=max_tokens,\n        temperature=temperature,\n        top_p=top_p,\n        stream=True)\n    \n    for chunk in response:\n      if chunk.choices:\n        if chunk.choices[0].delta.tool_calls:\n          # If the response contains tool calls, return the first one as a string\n          import json\n          tool_calls = chunk.choices[0].delta.tool_calls\n          tool_calls_json = [tc.to_dict() for tc in tool_calls]\n          # Convert to JSON string\n          json_string = json.dumps(tool_calls_json, indent=2)\n          # Yield the JSON string\n          yield json_string\n        else:\n          # Otherwise, return the content of the first choice\n          text = (chunk.choices[0].delta.content\n                  if (chunk and chunk.choices[0].delta.content) is not None else \'\')\n          yield text',F="tokenizers==0.21.0\naccelerate==1.2.0\noptimum==1.23.3\n# xformers\neinops==0.8.0\npackaging\nninja\n\nqwen-vl-utils==0.0.8\ntimm\nopenai\nclarifai\npsutil\n\ntorch==2.6.0\nvllm==0.8.0\ntransformers==4.50.1\nbackoff==2.2.1\npeft>=0.13.2\nsoundfile>=0.13.1\nscipy==1.15.2\nlibrosa\ndecord",q='model:\n  id: "llama-3_1-8B-instruct-tool-calling"\n  user_id: "user_id"\n  app_id: "app_id"\n  model_type_id: "text-to-text"\n\nbuild_info:\n  python_version: \'3.12\'\n\ninference_compute_info:\n  cpu_limit: \'1\'\n  cpu_memory: 12Gi\n  num_accelerators: 1\n  accelerator_type: ["NVIDIA-*"]\n  accelerator_memory: 44Gi\n\ncheckpoints:\n  type: huggingface\n  repo_id: meta-llama/Llama-3.1-8B-Instruct\n  hf_token: "hf_token"\n  when: runtime',O='import json\nimport os\nfrom typing import Iterator, List\n\nfrom clarifai.runners.models.openai_class import OpenAIModelClass\nfrom clarifai.runners.utils.data_types import Image\nfrom clarifai.runners.utils.data_utils import Param\nfrom clarifai.runners.utils.openai_convertor import build_openai_messages\nfrom clarifai.utils.logging import logger\nfrom openai import OpenAI\n\n# Set default host\nif not os.environ.get(\'OLLAMA_HOST\'):\n    os.environ["OLLAMA_HOST"] = \'127.0.0.1:23333\'\nOLLAMA_HOST = os.environ.get(\'OLLAMA_HOST\')\n\nif not os.environ.get(\'OLLAMA_CONTEXT_LENGTH\'):\n    # Set default context length if not set\n    os.environ["OLLAMA_CONTEXT_LENGTH"] = \'8192\'  # Default context length for Llama 3.2\nOLLAMA_CONTEXT_LENGTH = os.environ.get(\'OLLAMA_CONTEXT_LENGTH\')\n\n\ndef run_ollama_server(model_name: str = \'llama3.2\'):\n    """\n    start the Ollama server.\n    """\n    from clarifai.runners.utils.model_utils import execute_shell_command, terminate_process\n\n    try:\n        logger.info(f"Starting Ollama server in the host: {OLLAMA_HOST}")\n        start_process = execute_shell_command("ollama serve")\n        if start_process:\n            pull_model = execute_shell_command(f"ollama pull {model_name}")\n            logger.info(f"Model {model_name} pulled successfully.")\n            logger.info(f"Ollama server started successfully on {OLLAMA_HOST}")\n\n    except Exception as e:\n        logger.error(f"Error starting Ollama server: {e}")\n        if \'start_process\' in locals():\n            terminate_process(start_process)\n        raise RuntimeError(f"Failed to start Ollama server: {e}")\n\n\n# Check if Image has content before building messages\ndef has_image_content(image: Image) -> bool:\n    """Check if Image object has either bytes or URL."""\n    return bool(getattr(image, \'url\', None) or getattr(image, \'bytes\', None))\n\n\nclass OllamaModelClass(OpenAIModelClass):\n    client = True\n    model = True\n\n    def load_model(self):\n        """\n        Load the Ollama model.\n        """\n        # set the model name here or via OLLAMA_MODEL_NAME\n        self.model = os.environ.get("OLLAMA_MODEL_NAME", \'llama3.2\')  #\'devstral:latest\')\n\n        # start ollama server\n        run_ollama_server(model_name=self.model)\n\n        self.client = OpenAI(api_key="notset", base_url=f"http://{OLLAMA_HOST}/v1")\n\n        logger.info(f"Ollama model loaded successfully: {self.model}")\n\n    @OpenAIModelClass.method\n    def predict(\n        self,\n        prompt: str,\n        image: Image = None,\n        images: List[Image] = None,\n        chat_history: List[dict] = None,\n        tools: List[dict] = None,\n        tool_choice: str = None,\n        max_tokens: int = Param(\n            default=2048,\n            description="The maximum number of tokens to generate. Shorter token lengths will provide faster performance.",\n        ),\n        temperature: float = Param(\n            default=0.7,\n            description="A decimal number that determines the degree of randomness in the response",\n        ),\n        top_p: float = Param(\n            default=0.95,\n            description="An alternative to sampling with temperature, where the model considers the results of the tokens with top_p probability mass.",\n        ),\n    ) -> str:\n        """\n        This method is used to predict the response for the given prompt and chat history using the model and tools.\n        """\n        if tools is not None and tool_choice is None:\n            tool_choice = "auto"\n\n        img_content = image if has_image_content(image) else None\n\n        messages = build_openai_messages(\n            prompt=prompt, image=img_content, images=images, messages=chat_history\n        )\n        response = self.client.chat.completions.create(\n            model=self.model,\n            messages=messages,\n            tools=tools,\n            tool_choice=tool_choice,\n            max_completion_tokens=max_tokens,\n            temperature=temperature,\n            top_p=top_p,\n        )\n\n        if response.usage is not None:\n            self.set_output_context(\n                prompt_tokens=response.usage.prompt_tokens,\n                completion_tokens=response.usage.completion_tokens,\n            )\n            if len(response.choices) == 0:\n                # still need to send the usage back.\n                return ""\n\n        if response.choices[0] and response.choices[0].message.tool_calls:\n            # If the response contains tool calls, return as a string\n            tool_calls = response.choices[0].message.tool_calls\n            tool_calls_json = json.dumps([tc.to_dict() for tc in tool_calls], indent=2)\n            return tool_calls_json\n        else:\n            # Otherwise, return the content of the first choice\n            return response.choices[0].message.content\n\n    @OpenAIModelClass.method\n    def generate(\n        self,\n        prompt: str,\n        image: Image = None,\n        images: List[Image] = None,\n        chat_history: List[dict] = None,\n        tools: List[dict] = None,\n        tool_choice: str = None,\n        max_tokens: int = Param(\n            default=2048,\n            description="The maximum number of tokens to generate. Shorter token lengths will provide faster performance.",\n        ),\n        temperature: float = Param(\n            default=0.7,\n            description="A decimal number that determines the degree of randomness in the response",\n        ),\n        top_p: float = Param(\n            default=0.95,\n            description="An alternative to sampling with temperature, where the model considers the results of the tokens with top_p probability mass.",\n        ),\n    ) -> Iterator[str]:\n        """\n        This method is used to stream generated text tokens from a prompt + optional chat history and tools.\n        """\n        if tools is not None and tool_choice is None:\n            tool_choice = "auto"\n\n        img_content = image if has_image_content(image) else None\n\n        messages = build_openai_messages(\n            prompt=prompt, image=img_content, images=images, messages=chat_history\n        )\n        for chunk in self.client.chat.completions.create(\n            model=self.model,\n            messages=messages,\n            tools=tools,\n            tool_choice=tool_choice,\n            max_completion_tokens=max_tokens,\n            temperature=temperature,\n            top_p=top_p,\n            stream=True,\n            stream_options={"include_usage": True},\n        ):\n            if chunk.usage is not None:\n                if chunk.usage.prompt_tokens or chunk.usage.completion_tokens:\n                    self.set_output_context(\n                        prompt_tokens=chunk.usage.prompt_tokens,\n                        completion_tokens=chunk.usage.completion_tokens,\n                    )\n                if len(chunk.choices) == 0:  # still need to send the usage back.\n                    yield ""\n            if chunk.choices:\n                if chunk.choices[0].delta.tool_calls:\n                    # If the response contains tool calls, return the first one as a string\n                    import json\n\n                    tool_calls = chunk.choices[0].delta.tool_calls\n                    tool_calls_json = [tc.to_dict() for tc in tool_calls]\n                    # Convert to JSON string\n                    json_string = json.dumps(tool_calls_json, indent=2)\n                    # Yield the JSON string\n                    yield json_string\n                else:\n                    # Otherwise, return the content of the first choice\n                    text = (\n                        chunk.choices[0].delta.content\n                        if (chunk and chunk.choices[0].delta.content) is not None\n                        else \'\'\n                    )\n                    yield text',R="ollama\nclarifai\nopenai",z="model:\n  app_id: local-dev-runner-app\n  id: local-dev-model\n  model_type_id: text-to-text\n  user_id: clarifai-user-id\n\nbuild_info:\n  python_version: '3.12'\n\ninference_compute_info:\n  cpu_limit: '3'\n  cpu_memory: 14Gi\n  num_accelerators: 0",G='from typing import Annotated\nfrom urllib.parse import urljoin, urlparse\n\nfrom fastmcp import FastMCP  # use fastmcp v2 not the built in mcp\nfrom pydantic import Field\n\nserver = FastMCP(\n    "browser-tools-mcp-server",\n    instructions="Web browsing, scraping, and search tools for gathering information from the internet",\n    stateless_http=True,\n)\n\n\ndef make_http_request(\n    url: str, method: str = "GET", headers: dict = None, timeout: int = 30\n) -> tuple[bool, dict]:\n    """Make an HTTP request with proper error handling."""\n    import requests\n    from requests.adapters import HTTPAdapter\n    from urllib3.util.retry import Retry\n\n    try:\n        # Setup session with retries\n        session = requests.Session()\n        retry_strategy = Retry(\n            total=3,\n            backoff_factor=1,\n            status_forcelist=[429, 500, 502, 503, 504],\n        )\n        adapter = HTTPAdapter(max_retries=retry_strategy)\n        session.mount("http://", adapter)\n        session.mount("https://", adapter)\n\n        # Default headers\n        default_headers = {\n            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"\n        }\n        if headers:\n            default_headers.update(headers)\n\n        response = session.request(method, url, headers=default_headers, timeout=timeout)\n\n        return True, {\n            "status_code": response.status_code,\n            "headers": dict(response.headers),\n            "text": response.text,\n            "url": response.url,\n        }\n    except Exception as e:\n        return False, {"error": str(e)}\n\n\n@server.tool("fetch_webpage", description="Fetch and return the content of a webpage")\ndef fetch_webpage(\n    url: Annotated[str, Field(description="URL of the webpage to fetch")],\n    include_headers: Annotated[\n        bool, Field(description="Include HTTP headers in response")\n    ] = False,\n    max_length: Annotated[\n        int, Field(description="Maximum content length to return", ge=100, le=50000)\n    ] = 10000,\n) -> str:\n    """Fetch the content of a webpage and return the text."""\n    success, response = make_http_request(url)\n\n    if not success:\n        return f"Error fetching webpage: {response.get(\'error\', \'Unknown error\')}"\n\n    content = response.get(\'text\', \'\')\n\n    # Truncate if too long\n    if len(content) > max_length:\n        content = (\n            content[:max_length]\n            + f"\\n\\n... (content truncated, showing first {max_length} characters)"\n        )\n\n    result = f"URL: {response.get(\'url\', url)}\\n"\n    result += f"Status Code: {response.get(\'status_code\')}\\n\\n"\n\n    if include_headers:\n        headers = response.get(\'headers\', {})\n        result += "Headers:\\n"\n        for key, value in headers.items():\n            result += f"  {key}: {value}\\n"\n        result += "\\n"\n\n    result += "Content:\\n"\n    result += content\n\n    return result\n\n\n@server.tool("extract_text", description="Extract clean text from HTML content")\ndef extract_text(\n    url: Annotated[str, Field(description="URL of the webpage")],\n    remove_scripts: Annotated[bool, Field(description="Remove script and style tags")] = True,\n    max_length: Annotated[\n        int, Field(description="Maximum text length to return", ge=100, le=50000)\n    ] = 5000,\n) -> str:\n    """Extract clean text from a webpage by removing HTML tags."""\n    success, response = make_http_request(url)\n\n    if not success:\n        return f"Error fetching webpage: {response.get(\'error\', \'Unknown error\')}"\n\n    try:\n        from bs4 import BeautifulSoup\n\n        soup = BeautifulSoup(response.get(\'text\', \'\'), \'html.parser\')\n\n        # Remove script and style elements if requested\n        if remove_scripts:\n            for script in soup(["script", "style"]):\n                script.decompose()\n\n        # Extract text\n        text = soup.get_text()\n\n        # Clean up whitespace\n        lines = (line.strip() for line in text.splitlines())\n        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))\n        text = \' \'.join(chunk for chunk in chunks if chunk)\n\n        # Truncate if too long\n        if len(text) > max_length:\n            text = (\n                text[:max_length]\n                + f"\\n\\n... (text truncated, showing first {max_length} characters)"\n            )\n\n        return f"URL: {response.get(\'url\', url)}\\nExtracted Text:\\n\\n{text}"\n\n    except ImportError:\n        return "Error: BeautifulSoup4 not available for HTML parsing"\n    except Exception as e:\n        return f"Error extracting text: {str(e)}"\n\n\n@server.tool("search_google", description="Search Google and return results")\ndef search_google(\n    query: Annotated[str, Field(description="Search query")],\n    num_results: Annotated[int, Field(description="Number of results to return", ge=1, le=20)] = 5,\n    safe_search: Annotated[bool, Field(description="Enable safe search")] = True,\n) -> str:\n    """Search Google and return search results. Note: This is a mock implementation."""\n    # This is a simplified mock implementation\n    # In production, you\'d use Google Custom Search API or similar service\n\n    # Mock search results based on common queries\n    mock_results = {\n        "python": [\n            {\n                "title": "Welcome to Python.org",\n                "url": "https://www.python.org/",\n                "snippet": "The official home of the Python Programming Language",\n            },\n            {\n                "title": "Python Tutorial",\n                "url": "https://docs.python.org/3/tutorial/",\n                "snippet": "Python is an easy to learn, powerful programming language",\n            },\n            {\n                "title": "Learn Python",\n                "url": "https://www.learnpython.org/",\n                "snippet": "Learn Python programming with interactive tutorials",\n            },\n        ],\n        "machine learning": [\n            {\n                "title": "Machine Learning | Coursera",\n                "url": "https://www.coursera.org/learn/machine-learning",\n                "snippet": "Learn Machine Learning online with courses from top universities",\n            },\n            {\n                "title": "Machine Learning - Wikipedia",\n                "url": "https://en.wikipedia.org/wiki/Machine_learning",\n                "snippet": "Machine learning is a method of data analysis that automates analytical model building",\n            },\n            {\n                "title": "Introduction to Machine Learning",\n                "url": "https://scikit-learn.org/",\n                "snippet": "Simple and efficient tools for predictive data analysis",\n            },\n        ],\n    }\n\n    # Find relevant results\n    results = []\n    query_lower = query.lower()\n\n    for key, search_results in mock_results.items():\n        if any(word in query_lower for word in key.split()):\n            results.extend(search_results)\n\n    if not results:\n        # Generic results for unknown queries\n        results = [\n            {\n                "title": f"Search results for: {query}",\n                "url": "https://www.google.com/search?q=" + query.replace(" ", "+"),\n                "snippet": f"Various results related to {query}",\n            },\n            {\n                "title": f"Wikipedia: {query}",\n                "url": f"https://en.wikipedia.org/wiki/{query.replace(\' \', \'_\')}",\n                "snippet": f"Wikipedia article about {query}",\n            },\n        ]\n\n    # Limit results\n    results = results[:num_results]\n\n    # Format output\n    output = f"Google Search Results for \'{query}\':\\n\\n"\n    for i, result in enumerate(results, 1):\n        output += f"{i}. {result[\'title\']}\\n"\n        output += f"   URL: {result[\'url\']}\\n"\n        output += f"   {result[\'snippet\']}\\n\\n"\n\n    return output\n\n\n@server.tool("extract_links", description="Extract all links from a webpage")\ndef extract_links(\n    url: Annotated[str, Field(description="URL of the webpage")],\n    filter_domain: Annotated[\n        bool, Field(description="Only return links from the same domain")\n    ] = False,\n    max_links: Annotated[\n        int, Field(description="Maximum number of links to return", ge=1, le=100)\n    ] = 20,\n) -> str:\n    """Extract all links from a webpage."""\n    success, response = make_http_request(url)\n\n    if not success:\n        return f"Error fetching webpage: {response.get(\'error\', \'Unknown error\')}"\n\n    try:\n        from bs4 import BeautifulSoup\n\n        soup = BeautifulSoup(response.get(\'text\', \'\'), \'html.parser\')\n\n        # Find all links\n        links = []\n        base_domain = urlparse(url).netloc\n\n        for link in soup.find_all(\'a\', href=True):\n            href = link[\'href\']\n            text = link.get_text(strip=True)\n\n            # Convert relative URLs to absolute\n            if href.startswith(\'/\'):\n                href = urljoin(url, href)\n            elif not href.startswith((\'http://\', \'https://\')):\n                href = urljoin(url, href)\n\n            # Filter by domain if requested\n            if filter_domain:\n                link_domain = urlparse(href).netloc\n                if link_domain != base_domain:\n                    continue\n\n            links.append({"url": href, "text": text or "No text"})\n\n        # Remove duplicates and limit\n        unique_links = []\n        seen_urls = set()\n        for link in links:\n            if link["url"] not in seen_urls:\n                unique_links.append(link)\n                seen_urls.add(link["url"])\n                if len(unique_links) >= max_links:\n                    break\n\n        # Format output\n        output = f"Links extracted from {url} ({len(unique_links)} unique links):\\n\\n"\n        for i, link in enumerate(unique_links, 1):\n            output += f"{i}. {link[\'text\']}\\n"\n            output += f"   URL: {link[\'url\']}\\n\\n"\n\n        return output\n\n    except ImportError:\n        return "Error: BeautifulSoup4 not available for HTML parsing"\n    except Exception as e:\n        return f"Error extracting links: {str(e)}"\n\n\n@server.tool("take_screenshot", description="Take a screenshot of a webpage")\ndef take_screenshot(\n    url: Annotated[str, Field(description="URL of the webpage")],\n    width: Annotated[int, Field(description="Screenshot width", ge=100, le=3840)] = 1280,\n    height: Annotated[int, Field(description="Screenshot height", ge=100, le=2160)] = 720,\n    full_page: Annotated[bool, Field(description="Capture full page")] = False,\n) -> str:\n    """Take a screenshot of a webpage using headless browser (mock implementation)."""\n    # This is a mock implementation\n    # In production, you\'d use Selenium, Playwright, or similar\n\n    try:\n        # Mock screenshot functionality\n        screenshot_path = f"/tmp/screenshot_{url.replace(\'://\', \'_\').replace(\'/\', \'_\')}.png"\n\n        # Simulate screenshot creation\n        return (\n            f"Screenshot would be saved to: {screenshot_path}\\n"\n            f"URL: {url}\\n"\n            f"Dimensions: {width}x{height}\\n"\n            f"Full page: {full_page}\\n\\n"\n            f"Note: This is a mock implementation. In production, this would use:\\n"\n            f"- Selenium WebDriver\\n"\n            f"- Playwright\\n"\n            f"- Puppeteer\\n"\n            f"- Or similar headless browser automation tools"\n        )\n\n    except Exception as e:\n        return f"Error taking screenshot: {str(e)}"\n\n\n@server.tool(\n    "check_website_status",\n    description="Check if a website is accessible and get status information",\n)\ndef check_website_status(\n    url: Annotated[str, Field(description="URL to check")],\n    check_ssl: Annotated[bool, Field(description="Check SSL certificate validity")] = True,\n) -> str:\n    """Check website accessibility and status."""\n    success, response = make_http_request(url, timeout=10)\n\n    if not success:\n        return f"Website is not accessible: {response.get(\'error\', \'Unknown error\')}"\n\n    status_code = response.get(\'status_code\')\n    headers = response.get(\'headers\', {})\n\n    result = f"Website Status for {url}:\\n\\n"\n    result += f"Status Code: {status_code}\\n"\n    result += f"Status: {\'\u2713 Accessible\' if 200 <= status_code < 400 else \'\u2717 Error\'}\\n"\n    result += f"Server: {headers.get(\'server\', \'Unknown\')}\\n"\n    result += f"Content-Type: {headers.get(\'content-type\', \'Unknown\')}\\n"\n    result += f"Content-Length: {headers.get(\'content-length\', \'Unknown\')}\\n"\n    result += f"Last-Modified: {headers.get(\'last-modified\', \'Unknown\')}\\n"\n\n    if check_ssl and url.startswith(\'https://\'):\n        result += "\\nSSL Status: \u2713 HTTPS enabled"\n\n    return result\n\n\n@server.tool("search_webpage_content", description="Search for specific content within a webpage")\ndef search_webpage_content(\n    url: Annotated[str, Field(description="URL of the webpage")],\n    search_term: Annotated[str, Field(description="Term to search for")],\n    case_sensitive: Annotated[bool, Field(description="Case sensitive search")] = False,\n    max_matches: Annotated[\n        int, Field(description="Maximum number of matches to return", ge=1, le=50)\n    ] = 10,\n) -> str:\n    """Search for specific content within a webpage."""\n    success, response = make_http_request(url)\n\n    if not success:\n        return f"Error fetching webpage: {response.get(\'error\', \'Unknown error\')}"\n\n    try:\n        from bs4 import BeautifulSoup\n\n        soup = BeautifulSoup(response.get(\'text\', \'\'), \'html.parser\')\n        text = soup.get_text()\n\n        # Perform search\n        if not case_sensitive:\n            text = text.lower()\n            search_term = search_term.lower()\n\n        matches = []\n        lines = text.split(\'\\n\')\n\n        for line_num, line in enumerate(lines, 1):\n            if search_term in line:\n                # Get context around the match\n                context_start = max(0, line.find(search_term) - 50)\n                context_end = min(len(line), line.find(search_term) + len(search_term) + 50)\n                context = line[context_start:context_end]\n\n                matches.append(\n                    {"line": line_num, "context": context.strip(), "full_line": line.strip()}\n                )\n\n                if len(matches) >= max_matches:\n                    break\n\n        if not matches:\n            return f"No matches found for \'{search_term}\' in {url}"\n\n        result = f"Found {len(matches)} matches for \'{search_term}\' in {url}:\\n\\n"\n        for i, match in enumerate(matches, 1):\n            result += f"{i}. Line {match[\'line\']}: ...{match[\'context\']}...\\n\\n"\n\n        return result\n\n    except ImportError:\n        return "Error: BeautifulSoup4 not available for HTML parsing"\n    except Exception as e:\n        return f"Error searching content: {str(e)}"\n\n\n# Static resource\n@server.resource("config://browser_settings")\ndef get_browser_settings():\n    return {\n        "default_timeout": 30,\n        "max_content_length": 10000,\n        "supported_formats": ["html", "text", "json"],\n        "user_agent": "MCP-Browser-Tools/1.0",\n        "screenshot_formats": ["png", "jpg"],\n    }\n\n\n# Dynamic resource template\n@server.resource("site://{domain}/info")\ndef get_site_info(domain: str):\n    return {\n        "domain": domain,\n        "note": "Use check_website_status tool to get actual site information",\n    }\n\n\n@server.prompt()\ndef web_research_prompt(research_type: str) -> str:\n    """Generate prompts for web research tasks."""\n    prompts = {\n        "content": "To research web content:\\n1. Use search_google to find relevant pages\\n2. Use fetch_webpage to get page content\\n3. Use extract_text for clean text extraction\\n4. Use search_webpage_content to find specific information",\n        "links": "To analyze website links:\\n1. Use extract_links to get all links\\n2. Filter by domain if needed\\n3. Check link status with check_website_status",\n        "monitoring": "To monitor websites:\\n1. Use check_website_status for availability\\n2. Use fetch_webpage to check content changes\\n3. Use take_screenshot for visual monitoring",\n    }\n\n    return prompts.get(research_type, f"Web research guidance for: {research_type}")\n\n\nfrom clarifai.runners.models.mcp_class import MCPModelClass\n\n\nclass MyModelClass(MCPModelClass):\n    def get_server(self) -> FastMCP:\n        return server',U='import asyncio\nimport os\n\nfrom clarifai.urls.helper import ClarifaiUrlHelper\nfrom fastmcp import Client\nfrom fastmcp.client.transports import StreamableHttpTransport\n\nPAT = os.environ[\'CLARIFAI_PAT\']\nurl = ClarifaiUrlHelper().mcp_api_url()  # get url from the current clarifai config\n\nprint(url)\n\ntransport = StreamableHttpTransport(url=url, headers={"Authorization": "Bearer " + PAT})\n\n\nasync def main():\n    print("=== Browser Tools MCP Server Examples ===\\n")\n\n    async with Client(transport) as client:\n        # List available tools first\n        print("Available tools:")\n        tools = await client.list_tools()\n        for tool in tools:\n            print(f"- {tool.name}: {tool.description}")\n        print("\\n" + "=" * 50 + "\\n")\n\n        # Example 1: Fetch webpage content\n        print("1. Fetching webpage content:")\n        try:\n            result = await client.call_tool(\n                "fetch_webpage", {"url": "https://httpbin.org/get", "max_length": 2000}\n            )\n            print(result[0].text)\n        except Exception as e:\n            print(f"Error: {e}")\n        print("\\n" + "=" * 50 + "\\n")\n\n        # Example 2: Extract clean text from webpage\n        print("2. Extracting clean text:")\n        try:\n            result = await client.call_tool(\n                "extract_text", {"url": "https://httpbin.org/html", "max_length": 1000}\n            )\n            print(result[0].text)\n        except Exception as e:\n            print(f"Error: {e}")\n        print("\\n" + "=" * 50 + "\\n")\n\n        # Example 3: Search Google (mock results)\n        print("3. Searching Google (mock implementation):")\n        try:\n            result = await client.call_tool(\n                "search_google", {"query": "python programming", "num_results": 3}\n            )\n            print(result[0].text)\n        except Exception as e:\n            print(f"Error: {e}")\n        print("\\n" + "=" * 50 + "\\n")\n\n        # Example 4: Check website status\n        print("4. Checking website status:")\n        try:\n            result = await client.call_tool(\n                "check_website_status", {"url": "https://httpbin.org", "check_ssl": True}\n            )\n            print(result[0].text)\n        except Exception as e:\n            print(f"Error: {e}")\n        print("\\n" + "=" * 50 + "\\n")\n\n        # Example 5: Extract links from webpage\n        print("5. Extracting links from webpage:")\n        try:\n            result = await client.call_tool(\n                "extract_links", {"url": "https://httpbin.org", "max_links": 5}\n            )\n            print(result[0].text)\n        except Exception as e:\n            print(f"Error: {e}")\n        print("\\n" + "=" * 50 + "\\n")\n\n\nif __name__ == "__main__":\n    asyncio.run(main())',B="clarifai\nanyio==4.9.0\nmcp==1.9.0\nfastmcp==2.3.4\nrequests>=2.31.0\nbeautifulsoup4==4.12.2\nlxml==4.9.3",Q="build_info:\n  python_version: '3.11'\ninference_compute_info:\n  cpu_limit: 1000m\n  cpu_memory: 1Gi\n  num_accelerators: 0\nmodel:\n  app_id: mcp-examples-app\n  id: browser-tools-mcp-server\n  model_type_id: text-to-text\n  user_id: mcp-examples-user",V='import io\nimport tarfile\nfrom typing import Annotated, Any, Dict\n\nimport docker\nfrom clarifai.runners.models.mcp_class import MCPModelClass\nfrom fastmcp import FastMCP\nfrom pydantic import Field\n\nserver = FastMCP(\n    "python-execution-server",\n    instructions="Execute Python code securely in Docker containers",\n    stateless_http=True,\n)\n\n_docker_client = None\n\n\ndef get_docker_client():\n    """Get or create Docker client."""\n    global _docker_client\n    if _docker_client is None:\n        try:\n            _docker_client = docker.from_env()\n            _docker_client.ping()\n        except Exception as e:\n            # Try alternative connection methods for different Docker setups\n            try:\n                # TODO: replace base_url with your local machine\'s docker socket \n                _docker_client = docker.DockerClient(\n                    base_url=\'unix:///Users/YOUR_USER_NAME/.rd/docker.sock\'\n                )\n                _docker_client.ping()\n            except:\n                try:\n                    _docker_client = docker.DockerClient(base_url=\'unix://var/run/docker.sock\')\n                    _docker_client.ping()\n                except:\n                    raise Exception(f"Cannot connect to Docker daemon. Original error: {e}")\n    return _docker_client\n\n\ndef execute_python_code_fresh_container(code: str) -> Dict[str, Any]:\n    """\n    Execute Python code in a fresh Docker container (OpenAI approach).\n    Each execution gets a completely clean environment.\n    """\n    try:\n        client = get_docker_client()\n\n        # Pull Python image if not present\n        try:\n            client.images.get("python:3.11")\n        except docker.errors.ImageNotFound:\n            client.images.pull("python:3.11")\n\n        # Create a temporary tar archive containing the script (like OpenAI)\n        script_name = "script.py"\n        tarstream = io.BytesIO()\n        with tarfile.open(fileobj=tarstream, mode="w") as tar:\n            script_bytes = code.encode("utf-8")\n            tarinfo = tarfile.TarInfo(name=script_name)\n            tarinfo.size = len(script_bytes)\n            tar.addfile(tarinfo, io.BytesIO(script_bytes))\n        tarstream.seek(0)\n\n        # Start fresh container\n        container = client.containers.create("python:3.11", command="sleep infinity", detach=True)\n\n        try:\n            container.start()\n            # Put the script into the container\n            container.put_archive(path="/tmp", data=tarstream.read())\n            # Execute the script\n            exec_result = container.exec_run(f"python /tmp/{script_name}")\n\n            return {\n                "stdout": exec_result.output.decode("utf-8", errors=\'replace\'),\n                "stderr": "",\n                "status": exec_result.exit_code,\n            }\n        finally:\n            # Always clean up container\n            container.remove(force=True)\n\n    except docker.errors.ContainerError as e:\n        return {"stdout": "", "stderr": str(e), "status": 1}\n    except Exception as e:\n        return {"stdout": "", "stderr": f"Execution error: {str(e)}", "status": 1}\n\n\ndef execute_with_packages(code: str, packages: list = None) -> Dict[str, Any]:\n    """\n    Execute Python code with pre-installed packages.\n    This is the key enhancement over #1 - allows package installation.\n    """\n    if packages:\n        # Prepend package installation to the code\n        install_code = "\\n".join(\n            [\n                f"import subprocess; subprocess.run([\'pip\', \'install\', \'{pkg}\'], check=True)"\n                for pkg in packages\n            ]\n        )\n        full_code = f"{install_code}\\n\\n{code}"\n    else:\n        full_code = code\n\n    return execute_python_code_fresh_container(full_code)\n\n\n@server.tool(\n    "execute_with_packages", description="Execute Python code with packages pre-installed"\n)\ndef execute_with_packages_tool(\n    code: Annotated[str, Field(description="Python code to execute")],\n    packages: Annotated[\n        list[str], Field(description="List of packages to install before execution")\n    ] = None,\n) -> str:\n    """\n    Execute Python code with specified packages installed on top of the base Python image.\n    This enables users to work with the full Python ecosystem.\n    Example: execute_with_packages("import requests; print(requests.get(\'https://httpbin.org/json\').json())", ["requests"])\n    """\n    if not code.strip():\n        return "Error: No code provided"\n\n    result = execute_with_packages(code, packages or [])\n\n    if result["status"] == 0:\n        output = "--- Execution Successful ---\\n"\n        if packages:\n            output += f"Packages installed: {\', \'.join(packages)}\\n\\n"\n        if result["stdout"].strip():\n            output += result["stdout"]\n        else:\n            output += "(No output - use print() to see results)"\n    else:\n        output = f"--- Execution Error (status: {result[\'status\']}) ---\\n"\n        if result["stderr"].strip():\n            output += result["stderr"]\n        if result["stdout"].strip():\n            output += "\\n--- Output ---\\n" + result["stdout"]\n\n    return output\n\n\nclass MyModel(MCPModelClass):\n    def get_server(self) -> FastMCP:\n        """Return the FastMCP server instance."""\n        return server',H='import asyncio\nimport os\n\nfrom clarifai.urls.helper import ClarifaiUrlHelper\nfrom fastmcp import Client\nfrom fastmcp.client.transports import StreamableHttpTransport\n\nPAT = os.environ[\'CLARIFAI_PAT\']\nurl = ClarifaiUrlHelper().mcp_api_url()\n\ntransport = StreamableHttpTransport(url=url, headers={"Authorization": "Bearer " + PAT})\n\n\nasync def main():\n    async with Client(transport) as client:\n        # List available tools\n        print("=== Available Tools ===")\n        tools = await client.list_tools()\n        for tool in tools:\n            print(f"- {tool.name}: {tool.description}")\n        print()\n\n        # Test 1: execute_with_packages\n        print("=== Test 1: execute_with_packages ===")\n        code_with_pkg = \'import requests; print(f"Requests version: {requests.__version__}"); print("Package imported successfully!")\'\n        result = await client.call_tool(\n            "execute_with_packages", {"code": code_with_pkg, "packages": ["requests"]}\n        )\n        print(result.content[0].text)\n\n\nif __name__ == "__main__":\n    asyncio.run(main())',W="# Clarifai SDK - required\nclarifai\nfastmcp\npydantic\ndocker",Y="build_info:\n  python_version: '3.11'\ninference_compute_info:\n  cpu_limit: 1\n  cpu_memory: 500Mi\n  num_accelerators: 0\nmodel:\n  app_id: code-execution-app\n  id: code-execution-model\n  model_type_id: text-to-text\n  user_id: your_user_id",J='import subprocess\nimport tempfile\nimport os\nfrom typing import Annotated, Any, Dict\n\nfrom clarifai.runners.models.mcp_class import MCPModelClass\nfrom fastmcp import FastMCP\nfrom pydantic import Field\n\nserver = FastMCP(\n    "python-execution-server",\n    instructions="Execute Python code using local Python environment",\n    stateless_http=True,\n)\n\n\ndef execute_python_code_fresh_container(code: str) -> Dict[str, Any]:\n    """\n    Execute Python code using local Python environment.\n    Each execution gets a clean temporary file.\n    """\n    try:\n        # Create a temporary file for the code\n        with tempfile.NamedTemporaryFile(mode=\'w\', suffix=\'.py\', delete=False) as temp_file:\n            temp_file.write(code)\n            temp_path = temp_file.name\n\n        try:\n            # Execute the script using subprocess\n            result = subprocess.run(\n                ["python", temp_path],\n                capture_output=True,\n                text=True,\n                check=False\n            )\n\n            return {\n                "stdout": result.stdout,\n                "stderr": result.stderr,\n                "status": result.returncode,\n            }\n        finally:\n            # Clean up temp file\n            try:\n                os.unlink(temp_path)\n            except Exception:\n                pass\n\n    except Exception as e:\n        return {"stdout": "", "stderr": f"Execution error: {str(e)}", "status": 1}\n\n\ndef execute_with_packages(code: str, packages: list = None) -> Dict[str, Any]:\n    """\n    Execute Python code with pre-installed packages.\n    This is the key enhancement over #1 - allows package installation.\n    """\n    if packages:\n        # Prepend package installation to the code\n        install_code = "\\n".join(\n            [\n                f"import subprocess; subprocess.run([\'pip\', \'install\', \'{pkg}\'], check=True)"\n                for pkg in packages\n            ]\n        )\n        full_code = f"{install_code}\\n\\n{code}"\n    else:\n        full_code = code\n\n    return execute_python_code_fresh_container(full_code)\n\n\n@server.tool(\n    "execute_with_packages", description="Execute Python code with packages pre-installed"\n)\ndef execute_with_packages_tool(\n    code: Annotated[str, Field(description="Python code to execute")],\n    packages: Annotated[\n        list[str], Field(description="List of packages to install before execution")\n    ] = None,\n) -> str:\n    """\n    Execute Python code with specified packages installed on top of the base Python image.\n    This enables users to work with the full Python ecosystem.\n    Example: execute_with_packages("import requests; print(requests.get(\'https://httpbin.org/json\').json())", ["requests"])\n    """\n    if not code.strip():\n        return "Error: No code provided"\n\n    result = execute_with_packages(code, packages or [])\n\n    if result["status"] == 0:\n        output = "--- Execution Successful ---\\n"\n        if packages:\n            output += f"Packages installed: {\', \'.join(packages)}\\n\\n"\n        if result["stdout"].strip():\n            output += result["stdout"]\n        else:\n            output += "(No output - use print() to see results)"\n    else:\n        output = f"--- Execution Error (status: {result[\'status\']}) ---\\n"\n        if result["stderr"].strip():\n            output += result["stderr"]\n        if result["stdout"].strip():\n            output += "\\n--- Output ---\\n" + result["stdout"]\n\n    return output\n\n\nclass MyModel(MCPModelClass):\n    def get_server(self) -> FastMCP:\n        """Return the FastMCP server instance."""\n        return server',X='import asyncio\nimport os\n\nfrom clarifai.urls.helper import ClarifaiUrlHelper\nfrom fastmcp import Client\nfrom fastmcp.client.transports import StreamableHttpTransport\n\nPAT = os.environ[\'CLARIFAI_PAT\']\nurl = ClarifaiUrlHelper().mcp_api_url()\n\ntransport = StreamableHttpTransport(url=url, headers={"Authorization": "Bearer " + PAT})\n\n\nasync def main():\n    async with Client(transport) as client:\n        # List available tools\n        print("=== Available Tools ===")\n        tools = await client.list_tools()\n        for tool in tools:\n            print(f"- {tool.name}: {tool.description}")\n        print()\n\n        # Test 1: execute_with_packages\n        print("=== Test 1: execute_with_packages ===")\n        code_with_pkg = \'import requests; print(f"Requests version: {requests.__version__}"); print("Package imported successfully!")\'\n        result = await client.call_tool(\n            "execute_with_packages", {"code": code_with_pkg, "packages": ["requests"]}\n        )\n        print(result.content[0].text)\n\n\nif __name__ == "__main__":\n    asyncio.run(main())',K="# Clarifai SDK - required\nclarifai==11.7.5\nfastmcp\npydantic",$="build_info:\n  python_version: '3.11'\ninference_compute_info:\n  cpu_limit: 1\n  cpu_memory: 500Mi\n  num_accelerators: 0\nmodel:\n  app_id: code-execution-app-no-docker\n  id: code-execution-model-no-docker\n  model_type_id: text-to-text\n  user_id: your_user_id",Z='import os\nfrom typing import Annotated\n\nfrom clarifai.utils.logging import logger\nfrom fastmcp import FastMCP  # use fastmcp v2 not the built in mcp\nfrom pydantic import Field\n\nserver = FastMCP(\n    "google-drive-mcp-server",\n    instructions="Google Drive operations for file storage, sharing, and collaboration",\n    stateless_http=True,\n)\n\n\ndef get_drive_service():\n    """Create and return a Google Drive service object."""\n    try:\n        from google.auth.transport.requests import Request\n        from google.oauth2.credentials import Credentials\n        from google_auth_oauthlib.flow import InstalledAppFlow\n        from googleapiclient.discovery import build\n\n        # Define the scope\n        SCOPES = [\'https://www.googleapis.com/auth/drive\']\n\n        creds = None\n        # Check for existing token\n        if os.path.exists(\'token.json\'):\n            creds = Credentials.from_authorized_user_file(\'token.json\', SCOPES)\n\n        # If there are no valid credentials, use mock data\n        if not creds or not creds.valid:\n            logger.warning("Google Drive credentials not available, using mock data")\n            return None\n\n        service = build(\'drive\', \'v3\', credentials=creds)\n        return service\n    except ImportError:\n        logger.error("Google API client libraries not available")\n        return None\n    except Exception as e:\n        logger.error(f"Failed to create Drive service: {str(e)}")\n        return None\n\n\ndef handle_drive_operation(operation_func):\n    """Decorator to handle Drive operations with proper error handling."""\n\n    def wrapper(*args, **kwargs):\n        try:\n            service = get_drive_service()\n            if not service:\n                # Return mock data if service not available\n                return operation_func(None, *args, **kwargs)\n\n            return operation_func(service, *args, **kwargs)\n        except Exception as e:\n            return f"Google Drive operation failed: {str(e)}"\n\n    return wrapper\n\n\n@server.tool("drive_list_files", description="List files in Google Drive")\ndef drive_list_files(\n    folder_id: Annotated[str, Field(description="Folder ID to list files from (optional)")] = "",\n    file_type: Annotated[\n        str,\n        Field(\n            description="Filter by file type (document, spreadsheet, presentation, folder, etc.)"\n        ),\n    ] = "",\n    max_results: Annotated[\n        int, Field(description="Maximum number of files to return", ge=1, le=100)\n    ] = 20,\n    order_by: Annotated[\n        str, Field(description="Order by: name, modifiedTime, createdTime")\n    ] = "modifiedTime desc",\n) -> str:\n    """List files in Google Drive."""\n\n    @handle_drive_operation\n    def _list_files(service, folder_id, file_type, max_results, order_by):\n        if not service:\n            # Mock response\n            mock_files = [\n                {\n                    "id": "1abc123",\n                    "name": "Project Document.docx",\n                    "mimeType": "application/vnd.google-apps.document",\n                    "modifiedTime": "2024-01-15T10:30:00Z",\n                    "size": "2048576",\n                    "owners": [{"displayName": "John Doe"}],\n                },\n                {\n                    "id": "2def456",\n                    "name": "Budget Spreadsheet.xlsx",\n                    "mimeType": "application/vnd.google-apps.spreadsheet",\n                    "modifiedTime": "2024-01-14T14:22:00Z",\n                    "size": "1048576",\n                    "owners": [{"displayName": "Jane Smith"}],\n                },\n                {\n                    "id": "3ghi789",\n                    "name": "Presentation.pptx",\n                    "mimeType": "application/vnd.google-apps.presentation",\n                    "modifiedTime": "2024-01-13T09:15:00Z",\n                    "size": "5242880",\n                    "owners": [{"displayName": "Bob Johnson"}],\n                },\n            ]\n\n            output = f"Google Drive Files (showing {len(mock_files)} files):\\n\\n"\n            for file in mock_files:\n                file_size_mb = int(file.get(\'size\', 0)) / (1024 * 1024)\n                output += f"\u2022 {file[\'name\']}\\n"\n                output += f"  ID: {file[\'id\']}\\n"\n                output += f"  Type: {file[\'mimeType\'].split(\'.\')[-1] if \'.\' in file[\'mimeType\'] else \'Google App\'}\\n"\n                output += f"  Size: {file_size_mb:.2f} MB\\n"\n                output += f"  Modified: {file[\'modifiedTime\']}\\n"\n                output += f"  Owner: {file[\'owners\'][0][\'displayName\']}\\n\\n"\n\n            return output\n\n        # Real implementation would use service.files().list()\n        query_parts = []\n\n        if folder_id:\n            query_parts.append(f"\'{folder_id}\' in parents")\n\n        if file_type:\n            mime_type_map = {\n                "document": "application/vnd.google-apps.document",\n                "spreadsheet": "application/vnd.google-apps.spreadsheet",\n                "presentation": "application/vnd.google-apps.presentation",\n                "folder": "application/vnd.google-apps.folder",\n                "pdf": "application/pdf",\n            }\n            if file_type in mime_type_map:\n                query_parts.append(f"mimeType=\'{mime_type_map[file_type]}\'")\n\n        query = " and ".join(query_parts) if query_parts else None\n\n        results = (\n            service.files()\n            .list(\n                q=query,\n                pageSize=max_results,\n                orderBy=order_by,\n                fields="files(id,name,mimeType,size,modifiedTime,owners)",\n            )\n            .execute()\n        )\n\n        files = results.get(\'files\', [])\n\n        if not files:\n            return "No files found in Google Drive"\n\n        output = f"Google Drive Files ({len(files)} files):\\n\\n"\n        for file in files:\n            file_size = int(file.get(\'size\', 0))\n            file_size_mb = file_size / (1024 * 1024) if file_size > 0 else 0\n\n            output += f"\u2022 {file[\'name\']}\\n"\n            output += f"  ID: {file[\'id\']}\\n"\n            output += f"  Type: {file[\'mimeType\'].split(\'.\')[-1] if \'.\' in file[\'mimeType\'] else \'Google App\'}\\n"\n            output += f"  Size: {file_size_mb:.2f} MB\\n"\n            output += f"  Modified: {file[\'modifiedTime\']}\\n"\n            if file.get(\'owners\'):\n                output += f"  Owner: {file[\'owners\'][0][\'displayName\']}\\n"\n            output += "\\n"\n\n        return output\n\n    return _list_files(folder_id, file_type, max_results, order_by)\n\n\n@server.tool("drive_upload_file", description="Upload a file to Google Drive")\ndef drive_upload_file(\n    file_path: Annotated[str, Field(description="Local file path to upload")],\n    file_name: Annotated[str, Field(description="Name for the file in Drive (optional)")] = "",\n    folder_id: Annotated[str, Field(description="Folder ID to upload to (optional)")] = "",\n    description: Annotated[str, Field(description="File description")] = "",\n) -> str:\n    """Upload a file to Google Drive."""\n\n    @handle_drive_operation\n    def _upload_file(service, file_path, file_name, folder_id, description):\n        if not os.path.exists(file_path):\n            return f"Error: File \'{file_path}\' does not exist"\n\n        if not service:\n            # Mock response\n            file_name = file_name or os.path.basename(file_path)\n            file_size = os.path.getsize(file_path)\n            file_size_mb = file_size / (1024 * 1024)\n\n            return (\n                f"Mock upload successful!\\n"\n                f"File: {file_name}\\n"\n                f"Size: {file_size_mb:.2f} MB\\n"\n                f"Drive ID: mock-file-id-{hash(file_path) % 10000}\\n"\n                f"Note: This is a mock response. Install Google API libraries for actual uploads."\n            )\n\n        from googleapiclient.http import MediaFileUpload\n\n        file_name = file_name or os.path.basename(file_path)\n\n        file_metadata = {\'name\': file_name, \'description\': description}\n\n        if folder_id:\n            file_metadata[\'parents\'] = [folder_id]\n\n        media = MediaFileUpload(file_path, resumable=True)\n\n        file = (\n            service.files()\n            .create(body=file_metadata, media_body=media, fields=\'id,name,size,webViewLink\')\n            .execute()\n        )\n\n        file_size = int(file.get(\'size\', 0))\n        file_size_mb = file_size / (1024 * 1024) if file_size > 0 else 0\n\n        return (\n            f"Successfully uploaded \'{file_name}\' to Google Drive\\n"\n            f"File ID: {file[\'id\']}\\n"\n            f"Size: {file_size_mb:.2f} MB\\n"\n            f"View Link: {file.get(\'webViewLink\', \'N/A\')}"\n        )\n\n    return _upload_file(file_path, file_name, folder_id, description)\n\n\n@server.tool("drive_download_file", description="Download a file from Google Drive")\ndef drive_download_file(\n    file_id: Annotated[str, Field(description="Google Drive file ID")],\n    local_path: Annotated[str, Field(description="Local path to save the file")] = "",\n    export_format: Annotated[\n        str, Field(description="Export format for Google Docs (pdf, docx, etc.)")\n    ] = "",\n) -> str:\n    """Download a file from Google Drive."""\n\n    @handle_drive_operation\n    def _download_file(service, file_id, local_path, export_format):\n        if not service:\n            # Mock response\n            local_path = local_path or f"downloaded_file_{file_id}"\n            return (\n                f"Mock download successful!\\n"\n                f"File ID: {file_id}\\n"\n                f"Saved to: {local_path}\\n"\n                f"Note: This is a mock response. Install Google API libraries for actual downloads."\n            )\n\n        # Get file metadata\n        file_metadata = service.files().get(fileId=file_id).execute()\n        file_name = file_metadata[\'name\']\n\n        if not local_path:\n            local_path = file_name\n\n        # Check if it\'s a Google Workspace file that needs to be exported\n        google_mime_types = [\n            \'application/vnd.google-apps.document\',\n            \'application/vnd.google-apps.spreadsheet\',\n            \'application/vnd.google-apps.presentation\',\n        ]\n\n        if file_metadata[\'mimeType\'] in google_mime_types:\n            if not export_format:\n                export_format = \'pdf\'  # Default export format\n\n            export_mime_types = {\n                \'pdf\': \'application/pdf\',\n                \'docx\': \'application/vnd.openxmlformats-officedocument.wordprocessingml.document\',\n                \'xlsx\': \'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\',\n                \'pptx\': \'application/vnd.openxmlformats-officedocument.presentationml.presentation\',\n            }\n\n            request = service.files().export_media(\n                fileId=file_id,\n                mimeType=export_mime_types.get(export_format, export_mime_types[\'pdf\']),\n            )\n        else:\n            request = service.files().get_media(fileId=file_id)\n\n        # Download the file\n        import io\n\n        fh = io.BytesIO()\n        downloader = MediaIoBaseDownload(fh, request)\n        done = False\n        while done is False:\n            status, done = downloader.next_chunk()\n\n        # Save to local file\n        with open(local_path, \'wb\') as f:\n            f.write(fh.getvalue())\n\n        file_size = os.path.getsize(local_path)\n        file_size_mb = file_size / (1024 * 1024)\n\n        return (\n            f"Successfully downloaded \'{file_name}\' from Google Drive\\n"\n            f"Saved to: {local_path}\\n"\n            f"Size: {file_size_mb:.2f} MB"\n        )\n\n    return _download_file(file_id, local_path, export_format)\n\n\n@server.tool("drive_share_file", description="Share a file or folder in Google Drive")\ndef drive_share_file(\n    file_id: Annotated[str, Field(description="Google Drive file or folder ID")],\n    email: Annotated[str, Field(description="Email address to share with (optional)")] = "",\n    role: Annotated[\n        str, Field(description="Permission role: reader, writer, commenter")\n    ] = "reader",\n    anyone_can_view: Annotated[\n        bool, Field(description="Make file viewable by anyone with the link")\n    ] = False,\n) -> str:\n    """Share a file or folder in Google Drive."""\n\n    @handle_drive_operation\n    def _share_file(service, file_id, email, role, anyone_can_view):\n        if not service:\n            # Mock response\n            share_url = f"https://drive.google.com/file/d/{file_id}/view"\n            return (\n                f"Mock sharing successful!\\n"\n                f"File ID: {file_id}\\n"\n                f"Share URL: {share_url}\\n"\n                f"Permissions: {role}\\n"\n                f"Note: This is a mock response. Install Google API libraries for actual sharing."\n            )\n\n        permissions_created = []\n\n        # Share with specific email if provided\n        if email:\n            permission = {\'type\': \'user\', \'role\': role, \'emailAddress\': email}\n\n            result = (\n                service.permissions()\n                .create(fileId=file_id, body=permission, sendNotificationEmail=True)\n                .execute()\n            )\n\n            permissions_created.append(f"Shared with {email} as {role}")\n\n        # Make viewable by anyone with link if requested\n        if anyone_can_view:\n            permission = {\'type\': \'anyone\', \'role\': \'reader\'}\n\n            service.permissions().create(fileId=file_id, body=permission).execute()\n\n            permissions_created.append("Made viewable by anyone with the link")\n\n        # Get the file\'s web view link\n        file_metadata = service.files().get(fileId=file_id, fields=\'name,webViewLink\').execute()\n\n        result = f"Successfully shared \'{file_metadata[\'name\']}\'\\n"\n        result += f"File ID: {file_id}\\n"\n        result += f"Share URL: {file_metadata.get(\'webViewLink\', \'N/A\')}\\n"\n\n        if permissions_created:\n            result += "\\nPermissions:\\n"\n            for perm in permissions_created:\n                result += f"  \u2022 {perm}\\n"\n\n        return result\n\n    return _share_file(file_id, email, role, anyone_can_view)\n\n\n@server.tool("drive_create_folder", description="Create a new folder in Google Drive")\ndef drive_create_folder(\n    folder_name: Annotated[str, Field(description="Name for the new folder")],\n    parent_folder_id: Annotated[str, Field(description="Parent folder ID (optional)")] = "",\n    description: Annotated[str, Field(description="Folder description")] = "",\n) -> str:\n    """Create a new folder in Google Drive."""\n\n    @handle_drive_operation\n    def _create_folder(service, folder_name, parent_folder_id, description):\n        if not service:\n            # Mock response\n            return (\n                f"Mock folder creation successful!\\n"\n                f"Folder name: {folder_name}\\n"\n                f"Folder ID: mock-folder-id-{hash(folder_name) % 10000}\\n"\n                f"Note: This is a mock response. Install Google API libraries for actual folder creation."\n            )\n\n        folder_metadata = {\n            \'name\': folder_name,\n            \'mimeType\': \'application/vnd.google-apps.folder\',\n            \'description\': description,\n        }\n\n        if parent_folder_id:\n            folder_metadata[\'parents\'] = [parent_folder_id]\n\n        folder = (\n            service.files().create(body=folder_metadata, fields=\'id,name,webViewLink\').execute()\n        )\n\n        return (\n            f"Successfully created folder \'{folder_name}\'\\n"\n            f"Folder ID: {folder[\'id\']}\\n"\n            f"View Link: {folder.get(\'webViewLink\', \'N/A\')}"\n        )\n\n    return _create_folder(folder_name, parent_folder_id, description)\n\n\n@server.tool("drive_delete_file", description="Delete a file or folder from Google Drive")\ndef drive_delete_file(\n    file_id: Annotated[str, Field(description="Google Drive file or folder ID to delete")],\n    permanent: Annotated[bool, Field(description="Permanently delete (bypass trash)")] = False,\n) -> str:\n    """Delete a file or folder from Google Drive."""\n\n    @handle_drive_operation\n    def _delete_file(service, file_id, permanent):\n        if not service:\n            # Mock response\n            return (\n                f"Mock deletion successful!\\n"\n                f"File ID: {file_id}\\n"\n                f"Permanent: {permanent}\\n"\n                f"Note: This is a mock response. Install Google API libraries for actual deletion."\n            )\n\n        # Get file metadata before deletion\n        try:\n            file_metadata = service.files().get(fileId=file_id, fields=\'name\').execute()\n            file_name = file_metadata[\'name\']\n        except:\n            return f"Error: File with ID \'{file_id}\' not found"\n\n        if permanent:\n            service.files().delete(fileId=file_id).execute()\n            return f"Permanently deleted \'{file_name}\' (ID: {file_id})"\n        else:\n            # Move to trash\n            service.files().update(fileId=file_id, body={\'trashed\': True}).execute()\n            return f"Moved \'{file_name}\' to trash (ID: {file_id})"\n\n    return _delete_file(file_id, permanent)\n\n\n@server.tool("drive_search_files", description="Search for files in Google Drive")\ndef drive_search_files(\n    query: Annotated[str, Field(description="Search query")],\n    file_type: Annotated[str, Field(description="File type filter")] = "",\n    max_results: Annotated[int, Field(description="Maximum number of results", ge=1, le=100)] = 10,\n) -> str:\n    """Search for files in Google Drive."""\n\n    @handle_drive_operation\n    def _search_files(service, query, file_type, max_results):\n        if not service:\n            # Mock response\n            mock_results = [\n                {\n                    "id": "search1",\n                    "name": f"Document about {query}.docx",\n                    "mimeType": "application/vnd.google-apps.document",\n                },\n                {\n                    "id": "search2",\n                    "name": f"{query} Spreadsheet.xlsx",\n                    "mimeType": "application/vnd.google-apps.spreadsheet",\n                },\n            ]\n\n            output = f"Search results for \'{query}\' (mock data):\\n\\n"\n            for i, file in enumerate(mock_results, 1):\n                output += f"{i}. {file[\'name\']}\\n"\n                output += f"   ID: {file[\'id\']}\\n"\n                output += f"   Type: {file[\'mimeType\'].split(\'.\')[-1]}\\n\\n"\n\n            return output\n\n        search_query = f"name contains \'{query}\'"\n\n        if file_type:\n            mime_type_map = {\n                "document": "application/vnd.google-apps.document",\n                "spreadsheet": "application/vnd.google-apps.spreadsheet",\n                "presentation": "application/vnd.google-apps.presentation",\n            }\n            if file_type in mime_type_map:\n                search_query += f" and mimeType=\'{mime_type_map[file_type]}\'"\n\n        results = (\n            service.files()\n            .list(\n                q=search_query, pageSize=max_results, fields="files(id,name,mimeType,modifiedTime)"\n            )\n            .execute()\n        )\n\n        files = results.get(\'files\', [])\n\n        if not files:\n            return f"No files found matching \'{query}\'"\n\n        output = f"Search results for \'{query}\' ({len(files)} files):\\n\\n"\n        for i, file in enumerate(files, 1):\n            output += f"{i}. {file[\'name\']}\\n"\n            output += f"   ID: {file[\'id\']}\\n"\n            output += f"   Type: {file[\'mimeType\'].split(\'.\')[-1] if \'.\' in file[\'mimeType\'] else \'Google App\'}\\n"\n            output += f"   Modified: {file[\'modifiedTime\']}\\n\\n"\n\n        return output\n\n    return _search_files(query, file_type, max_results)\n\n\n# Static resource\n@server.resource("config://drive_settings")\ndef get_drive_settings():\n    return {\n        "api_version": "v3",\n        "scopes": ["https://www.googleapis.com/auth/drive"],\n        "supported_formats": ["pdf", "docx", "xlsx", "pptx"],\n        "max_upload_size": "5TB",\n        "permission_roles": ["reader", "writer", "commenter", "owner"],\n    }\n\n\n# Dynamic resource template\n@server.resource("drive://file/{file_id}/info")\ndef get_file_info(file_id: str):\n    return {\n        "file_id": file_id,\n        "note": "Use drive_list_files or other tools to get actual file information",\n    }\n\n\n@server.prompt()\ndef drive_workflow_prompt(workflow_type: str) -> str:\n    """Generate prompts for Google Drive workflows."""\n    prompts = {\n        "collaboration": "For collaboration workflow:\\n1. Use drive_upload_file to share documents\\n2. Use drive_share_file to set permissions\\n3. Use drive_create_folder for organization\\n4. Grant appropriate roles (reader, writer, commenter)",\n        "backup": "For backup workflow:\\n1. Use drive_create_folder for organized storage\\n2. Use drive_upload_file to backup important files\\n3. Consider using drive_share_file for team access\\n4. Regular sync using automated tools",\n        "document_management": "For document management:\\n1. Create folder structure with drive_create_folder\\n2. Upload files with drive_upload_file\\n3. Use drive_search_files to find documents\\n4. Share selectively with drive_share_file",\n    }\n\n    return prompts.get(workflow_type, f"Google Drive workflow guidance for: {workflow_type}")\n\n\nfrom clarifai.runners.models.mcp_class import MCPModelClass\n\n\nclass MyModelClass(MCPModelClass):\n    def get_server(self) -> FastMCP:\n        return server',ee='import asyncio\nimport os\nimport tempfile\n\nfrom clarifai.urls.helper import ClarifaiUrlHelper\nfrom fastmcp import Client\nfrom fastmcp.client.transports import StreamableHttpTransport\n\nPAT = os.environ[\'CLARIFAI_PAT\']\nurl = ClarifaiUrlHelper().mcp_api_url()  # get url from the current clarifai config\n\ntransport = StreamableHttpTransport(url=url, headers={"Authorization": "Bearer " + PAT})\n\n\nasync def main():\n    print("=== Google Drive MCP Server Examples ===\\n")\n\n    async with Client(transport) as client:\n        # List available tools first\n        print("Available tools:")\n        tools = await client.list_tools()\n        for tool in tools:\n            print(f"- {tool.name}: {tool.description}")\n        print("\\n" + "=" * 50 + "\\n")\n\n        # Example 1: List files in Google Drive\n        print("1. Listing files in Google Drive:")\n        try:\n            result = await client.call_tool(\n                "drive_list_files", {"max_results": 5, "order_by": "modifiedTime desc"}\n            )\n            print(result[0].text)\n        except Exception as e:\n            print(f"Error: {e}")\n        print("\\n" + "=" * 50 + "\\n")\n\n        # Example 2: Search for files\n        print("2. Searching for files:")\n        try:\n            result = await client.call_tool(\n                "drive_search_files", {"query": "project", "max_results": 3}\n            )\n            print(result[0].text)\n        except Exception as e:\n            print(f"Error: {e}")\n        print("\\n" + "=" * 50 + "\\n")\n\n        # Example 3: Create a folder\n        print("3. Creating a folder:")\n        try:\n            result = await client.call_tool(\n                "drive_create_folder",\n                {"folder_name": "MCP Demo Folder", "description": "Folder created by MCP example"},\n            )\n            print(result[0].text)\n        except Exception as e:\n            print(f"Error: {e}")\n        print("\\n" + "=" * 50 + "\\n")\n\n        # Example 4: Upload a file (creates a temporary file for demo)\n        print("4. Uploading a file:")\n\n        # Create a temporary file for demonstration\n        with tempfile.NamedTemporaryFile(mode=\'w\', delete=False, suffix=\'.txt\') as f:\n            f.write("This is a test file for Google Drive upload demonstration.")\n            temp_file_path = f.name\n\n        try:\n            result = await client.call_tool(\n                "drive_upload_file",\n                {\n                    "file_path": temp_file_path,\n                    "file_name": "MCP Demo File.txt",\n                    "description": "File uploaded by MCP example",\n                },\n            )\n            print(result[0].text)\n        except Exception as e:\n            print(f"Error: {e}")\n        finally:\n            # Clean up temporary file\n            os.unlink(temp_file_path)\n        print("\\n" + "=" * 50 + "\\n")\n\n        # Example 5: Share a file\n        print("5. Sharing a file:")\n        try:\n            result = await client.call_tool(\n                "drive_share_file", {"file_id": "mock-file-id-123", "anyone_can_view": True}\n            )\n            print(result[0].text)\n        except Exception as e:\n            print(f"Error: {e}")\n        print("\\n" + "=" * 50 + "\\n")\n\n        # Example 6: Download a file\n        print("6. Downloading a file:")\n        try:\n            result = await client.call_tool(\n                "drive_download_file",\n                {\n                    "file_id": "mock-file-id-123",\n                    "local_path": "downloaded_document.pdf",\n                    "export_format": "pdf",\n                },\n            )\n            print(result[0].text)\n        except Exception as e:\n            print(f"Error: {e}")\n        print("\\n" + "=" * 50 + "\\n")\n\n    print("\\n" + "=" * 50)\n    print("Note: Google Drive authentication required for actual operations:")\n    print("- Set up OAuth 2.0 credentials in Google Cloud Console")\n    print("- Download credentials.json file")\n    print("- Run authentication flow to generate token.json")\n    print("- This example uses mock data for demonstration")\n    print("=" * 50)\n\n\nif __name__ == "__main__":\n    asyncio.run(main())',ne="clarifai\nanyio==4.9.0\nmcp==1.9.0\nfastmcp==2.3.4\ngoogle-api-python-client==2.111.0\ngoogle-auth-httplib2==0.2.0\ngoogle-auth-oauthlib==1.1.0",te="build_info:\n  python_version: '3.11'\ninference_compute_info:\n  cpu_limit: 1000m\n  cpu_memory: 1Gi\n  num_accelerators: 0\nmodel:\n  app_id: mcp-examples-app\n  id: google-drive-mcp-server\n  model_type_id: text-to-text\n  user_id: mcp-examples-user",re='from fastmcp import FastMCP\nfrom pydantic import Field\nfrom typing import Annotated\n\nfrom clarifai.runners.models.mcp_class import MCPModelClass\n\nserver = FastMCP("my-mcp-server", instructions="", stateless_http=True)\n\n\n@server.tool("addition_tool", description="Add two numbers")\ndef addition_tool(\n    a: Annotated[float, Field(description="First number")],\n    b: Annotated[float, Field(description="Second number")],\n) -> float:\n    """Add two numbers"""\n    return a + b\n\n\n@server.tool("subtraction_tool", description="Subtract two numbers")\ndef subtraction_tool(\n    a: Annotated[float, Field(description="First number")],\n    b: Annotated[float, Field(description="Second number")],\n) -> float:\n    """Subtract two numbers"""\n    return a - b\n\n\n@server.tool("multiplication_tool", description="Multiply two numbers")\ndef multiplication_tool(\n    a: Annotated[float, Field(description="First number")],\n    b: Annotated[float, Field(description="Second number")],\n) -> float:\n    """Multiply two numbers"""\n    return a * b\n\n\n@server.tool("division_tool", description="Divide two numbers")\ndef division_tool(\n    a: Annotated[float, Field(description="First number")],\n    b: Annotated[float, Field(description="Second number")],\n) -> float:\n    """Divide two numbers"""\n    return a / b\n\n\nclass MyModel(MCPModelClass):\n    def get_server(self) -> FastMCP:\n        """Return the FastMCP server instance."""\n        return server',se='import asyncio\nimport os\n\nfrom clarifai.urls.helper import ClarifaiUrlHelper\nfrom fastmcp import Client\nfrom fastmcp.client.transports import StreamableHttpTransport\n\nPAT = os.environ[\'CLARIFAI_PAT\']\nurl = ClarifaiUrlHelper().mcp_api_url()  # get url from the current clarifai config\n\ntransport = StreamableHttpTransport(url=url, headers={"Authorization": "Bearer " + PAT})\n\n\nasync def main():\n    async with Client(transport) as client:\n        tools = await client.list_tools()\n        for tool in tools:\n            print(f"- {tool.name}: {tool.description}")\n\n        result = await client.call_tool("addition_tool", {"a": 10.1, "b": 5.2})\n        print(f"10.1 + 5.2 = {result.content[0].text}")\n\n        result = await client.call_tool("subtraction_tool", {"a": 10.1, "b": 3.2})\n        print(f"10.1 - 3.2 = {result.content[0].text}")\n\n        result = await client.call_tool("multiplication_tool", {"a": 4.1, "b": 7.2})\n        print(f"4.1 * 7.2 = {result.content[0].text}")\n\n        result = await client.call_tool("division_tool", {"a": 20.1, "b": 4.2})\n        print(f"20.1 / 4.2 = {result.content[0].text}")\n\n\nif __name__ == "__main__":\n    asyncio.run(main())',oe="# Clarifai SDK - required\nclarifai\nfastmcp\npydantic",ie="build_info:\n  python_version: '3.11'\ninference_compute_info:\n  cpu_limit: 1\n  cpu_memory: 500Mi\n  num_accelerators: 0\nmodel:\n  app_id: mcp-math-app\n  id: mcp-math-model\n  model_type_id: text-to-text\n  user_id: mcp-math-user",ae='import os\nfrom typing import Annotated, Any\n\nfrom clarifai.utils.logging import logger\nfrom fastmcp import FastMCP  # use fastmcp v2 not the built in mcp\nfrom pydantic import Field\n\nserver = FastMCP(\n    "postgres-mcp-server",\n    instructions="PostgreSQL database operations and management",\n    stateless_http=True,\n)\n\n\ndef get_postgres_connection(host: str, port: int, user: str, password: str, database: str):\n    """Create a PostgreSQL connection. Returns connection object or None if failed."""\n    try:\n        import psycopg2\n\n        connection = psycopg2.connect(\n            host=host, port=port, user=user, password=password, database=database\n        )\n        connection.autocommit = True\n        return connection\n    except Exception as e:\n        logger.error(f"PostgreSQL connection failed: {str(e)}")\n        return None\n\n\ndef execute_postgres_query(connection, query: str, params: tuple = None) -> tuple[bool, Any]:\n    """Execute a PostgreSQL query and return results."""\n    try:\n        from psycopg2.extras import RealDictCursor\n\n        cursor = connection.cursor(cursor_factory=RealDictCursor)\n        cursor.execute(query, params or ())\n\n        # Handle different query types\n        if query.strip().upper().startswith((\'SELECT\', \'WITH\', \'SHOW\', \'EXPLAIN\')):\n            results = cursor.fetchall()\n            # Convert to list of dicts for JSON serialization\n            results = [dict(row) for row in results]\n            cursor.close()\n            return True, results\n        else:\n            # For INSERT, UPDATE, DELETE, etc.\n            affected_rows = cursor.rowcount\n            cursor.close()\n            return True, {"affected_rows": affected_rows, "message": "Query executed successfully"}\n\n    except Exception as e:\n        return False, {"error": str(e)}\n\n\n@server.tool("postgres_connect", description="Test PostgreSQL database connection")\ndef postgres_connect(\n    host: Annotated[str, Field(description="PostgreSQL host")] = "localhost",\n    port: Annotated[int, Field(description="PostgreSQL port", ge=1, le=65535)] = 5432,\n    user: Annotated[str, Field(description="PostgreSQL username")] = "postgres",\n    password: Annotated[str, Field(description="PostgreSQL password")] = "",\n    database: Annotated[str, Field(description="Database name")] = "postgres",\n) -> str:\n    """Test connection to PostgreSQL database."""\n    connection = get_postgres_connection(host, port, user, password, database)\n\n    if connection:\n        try:\n            cursor = connection.cursor()\n            cursor.execute("SELECT version();")\n            version = cursor.fetchone()[0]\n            cursor.close()\n            connection.close()\n            return f"Successfully connected to PostgreSQL database \'{database}\' on {host}:{port}\\nPostgreSQL Version: {version}"\n        except Exception as e:\n            connection.close()\n            return f"Connected but failed to get version: {str(e)}"\n    else:\n        return f"Failed to connect to PostgreSQL database \'{database}\' on {host}:{port}"\n\n\n@server.tool("postgres_execute_query", description="Execute a PostgreSQL query")\ndef postgres_execute_query(\n    query: Annotated[str, Field(description="SQL query to execute")],\n    host: Annotated[str, Field(description="PostgreSQL host")] = "localhost",\n    port: Annotated[int, Field(description="PostgreSQL port", ge=1, le=65535)] = 5432,\n    user: Annotated[str, Field(description="PostgreSQL username")] = "postgres",\n    password: Annotated[str, Field(description="PostgreSQL password")] = "",\n    database: Annotated[str, Field(description="Database name")] = "postgres",\n    limit: Annotated[\n        int, Field(description="Limit results for SELECT queries", ge=1, le=1000)\n    ] = 100,\n) -> str:\n    """Execute a SQL query on PostgreSQL database."""\n    connection = get_postgres_connection(host, port, user, password, database)\n\n    if not connection:\n        return f"Failed to connect to PostgreSQL database \'{database}\' on {host}:{port}"\n\n    try:\n        # Add LIMIT to SELECT queries if not already present\n        print("QQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQ")\n        print(query)\n\n        if query.strip().upper().startswith(\'SELECT\') and \'LIMIT\' not in query.upper():\n            l = limit\n            query = f"{query.strip().rstrip(\';\')} LIMIT {l};"\n\n        print(query)\n\n        success, result = execute_postgres_query(connection, query)\n        connection.close()\n\n        if not success:\n            return f"Query failed: {result.get(\'error\', \'Unknown error\')}"\n\n        if isinstance(result, list):\n            # Format SELECT results\n            if not result:\n                return "Query executed successfully. No rows returned."\n\n            # Create a formatted table\n            output = f"Query results ({len(result)} rows):\\n\\n"\n\n            if result:\n                # Get column names\n                columns = list(result[0].keys())\n\n                # Calculate column widths\n                col_widths = {}\n                for col in columns:\n                    col_widths[col] = max(\n                        len(col), max(len(str(row.get(col, \'\'))) for row in result)\n                    )\n\n                # Create header\n                header = " | ".join(col.ljust(col_widths[col]) for col in columns)\n                separator = " | ".join("-" * col_widths[col] for col in columns)\n\n                output += header + "\\n"\n                output += separator + "\\n"\n\n                # Add data rows\n                for row in result:\n                    row_str = " | ".join(\n                        str(row.get(col, \'\')).ljust(col_widths[col]) for col in columns\n                    )\n                    output += row_str + "\\n"\n\n            return output\n        else:\n            # Non-SELECT query result\n            return f"Query executed successfully. {result.get(\'message\', \'\')} Affected rows: {result.get(\'affected_rows\', 0)}"\n\n    except Exception as e:\n        if connection:\n            connection.close()\n        return f"Error executing query: {str(e)}"\n\n\n@server.tool("postgres_list_tables", description="List all tables in the database")\ndef postgres_list_tables(\n    host: Annotated[str, Field(description="PostgreSQL host")] = "localhost",\n    port: Annotated[int, Field(description="PostgreSQL port", ge=1, le=65535)] = 5432,\n    user: Annotated[str, Field(description="PostgreSQL username")] = "postgres",\n    password: Annotated[str, Field(description="PostgreSQL password")] = "",\n    database: Annotated[str, Field(description="Database name")] = "postgres",\n    schema: Annotated[str, Field(description="Schema name")] = "public",\n) -> str:\n    """List all tables in the PostgreSQL database."""\n    query = f"""\n    SELECT table_name, table_type\n    FROM information_schema.tables\n    WHERE table_schema = \'{schema}\'\n    ORDER BY table_name;\n    """\n    return postgres_execute_query(query, host, port, user, password, database)\n\n\n@server.tool("postgres_describe_table", description="Describe the structure of a table")\ndef postgres_describe_table(\n    table_name: Annotated[str, Field(description="Table name to describe")],\n    host: Annotated[str, Field(description="PostgreSQL host")] = "localhost",\n    port: Annotated[int, Field(description="PostgreSQL port", ge=1, le=65535)] = 5432,\n    user: Annotated[str, Field(description="PostgreSQL username")] = "postgres",\n    password: Annotated[str, Field(description="PostgreSQL password")] = "",\n    database: Annotated[str, Field(description="Database name")] = "postgres",\n    schema: Annotated[str, Field(description="Schema name")] = "public",\n) -> str:\n    """Describe the structure of a PostgreSQL table."""\n    query = f"""\n    SELECT\n        column_name,\n        data_type,\n        character_maximum_length,\n        is_nullable,\n        column_default\n    FROM information_schema.columns\n    WHERE table_schema = \'{schema}\' AND table_name = \'{table_name}\'\n    ORDER BY ordinal_position;\n    """\n    return postgres_execute_query(query, host, port, user, password, database)\n\n\n@server.tool("postgres_list_databases", description="List all databases")\ndef postgres_list_databases(\n    host: Annotated[str, Field(description="PostgreSQL host")] = "localhost",\n    port: Annotated[int, Field(description="PostgreSQL port", ge=1, le=65535)] = 5432,\n    user: Annotated[str, Field(description="PostgreSQL username")] = "postgres",\n    password: Annotated[str, Field(description="PostgreSQL password")] = "",\n) -> str:\n    """List all databases on the PostgreSQL server."""\n    # Use postgres database as default for this query\n    query = "SELECT datname as database_name FROM pg_database WHERE datistemplate = false ORDER BY datname;"\n    return postgres_execute_query(query, host, port, user, password, "postgres")\n\n\n@server.tool("postgres_table_stats", description="Get statistics for a table")\ndef postgres_table_stats(\n    table_name: Annotated[str, Field(description="Table name")],\n    host: Annotated[str, Field(description="PostgreSQL host")] = "localhost",\n    port: Annotated[int, Field(description="PostgreSQL port", ge=1, le=65535)] = 5432,\n    user: Annotated[str, Field(description="PostgreSQL username")] = "postgres",\n    password: Annotated[str, Field(description="PostgreSQL password")] = "",\n    database: Annotated[str, Field(description="Database name")] = "postgres",\n    schema: Annotated[str, Field(description="Schema name")] = "public",\n) -> str:\n    """Get statistics for a PostgreSQL table."""\n    query = f"""\n    SELECT\n        schemaname,\n        tablename,\n        attname as column_name,\n        n_distinct,\n        correlation\n    FROM pg_stats\n    WHERE schemaname = \'{schema}\' AND tablename = \'{table_name}\'\n    ORDER BY attname;\n    """\n    return postgres_execute_query(query, host, port, user, password, database)\n\n\n@server.tool("postgres_table_size", description="Get size information for tables")\ndef postgres_table_size(\n    host: Annotated[str, Field(description="PostgreSQL host")] = "localhost",\n    port: Annotated[int, Field(description="PostgreSQL port", ge=1, le=65535)] = 5432,\n    user: Annotated[str, Field(description="PostgreSQL username")] = "postgres",\n    password: Annotated[str, Field(description="PostgreSQL password")] = "",\n    database: Annotated[str, Field(description="Database name")] = "postgres",\n    schema: Annotated[str, Field(description="Schema name")] = "public",\n) -> str:\n    """Get size information for all tables in the schema."""\n    query = f"""\n    SELECT\n        table_name,\n        pg_size_pretty(pg_total_relation_size(schemaname||\'.\'||tablename)) AS size,\n        pg_size_pretty(pg_relation_size(schemaname||\'.\'||tablename)) AS table_size,\n        pg_size_pretty(pg_total_relation_size(schemaname||\'.\'||tablename) - pg_relation_size(schemaname||\'.\'||tablename)) AS index_size\n    FROM pg_tables\n    WHERE schemaname = \'{schema}\'\n    ORDER BY pg_total_relation_size(schemaname||\'.\'||tablename) DESC;\n    """\n    return postgres_execute_query(query, host, port, user, password, database)\n\n\n@server.tool("postgres_active_connections", description="Show active database connections")\ndef postgres_active_connections(\n    host: Annotated[str, Field(description="PostgreSQL host")] = "localhost",\n    port: Annotated[int, Field(description="PostgreSQL port", ge=1, le=65535)] = 5432,\n    user: Annotated[str, Field(description="PostgreSQL username")] = "postgres",\n    password: Annotated[str, Field(description="PostgreSQL password")] = "",\n    database: Annotated[str, Field(description="Database name")] = "postgres",\n) -> str:\n    """Show active connections to the PostgreSQL database."""\n    query = """\n    SELECT\n        pid,\n        usename,\n        datname,\n        client_addr,\n        application_name,\n        state,\n        query_start,\n        state_change\n    FROM pg_stat_activity\n    WHERE state = \'active\'\n    ORDER BY query_start DESC;\n    """\n    return postgres_execute_query(query, host, port, user, password, database)\n\n\n@server.tool("postgres_create_backup", description="Create a backup using pg_dump")\ndef postgres_create_backup(\n    backup_type: Annotated[str, Field(description="Backup type: \'database\' or \'table\'")],\n    target_name: Annotated[str, Field(description="Database name or table name")],\n    host: Annotated[str, Field(description="PostgreSQL host")] = "localhost",\n    port: Annotated[int, Field(description="PostgreSQL port", ge=1, le=65535)] = 5432,\n    user: Annotated[str, Field(description="PostgreSQL username")] = "postgres",\n    password: Annotated[str, Field(description="PostgreSQL password")] = "",\n    database: Annotated[str, Field(description="Database name (for table backup)")] = "postgres",\n) -> str:\n    """Create a backup using pg_dump."""\n    import subprocess\n\n    try:\n        env = os.environ.copy()\n        env[\'PGPASSWORD\'] = password\n\n        if backup_type == "database":\n            cmd = [\n                "pg_dump",\n                f"--host={host}",\n                f"--port={port}",\n                f"--username={user}",\n                "--format=custom",\n                "--no-password",\n                target_name,\n            ]\n            backup_file = f"{target_name}_backup.dump"\n        elif backup_type == "table":\n            cmd = [\n                "pg_dump",\n                f"--host={host}",\n                f"--port={port}",\n                f"--username={user}",\n                "--format=custom",\n                "--no-password",\n                f"--table={target_name}",\n                database,\n            ]\n            backup_file = f"{database}_{target_name}_backup.dump"\n        else:\n            return "Error: backup_type must be \'database\' or \'table\'"\n\n        # Execute pg_dump\n        with open(backup_file, \'wb\') as f:\n            result = subprocess.run(\n                cmd, stdout=f, stderr=subprocess.PIPE, env=env, timeout=300, check=False\n            )\n\n        if result.returncode == 0:\n            return f"Backup created successfully: {backup_file}"\n        else:\n            return f"Backup failed: {result.stderr.decode()}"\n\n    except Exception as e:\n        return f"Backup failed: {str(e)}"\n\n\n@server.tool("postgres_analyze_table", description="Analyze a table to update statistics")\ndef postgres_analyze_table(\n    table_name: Annotated[str, Field(description="Table name to analyze")],\n    host: Annotated[str, Field(description="PostgreSQL host")] = "localhost",\n    port: Annotated[int, Field(description="PostgreSQL port", ge=1, le=65535)] = 5432,\n    user: Annotated[str, Field(description="PostgreSQL username")] = "postgres",\n    password: Annotated[str, Field(description="PostgreSQL password")] = "",\n    database: Annotated[str, Field(description="Database name")] = "postgres",\n) -> str:\n    """Analyze a PostgreSQL table to update statistics."""\n    return postgres_execute_query(f"ANALYZE {table_name};", host, port, user, password, database)\n\n\n# Static resource\n@server.resource("config://postgres_settings")\ndef get_postgres_settings():\n    return {\n        "default_port": 5432,\n        "default_host": "localhost",\n        "default_schema": "public",\n        "max_query_limit": 1000,\n        "supported_operations": [\n            "SELECT",\n            "INSERT",\n            "UPDATE",\n            "DELETE",\n            "CREATE",\n            "DROP",\n            "ALTER",\n        ],\n        "backup_formats": ["custom", "plain", "tar"],\n    }\n\n\n# Dynamic resource template\n@server.resource("database://{database_name}/schema_info")\ndef get_database_schema_info(database_name: str):\n    return {\n        "database": database_name,\n        "note": "Use postgres_list_tables and postgres_describe_table tools to get actual schema information",\n    }\n\n\n@server.prompt()\ndef postgres_query_prompt(query_type: str) -> str:\n    """Generate prompts for PostgreSQL query construction."""\n    prompts = {\n        "select": "To write a SELECT query:\\nSELECT column1, column2 FROM schema.table_name WHERE condition ORDER BY column LIMIT n;",\n        "insert": "To write an INSERT query:\\nINSERT INTO schema.table_name (column1, column2) VALUES (value1, value2);",\n        "update": "To write an UPDATE query:\\nUPDATE schema.table_name SET column1 = value1 WHERE condition;",\n        "delete": "To write a DELETE query:\\nDELETE FROM schema.table_name WHERE condition;",\n        "create": "To create a table:\\nCREATE TABLE schema.table_name (column1 datatype constraints, column2 datatype constraints);",\n        "index": "To create an index:\\nCREATE INDEX index_name ON schema.table_name (column1, column2);",\n        "jsonb": "For JSONB operations:\\nSELECT data->>\'key\' FROM table WHERE data @> \'{\\"key\\": \\"value\\"}\';",\n    }\n\n    return prompts.get(query_type, f"PostgreSQL query guidance for: {query_type}")\n\n\nfrom clarifai.runners.models.mcp_class import MCPModelClass\n\n\nclass MyModelClass(MCPModelClass):\n    def get_server(self) -> FastMCP:\n        return server',le='import asyncio\nimport os\n\nfrom clarifai.urls.helper import ClarifaiUrlHelper\nfrom fastmcp import Client\nfrom fastmcp.client.transports import StreamableHttpTransport\n\nPAT = os.environ[\'CLARIFAI_PAT\']\nurl = ClarifaiUrlHelper().mcp_api_url()  # get url from the current clarifai config\n\ntransport = StreamableHttpTransport(url=url, headers={"Authorization": "Bearer " + PAT})\n\n\nasync def main():\n    print("=== PostgreSQL MCP Server Examples ===\\n")\n\n    # Note: These examples assume you have PostgreSQL credentials\n    # Set environment variables: POSTGRES_USER, POSTGRES_PASSWORD, POSTGRES_HOST, POSTGRES_DATABASE\n\n    postgres_config = {\n        "host": os.environ.get("POSTGRES_HOST", "localhost"),\n        "port": os.environ.get("POSTGRES_PORT", "5432"),\n        "user": os.environ.get("POSTGRES_USER", "postgres"),\n        "password": os.environ.get("POSTGRES_PASSWORD", ""),\n        "database": os.environ.get("POSTGRES_DATABASE", "postgres"),\n    }\n\n    async with Client(transport) as client:\n        # List available tools first\n        print("Available tools:")\n        tools = await client.list_tools()\n        for tool in tools:\n            print(f"- {tool.name}: {tool.description}")\n        print("\\n" + "=" * 50 + "\\n")\n\n        # Example 1: Test PostgreSQL connection\n        print("1. Testing PostgreSQL connection:")\n        try:\n            result = await client.call_tool("postgres_connect", postgres_config)\n            print(result[0].text)\n        except Exception as e:\n            print(f"Error: {e}")\n        print("\\n" + "=" * 50 + "\\n")\n\n        # Example 2: List databases\n        print("2. Listing databases:")\n        try:\n            result = await client.call_tool(\n                "postgres_list_databases",\n                {\n                    "host": postgres_config["host"],\n                    "port": postgres_config["port"],\n                    "user": postgres_config["user"],\n                    "password": postgres_config["password"],\n                },\n            )\n            print(result[0].text)\n        except Exception as e:\n            print(f"Error: {e}")\n        print("\\n" + "=" * 50 + "\\n")\n\n        # Example 3: List tables in database\n        print("3. Listing tables:")\n        try:\n            result = await client.call_tool("postgres_list_tables", postgres_config)\n            print(result[0].text)\n        except Exception as e:\n            print(f"Error: {e}")\n        print("\\n" + "=" * 50 + "\\n")\n\n        # Example 4: Execute a sample query\n        print("4. Executing a sample query (SELECT version):")\n        try:\n            result = await client.call_tool(\n                "postgres_execute_query",\n                {\n                    **postgres_config,\n                    "query": "SELECT version() as postgres_version, current_timestamp as current_time;",\n                },\n            )\n            print(result[0].text)\n        except Exception as e:\n            print(f"Error: {e}")\n        print("\\n" + "=" * 50 + "\\n")\n\n        # Example 5: Get table sizes\n        print("5. Getting table sizes:")\n        try:\n            result = await client.call_tool("postgres_table_size", postgres_config)\n            print(result[0].text)\n        except Exception as e:\n            print(f"Error: {e}")\n        print("\\n" + "=" * 50 + "\\n")\n\n    print("\\n" + "=" * 50)\n    print("Note: Set these environment variables for actual PostgreSQL connections:")\n    print("- POSTGRES_HOST (default: localhost)")\n    print("- POSTGRES_USER (default: postgres)")\n    print("- POSTGRES_PASSWORD")\n    print("- POSTGRES_DATABASE (default: postgres)")\n    print("=" * 50)\n\n\nif __name__ == "__main__":\n    asyncio.run(main())',de="clarifai\nanyio==4.9.0\nmcp==1.9.0\nfastmcp==2.3.4\npsycopg2-binary==2.9.9",ce="build_info:\n  python_version: '3.11'\ninference_compute_info:\n  cpu_limit: 1000m\n  cpu_memory: 1Gi\n  num_accelerators: 0\nmodel:\n  app_id: mcp-examples-app\n  id: postgres-mcp-server\n  model_type_id: text-to-text\n  user_id: mcp-examples-user",me='from __future__ import annotations\nimport os, re\nfrom dataclasses import dataclass, field\nfrom typing import Dict, List, Optional, Union\nfrom urllib.parse import urlparse, parse_qs, unquote\n\nimport httpx  # type: ignore\nfrom bs4 import BeautifulSoup  # type: ignore\nfrom mcp.server.fastmcp import FastMCP, Context  # type: ignore\n\nUSER_AGENT = (\n    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "\n    "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0 Safari/537.36"\n)\n\ntry:\n    # Optional SDK for DuckDuckGo (pip install duckduckgo_search)\n   from ddgs import DDGS  # type: ignore\nexcept ImportError:\n    DDGS = None\n\n# ---------------- Data Models ----------------\n@dataclass\nclass SearchResult:\n    id: int\n    title: str\n    url: str\n    snippet: str\n\n@dataclass\nclass PageCache:\n    url: str\n    text_lines: List[str]\n\n@dataclass\nclass SessionState:\n    last_query: Optional[str] = None\n    results: List[SearchResult] = field(default_factory=list)\n    opened_pages: Dict[str, PageCache] = field(default_factory=dict)\n    last_open_url: Optional[str] = None\n\nasync def ddg_sdk_search(query: str, topn: int, region: str, safesearch: str) -> List[SearchResult]:\n    """\n    Use duckduckgo_search AsyncDDGS instead of manual HTML scraping.\n    Falls back to manual method if SDK not available.\n    """\n    if DDGS is None:\n        raise RuntimeError("duckduckgo_search not installed")\n    # The library expects: region like \'wt-wt\', safesearch one of: \'moderate\',\'off\',\'strict\'\n    safe = safesearch.lower()\n    if safe not in {"moderate", "off", "strict"}:\n        safe = "moderate"\n    results: List[SearchResult] = []\n    try:\n        results_gen = DDGS().text(query, region=region, safesearch=safe, max_results=topn)\n        for r in results_gen:\n            # r keys commonly: title, href, body\n            title = (r.get("title") or "").strip()\n            url = (r.get("href") or "").strip()\n            snippet = (r.get("body") or "").strip()\n            if title and url.startswith("http"):\n                results.append(SearchResult(id=len(results) + 1, title=title, url=url, snippet=snippet))\n            if len(results) >= topn:\n                break\n    except Exception as e:\n        print(f"[ddg_sdk_search] Exception: {e}")\n        raise\n    return results\n\ndef _clean_duckduckgo_href(raw: str) -> str:\n    print(f"[_clean_duckduckgo_href] raw={raw}")\n    if not raw:\n        return ""\n    if raw.startswith("//"):\n        raw = "https:" + raw\n    p = urlparse(raw)\n    if p.netloc.endswith("duckduckgo.com"):\n        if p.path.startswith("/l/"):\n            qs = parse_qs(p.query)\n            if qs.get("uddg"):\n                target = unquote(qs["uddg"][0])\n                if "duckduckgo.com" in urlparse(target).netloc:\n                    return ""\n                return target\n        if p.path.endswith(".js") or p.path.startswith("/y.js"):\n            return ""\n    if not p.scheme:\n        return ""\n    return raw\n\ndef parse_duckduckgo(html: str, topn: int) -> List[SearchResult]:\n    print(f"[parse_duckduckgo] Parsing HTML, topn={topn}")\n    soup = BeautifulSoup(html, "html.parser")\n    results: List[SearchResult] = []\n    for block in soup.select("div.result"):\n        a = block.select_one("a.result__a")\n        if not a:\n            continue\n        title = a.get_text(" ", strip=True)\n        url = _clean_duckduckgo_href(a.get("href") or "")\n        if not url or "duckduckgo.com" in urlparse(url).netloc:\n            continue\n        snip_el = block.select_one(".result__snippet, .snippet")\n        snippet = snip_el.get_text(" ", strip=True) if snip_el else ""\n        if not title or not url.startswith("http"):\n            continue\n        results.append(SearchResult(id=len(results) + 1, title=title, url=url, snippet=snippet))\n        if len(results) >= topn:\n            break\n    print(f"[parse_duckduckgo] Found {len(results)} results")\n    return results\n\ndef format_search_results(results: List[SearchResult]) -> str:\n    print(f"[format_search_results] Formatting {len(results)} results")\n    if not results:\n        return "No results."\n    return "\\n\\n".join(\n        f"[{r.id}] {r.title}\\nURL: {r.url}\\nSnippet: {r.snippet or \'(no snippet)\'}"\n        for r in results\n    )\n\nasync def fetch_page(url: str, timeout: int = 20) -> PageCache:\n    print(f"[fetch_page] Fetching URL: {url} with timeout={timeout}")\n    headers = {"User-Agent": USER_AGENT, "Accept-Language": "en-US,en;q=0.9"}\n    try:\n        async with httpx.AsyncClient(timeout=timeout, follow_redirects=True) as client:\n            r = await client.get(url, headers=headers)\n            print(f"[fetch_page] HTTP status: {r.status_code}")\n            r.raise_for_status()\n            html = r.text\n    except Exception as e:\n        print(f"[fetch_page] Exception: {e}")\n        raise\n    soup = BeautifulSoup(html, "html.parser")\n    for tag in soup(["script", "style", "noscript"]):\n        tag.decompose()\n    text = soup.get_text("\\n")\n    lines = [ln.rstrip() for ln in text.splitlines() if ln.strip()]\n    print(f"[fetch_page] Extracted {len(lines)} lines")\n    return PageCache(url=url, text_lines=lines)\n\n# ---------------- FastMCP Server (in-process) ----------------\nmcp = FastMCP(\n    name="duckduckgo-browser",\n    instructions=r"""\nTool for browsing.\nThe `cursor` appears in brackets before each browsing display: `[{cursor}]`.\nCite information from the tool using the following format:\n`\u3010{cursor}\u2020L{line_start}(-L{line_end})?\u3011`, for example: `\u30106\u2020L9-L11\u3011` or `\u30108\u2020L3\u3011`. \nDo not quote more than 10 words directly from the tool output.\nsources=web\n""".strip(),\n    port=8001,\n)\n\n@mcp.tool(\n    name="search",\n    title="Search for information",\n    description=\n    "Searches for information related to `query` and displays `topn` results.",\n)\nasync def search(query: str, topn: int = 10,\n                 region: str = "wt-wt", safesearch: str = "moderate") -> str:\n    print(f"[search] Query: {query}, topn={topn}, region={region}, safesearch={safesearch}")\n    \n    try:\n        print("[search] Using DuckDuckGo SDK")\n        results = await ddg_sdk_search(query, topn, region, safesearch)\n           \n    except Exception as e:  # noqa: BLE001\n        print(f"[search] Exception: {e}")\n        return f"Search error: {e}"\n    return format_search_results(results)\n\n@mcp.tool(\n    name="open",\n    title="Open a link or page",\n    description="""\nOpens the link `id` from the page indicated by `cursor` starting at line number `loc`, showing `num_lines` lines.\nValid link ids are displayed with the formatting: `\u3010{id}\u2020.*\u3011`.\nIf `cursor` is not provided, the most recent page is implied.\nIf `id` is a string, it is treated as a fully qualified URL associated with `source`.\nIf `loc` is not provided, the viewport will be positioned at the beginning of the document or centered on the most relevant passage, if available.\nUse this function without `id` to scroll to a new location of an opened page.\n""".strip(),\n)\nasync def open(id: Union[int, str] = -1,\n               loc: int = 0, num_lines: int = 60) -> str:\n    print(f"[open] id={id}, loc={loc}, num_lines={num_lines}")\n    if isinstance(id, int) and id != -1:\n        print("[open] Invalid id (int but not -1)")\n        return "Result id not found. Run search."\n    else:\n        if not isinstance(id, str) or not id.startswith("http"):\n            print("[open] Invalid id (not a valid URL)")\n            return "Provide result id or full http(s) URL."\n        url = id\n    try:\n        page = await fetch_page(url)\n    except Exception as e:  # noqa: BLE001\n        print(f"[open] Exception: {e}")\n        return f"Open failed: {e}"\n    total = len(page.text_lines)\n    if loc < 0: loc = 0\n    if num_lines <= 0: num_lines = 60\n    end = min(total, loc + num_lines)\n    body = "\\n".join(f"L{loc+i+1}: {line}" for i, line in enumerate(page.text_lines[loc:end]))\n    print(f"[open] Returning lines {loc+1}-{end} of {total}")\n    return f"URL: {url}\\nLines {loc+1}-{end} of {total}\\n" + "-"*60 + "\\n" + body\n\n@mcp.tool(\n    name="find",\n    title="Find pattern in page",\n    description=\n    "Finds exact matches of `pattern` in the current page, or the page given by `cursor`.",\n)\nasync def find(pattern: str, url: Optional[str] = None, max_matches: int = 50) -> str:\n    print(f"[find] pattern={pattern}, url={url}, max_matches={max_matches}")\n    if not url:\n        print("[find] No URL provided")\n        return "No page open."\n    try:\n        page = await fetch_page(url)\n    except Exception as e:\n        print(f"[find] Exception: {e}")\n        return f"Fetch failed: {e}"\n    rx = re.compile(re.escape(pattern), re.IGNORECASE)\n    hits: List[str] = []\n    for i, line in enumerate(page.text_lines, start=1):\n        if rx.search(line):\n            hits.append(f"L{i}: {rx.sub(lambda m: \'**\'+m.group(0)+\'**\', line)}")\n            if len(hits) >= max_matches:\n                break\n    print(f"[find] Found {len(hits)} matches")\n    return "\\n".join(hits) if hits else f"No matches for \'{pattern}\'."\n\n@mcp.resource("config://browser_search_settings")\ndef get_browser_search_settings() -> Dict[str, str]:\n    return {\n        "available_tools": ["search", "open", "find"],\n    }\n    \n@mcp.prompt()\ndef web_browsing_prompt(task_type: str) -> str:\n    """\n    Generate a usage prompt for web browsing and extraction tasks, tailored to the available tools in this file.\n    """\n    prompts = {\n        "content_research": (\n            "To research web content:\\n"\n            "1. Use the \'search\' tool to find relevant pages.\\n"\n            "2. Use \'open\' to read the content of a result or URL.\\n"\n            "3. Use \'find\' to locate specific information within an opened page."\n        ),\n        "data_extraction": (\n            "To extract structured data:\\n"\n            "1. Use \'search\' to find pages with the data you need.\\n"\n            "2. Use \'open\' to load the page content.\\n"\n            "3. Use \'find\' to extract or locate specific patterns or fields."\n        ),\n        "website_analysis": (\n            "To analyze a website:\\n"\n            "1. Use \'search\' to discover relevant pages.\\n"\n            "2. Use \'open\' to inspect the content of those pages.\\n"\n            "3. Use \'find\' to search for keywords, features, or patterns."\n        ),\n        "competitive_research": (\n            "To research competitors:\\n"\n            "1. Use \'search\' to find competitor websites or pages.\\n"\n            "2. Use \'open\' to review their content.\\n"\n            "3. Use \'find\' to compare features, pricing, or other details."\n        ),\n        "market_research": (\n            "To conduct market research:\\n"\n            "1. Use \'search\' for industry trends and news.\\n"\n            "2. Use \'open\' to read relevant articles or reports.\\n"\n            "3. Use \'find\' to extract market insights or statistics."\n        ),\n        "content_monitoring": (\n            "To monitor web content:\\n"\n            "1. Use \'search\' to discover new or updated content.\\n"\n            "2. Use \'open\' to review the latest pages.\\n"\n            "3. Use \'find\' to detect changes or specific updates."\n        ),\n    }\n    # Default fallback if task_type is not recognized\n    return prompts.get(\n        task_type,\n        (\n            "Web browsing guidance:\\n"\n            "\u2022 Use \'search\' to find information.\\n"\n            "\u2022 Use \'open\' to read a page.\\n"\n            "\u2022 Use \'find\' to locate details within a page."\n        ),\n    )\n\n\nfrom clarifai.runners.models.mcp_class import MCPModelClass\n\nclass MyBrowserSearchToolClass(MCPModelClass):\n    def get_server(self) -> FastMCP:\n        return mcp\n\n# Main function to run the MCP server\nif __name__ == "__main__":\n    import asyncio\n    import sys\n    \n    # Simple approach - just run the server\n    try:\n        asyncio.run(mcp.run())\n    except KeyboardInterrupt:\n        print("Server stopped by user", file=sys.stderr)\n        sys.exit(0)\n    except Exception as e:\n        print(f"Server error: {e}", file=sys.stderr)\n        sys.exit(1)',pe='import asyncio\nimport os\nimport json\nfrom openai import AsyncOpenAI\nfrom fastmcp import Client\nfrom fastmcp.client.transports import StreamableHttpTransport\nfrom clarifai.urls.helper import ClarifaiUrlHelper\n\nAPI_KEY = os.getenv("CLARIFAI_PAT", "None")  # Set env var; avoid hardcoding secrets.\n\ntransport = StreamableHttpTransport(\n    url=ClarifaiUrlHelper().mcp_api_url(),\n    headers={"Authorization": f"Bearer {API_KEY}"},\n)\n\nopenai_client = AsyncOpenAI(\n    api_key=API_KEY,\n    base_url="https://api.clarifai.com/v2/ext/openai/v1"\n)\n\nMODEL_ID = "https://clarifai.com/openai/chat-completion/models/gpt-oss-120b"\n\ndef format_tools_to_openai_function(tools):\n    return [\n        {\n            "type": "function",\n            "function": {\n                "name": tool.name,\n                "description": f"[{tool.name}] {tool.description}",\n                "parameters": tool.inputSchema,\n            },\n        }\n        for tool in tools\n    ]\n\nasync def first_model_call(messages, tools):\n    return await openai_client.chat.completions.create(\n        model=MODEL_ID,\n        messages=messages,\n        temperature=0.4,\n        tools=tools,\n        tool_choice="auto",\n        stream=False,\n    )\n\nasync def final_model_call(messages):\n    # Force answer without further tool use\n    return await openai_client.chat.completions.create(\n        model=MODEL_ID,\n        messages=messages,\n        temperature=0.4,\n        tool_choice="none",\n        stream=False,\n    )\n\nasync def run_two_step_answer(user_prompt: str):\n    # 1. Base messages\n    messages = [\n        {\n            "role": "system",\n            "content": (\n                "You can call tools only in the FIRST response if needed to gather info. "\n                "After tool results are provided, you MUST produce the final answer without more tool calls."\n            ),\n        },\n        {"role": "user", "content": user_prompt},\n    ]\n\n    # Load tool definitions\n    async with Client(transport) as client:\n        tools_raw = await client.list_tools()\n    tools = format_tools_to_openai_function(tools_raw)\n\n    # 2. First model pass (may request tool calls)\n    first_resp = await first_model_call(messages, tools)\n    first_msg = first_resp.choices[0].message\n    messages.append({\n        "role": "assistant",\n        "content": first_msg.content,\n        "tool_calls": getattr(first_msg, "tool_calls", None)\n    })\n\n    # If no tool calls, just answer now\n    if not first_msg.tool_calls:\n        print("Assistant (no tools needed):", first_msg.content)\n        return first_msg.content\n\n    # 3. Execute ALL requested tool calls once\n    async with Client(transport) as client:\n        for tool_call in first_msg.tool_calls:\n            tool_name = tool_call.function.name\n            raw_args = tool_call.function.arguments or "{}"\n            try:\n                args = json.loads(raw_args)\n            except json.JSONDecodeError:\n                args = {}\n            print(f"\\n== Executing tool: {tool_name} | args: {raw_args}")\n            try:\n                result = await client.call_tool(tool_name, arguments=args)\n                # Collect text parts\n                if hasattr(result, "content"):\n                    parts = [getattr(seg, "text", "") for seg in result.content if getattr(seg, "text", "")]\n                    result_text = "\\n".join(parts) if parts else str(result)\n                else:\n                    result_text = str(result)\n            except Exception as e:\n                result_text = f"Tool {tool_name} failed: {e}"\n                print(result_text)\n\n            if len(result_text) > 4000:\n                result_text = result_text[:3500] + "\\n...[truncated]..."\n            messages.append({\n                "role": "tool",\n                "tool_call_id": tool_call.id,\n                "content": result_text\n            })\n\n    # 4. Final model call (NO more tools)\n    messages.append({\n        "role": "system",\n        "content": "Use the tool outputs above to craft the final answer. Do not call tools again."\n    })\n    final_resp = await final_model_call(messages)\n    final_msg = final_resp.choices[0].message.content\n    print("\\nFinal Answer:\\n", final_msg)\n    return final_msg\n\nasync def main():\n    await run_two_step_answer("Who won the 2025 Ballon d\'Or?")\n\nif __name__ == "__main__":\n    asyncio.run(main())',ue="clarifai==11.7.5\nanyio==4.9.0\nmcp==1.9.0\nfastmcp==2.3.4\nrequests>=2.31.0\nbeautifulsoup4==4.12.2\nlxml>=4.9.3\nddgs",fe="build_info:\n  python_version: '3.11'\ninference_compute_info:\n  cpu_limit: 1000m\n  cpu_memory: 1Gi\n  num_accelerators: 0\nmodel:\n  app_id: test-mcp\n  id: browser-search-mcp-model\n  model_type_id: text-to-text\n  user_id: clarifai-user-id",_e='import os\nimport sys\n\nsys.path.append(os.path.dirname(__file__))\nfrom typing import Iterator, List\n\nfrom clarifai.runners.models.model_builder import ModelBuilder\nfrom clarifai.runners.models.openai_class import OpenAIModelClass\nfrom clarifai.runners.utils.data_types import Image\nfrom clarifai.runners.utils.data_utils import Param\nfrom clarifai.runners.utils.openai_convertor import build_openai_messages\nfrom clarifai.utils.logging import logger\nfrom openai import OpenAI\nfrom openai_server_starter import OpenAI_APIServer\n\n\nclass VLLMModel(OpenAIModelClass):\n    """\n    A custom runner that integrates with the Clarifai platform and uses Server inference\n    to process inputs, including text and images.\n    """\n\n    client = True  # This will be set in load_model method\n    model = True  # This will be set in load_model method\n\n    def load_model(self):\n        """Load the model here and start the server."""\n        os.path.join(os.path.dirname(__file__))\n\n        server_args = {\n            \'max_model_len\': \'8192\',\n            \'gpu_memory_utilization\': 0.95,\n            \'dtype\': \'auto\',\n            \'task\': \'auto\',\n            \'kv_cache_dtype\': \'auto\',\n            \'tensor_parallel_size\': 1,\n            \'chat_template\': None,\n            \'cpu_offload_gb\': 0.0,\n            \'quantization\': None,\n            \'port\': 23333,\n            \'host\': \'localhost\',\n            \'checkpoints\': \'runtime\'\n        }\n\n        stage = server_args.get("checkpoints")\n        if stage in ["build", "runtime"]:\n            config_path = os.path.dirname(os.path.dirname(__file__))\n            builder = ModelBuilder(config_path, download_validation_only=True)\n            checkpoints = builder.download_checkpoints(stage=stage)\n            server_args.update({"checkpoints": checkpoints})\n\n        if server_args.get("additional_list_args") == [\'\']:\n            server_args.pop("additional_list_args")\n\n        # Start server\n        # This line were generated by `upload` module\n        self.server = OpenAI_APIServer.from_vllm_backend(**server_args)\n\n        self.client = OpenAI(\n                api_key="notset",\n                base_url=VLLMModel.make_api_url(self.server.host, self.server.port))\n        self.model = self._get_model()\n\n        logger.info(f"OpenAI {self.model} model loaded successfully!")\n\n    def _get_model(self):\n        try:\n            return self.client.models.list().data[0].id\n        except Exception as e:\n            raise ConnectionError("Failed to retrieve model ID from API") from e\n\n    @staticmethod\n    def make_api_url(host: str, port: int, version: str = "v1") -> str:\n        return f"http://{host}:{port}/{version}"\n\n    @OpenAIModelClass.method\n    def predict(self,\n                prompt: str,\n                image: Image = None,\n                images: List[Image] = None,\n                chat_history: List[dict] = None,\n                max_tokens: int = Param(default=512, description="The maximum number of tokens to generate. Shorter token lengths will provide faster performance.", ),\n                temperature: float = Param(default=0.7, description="A decimal number that determines the degree of randomness in the response", ),\n                top_p: float = Param(default=0.95, description="An alternative to sampling with temperature, where the model considers the results of the tokens with top_p probability mass.", )\n                ) -> str:\n        """This is the method that will be called when the runner is run. It takes in an input and\n        returns an output.\n        """\n        openai_messages = build_openai_messages(prompt=prompt, image=image, images=images, messages=chat_history)\n        response = self.client.chat.completions.create(\n            model=self.model,\n            messages=openai_messages,\n            max_completion_tokens=max_tokens,\n            temperature=temperature,\n            top_p=top_p)\n        if response.usage and response.usage.prompt_tokens and response.usage.completion_tokens:\n            self.set_output_context(prompt_tokens=response.usage.prompt_tokens, completion_tokens=response.usage.completion_tokens)\n        return response.choices[0].message.content\n\n    @OpenAIModelClass.method\n    def generate(self,\n                prompt: str,\n                image: Image = None,\n                images: List[Image] = None,\n                chat_history: List[dict] = None,\n                max_tokens: int = Param(default=512, description="The maximum number of tokens to generate. Shorter token lengths will provide faster performance.", ),\n                temperature: float = Param(default=0.7, description="A decimal number that determines the degree of randomness in the response", ),\n                top_p: float = Param(default=0.95, description="An alternative to sampling with temperature, where the model considers the results of the tokens with top_p probability mass.", )\n                ) -> Iterator[str]:\n        """Example yielding a whole batch of streamed stuff back."""\n        openai_messages = build_openai_messages(prompt=prompt, image=image, images=images, messages=chat_history)\n        for chunk in self.client.chat.completions.create(\n            model=self.model,\n            messages=openai_messages,\n            max_completion_tokens=max_tokens,\n            temperature=temperature,\n            top_p=top_p,\n            stream=True):\n            if chunk.choices:\n                text = (chunk.choices[0].delta.content\n                        if (chunk and chunk.choices[0].delta.content) is not None else \'\')\n                yield text\n\n    def test(self):\n        try:\n            print("Testing predict...")\n            print(self.predict(prompt="Explain why cat can\'t fly"))\n            print(self.predict(prompt="Describe this image", image=Image.from_url("https://samples.clarifai.com/metro-north.jpg")))\n        except Exception as e:\n            print(f"Error in predict {e}")\n\n        try:\n            print("Testing generate...")\n            for each in self.generate(prompt="Explain why cat can\'t fly"):\n                print(each, end=" ")\n            print()\n            for each in self.generate(prompt="Describe this image", image=Image.from_url("https://samples.clarifai.com/metro-north.jpg")):\n                print(each, end=" ")\n            print()\n        except Exception as e:\n            print(f"Error in generate {e}")',he='import os\nimport signal\nimport subprocess\nimport sys\nimport threading\nfrom typing import List\n\nimport psutil\nfrom clarifai.utils.logging import logger\n\nPYTHON_EXEC = sys.executable\n\n\ndef kill_process_tree(parent_pid, include_parent: bool = True, skip_pid: int = None):\n  """Kill the process and all its child processes."""\n  if parent_pid is None:\n    parent_pid = os.getpid()\n    include_parent = False\n\n  try:\n    itself = psutil.Process(parent_pid)\n  except psutil.NoSuchProcess:\n    return\n\n  children = itself.children(recursive=True)\n  for child in children:\n    if child.pid == skip_pid:\n      continue\n    try:\n      child.kill()\n    except psutil.NoSuchProcess:\n      pass\n\n  if include_parent:\n    try:\n      itself.kill()\n\n      # Sometime processes cannot be killed with SIGKILL (e.g, PID=1 launched by kubernetes),\n      # so we send an additional signal to kill them.\n      itself.send_signal(signal.SIGQUIT)\n    except psutil.NoSuchProcess:\n      pass\n\n\nclass OpenAI_APIServer:\n\n  def __init__(self, **kwargs):\n    self.server_started_event = threading.Event()\n    self.process = None\n    self.backend = None\n    self.server_thread = None\n\n  def __del__(self, *exc):\n    # This is important\n    # close the server when exit the program\n    self.close()\n\n  def close(self):\n    if self.process:\n      try:\n        kill_process_tree(self.process.pid)\n      except:\n        self.process.terminate()\n    if self.server_thread:\n      self.server_thread.join()\n\n  def wait_for_startup(self):\n    self.server_started_event.wait()\n\n  def validate_if_server_start(self, line: str):\n    line_lower = line.lower()\n    if self.backend in ["vllm", "sglang", "lmdeploy"]:\n      if self.backend == "vllm":\n        return "application startup complete" in line_lower or "vllm api server on" in line_lower\n      else:\n        return f" running on http://{self.host}:" in line.strip()\n    elif self.backend == "llamacpp":\n      return "waiting for new tasks" in line_lower\n    elif self.backend == "tgi":\n      return "Connected" in line.strip()\n\n  def _start_server(self, cmds):\n    try:\n      env = os.environ.copy()\n      env["VLLM_USAGE_SOURCE"] = "production-docker-image"\n      self.process = subprocess.Popen(\n          cmds,\n          stdout=subprocess.PIPE,\n          stderr=subprocess.STDOUT,\n          text=True,\n      )\n      for line in self.process.stdout:\n        logger.info("Server Log:  " + line.strip())\n        if self.validate_if_server_start(line):\n          self.server_started_event.set()\n          # break\n    except Exception as e:\n      if self.process:\n        self.process.terminate()\n      raise RuntimeError(f"Failed to start Server server: {e}")\n\n  def start_server_thread(self, cmds: str):\n    try:\n      # Start the  server in a separate thread\n      self.server_thread = threading.Thread(target=self._start_server, args=(cmds,), daemon=None)\n      self.server_thread.start()\n\n      # Wait for the server to start\n      self.wait_for_startup()\n    except Exception as e:\n      raise Exception(e)\n\n  @classmethod\n  def from_vllm_backend(cls,\n                        checkpoints,\n                        limit_mm_per_prompt: str = \'\',\n                        max_model_len: float = None,\n                        gpu_memory_utilization: float = 0.9,\n                        dtype="auto",\n                        task="auto",\n                        kv_cache_dtype: str = "auto",\n                        tensor_parallel_size=1,\n                        chat_template: str = None,\n                        cpu_offload_gb: float = 0.,\n                        quantization: str = None,\n                        port=23333,\n                        host="localhost",\n                        additional_list_args: List[str] = []):\n    """Run VLLM OpenAI compatible server\n\n    Args:\n      checkpoints (str): model id or path\n      limit_mm_per_prompt (str, optional): For each multimodal plugin, limit how many input instances to allow for each prompt. Expects a comma-separated list of items, e.g.: image=16,video=2 allows a maximum of 16 images and 2 videos per prompt. Defaults to 1 for each modality.\n      max_model_len (float, optional):Model context length. If unspecified, will be automatically derived from the model config. Defaults to None.\n      gpu_memory_utilization (float, optional): The fraction of GPU memory to be used for the model executor, which can range from 0 to 1. For example, a value of 0.5 would imply 50% GPU memory utilization. If unspecified, will use the default value of 0.9. This is a per-instance limit, and only applies to the current vLLM instance.It does not matter if you have another vLLM instance running on the same GPU. For example, if you have two vLLM instances running on the same GPU, you can set the GPU memory utilization to 0.5 for each instance. Defaults to 0.9.\n      dtype (str, optional): dtype. Defaults to "float16".\n      task (str, optional): The task to use the model for. Each vLLM instance only supports one task, even if the same model can be used for multiple tasks. When the model only supports one task, "auto" can be used to select it; otherwise, you must specify explicitly which task to use. Choices {auto, generate, embedding, embed, classify, score, reward, transcription}. Defaults to "auto".\n      kv_cache_dtype (str, optional): Data type for kv cache storage. If \u201cauto\u201d, will use model data type. CUDA 11.8+ supports fp8 (=fp8_e4m3) and fp8_e5m2. ROCm (AMD GPU) supports fp8 (=fp8_e4m3). Defaults to "auto".\n      tensor_parallel_size (int, optional): n gpus. Defaults to 1.\n      chat_template (str, optional): The file path to the chat template, or the template in single-line form for the specified model. Defaults to None.\n      cpu_offload_gb (float, optional): The space in GiB to offload to CPU, per GPU. Default is 0, which means no offloading. Intuitively, this argument can be seen as a virtual way to increase the GPU memory size. For example, if you have one 24 GB GPU and set this to 10, virtually you can think of it as a 34 GB GPU. Then you can load a 13B model with BF16 weight, which requires at least 26GB GPU memory. Note that this requires fast CPU-GPU interconnect, as part of the model is loaded from CPU memory to GPU memory on the fly in each model forward pass. Defaults to 0.\n      quantization (str, optional): quantization format {aqlm,awq,deepspeedfp,tpu_int8,fp8,fbgemm_fp8,modelopt,marlin,gguf,gptq_marlin_24,gptq_marlin,awq_marlin,gptq,compressed-tensors,bitsandbytes,qqq,hqq,experts_int8,neuron_quant,ipex,quark,moe_wna16,None}. Defaults to None.\n      port (int, optional): port. Defaults to 23333.\n      host (str, optional): host name. Defaults to "localhost".\n      additional_list_args (List[str], optional): additional args to run subprocess cmd e.g. ["--arg-name", "arg value"]. See more at [this document](https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html#vllm-serve). Defaults to [].\n\n    """\n    cmds = [\n        PYTHON_EXEC, \'-m\', \'vllm.entrypoints.openai.api_server\', \'--model\', checkpoints, \'--dtype\',\n        str(dtype), \'--task\',\n        str(task), \'--kv-cache-dtype\',\n        str(kv_cache_dtype), \'--tensor-parallel-size\',\n        str(tensor_parallel_size), \'--gpu-memory-utilization\',\n        str(gpu_memory_utilization), \'--cpu-offload-gb\',\n        str(cpu_offload_gb), \'--port\',\n        str(port), \'--host\',\n        str(host), "--trust-remote-code"\n    ]\n\n    if quantization:\n      cmds += [\n          \'--quantization\',\n          str(quantization),\n      ]\n    if chat_template:\n      cmds += [\n          \'--chat-template\',\n          str(chat_template),\n      ]\n    if max_model_len:\n      cmds += [\n          \'--max-model-len\',\n          str(max_model_len),\n      ]\n    if limit_mm_per_prompt:\n      cmds += [\n          \'--limit-mm-per-prompt\',\n          str(limit_mm_per_prompt),\n      ]\n\n    if additional_list_args != []:\n      cmds += additional_list_args\n\n    print("CMDS to run vllm server: ", cmds)\n\n    _self = cls()\n\n    _self.host = host\n    _self.port = port\n    _self.backend = "vllm"\n    _self.start_server_thread(cmds)\n    import time\n    time.sleep(5)\n\n    return _self',ge="torch==2.6.0\ntokenizers==0.21.1\naccelerate==1.2.0\noptimum==1.23.3\nxformers\neinops==0.8.0\npackaging\nninja\n\nqwen-vl-utils==0.0.8\ntimm==1.0.12\nopenai\nclarifai>=11.5.0,<12.0.0\npsutil\n\nvllm==0.8.5\ntransformers==4.51.1",xe='# Config file for the vLLM runner\n\nmodel:\n  id: "Qwen2_5-VL-3B-Instruct"\n  user_id: "user_id"\n  app_id: "app_id"\n  model_type_id: "multimodal-to-text"\n\nbuild_info:\n  python_version: "3.11"\n\ninference_compute_info:\n  cpu_limit: "3"\n  cpu_memory: "14Gi"\n  num_accelerators: 1\n  accelerator_type: ["NVIDIA-*"]\n  accelerator_memory: "44Gi"\n\ncheckpoints:\n  type: "huggingface"\n  repo_id: "Qwen/Qwen2.5-VL-3B-Instruct"\n  hf_token: "hf_token"',ye='import os\nfrom io import BytesIO\n\n# Third-party imports\nimport torch\nfrom PIL import Image as PILImage\nfrom transformers import AutoTokenizer, AutoProcessor, AutoModelForImageTextToText\n\n# Clarifai imports\nfrom clarifai.runners.models.model_builder import ModelBuilder\nfrom clarifai.runners.models.model_class import ModelClass\nfrom clarifai.runners.utils.data_types import Image\nfrom clarifai.runners.utils.data_utils import Param\nfrom clarifai.utils.logging import logger\n\nDEFAULT_PROMPT = """Extract the text from the above document as if you were reading it naturally. Return the tables in html format. Return the equations in LaTeX representation. If there is an image in the document and image caption is not present, add a small description of the image inside the <img></img> tag; otherwise, add the image caption inside <img></img>. Watermarks should be wrapped in brackets. Ex: <watermark>OFFICIAL COPY</watermark>. Page numbers should be wrapped in brackets. Ex: <page_number>14</page_number> or <page_number>9/22</page_number>. Prefer using \u2610 and \u2611 for check boxes."""\n\n\ndef preprocess_image(image_bytes: bytes) -> PILImage:\n    """Convert image bytes to PIL Image."""\n    return PILImage.open(BytesIO(image_bytes)).convert("RGB")\n\n\nclass MyRunner(ModelClass):\n    """A custom runner that loads the OCR model and runs it on the input image."""\n\n    def load_model(self):\n        """Load the model here."""\n\n        model_path = os.path.dirname(os.path.dirname(__file__))\n        builder = ModelBuilder(model_path, download_validation_only=True)\n        checkpoint_path = builder.download_checkpoints(stage="runtime")\n\n        self.device = "cuda" if torch.cuda.is_available() else "cpu"\n        logger.info(f"Running on device: {self.device}")\n\n        self.model = AutoModelForImageTextToText.from_pretrained(checkpoint_path).to(\n            self.device\n        )\n        self.model.eval()\n        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n        self.processor = AutoProcessor.from_pretrained(checkpoint_path)\n\n        logger.info("Done loading!")\n\n    @ModelClass.method\n    def predict(\n        self,\n        image: Image,\n        prompt: str = Param(\n            default=DEFAULT_PROMPT, description="The prompt to use for the OCR model."\n        ),\n        max_new_tokens: int = Param(\n            default=512,\n            description="The maximum number of tokens to generate. Shorter token lengths will provide faster performance.",\n        ),\n    ) -> str:\n        """This is the method that will be called when the runner is run. It takes in an input and returns an output."""\n        messages = [\n            {"role": "system", "content": "You are a helpful assistant."},\n            {\n                "role": "user",\n                "content": [\n                    {\n                        "type": "image_url",\n                        "image_url": f"data:image/png;base64,{image.bytes}",\n                    },\n                    {"type": "text", "text": prompt},\n                ],\n            },\n        ]\n        image = preprocess_image(image.bytes)\n        text = self.processor.apply_chat_template(\n            messages, tokenize=False, add_generation_prompt=True\n        )\n        inputs = self.processor(\n            text=[text], images=[image], padding=True, return_tensors="pt"\n        )\n        inputs = inputs.to(self.model.device)\n\n        output_ids = self.model.generate(\n            **inputs, max_new_tokens=max_new_tokens, do_sample=False\n        )\n        generated_ids = [\n            output_ids[len(input_ids) :]\n            for input_ids, output_ids in zip(inputs.input_ids, output_ids)\n        ]\n\n        output_text = self.processor.batch_decode(\n            generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n        )\n        return output_text[0]\n\n    def test(self):\n        """Test the model with a sample image."""\n        import requests  # Import moved here as it\'s only used for testing\n\n        # Load a sample image from the IAM database\n        url = "https://dl.a9t9.com/ocr/solarcell.jpg"\n        image = Image(bytes=requests.get(url).content)\n        # image = Image.from_url(url)\n        generated_text = self.predict(image)\n        # Log the detected text\n        logger.info(f"Detected text:\\n{generated_text}")',ve="clarifai>=11.5.2,<12.0.0\npillow==10.4.0\ntorch==2.6.0\ntorchvision==0.21.0\ntransformers>=4.51.1",ke='model:\n  id: "nanonets-ocr-s"\n  user_id: "user_id"\n  app_id: "app_id"\n  model_type_id: "multimodal-to-text"\n\nbuild_info:\n  python_version: "3.11"\n\ninference_compute_info:\n  cpu_limit: "2"\n  cpu_memory: "18Gi"\n  num_accelerators: 1\n  accelerator_type: ["NVIDIA-*"]\n  accelerator_memory: "18Gi"\n\ncheckpoints:\n  type: "huggingface"\n  repo_id: "nanonets/Nanonets-OCR-s"',be='from typing import List, Iterator\nimport os\nfrom clarifai.runners.models.openai_class import OpenAIModelClass\nfrom clarifai.runners.models.model_builder import ModelBuilder\n\nimport onnxruntime\nimport numpy as np\nfrom transformers import AutoTokenizer, PretrainedConfig\n\nfrom clarifai.utils.logging import logger\n\n#Helper function Mean pool function\ndef mean_pooling(model_output: np.ndarray, attention_mask: np.ndarray):\n            token_embeddings = model_output\n            input_mask_expanded = np.expand_dims(attention_mask, axis=-1)\n            input_mask_expanded = np.broadcast_to(input_mask_expanded, token_embeddings.shape)\n            sum_embeddings = np.sum(token_embeddings * input_mask_expanded, axis=1)\n            sum_mask = np.clip(np.sum(input_mask_expanded, axis=1), a_min=1e-9, a_max=None)\n            return sum_embeddings / sum_mask\n        \nclass Jinaai_embedding_v2(OpenAIModelClass):\n    """\n    A custom runner that integrates with the Clarifai platform and uses Jinaai embedding v2 model to process inputs, including text and images.\n    """\n    client = True\n    model = "jinaai-embedding-v3"\n    \n    def load_model(self):\n        """Load the model here and start the server."""\n        \n        # Load checkpoints\n        model_path = os.path.dirname(os.path.dirname(__file__))\n        builder = ModelBuilder(model_path, download_validation_only=True)\n        logger.info(f"\\nDownloading Jinaai {self.model} model checkpoints...\\n")\n        self.checkpoints =  builder.download_checkpoints(stage=\'runtime\')\n        logger.info(f"Checkpoints downloaded to {self.checkpoints}")\n        \n        #logger.info("Loading Jinaai embedding v3 model...")\n        self.tokenizer = AutoTokenizer.from_pretrained(self.checkpoints)\n        self.config = PretrainedConfig.from_pretrained(self.checkpoints)\n        \n        #logger.info(f"Tokenizer and config loaded from \'jinaai/jinaai-embeddings-v3\'")\n        self.session = onnxruntime.InferenceSession(self.checkpoints + \'/onnx/model.onnx\')\n        \n        # log that system is ready\n        logger.info(f"Jinaai {self.model} model loaded successfully!")\n    \n    def tokenize_and_embed(self, input: str):\n        """\n        Tokenize the input text and return the embedding vector.\n        \n        Args:\n            input (str): The input text to be tokenized and embedded.\n        \n        Returns:\n            np.ndarray: The embedding vector for the input text.\n        """\n        # Tokenize input\n        input_text = self.tokenizer(input, return_tensors=\'np\')\n           # Prepare inputs for ONNX model\n        task_type = \'text-matching\'\n        task_id = np.array(self.config.lora_adaptations.index(task_type), dtype=np.int64)\n        inputs = {\n            \'input_ids\': input_text[\'input_ids\'],\n            \'attention_mask\': input_text[\'attention_mask\'],\n            \'task_id\': task_id\n        }\n\n        # Run model\n        outputs = self.session.run(None, inputs)[0]\n        return outputs, input_text\n\n    @OpenAIModelClass.method\n    def predict(self,\n                input: str,) -> List[float]:\n        """\n        Predict method to process the input text and return the embedding vector.\n        \n        Args:\n            input (str): The input text to be processed.\n        \n        Returns:\n            List[float]: The embedding vector for the input text.\n       """\n        # Tokenize and embed the input text\n        outputs, input_text = self.tokenize_and_embed(input)\n        \n        # Apply mean pooling and normalization to the model outputs\n        embeddings = mean_pooling(outputs, input_text["attention_mask"])\n        embeddings = embeddings / np.linalg.norm(embeddings, ord=2, axis=1, keepdims=True)\n        \n        return embeddings[0].tolist()',we="transformers #4.52.4\nclarifai #11.4.10\nonnxruntime\nnumpy",Te='model:\n  id: "jinaai-embeddings-v3"\n  user_id: "user_id"\n  app_id: "app_id"\n  model_type_id: "text-embedder"\n\nbuild_info:\n  python_version: "3.11"\n\ninference_compute_info:\n  cpu_limit: "1"\n  cpu_memory: "5Gi"\n  num_accelerators: 0\n\ncheckpoints:\n  type: "huggingface"\n  repo_id: "jinaai/jina-embeddings-v3"',Ae='import os\nfrom typing import List\n\nfrom clarifai.runners.models.model_class import ModelClass\nfrom clarifai.runners.utils.data_utils import Param\nfrom clarifai.runners.utils.data_types import Image\nfrom clarifai.runners.models.model_builder import ModelBuilder\n\nfrom diffusers import FluxPipeline\nimport torch\n\n\nclass TextToImageModel(ModelClass):\n  """\n  A custom runner for the FLUX model that integrates with the Clarifai platform.\n  """\n\n  def load_model(self):\n    """Load the model here."""\n    # "black-forest-labs/FLUX.1-schnell"\n    \n    self.device = "cuda"\n    \n    model_path = os.path.dirname(os.path.dirname(__file__))\n    builder = ModelBuilder(model_path, download_validation_only=True)\n    checkpoints = builder.download_checkpoints(stage="runtime")\n    # load model and scheduler\n    self.pipeline = FluxPipeline.from_pretrained(\n      checkpoints,\n      torch_dtype=torch.bfloat16\n    )\n    \n    self.pipeline = self.pipeline.to(self.device)\n\n  @ModelClass.method\n  def predict(\n    self,\n    prompt: str,\n    num_inference_steps: int = Param(default=28, description="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference."),\n    guidance_scale: float = Param(default=3.5, description="The `guidance_scale` controls how strongly the model follows the conditioning input during generation."),\n    negative_prompt: str = Param(default="", description="The prompt to guide what to not include in image generation. Ignored when not using guidance (guidance_scale < 1)"),\n    true_cfg_scale: float = Param(default=1.0, description="When > 1.0 and a provided negative_prompt, enables true classifier-free guidance"),\n    height: int = Param(default=1024, description="The height in pixels of the generated image. This is set to 1024 by default for the best results."),\n    width: int = Param(default=1024, description="The width in pixels of the generated image. This is set to 1024 by default for the best results."),\n    max_sequence_length: int = Param(default=256, description="Maximum sequence length to use with the prompt"),\n    seed: int = Param(default=None, description="Seed value to make generation deterministic."),\n    # No need\n    sigmas: List[float] = None,\n  ) -> Image:\n    \n    image = self.pipeline(\n        prompt=prompt,\n        negative_prompt=negative_prompt,\n        guidance_scale=guidance_scale,\n        num_inference_steps=num_inference_steps,\n        max_sequence_length=max_sequence_length,\n        width=width,\n        height=height,\n        true_cfg_scale=true_cfg_scale,\n        generator=torch.Generator("cpu").manual_seed(seed) if seed else None,\n        sigmas=sigmas,\n    ).images[0]\n    \n    # this is important, delete all model cache to avoid OOM\n    torch.cuda.empty_cache()\n    \n    return Image.from_pil(image)\n  \n  \n  @ModelClass.method\n  def create(\n    self,\n    prompt: List[str],\n    prompt_2: List[str] = None,\n    negative_prompt: List[str] = None,\n    negative_prompt_2: List[str] = None,\n    true_cfg_scale: float = 1.0,\n    height: int = None,\n    width: int = None,\n    max_sequence_length: int = 256,\n    num_inference_steps: int = 28,\n    guidance_scale: float = 3.5,\n    seed: int = None,\n    sigmas: List[float] = None,\n  ) -> List[Image]:\n    """\n    Generate an image from the given prompt using the FLUX model.\n    Args:\n      * prompt (`List[str]`): The prompt or prompts to guide the image generation.\n      * prompt_2 (`List[str]`, *optional*): The prompt to be sent to `tokenizer_2` and `text_encoder_2`. If not defined, `prompt` is will be used instead.\n      * negative_prompt (`List[str]`, *optional*): The prompt to guide what to not include in image generation. Ignored when not using guidance (guidance_scale < 1).\n      * negative_prompt_2 (`List[str]`, *optional*): The negative_prompt to be sent to `tokenizer_2` and `text_encoder_2`. If not defined, `negative_prompt` is will be used instead.\n      * height (`int`, *optional*, defaults to model.unet.config.sample_size * model.vae_scale_factor): The height in pixels of the generated image. This is set to 1024 by default for the best results.\n      * width (`int`, *optional*, defaults to model.unet.config.sample_size * model.vae_scale_factor): The width in pixels of the generated image. This is set to 1024 by default for the best results.\n      * num_inference_steps (`int`, *optional*, defaults to 28): The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\n      * guidance_scale (`float`, *optional*, defaults to 3.5):\n          Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\n          `guidance_scale` is defined as `w` of equation 2. of [Imagen\n          Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >\n          1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,\n          usually at the expense of lower image quality.\n      * seed (`int`, *optional*, defaults to None):\n          Seed value passed to `torch.Generator("cpu").manual_seed(seed)` (see more [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)) to make generation deterministic.\n      * sigmas (`List[float]`, *optional*): Custom sigmas to use for the denoising process with schedulers which support a `sigmas` argument in their `set_timesteps` method. If not defined, the default behavior when `num_inference_steps` is passed will be used.\n      * max_sequence_length (`int` defaults to `256`): Maximum sequence length to use with the prompt.\n    \n    (see more at this [doc](https://huggingface.co/docs/diffusers/v0.32.2/en/api/pipelines/flux#diffusers.FluxPipeline.__call__))\n    """\n    assert isinstance(prompt, list), ValueError("prompt must be a list of string")\n    assert len(prompt) <= 4, ValueError(\n        f"The provided prompt length ({len(prompt)}) exceeds the maximum limit (4). Please reduce the number of prompts in `prompt`.")\n    images = self.pipeline(\n        prompt=prompt,\n        prompt_2=prompt_2,\n        negative_prompt=negative_prompt,\n        negative_prompt_2=negative_prompt_2,\n        guidance_scale=guidance_scale,\n        num_inference_steps=num_inference_steps,\n        max_sequence_length=max_sequence_length,\n        width=width,\n        height=height,\n        true_cfg_scale=true_cfg_scale,\n        generator=torch.Generator("cpu").manual_seed(seed) if seed else None,\n        sigmas=sigmas,\n    ).images\n    \n    # this is important, delete all model cache to avoid OOM\n    torch.cuda.empty_cache()\n    \n    return [Image.from_pil(image) for image in images]\n  \n\n  def test(self):\n    """ \n    Test cases only executed when running `clarifai model test-locally`\n    """\n    image = self.predict(\n    prompt="A Ghibli animated orange cat, panicked about a deadline, sits in front of a Banana-brand laptop.", \n    negative_prompt="Ugly, cute", guidance_scale=7)\n    print(image)\n\n    images = self.create(\n        prompt=["A Ghibli animated orange cat, panicked about a deadline, sits in front of a Banana-brand laptop."]*3, \n        negative_prompt=["Ugly, cute"]*2, guidance_scale=7)\n    print(images)',je="tokenizers==0.21.0\ntransformers>=4.48\ndiffusers==0.32.2\naccelerate==1.2.0\noptimum==1.23.3\nxformers\neinops==0.8.0\nrequests==2.32.3\nsentencepiece==0.2.0\nnumpy>2.0\nninja\naiohttp\npackaging\ntorch==2.5.1\nclarifai\nclarifai-protocol",Le='model: \n  id: "flux_1-schnell"\n  user_id: \n  app_id: \n  model_type_id: "text-to-image"\n\nbuild_info:\n  python_version: "3.11"\n\ninference_compute_info:\n  cpu_limit: "3"\n  cpu_memory: "18Gi"\n  num_accelerators: 1\n  accelerator_type: ["NVIDIA-L40S", "NVIDIA-A100", "NVIDIA-H100"]\n  accelerator_memory: "44Gi"\n\ncheckpoints:\n  type: huggingface\n  repo_id: black-forest-labs/FLUX.1-schnell\n  hf_token: ',Se={description:"Learn how to upload and customize your own models on the Clarifai platform",sidebar_position:5},Ie="Model Upload Examples",Ee={},Pe=[{value:"Hello World",id:"hello-world",level:2},{value:"NSFW Image Classifier",id:"nsfw-image-classifier",level:2},{value:"DETR ResNet Image Detector",id:"detr-resnet-image-detector",level:2},{value:"Image Segmenter",id:"image-segmenter",level:2},{value:"Mask2Former ADE",id:"mask2former-ade",level:3},{value:"Image-Text-to-Image",id:"image-text-to-image",level:2},{value:"Stable Diffusion 2 Depth",id:"stable-diffusion-2-depth",level:3},{value:"LLM",id:"llm",level:2},{value:"SmolLM2 1.7B Instruct (SGLang)",id:"smollm2-17b-instruct-sglang",level:3},{value:"LLaMA 3.2 1B Instruct (Hugging Face)",id:"llama-32-1b-instruct-hugging-face",level:3},{value:"LLaMA 3.2 3B Instruct (LMDeploy)",id:"llama-32-3b-instruct-lmdeploy",level:3},{value:"Gemma 3 1B Instruct (vLLM)",id:"gemma-3-1b-instruct-vllm",level:3},{value:"LLaMA 3.1 8B Tool Calling (vLLM)",id:"llama-31-8b-tool-calling-vllm",level:3},{value:"Local Runners",id:"local-runners",level:2},{value:"Ollama",id:"ollama",level:3},{value:"MCP",id:"mcp",level:2},{value:"Browser Tools",id:"browser-tools",level:3},{value:"Code Execution",id:"code-execution",level:3},{value:"Code Execution Without Docker Version",id:"code-execution-without-docker-version",level:3},{value:"Google Drive",id:"google-drive",level:3},{value:"Math",id:"math",level:3},{value:"Postgres",id:"postgres",level:3},{value:"Web Search",id:"web-search",level:3},{value:"Multimodal Models",id:"multimodal-models",level:2},{value:"Qwen2.5 VL 3B Instruct",id:"qwen25-vl-3b-instruct",level:3},{value:"OCR",id:"ocr",level:2},{value:"NanoNets OCR Small",id:"nanonets-ocr-small",level:3},{value:"Text Embedder",id:"text-embedder",level:2},{value:"Jina Embeddings v3",id:"jina-embeddings-v3",level:3},{value:"Text-to-Image",id:"text-to-image",level:2},{value:"FLUX Schnell",id:"flux-schnell",level:3}];function Ce(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components},{Details:t}=n;return t||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"model-upload-examples",children:"Model Upload Examples"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Learn how to upload and customize your own models on the Clarifai platform"})}),"\n",(0,s.jsx)("hr",{}),"\n",(0,s.jsx)(n.p,{children:"This section provides examples that guide you through uploading custom models to Clarifai. You\u2019ll learn how to adapt pre-built examples, configure model settings, and prepare your project for deployment on the platform."}),"\n",(0,s.jsxs)(n.admonition,{type:"tip",children:[(0,s.jsxs)(n.p,{children:["All the examples here are available in the Clarifai Runners Examples ",(0,s.jsx)(n.a,{href:"https://github.com/Clarifai/runners-examples",children:"GitHub repository"}),". You can use the Clarifai CLI to ",(0,s.jsx)(n.a,{href:"https://docs.clarifai.com/resources/api-overview/cli#initialize-with-github-template",children:"download a model template"}),". For example:"]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"clarifai model init --github-url https://github.com/Clarifai/runners-examples/tree/main/local-runners/ollama-model-upload\n"})})]}),"\n",(0,s.jsx)(n.p,{children:"To use these examples, create a project directory and organize your files as shown below. This structure is required for successfully uploading models to the Clarifai platform:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"your_model_directory/\n\u251c\u2500\u2500 1/\n\u2502   \u2514\u2500\u2500 model.py\n\u251c\u2500\u2500 requirements.txt\n\u2514\u2500\u2500 config.yaml\n"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"your_model_directory/"})})," \u2013 Root directory containing all files related to your custom model.","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"1/"})})," \u2013 A subdirectory (named ",(0,s.jsx)(n.code,{children:"1"}),") that contains the main model file."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"model.py"})})," \u2013 Defines your model logic, including loading the model and handling inference."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"requirements.txt"})})," \u2013 Lists all Python dependencies required to run your model."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"config.yaml"})})," \u2013 Includes metadata and configuration details for building the model, such as compute resources and environment settings."]}),"\n"]}),"\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n",(0,s.jsx)(n.h2,{id:"hello-world",children:"Hello World"}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"1/model.py"})}),(0,s.jsx)(i.A,{className:"language-text",children:a})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"requirements.txt"})}),(0,s.jsx)(i.A,{className:"language-text",children:l})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"config.yaml"})}),(0,s.jsx)(i.A,{className:"language-text",children:d})]}),"\n",(0,s.jsx)(n.h2,{id:"nsfw-image-classifier",children:"NSFW Image Classifier"}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"1/model.py"})}),(0,s.jsx)(i.A,{className:"language-text",children:c})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"requirements.txt"})}),(0,s.jsx)(i.A,{className:"language-text",children:m})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"config.yaml"})}),(0,s.jsx)(i.A,{className:"language-text",children:p})]}),"\n",(0,s.jsx)(n.h2,{id:"detr-resnet-image-detector",children:"DETR ResNet Image Detector"}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"1/model.py"})}),(0,s.jsx)(i.A,{className:"language-text",children:u})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"requirements.txt"})}),(0,s.jsx)(i.A,{className:"language-text",children:f})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"config.yaml"})}),(0,s.jsx)(i.A,{className:"language-text",children:_})]}),"\n",(0,s.jsx)(n.h2,{id:"image-segmenter",children:"Image Segmenter"}),"\n",(0,s.jsx)(n.h3,{id:"mask2former-ade",children:"Mask2Former ADE"}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"1/model.py"})}),(0,s.jsx)(i.A,{className:"language-text",children:h})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"requirements.txt"})}),(0,s.jsx)(i.A,{className:"language-text",children:g})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"config.yaml"})}),(0,s.jsx)(i.A,{className:"language-text",children:x})]}),"\n",(0,s.jsx)(n.h2,{id:"image-text-to-image",children:"Image-Text-to-Image"}),"\n",(0,s.jsx)(n.h3,{id:"stable-diffusion-2-depth",children:"Stable Diffusion 2 Depth"}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"1/model.py"})}),(0,s.jsx)(i.A,{className:"language-text",children:y})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"requirements.txt"})}),(0,s.jsx)(i.A,{className:"language-text",children:v})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"config.yaml"})}),(0,s.jsx)(i.A,{className:"language-text",children:k})]}),"\n",(0,s.jsx)(n.h2,{id:"llm",children:"LLM"}),"\n",(0,s.jsx)(n.h3,{id:"smollm2-17b-instruct-sglang",children:"SmolLM2 1.7B Instruct (SGLang)"}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"1/model.py"})}),(0,s.jsx)(i.A,{className:"language-text",children:b})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"1/openai_server_starter.py"})}),(0,s.jsx)(i.A,{className:"language-text",children:w})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"requirements.txt"})}),(0,s.jsx)(i.A,{className:"language-text",children:T})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"config.yaml"})}),(0,s.jsx)(i.A,{className:"language-text",children:A})]}),"\n",(0,s.jsx)(n.h3,{id:"llama-32-1b-instruct-hugging-face",children:"LLaMA 3.2 1B Instruct (Hugging Face)"}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"1/model.py"})}),(0,s.jsx)(i.A,{className:"language-text",children:j})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"requirements.txt"})}),(0,s.jsx)(i.A,{className:"language-text",children:L})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"config.yaml"})}),(0,s.jsx)(i.A,{className:"language-text",children:S})]}),"\n",(0,s.jsx)(n.h3,{id:"llama-32-3b-instruct-lmdeploy",children:"LLaMA 3.2 3B Instruct (LMDeploy)"}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"1/model.py"})}),(0,s.jsx)(i.A,{className:"language-text",children:I})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"requirements.txt"})}),(0,s.jsx)(i.A,{className:"language-text",children:E})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"config.yaml"})}),(0,s.jsx)(i.A,{className:"language-text",children:P})]}),"\n",(0,s.jsx)(n.h3,{id:"gemma-3-1b-instruct-vllm",children:"Gemma 3 1B Instruct (vLLM)"}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"1/model.py"})}),(0,s.jsx)(i.A,{className:"language-text",children:C})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"requirements.txt"})}),(0,s.jsx)(i.A,{className:"language-text",children:N})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"config.yaml"})}),(0,s.jsx)(i.A,{className:"language-text",children:M})]}),"\n",(0,s.jsx)(n.h3,{id:"llama-31-8b-tool-calling-vllm",children:"LLaMA 3.1 8B Tool Calling (vLLM)"}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"1/model.py"})}),(0,s.jsx)(i.A,{className:"language-text",children:D})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"requirements.txt"})}),(0,s.jsx)(i.A,{className:"language-text",children:F})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"config.yaml"})}),(0,s.jsx)(i.A,{className:"language-text",children:q})]}),"\n",(0,s.jsx)(n.h2,{id:"local-runners",children:"Local Runners"}),"\n",(0,s.jsx)(n.h3,{id:"ollama",children:"Ollama"}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"1/model.py"})}),(0,s.jsx)(i.A,{className:"language-text",children:O})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"requirements.txt"})}),(0,s.jsx)(i.A,{className:"language-text",children:R})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"config.yaml"})}),(0,s.jsx)(i.A,{className:"language-text",children:z})]}),"\n",(0,s.jsx)(n.h2,{id:"mcp",children:"MCP"}),"\n",(0,s.jsx)(n.h3,{id:"browser-tools",children:"Browser Tools"}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"1/model.py"})}),(0,s.jsx)(i.A,{className:"language-text",children:G})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"client.py"})}),(0,s.jsx)(i.A,{className:"language-text",children:U})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"requirements.txt"})}),(0,s.jsx)(i.A,{className:"language-text",children:B})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"config.yaml"})}),(0,s.jsx)(i.A,{className:"language-text",children:Q})]}),"\n",(0,s.jsx)(n.h3,{id:"code-execution",children:"Code Execution"}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"1/model.py"})}),(0,s.jsx)(i.A,{className:"language-text",children:V})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"client.py"})}),(0,s.jsx)(i.A,{className:"language-text",children:H})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"requirements.txt"})}),(0,s.jsx)(i.A,{className:"language-text",children:W})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"config.yaml"})}),(0,s.jsx)(i.A,{className:"language-text",children:Y})]}),"\n",(0,s.jsx)(n.h3,{id:"code-execution-without-docker-version",children:"Code Execution Without Docker Version"}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"1/model.py"})}),(0,s.jsx)(i.A,{className:"language-text",children:J})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"client.py"})}),(0,s.jsx)(i.A,{className:"language-text",children:X})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"requirements.txt"})}),(0,s.jsx)(i.A,{className:"language-text",children:K})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"config.yaml"})}),(0,s.jsx)(i.A,{className:"language-text",children:$})]}),"\n",(0,s.jsx)(n.h3,{id:"google-drive",children:"Google Drive"}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"1/model.py"})}),(0,s.jsx)(i.A,{className:"language-text",children:Z})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"client.py"})}),(0,s.jsx)(i.A,{className:"language-text",children:ee})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"requirements.txt"})}),(0,s.jsx)(i.A,{className:"language-text",children:ne})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"config.yaml"})}),(0,s.jsx)(i.A,{className:"language-text",children:te})]}),"\n",(0,s.jsx)(n.h3,{id:"math",children:"Math"}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"1/model.py"})}),(0,s.jsx)(i.A,{className:"language-text",children:re})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"client.py"})}),(0,s.jsx)(i.A,{className:"language-text",children:se})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"requirements.txt"})}),(0,s.jsx)(i.A,{className:"language-text",children:oe})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"config.yaml"})}),(0,s.jsx)(i.A,{className:"language-text",children:ie})]}),"\n",(0,s.jsx)(n.h3,{id:"postgres",children:"Postgres"}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"1/model.py"})}),(0,s.jsx)(i.A,{className:"language-text",children:ae})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"client.py"})}),(0,s.jsx)(i.A,{className:"language-text",children:le})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"requirements.txt"})}),(0,s.jsx)(i.A,{className:"language-text",children:de})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"config.yaml"})}),(0,s.jsx)(i.A,{className:"language-text",children:ce})]}),"\n",(0,s.jsx)(n.h3,{id:"web-search",children:"Web Search"}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"1/model.py"})}),(0,s.jsx)(i.A,{className:"language-text",children:me})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"client.py"})}),(0,s.jsx)(i.A,{className:"language-text",children:pe})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"requirements.txt"})}),(0,s.jsx)(i.A,{className:"language-text",children:ue})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"config.yaml"})}),(0,s.jsx)(i.A,{className:"language-text",children:fe})]}),"\n",(0,s.jsx)(n.h2,{id:"multimodal-models",children:"Multimodal Models"}),"\n",(0,s.jsx)(n.h3,{id:"qwen25-vl-3b-instruct",children:"Qwen2.5 VL 3B Instruct"}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"1/model.py"})}),(0,s.jsx)(i.A,{className:"language-text",children:_e})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"1/openai_server_starter.py"})}),(0,s.jsx)(i.A,{className:"language-text",children:he})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"requirements.txt"})}),(0,s.jsx)(i.A,{className:"language-text",children:ge})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"config.yaml"})}),(0,s.jsx)(i.A,{className:"language-text",children:xe})]}),"\n",(0,s.jsx)(n.h2,{id:"ocr",children:"OCR"}),"\n",(0,s.jsx)(n.h3,{id:"nanonets-ocr-small",children:"NanoNets OCR Small"}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"1/model.py"})}),(0,s.jsx)(i.A,{className:"language-text",children:ye})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"requirements.txt"})}),(0,s.jsx)(i.A,{className:"language-text",children:ve})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"config.yaml"})}),(0,s.jsx)(i.A,{className:"language-text",children:ke})]}),"\n",(0,s.jsx)(n.h2,{id:"text-embedder",children:"Text Embedder"}),"\n",(0,s.jsx)(n.h3,{id:"jina-embeddings-v3",children:"Jina Embeddings v3"}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"1/model.py"})}),(0,s.jsx)(i.A,{className:"language-text",children:be})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"requirements.txt"})}),(0,s.jsx)(i.A,{className:"language-text",children:we})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"config.yaml"})}),(0,s.jsx)(i.A,{className:"language-text",children:Te})]}),"\n",(0,s.jsx)(n.h2,{id:"text-to-image",children:"Text-to-Image"}),"\n",(0,s.jsx)(n.h3,{id:"flux-schnell",children:"FLUX Schnell"}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"1/model.py"})}),(0,s.jsx)(i.A,{className:"language-text",children:Ae})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"requirements.txt"})}),(0,s.jsx)(i.A,{className:"language-text",children:je})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)(n.code,{children:"config.yaml"})}),(0,s.jsx)(i.A,{className:"language-text",children:Le})]})]})}function Ne(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(Ce,{...e})}):Ce(e)}}}]);
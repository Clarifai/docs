"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[5956],{19365:(e,t,n)=>{n.d(t,{A:()=>i});var a=n(96540),o=n(20053);const s={tabItem:"tabItem_Ymn6"};function i(e){let{children:t,hidden:n,className:i}=e;return a.createElement("div",{role:"tabpanel",className:(0,o.A)(s.tabItem,i),hidden:n},t)}},11470:(e,t,n)=>{n.d(t,{A:()=>w});var a=n(58168),o=n(96540),s=n(20053),i=n(23104),r=n(56347),l=n(57485),c=n(31682),u=n(89466);function p(e){return function(e){return o.Children.map(e,(e=>{if(!e||(0,o.isValidElement)(e)&&function(e){const{props:t}=e;return!!t&&"object"==typeof t&&"value"in t}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}(e).map((e=>{let{props:{value:t,label:n,attributes:a,default:o}}=e;return{value:t,label:n,attributes:a,default:o}}))}function m(e){const{values:t,children:n}=e;return(0,o.useMemo)((()=>{const e=t??p(n);return function(e){const t=(0,c.X)(e,((e,t)=>e.value===t.value));if(t.length>0)throw new Error(`Docusaurus error: Duplicate values "${t.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[t,n])}function d(e){let{value:t,tabValues:n}=e;return n.some((e=>e.value===t))}function h(e){let{queryString:t=!1,groupId:n}=e;const a=(0,r.W6)(),s=function(e){let{queryString:t=!1,groupId:n}=e;if("string"==typeof t)return t;if(!1===t)return null;if(!0===t&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:t,groupId:n});return[(0,l.aZ)(s),(0,o.useCallback)((e=>{if(!s)return;const t=new URLSearchParams(a.location.search);t.set(s,e),a.replace({...a.location,search:t.toString()})}),[s,a])]}function g(e){const{defaultValue:t,queryString:n=!1,groupId:a}=e,s=m(e),[i,r]=(0,o.useState)((()=>function(e){let{defaultValue:t,tabValues:n}=e;if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(t){if(!d({value:t,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${t}" but none of its children has the corresponding value. Available values are: ${n.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return t}const a=n.find((e=>e.default))??n[0];if(!a)throw new Error("Unexpected error: 0 tabValues");return a.value}({defaultValue:t,tabValues:s}))),[l,c]=h({queryString:n,groupId:a}),[p,g]=function(e){let{groupId:t}=e;const n=function(e){return e?`docusaurus.tab.${e}`:null}(t),[a,s]=(0,u.Dv)(n);return[a,(0,o.useCallback)((e=>{n&&s.set(e)}),[n,s])]}({groupId:a}),y=(()=>{const e=l??p;return d({value:e,tabValues:s})?e:null})();(0,o.useLayoutEffect)((()=>{y&&r(y)}),[y]);return{selectedValue:i,selectValue:(0,o.useCallback)((e=>{if(!d({value:e,tabValues:s}))throw new Error(`Can't select invalid tab value=${e}`);r(e),c(e),g(e)}),[c,g,s]),tabValues:s}}var y=n(92303);const f={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};function b(e){let{className:t,block:n,selectedValue:r,selectValue:l,tabValues:c}=e;const u=[],{blockElementScrollPositionUntilNextRender:p}=(0,i.a_)(),m=e=>{const t=e.currentTarget,n=u.indexOf(t),a=c[n].value;a!==r&&(p(t),l(a))},d=e=>{let t=null;switch(e.key){case"Enter":m(e);break;case"ArrowRight":{const n=u.indexOf(e.currentTarget)+1;t=u[n]??u[0];break}case"ArrowLeft":{const n=u.indexOf(e.currentTarget)-1;t=u[n]??u[u.length-1];break}}t?.focus()};return o.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,s.A)("tabs",{"tabs--block":n},t)},c.map((e=>{let{value:t,label:n,attributes:i}=e;return o.createElement("li",(0,a.A)({role:"tab",tabIndex:r===t?0:-1,"aria-selected":r===t,key:t,ref:e=>u.push(e),onKeyDown:d,onClick:m},i,{className:(0,s.A)("tabs__item",f.tabItem,i?.className,{"tabs__item--active":r===t})}),n??t)})))}function v(e){let{lazy:t,children:n,selectedValue:a}=e;const s=(Array.isArray(n)?n:[n]).filter(Boolean);if(t){const e=s.find((e=>e.props.value===a));return e?(0,o.cloneElement)(e,{className:"margin-top--md"}):null}return o.createElement("div",{className:"margin-top--md"},s.map(((e,t)=>(0,o.cloneElement)(e,{key:t,hidden:e.props.value!==a}))))}function L(e){const t=g(e);return o.createElement("div",{className:(0,s.A)("tabs-container",f.tabList)},o.createElement(b,(0,a.A)({},e,t)),o.createElement(v,(0,a.A)({},e,t)))}function w(e){const t=(0,y.A)();return o.createElement(L,(0,a.A)({key:String(t)},e))}},23982:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>w,contentTitle:()=>v,default:()=>N,frontMatter:()=>b,metadata:()=>L,toc:()=>k});var a=n(58168),o=(n(96540),n(15680)),s=n(11470),i=n(19365),r=n(77964);const l='from litellm import completion\n\nmessages = [{"role": "user","content": """Write a poem about history?"""}]\n# Using LLM from Clarifai Platform\nresponse=completion(\n            model="clarifai/mistralai.completion.mistral-large",\n            messages=messages,\n        )\n\nprint(f"Mistral large response : {response}")',c='from litellm import completion\n\nmessages = [{"role": "user","content": """Write a poem about history?"""}]\nresponse=completion(\n            model="clarifai/anthropic.completion.claude-2_1",\n            messages=messages,\n        )\n\nprint(f"Claude-2.1 response : {response}")',u='from litellm import completion\n\nmessages = [{"role": "user","content": """Write a poem about history?"""}]\nresponse = completion(\n                model="clarifai/openai.chat-completion.GPT-4",\n                messages=messages,\n                stream=True,\n                api_key = "OpenAI_API_KEY")\n\nfor chunk in response:\n  print(chunk)',p='from litellm import acompletion\nasync def test_get_response():\n    user_message = "Hello, how are you?"\n    messages = [{"content": user_message, "role": "user"}]\n    response = await acompletion(model="clarifai/openai.chat-completion.GPT-4", messages=messages, api_key="OpenAI_API_KEY")\n    return response\n\nresponse = await test_get_response()\nprint(response)',m='from litellm import acompletion\nimport asyncio, os, traceback\n\nasync def completion_call():\n    try:\n        print("test acompletion + streaming")\n        response = await acompletion(\n            model="clarifai/mistralai.completion.mistral-large", \n            messages=[{"content": "Hello, how are you?", "role": "user"}], \n            stream=True\n        )\n        print(f"response: {response}")\n        async for chunk in response:\n            print(chunk)\n    except:\n        print(f"error occurred: {traceback.format_exc()}")\n        pass\n\nawait completion_call()',d="Mistral large response : ModelResponse(id='chatcmpl-6eed494d-7ae2-4870-b9c2-6a64d50a6151', choices=[Choices\n(finish_reason='stop', index=1, message=Message(content=\"In the grand tapestry of time, where tales unfold,\\nLies the chronicle of ages, a sight to behold.\\nA tale of empires rising, and kings of old,\\nOf civilizations lost, and stories untold.\\n\\nOnce upon a yesterday, in a time so vast,\\nHumans took their first steps, \ncasting shadows in the past.\\nFrom the cradle of mankind, a journey they embarked,\\nThrough stone and bronze and iron, their skills they sharpened and marked.\\n\\nEgyptians built pyramids, reaching for the skies,\\nWhile Greeks sought wisdom, truth, in philosophies that lie.\\nRoman legions marched, their empire to expand,\\nAnd in the East, the Silk Road joined the world, hand in hand.\\n\\nThe Middle Ages came, \nwith knights in shining armor,\\nFeudal lords and serfs, a time of both clamor and calm order.\\nThen Renaissance bloomed, like a flower in the sun,\\nA rebirth of art and science, a new age had begun.\\n\\nAcross the vast oceans, explorers sailed with courage bold,\\nDiscovering new lands, stories of adventure, untold.\\nIndustrial Revolution churned, progress in its wake,\\nMachines and factories, a whole new world to make.\\n\\nTwo World Wars raged, a testament to man's strife,\\nYet from the ashes rose hope, a renewed will for life.\\nInto the modern era, technology took flight,\\nConnecting every corner, bathed in digital light.\\n\\nHistory, a symphony, a melody of time,\\nA testament to human will, resilience so sublime.\\nIn every page, a lesson, in every tale, a guide,\\nFor understanding our past, shapes our future's tide.\", role='assistant'))], created=1713896412, model='https://api.clarifai.com/v2/users/mistralai/apps/completion/models/mistral-large/outputs', object='chat.completion', system_fingerprint=None, usage=Usage(prompt_tokens=13, completion_tokens=338, total_tokens=351))",h="Claude-2.1 response : ModelResponse(id='chatcmpl-d126c919-4db4-4aa3-ac8f-7edea41e0b93', choices=[Choices(finish_reason='stop', index=1, message=Message\n(content=\" Here's a poem I wrote about history:\\n\\nThe Tides of Time\\n\\nThe tides of time ebb and flow,\\nCarrying stories of long ago.\\nFigures and events \ncome into light,\\nShaping the future with all their might.\\n\\nKingdoms rise, empires fall, \\nLeaving traces that echo down every hall.\\nRevolutions bring change with a fiery glow,\\nToppling structures from long ago.\\n\\nExplorers traverse each ocean and\nland,\\nSeeking treasures they don't understand.\\nWhile artists and writers try to make their mark,\\nHoping their works shine bright in the dark.\\n\\nThe cycle repeats again and again,\\nAs humanity struggles to learn from its pain.\\nThough the players may change on history's stage,\\nThe themes stay the same from age to age.\\n\\nWar and peace, life and death,\\nLove and strife with every breath.\\nThe tides of time continue their dance,\\nAs we join in, by luck or by chance.\\n\\nSo we study the past to light the way forward, \\nHeeding warnings from stories told and heard.\\nThe future unfolds from this unending flow -\\nWhere the tides of time ultimately go.\", role='assistant'))], created=1713896579, model='https://api.clarifai.com/v2/users/anthropic/apps/completion/models/claude-2_1/outputs', object='chat.completion', system_fingerprint=None, usage=Usage(prompt_tokens=12, completion_tokens=232, total_tokens=244))",g="ModelResponse(id='chatcmpl-40ae19af-3bf0-4eb4-99f2-33aec3ba84af', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=\"In the quiet corners \nof time's grand hall,\\nLies the tale of rise and fall.\\nFrom ancient \nruins to modern sprawl,\\nHistory, the greatest story of them all.\\n\\nEmpires have risen, empires have decayed,\\nThrough the eons, memories have stayed.\\nIn the book of time, history is laid,\\nA tapestry of events, meticulously displayed.\\n\\nThe pyramids of Egypt, standing tall,\\nThe Roman Empire's mighty sprawl.\\nFrom Alexander's conquest, to the Berlin Wall,\\nHistory, a silent witness to it all.\\n\\nIn the shadow of the past we tread,\\nWhere once kings and prophets led.\\nTheir stories in our hearts are spread,\\nEchoes of their words, in our minds are read.\\n\\nBattles fought and victories won,\\nActs of courage under the sun.\\nTales of love, of deeds done,\\nIn history's grand book, they all run.\\n\\nHeroes born, legends made,\\nIn the annals of time, they'll never fade.\\nTheir triumphs and failures all displayed,\\nIn the eternal march of history's parade.\\n\\nThe ink of the past is forever dry,\\nBut its lessons, we cannot deny.\\nIn its stories, truths lie,\\nIn its wisdom, we rely.\\n\\nHistory, a mirror to our past,\\nA guide for the future vast.\\nThrough its lens, we're ever cast,\\nIn the drama of life, forever vast.\", role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1714744515, model='https://api.clarifai.com/v2/users/openai/apps/chat-completion/models/GPT-4/outputs', object='chat.completion.chunk', system_fingerprint=None)\nModelResponse(id='chatcmpl-40ae19af-3bf0-4eb4-99f2-33aec3ba84af', choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(content=None, role=None, function_call=None, tool_calls=None), logprobs=None)], created=1714744515, model='https://api.clarifai.com/v2/users/openai/apps/chat-completion/models/GPT-4/outputs', object='chat.completion.chunk', system_fingerprint=None)",y="ModelResponse(id='chatcmpl-095f1f4f-66b0-4f1f-988d-159a809f0c9c', choices=[Choices(finish_reason='stop', index=1, message=Message(content=\"Hello! As an artificial intelligence, \nI don't have feelings, but I'm here and ready to assist you. How can I help you today?\", role='assistant'))], \ncreated=1717782689, model='https://api.clarifai.com/v2/users/openai/apps/chat-completion/models/GPT-4/outputs', object='chat.completion', system_fingerprint=None, usage=Usage(prompt_tokens=6, completion_tokens=30, total_tokens=36))",f="test acompletion + streaming\nresponse: <litellm.utils.CustomStreamWrapper object at 0x7e7b640d5250>\nModelResponse(id='chatcmpl-d70421b5-9701-4ed1-8926-09ccd79abd25', choices=[StreamingChoices(finish_reason=None, \nindex=0, delta=Delta(content=\"Hello! I'm an assistant designed to help answer your questions and provide information, so I don't have feelings, but I'm here and ready to assist you. How can I help you today?\", role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1718733588, model='https://api.clarifai.com/v2/users/mistralai/apps/completion/models/mistral-large/outputs', object='chat.completion.chunk', system_fingerprint=None)\nModelResponse(id='chatcmpl-d70421b5-9701-4ed1-8926-09ccd79abd25', \nchoices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(content=None, role=None, function_call=None, tool_calls=None), logprobs=None)], created=1718733588, model='https://api.clarifai.com/v2/users/mistralai/apps/completion/models/mistral-large/outputs', object='chat.completion.chunk', system_fingerprint=None)",b={},v="Using Clarifai Models In LiteLLM",L={unversionedId:"integrations/LiteLLM/clarifai-litellm",id:"integrations/LiteLLM/clarifai-litellm",title:"Using Clarifai Models In LiteLLM",description:"Learn how to use Clarifai Models in LiteLLM",source:"@site/docs/integrations/LiteLLM/clarifai-litellm.md",sourceDirName:"integrations/LiteLLM",slug:"/integrations/LiteLLM/clarifai-litellm",permalink:"/integrations/LiteLLM/clarifai-litellm",draft:!1,editUrl:"https://github.com/Clarifai/docs/blob/main/docs/integrations/LiteLLM/clarifai-litellm.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"LiteLLM Clarifai Integration",permalink:"/integrations/LiteLLM/"},next:{title:"Integrating Unstructred.io with Clarifai",permalink:"/integrations/unstructured/"}},w={},k=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Completion",id:"completion",level:2},{value:"Streaming",id:"streaming",level:2},{value:"Async Completion",id:"async-completion",level:2},{value:"Async Streaming",id:"async-streaming",level:2}],T={toc:k},A="wrapper";function N(e){let{components:t,...n}=e;return(0,o.yg)(A,(0,a.A)({},T,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("h1",{id:"using-clarifai-models-in-litellm"},"Using Clarifai Models In LiteLLM"),(0,o.yg)("p",null,(0,o.yg)("strong",{parentName:"p"},"Learn how to use Clarifai Models in LiteLLM")),(0,o.yg)("hr",null),(0,o.yg)("p",null,(0,o.yg)("a",{parentName:"p",href:"https://www.litellm.ai/"},"LiteLLM")," allows you to interact with LLM models from different providers. One such provider is Clarifai. Using LiteLLM users can easily use hosted LLM models in the Clarifai platform. Now let\u2019s look at how you can use Clarifai LLMs through LiteLLM."),(0,o.yg)("h2",{id:"prerequisites"},"Prerequisites"),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},"Setting up the Clarifai Python SDK along with PAT. Refer to the installation and configuration with the PAT token ",(0,o.yg)("a",{parentName:"li",href:"https://docs.clarifai.com/python-sdk/sdk-overview/"},"here"),".")),(0,o.yg)("admonition",{type:"note"},(0,o.yg)("p",{parentName:"admonition"},"Guide to get your ",(0,o.yg)("a",{parentName:"p",href:"https://docs.clarifai.com/clarifai-basics/authentication/personal-access-tokens"},"PAT"))),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},"import os\n#Replace your PAT\nos.environ['CLARIFAI_PAT'] =\"YOUR_PAT\"\n")),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},"Install the required packages.")),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre"},"!pip install litellm\n!pip install clarifai\n")),(0,o.yg)("h2",{id:"completion"},"Completion"),(0,o.yg)("p",null,"The ",(0,o.yg)("inlineCode",{parentName:"p"},"completion")," method is the core functionality of LiteLLM for interacting with large language models (LLMs) and generating text. It provides a standardized way to send prompts to various LLMs and retrieve the generated responses. You can set the ",(0,o.yg)("inlineCode",{parentName:"p"},"model")," field as a model URL from the Clarifai platform."),(0,o.yg)("p",null,"Click ",(0,o.yg)("a",{parentName:"p",href:"https://litellm.vercel.app/docs/completion"},"here")," to learn more about completion in LiteLLM."),(0,o.yg)("p",null,"In the example given below, we are going to chat with the ",(0,o.yg)("inlineCode",{parentName:"p"},"Mistral-Large")," model from Clarifai."),(0,o.yg)(s.A,{mdxType:"Tabs"},(0,o.yg)(i.A,{value:"python",label:"Python",mdxType:"TabItem"},(0,o.yg)(r.A,{className:"language-python",mdxType:"CodeBlock"},l))),(0,o.yg)("details",null,(0,o.yg)("summary",null,"Output"),(0,o.yg)(r.A,{className:"language-python",mdxType:"CodeBlock"},d)),(0,o.yg)("p",null,"Now let\u2019s ask the same question to ",(0,o.yg)("inlineCode",{parentName:"p"},"Claude-2.1")," model.  Claude models require special tokens as input. Therefore by using LiteLLM we are standardizing the inputs for LLM applications."),(0,o.yg)(s.A,{mdxType:"Tabs"},(0,o.yg)(i.A,{value:"python",label:"Python",mdxType:"TabItem"},(0,o.yg)(r.A,{className:"language-python",mdxType:"CodeBlock"},c))),(0,o.yg)("details",null,(0,o.yg)("summary",null,"Output"),(0,o.yg)(r.A,{className:"language-python",mdxType:"CodeBlock"},h)),(0,o.yg)("p",null,"If you observe the above outputs, the response format from both the models is the same even though they are different LLMs. This is one of the advantages of LiteLLM."),(0,o.yg)("h2",{id:"streaming"},"Streaming"),(0,o.yg)("p",null,"When using streaming, the completion method no longer returns a single dictionary with all the responses. Instead, it returns an iterator that yields dictionaries containing partial information as the LLM generates the response."),(0,o.yg)("p",null,"Click ",(0,o.yg)("a",{parentName:"p",href:"https://litellm.vercel.app/docs/completion/stream#streaming-responses"},"here")," to learn more about streaming in LiteLLM."),(0,o.yg)("admonition",{type:"info"},(0,o.yg)("p",{parentName:"admonition"},"Set stream=True as an argument to the completion function.")),(0,o.yg)(s.A,{mdxType:"Tabs"},(0,o.yg)(i.A,{value:"python",label:"Python",mdxType:"TabItem"},(0,o.yg)(r.A,{className:"language-python",mdxType:"CodeBlock"},u))),(0,o.yg)("details",null,(0,o.yg)("summary",null,"Output"),(0,o.yg)(r.A,{className:"language-python",mdxType:"CodeBlock"},g)),(0,o.yg)("h2",{id:"async-completion"},"Async Completion"),(0,o.yg)("p",null,"Async completion in LiteLLM uses asynchronous capabilities to handle language model completions efficiently. By using async function, LiteLLM can perform non-blocking I/O operations, making it well-suited for applications that require responsive and scalable interactions with language models."),(0,o.yg)("p",null,"Click ",(0,o.yg)("a",{parentName:"p",href:"https://litellm.vercel.app/docs/completion/stream#async-completion"},"here")," to learn more about async completion in LiteLLM."),(0,o.yg)(s.A,{mdxType:"Tabs"},(0,o.yg)(i.A,{value:"python",label:"Python",mdxType:"TabItem"},(0,o.yg)(r.A,{className:"language-python",mdxType:"CodeBlock"},p))),(0,o.yg)("details",null,(0,o.yg)("summary",null,"Output"),(0,o.yg)(r.A,{className:"language-python",mdxType:"CodeBlock"},y)),(0,o.yg)("h2",{id:"async-streaming"},"Async Streaming"),(0,o.yg)("p",null,"Async streaming in LiteLLM refers to the process of handling real-time data streams asynchronously when interacting with language models. This is particularly useful for applications that need to process data as it arrives without waiting for the entire response, such as chatbots, real-time data processing systems, or live user interactions."),(0,o.yg)("p",null,"Click ",(0,o.yg)("a",{parentName:"p",href:"https://litellm.vercel.app/docs/completion/stream#async-streaming"},"here")," to learn more about async streaming in LiteLLM."),(0,o.yg)(s.A,{mdxType:"Tabs"},(0,o.yg)(i.A,{value:"python",label:"Python",mdxType:"TabItem"},(0,o.yg)(r.A,{className:"language-python",mdxType:"CodeBlock"},m))),(0,o.yg)("details",null,(0,o.yg)("summary",null,"Output"),(0,o.yg)(r.A,{className:"language-python",mdxType:"CodeBlock"},f)))}N.isMDXComponent=!0}}]);
"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[1472],{37251:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>d,contentTitle:()=>c,default:()=>h,frontMatter:()=>a,metadata:()=>l,toc:()=>u});var r=o(74848),t=o(28453),s=o(3514),i=o(84142);const a={description:"Orchestrate your AI workloads better, avoid vendor lock-in, and use compute spend efficiently",sidebar_position:1,pagination_prev:null},c="Compute Orchestration",l={id:"portal-guide/compute-orchestration/README",title:"Compute Orchestration",description:"Orchestrate your AI workloads better, avoid vendor lock-in, and use compute spend efficiently",source:"@site/docs/portal-guide/compute-orchestration/README.mdx",sourceDirName:"portal-guide/compute-orchestration",slug:"/portal-guide/compute-orchestration/",permalink:"/portal-guide/compute-orchestration/",draft:!1,unlisted:!1,editUrl:"https://github.com/Clarifai/docs/blob/main/docs/portal-guide/compute-orchestration/README.mdx",tags:[],version:"current",sidebarPosition:1,frontMatter:{description:"Orchestrate your AI workloads better, avoid vendor lock-in, and use compute spend efficiently",sidebar_position:1,pagination_prev:null},sidebar:"tutorialSidebar",next:{title:"Clusters and Nodepools",permalink:"/portal-guide/compute-orchestration/set-up-compute"}},d={},u=[{value:"Compute Clusters and Nodepools",id:"compute-clusters-and-nodepools",level:2},{value:"Benefits of Compute Orchestration",id:"benefits-of-compute-orchestration",level:2}];function p(e){const n={a:"a",admonition:"admonition",h1:"h1",h2:"h2",li:"li",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h1,{id:"compute-orchestration",children:"Compute Orchestration"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Orchestrate your AI workloads better, avoid vendor lock-in, and use compute spend efficiently"})}),"\n",(0,r.jsx)("hr",{}),"\n",(0,r.jsx)(n.admonition,{type:"note",children:(0,r.jsxs)(n.p,{children:["Compute Orchestration is currently in ",(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/product-updates/changelog/release-types",children:"Public Preview"}),". To request access, please contact us ",(0,r.jsx)(n.a,{href:"https://www.clarifai.com/explore/contact-us-co",children:"here"}),"."]})}),"\n",(0,r.jsx)(n.p,{children:"Clarifai\u2019s Compute Orchestration provides efficient capabilities for you to deploy any model on any compute infrastructure, at any scale. These new platform capabilities bring the convenience of serverless autoscaling to any environment, regardless of deployment location or hardware, and dynamically scale resources to meet workload demands."}),"\n",(0,r.jsx)(n.p,{children:"Clarifai handles the containerization, model packing, time slicing, and other performance optimizations on your behalf."}),"\n",(0,r.jsx)(n.p,{children:"Previously, our platform supported the following deployment options:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Shared SaaS (Serverless)"})," \u2014 This is our default offering, which abstracts away infrastructure management and allows users to easily deploy models without worrying about the underlying compute resources. In this option, Clarifai maintains multi-tenant GPU pools users can access on-demand."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Full Platform Deployment"})," \u2014 This option is designed for organizations with high-security requirements. It deploys both the Clarifai control and compute planes into the user\u2019s preferred cloud, on-premises, or air-gapped infrastructure, ensuring full isolation."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"With Compute Orchestration, we are now providing users with the ability to manage any compute planes and access dedicated compute options. These capabilities enable our enterprise customers to deploy production models with enhanced control, performance, and scalability \u2014 while addressing specific problems around compute costs, latency, and control over hosted models."}),"\n",(0,r.jsx)(n.p,{children:"Compute Orchestration allows us to provide the following additional deployment options \u2014 all of which can be customized with your preferred settings for autoscaling, cold start, and more, ensuring maximum cost efficiency and performance:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Dedicated SaaS"})," \u2014 Provides exclusive access to Clarifai-managed nodes with customizable configurations. This is currently available in AWS US-East region, with plans to expand to other cloud providers and hardware options."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Self-Managed VPC (Virtual Private Cloud)"})," \u2014 Users securely connect their own cloud provider VPC, enabling Clarifai to orchestrate deployments within the user\u2019s cloud environment while leveraging existing cloud compute or spend commitments."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Self-Managed On-Premises"})," \u2014 Users securely connect their own on-premises or bare-metal infrastructure to leverage existing compute investments, which Clarifai then orchestrates for model deployment."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Multi-Site Deployment"})," \u2014 Supports deployments across multiple self-managed compute sources, with potential for future multi-cloud or multi-region dedicated SaaS solutions."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.admonition,{type:"info",children:(0,r.jsx)(n.p,{children:"If you\u2019re not using Compute Orchestration, the Shared SaaS (Serverless) deployment remains the default option."})}),"\n",(0,r.jsx)(n.h2,{id:"compute-clusters-and-nodepools",children:"Compute Clusters and Nodepools"}),"\n",(0,r.jsx)(n.p,{children:"We use clusters and nodepools to organize and manage the compute resources required for the Compute Orchestration capabilities."}),"\n",(0,r.jsx)(n.p,{children:"A compute cluster in Clarifai acts as the overarching computational environment where models are executed, whether for training or inference. A nodepool refers to a set of dedicated nodes (virtual machine instances) within a cluster that share similar configurations and resources, such as CPU or GPU type, memory size, and other performance parameters."}),"\n",(0,r.jsx)(n.p,{children:"Cluster configuration lets you specify where and how your models are run, ensuring better performance, lower latency, and adherence to regional regulations. You can specify a cloud provider, such as AWS, that will provide the underlying compute infrastructure for hosting and running your models. You can also specify the geographic location of the data center where the compute resources will be hosted."}),"\n",(0,r.jsx)(n.p,{children:"Nodepools are an important part of how compute resources are operated within a cluster. They provide flexibility in choosing the type of instances used to run your machine learning models and workflows and help determine how resources are scaled to meet demand."}),"\n",(0,r.jsx)(n.p,{children:"Nodepools specify the accelerator and instance that will run your models and other workloads. Accelerators are specialized hardware resources, such as GPUs or dedicated ML chips used for computation."}),"\n",(0,r.jsx)(n.p,{children:"Each nodepool can run containers or workloads, and you can have multiple nodepools within a single cluster to support different types of workloads or performance requirements. These nodes execute tasks like model training, inference, and workflow orchestration within a compute cluster."}),"\n",(0,r.jsx)(n.p,{children:"With compute orchestration, you can ensure these nodepools are properly scaled up or down depending on the workload's size, complexities, and costs."}),"\n",(0,r.jsx)(n.h2,{id:"benefits-of-compute-orchestration",children:"Benefits of Compute Orchestration"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"1. Performance and Deployment Flexibility"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"It provides access to a wide range of accelerator options tailored to your use case. You can configure multiple compute clusters each tailored to your AI development stage, performance requirements, and budget. You can also run affordable proof of concepts or compute-heavy LLMs or LVMs in production all from a single product."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"It offers flexibility to make deployments in any cloud service provider, on-premises, or air-gapped environment, allowing users to leverage their hardware of choice without being locked into a single vendor. Or, you can make deployments in Clarifai\u2019s compute to avoid having to worry about managing infrastructure."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"You can customize auto-scaling settings to prevent cold-start issues and handle traffic swings; and scale down to zero for cost efficiency.  The ability to scale from zero to infinity ensures both flexibility and cost management."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Just like with our previous offerings, we ensure efficient resource usage and cost savings through bin-packing (running multiple models per GPU), time slicing, and other optimizations."}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"2. Enhanced Security"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Users can run compute planes within their own cloud service provider or on-premise environments and securely connect to Clarifai\u2019s control plane, while only having to open outbound ports for traffic. This reduces networking complexities and security risks compared to opening inbound access or configuring cloud Identity and Access Management (IAM) access roles within your VPC."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Nodepool-based compute allows users to keep their resources isolated and provides precise control over scaling models and nodes. This allows users to specify where models are executed, addressing compliance and security needs for regulated industries."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Clarifai offers fine-grained access control across apps, teams, users, and compute resources."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Users can group CPU and GPU types into dedicated scaling nodepools, enabling them to handle diverse workloads or team-specific requirements while enhancing security and resource management."}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"3. Use Compute Cost-Efficiently and Abstract Away Complexity"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"An intuitive control plane enables users to efficiently govern access to AI resources, monitor performance, and manage costs. Clarifai\u2019s expertly designed platform takes care of dependencies and optimizations, offering features like model packing, streamlined dependency management, and customizable autoscaling options \u2014 including scale-to-zero for both model replicas and compute nodes."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"The advanced optimizations deliver exceptional efficiency, with model packing reducing compute usage by up to 3.7x and enabling support for over 1.6 million inputs per second with an impressive 99.9997% reliability. Depending on the chosen configuration, customers can achieve cost savings of at least 60%, and in some cases, up to 90%."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Organizations with pre-committed cloud spend or compute contracts with major cloud service providers, like AWS, Azure, or GCP, or existing GPU and hardware investments, can efficiently leverage their compute using Clarifai Compute Orchestration."}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"4. New Inference Capabilities and Developer Experience Improvements"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Resourceful features such as inference streaming improve time-to-first-token for LLM generations."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Faster cold starts and optimized frameworks improve performance for critical workloads."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Continuous batching is available to reduce costs by processing multiple inference requests in batches."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Clarifai containerizes your desired models into Docker images, ensuring model package requirements are encapsulated in a portable environment and dependencies are handled automatically."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Low-latency deployment minimizes gRPC hops, speeding up communication."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"New model types are easily supported with a unified protobuf format, and local inference runners allow users to test models before deploying to the cloud."}),"\n"]}),"\n"]}),"\n","\n",(0,r.jsx)(s.A,{items:(0,i.$S)().items})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(p,{...e})}):p(e)}},3514:(e,n,o)=>{o.d(n,{A:()=>y});o(96540);var r=o(18215),t=o(84142),s=o(28774),i=o(53465),a=o(16654),c=o(21312),l=o(51107);const d={cardContainer:"cardContainer_fWXF",cardTitle:"cardTitle_rnsV",cardDescription:"cardDescription_PWke"};var u=o(74848);function p(e){let{href:n,children:o}=e;return(0,u.jsx)(s.A,{href:n,className:(0,r.A)("card padding--lg",d.cardContainer),children:o})}function h(e){let{href:n,icon:o,title:t,description:s}=e;return(0,u.jsxs)(p,{href:n,children:[(0,u.jsxs)(l.A,{as:"h2",className:(0,r.A)("text--truncate",d.cardTitle),title:t,children:[o," ",t]}),s&&(0,u.jsx)("p",{className:(0,r.A)("text--truncate",d.cardDescription),title:s,children:s})]})}function m(e){let{item:n}=e;const o=(0,t.Nr)(n),r=function(){const{selectMessage:e}=(0,i.W)();return n=>e(n,(0,c.T)({message:"1 item|{count} items",id:"theme.docs.DocCard.categoryDescription.plurals",description:"The default description for a category card in the generated index about how many items this category includes"},{count:n}))}();return o?(0,u.jsx)(h,{href:o,icon:"\ud83d\uddc3\ufe0f",title:n.label,description:n.description??r(n.items.length)}):null}function f(e){let{item:n}=e;const o=(0,a.A)(n.href)?"\ud83d\udcc4\ufe0f":"\ud83d\udd17",r=(0,t.cC)(n.docId??void 0);return(0,u.jsx)(h,{href:n.href,icon:o,title:n.label,description:n.description??r?.description})}function g(e){let{item:n}=e;switch(n.type){case"link":return(0,u.jsx)(f,{item:n});case"category":return(0,u.jsx)(m,{item:n});default:throw new Error(`unknown item type ${JSON.stringify(n)}`)}}function x(e){let{className:n}=e;const o=(0,t.$S)();return(0,u.jsx)(y,{items:o.items,className:n})}function y(e){const{items:n,className:o}=e;if(!n)return(0,u.jsx)(x,{...e});const s=(0,t.d1)(n);return(0,u.jsx)("section",{className:(0,r.A)("row",o),children:s.map(((e,n)=>(0,u.jsx)("article",{className:"col col--6 margin-bottom--lg",children:(0,u.jsx)(g,{item:e})},n)))})}},53465:(e,n,o)=>{o.d(n,{W:()=>l});var r=o(96540),t=o(44586);const s=["zero","one","two","few","many","other"];function i(e){return s.filter((n=>e.includes(n)))}const a={locale:"en",pluralForms:i(["one","other"]),select:e=>1===e?"one":"other"};function c(){const{i18n:{currentLocale:e}}=(0,t.A)();return(0,r.useMemo)((()=>{try{return function(e){const n=new Intl.PluralRules(e);return{locale:e,pluralForms:i(n.resolvedOptions().pluralCategories),select:e=>n.select(e)}}(e)}catch(n){return console.error(`Failed to use Intl.PluralRules for locale "${e}".\nDocusaurus will fallback to the default (English) implementation.\nError: ${n.message}\n`),a}}),[e])}function l(){const e=c();return{selectMessage:(n,o)=>function(e,n,o){const r=e.split("|");if(1===r.length)return r[0];r.length>o.pluralForms.length&&console.error(`For locale=${o.locale}, a maximum of ${o.pluralForms.length} plural forms are expected (${o.pluralForms.join(",")}), but the message contains ${r.length}: ${e}`);const t=o.select(n),s=o.pluralForms.indexOf(t);return r[Math.min(s,r.length-1)]}(o,n,e)}}},28453:(e,n,o)=>{o.d(n,{R:()=>i,x:()=>a});var r=o(96540);const t={},s=r.createContext(t);function i(e){const n=r.useContext(s);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:i(e.components),r.createElement(s.Provider,{value:n},e.children)}}}]);
"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[5945],{84444:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>J,contentTitle:()=>q,default:()=>X,frontMatter:()=>F,metadata:()=>K,toc:()=>z});var a=t(74848),i=t(28453),s=t(11470),o=t(19365),r=t(21432);const c="##################################################################################################\n# In this section, we set the user authentication, user and app ID, model details, and the URL\n# of the image we want as an input. Change these strings to run your own example.\n#################################################################################################\n\n# Your PAT (Personal Access Token) can be found in the Account's Security section\nPAT = 'YOUR_PAT_HERE'\n# Specify the correct user_id/app_id pairings\n# Since you're making inferences outside your app's scope\nUSER_ID = 'clarifai'\nAPP_ID = 'main'\n# Change these to whatever model and image URL you want to use\nMODEL_ID = 'general-image-recognition'\nMODEL_VERSION_ID = 'aa7f35c01e0642fda5cf400f543e7c40'\nIMAGE_URL = 'https://samples.clarifai.com/metro-north.jpg'\n\n############################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n############################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (('authorization', 'Key ' + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\npost_model_outputs_response = stub.PostModelOutputs(\n    service_pb2.PostModelOutputsRequest(\n        user_app_id=userDataObject,  # The userDataObject is created in the overview and is required when using a PAT\n        model_id=MODEL_ID,\n        version_id=MODEL_VERSION_ID,  # This is optional. Defaults to the latest model version\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    image=resources_pb2.Image(\n                        url=IMAGE_URL\n                    )\n                )\n            )\n        ]\n    ),\n    metadata=metadata\n)\nif post_model_outputs_response.status.code != status_code_pb2.SUCCESS:\n    print(post_model_outputs_response.status)\n    raise Exception(\"Post model outputs failed, status: \" + post_model_outputs_response.status.description)\n\n# Since we have one input, one output will exist here\noutput = post_model_outputs_response.outputs[0]\n\nprint(\"Predicted concepts:\")\nfor concept in output.data.concepts:\n    print(\"%s %.2f\" % (concept.name, concept.value))\n\n# Uncomment this line to print the full Response JSON\n#print(output)",l="post_model_outputs_response = stub.PostModelOutputs(\n    service_pb2.PostModelOutputsRequest(\n        ...\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    image=resources_pb2.Image(\n                        url=IMAGE_URL\n                    )\n                )\n            ),\n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    image=resources_pb2.Image(\n                        url=IMAGE_URL\n                    )\n                )\n            ),\n            # and so on...\n        ]\n    ),\n    ...\n)",u="######################################################################################################\n# In this section, we set the user authentication, user and app ID, model details, and the location\n# of the image we want as an input. Change these strings to run your own example.\n#####################################################################################################\n\n# Your PAT (Personal Access Token) can be found in the Account's Security section\nPAT = 'YOUR_PAT_HERE'\n# Specify the correct user_id/app_id pairings\n# Since you're making inferences outside your app's scope\nUSER_ID = 'clarifai'\nAPP_ID = 'main'\n# Change these to whatever model and image input you want to use\nMODEL_ID = 'general-image-recognition'\nMODEL_VERSION_ID = 'aa7f35c01e0642fda5cf400f543e7c40'\nIMAGE_FILE_LOCATION = 'YOUR_IMAGE_FILE_LOCATION_HERE'\n\n############################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n############################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (('authorization', 'Key ' + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\nwith open(IMAGE_FILE_LOCATION, \"rb\") as f:\n    file_bytes = f.read()\n\npost_model_outputs_response = stub.PostModelOutputs(\n    service_pb2.PostModelOutputsRequest(\n        user_app_id=userDataObject,  # The userDataObject is created in the overview and is required when using a PAT\n        model_id=MODEL_ID,\n        version_id=MODEL_VERSION_ID,  # This is optional. Defaults to the latest model version\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    image=resources_pb2.Image(\n                        base64=file_bytes\n                    )\n                )\n            )\n        ]\n    ),\n    metadata=metadata\n)\nif post_model_outputs_response.status.code != status_code_pb2.SUCCESS:\n    print(post_model_outputs_response.status)\n    raise Exception(\"Post model outputs failed, status: \" + post_model_outputs_response.status.description)\n\n# Since we have one input, one output will exist here\noutput = post_model_outputs_response.outputs[0]\n\nprint(\"Predicted concepts:\")\nfor concept in output.data.concepts:\n    print(\"%s %.2f\" % (concept.name, concept.value))\n\n# Uncomment this line to print the full Response JSON\n#print(output)",p="##################################################################################################\n# In this section, we set the user authentication, user and app ID, model details, and the URL\n# of the image we want as an input. Change these strings to run your own example.\n#################################################################################################\n\n# Your PAT (Personal Access Token) can be found in the Account's Security section\nPAT = 'YOUR_PAT_HERE'\n# Specify the correct user_id/app_id pairings\n# Since you're making inferences outside your app's scope\nUSER_ID = 'clarifai'\nAPP_ID = 'main'\n# Change these to whatever model and image URL you want to use\nMODEL_ID = 'general-image-detection'\nMODEL_VERSION_ID = '1580bb1932594c93b7e2e04456af7c6f'\nIMAGE_URL = 'https://samples.clarifai.com/metro-north.jpg'\n# To use a local file, assign the location variable\n# IMAGE_FILE_LOCATION = 'YOUR_IMAGE_FILE_LOCATION_HERE'\n\n############################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n############################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (('authorization', 'Key ' + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\n# To use a local file, uncomment the following lines\n# with open(IMAGE_FILE_LOCATION, \"rb\") as f:\n#     file_bytes = f.read()\n\npost_model_outputs_response = stub.PostModelOutputs(\n    service_pb2.PostModelOutputsRequest(\n        user_app_id=userDataObject,  # The userDataObject is created in the overview and is required when using a PAT\n        model_id=MODEL_ID,\n        version_id=MODEL_VERSION_ID,  # This is optional. Defaults to the latest model version\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    image=resources_pb2.Image(\n                        url=IMAGE_URL\n                       # base64=file_bytes\n                    )\n                )\n            )\n        ]\n    ),\n    metadata=metadata\n)\nif post_model_outputs_response.status.code != status_code_pb2.SUCCESS:\n    print(post_model_outputs_response.status)\n    raise Exception(\"Post model outputs failed, status: \" + post_model_outputs_response.status.description)\n    \nregions = post_model_outputs_response.outputs[0].data.regions\n\nfor region in regions:\n    # Accessing and rounding the bounding box values\n    top_row = round(region.region_info.bounding_box.top_row, 3)\n    left_col = round(region.region_info.bounding_box.left_col, 3)\n    bottom_row = round(region.region_info.bounding_box.bottom_row, 3)\n    right_col = round(region.region_info.bounding_box.right_col, 3)\n    \n    for concept in region.data.concepts:\n        # Accessing and rounding the concept value\n        name = concept.name\n        value = round(concept.value, 4)\n\n        print((f\"{name}: {value} BBox: {top_row}, {left_col}, {bottom_row}, {right_col}\"))\n",d="##################################################################################################\n# In this section, we set the user authentication, user and app ID, model details, and the URL\n# of the image we want as an input. Change these strings to run your own example.\n#################################################################################################\n\n# Your PAT (Personal Access Token) can be found in the Account's Security section\nPAT = 'YOUR_PAT_HERE'\n# Specify the correct user_id/app_id pairings\n# Since you're making inferences outside your app's scope\nUSER_ID = 'clarifai'\nAPP_ID = 'main'\n# Change these to whatever model and image URL you want to use\nMODEL_ID = 'image-general-segmentation'\nMODEL_VERSION_ID = '1581820110264581908ce024b12b4bfb'\nIMAGE_URL = 'https://samples.clarifai.com/metro-north.jpg'\n# To use a local file, assign the location variable\n# IMAGE_FILE_LOCATION = 'YOUR_IMAGE_FILE_LOCATION_HERE'\n\n############################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n############################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (('authorization', 'Key ' + PAT),)\n\n# To use a local file, uncomment the following lines\n# with open(IMAGE_FILE_LOCATION, \"rb\") as f:\n#     file_bytes = f.read()\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\npost_model_outputs_response = stub.PostModelOutputs(\n    service_pb2.PostModelOutputsRequest(\n        user_app_id=userDataObject,  # The userDataObject is created in the overview and is required when using a PAT\n        model_id=MODEL_ID,\n        version_id=MODEL_VERSION_ID,  # This is optional. Defaults to the latest model version\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    image=resources_pb2.Image(\n                        url=IMAGE_URL\n                        # base64=file_bytes\n                    )\n                )\n            )\n        ]\n    ),\n    metadata=metadata\n)\nif post_model_outputs_response.status.code != status_code_pb2.SUCCESS:\n    print(post_model_outputs_response.status)\n    raise Exception(\"Post model outputs failed, status: \" + post_model_outputs_response.status.description)\n\nregions = post_model_outputs_response.outputs[0].data.regions\n\nfor region in regions:\n    for concept in region.data.concepts:\n        # Accessing and rounding the concept's percentage of image covered\n        name = concept.name\n        value = round(concept.value, 4)\n        print((f\"{name}: {value}\"))\n",h='##################################################################################################\n# In this section, we set the user authentication, user and app ID, model details, and the URL\n# of the image we want as an input. Change these strings to run your own example.\n#################################################################################################\n\n# Your PAT (Personal Access Token) can be found in the Account\'s Security section\nPAT = "YOUR_PAT_HERE"\n# Specify the correct user_id/app_id pairings\n# Since you\'re making inferences outside your app\'s scope\nUSER_ID = "salesforce"\nAPP_ID = "blip"\n# Change these to whatever model and image URL you want to use\nMODEL_ID = "general-english-image-caption-blip"\nMODEL_VERSION_ID = "cdb690f13e62470ea6723642044f95e4"\nIMAGE_URL = "https://samples.clarifai.com/metro-north.jpg"\n# To use a local file, assign the location variable\n# IMAGE_FILE_LOCATION = "YOUR_IMAGE_FILE_LOCATION_HERE"\n\n############################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n############################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (("authorization", "Key " + PAT),)\n\n# To use a local file, uncomment the following lines\n# with open(IMAGE_FILE_LOCATION, "rb") as f:\n#     file_bytes = f.read()\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\npost_model_outputs_response = stub.PostModelOutputs(\n    service_pb2.PostModelOutputsRequest(\n        user_app_id=userDataObject,  # The userDataObject is created in the overview and is required when using a PAT\n        model_id=MODEL_ID,\n        version_id=MODEL_VERSION_ID,  # This is optional. Defaults to the latest model version\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    image=resources_pb2.Image(\n                        url=IMAGE_URL\n                        # base64=file_bytes\n                        )\n                    )\n            )\n        ],\n    ),\n    metadata=metadata,\n)\nif post_model_outputs_response.status.code != status_code_pb2.SUCCESS:\n    print(post_model_outputs_response.status)\n    raise Exception(\n        "Post model outputs failed, status: "\n        + post_model_outputs_response.status.description\n    )\n\n# Since we have one input, one output will exist here\noutput = post_model_outputs_response.outputs[0]\n\nprint("Image caption:")\nprint(output.data.text.raw)\n\n# Uncomment this line to print the full Response JSON\n# print(output)\n',g="\x3c!--index.html file--\x3e\n\n<script>\n    ///////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, and the URL\n    // of the image we want as an input. Change these strings to run your own example.\n    //////////////////////////////////////////////////////////////////////////////////////////////////\n\n    // Your PAT (Personal Access Token) can be found in the Account's Security section\n    const PAT = 'YOUR_PAT_HERE';\n    // Specify the correct user_id/app_id pairings\n    // Since you're making inferences outside your app's scope\n    const USER_ID = 'clarifai';       \n    const APP_ID = 'main';\n    // Change these to whatever model and image URL you want to use\n    const MODEL_ID = 'general-image-recognition';\n    const MODEL_VERSION_ID = 'aa7f35c01e0642fda5cf400f543e7c40';    \n    const IMAGE_URL = 'https://samples.clarifai.com/metro-north.jpg';\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    const raw = JSON.stringify({\n        \"user_app_id\": {\n            \"user_id\": USER_ID,\n            \"app_id\": APP_ID\n        },\n        \"inputs\": [\n            {\n                \"data\": {\n                    \"image\": {\n                        \"url\": IMAGE_URL\n                    }\n                }\n            }\n        ]\n    });\n\n    const requestOptions = {\n        method: 'POST',\n        headers: {\n            'Accept': 'application/json',\n            'Authorization': 'Key ' + PAT\n        },\n        body: raw\n    };\n\n    // NOTE: MODEL_VERSION_ID is optional, you can also call prediction with the MODEL_ID only\n    // https://api.clarifai.com/v2/models/{YOUR_MODEL_ID}/outputs\n    // this will default to the latest version_id\n\n    fetch(\"https://api.clarifai.com/v2/models/\" + MODEL_ID + \"/versions/\" + MODEL_VERSION_ID + \"/outputs\", requestOptions)\n        .then(response => response.text())\n        .then(result => console.log(result))\n        .catch(error => console.log('error', error));\n<\/script>",A="\x3c!--index.html file--\x3e\n\n<script>\n    /////////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, and the location\n    // of the image we want as an input. Change these strings to run your own example.\n    ///////////////////////////////////////////////////////////////////////////////////////////////////////\n\n    // Your PAT (Personal Access Token) can be found in the Account's Security section\n    const PAT = 'YOUR_PAT_HERE';\n    // Specify the correct user_id/app_id pairings\n    // Since you're making inferences outside your app's scope\n    const USER_ID = 'clarifai';    \n    const APP_ID = 'main';\n    // Change these to whatever model and image input you want to use\n    const MODEL_ID = 'general-image-recognition';\n    const MODEL_VERSION_ID = 'aa7f35c01e0642fda5cf400f543e7c40';    \n    const IMAGE_BYTES_STRING = '/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAUDBAQEAwUEBAQFBQUGBwwIBwcHBw8LCwkMEQ8SEhEPERETFhwXExQaFRERGCEYGh0dHx8fExciJCIeJBweHx7/2wBDAQUFBQcGBw4ICA4eFBEUHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh7/wAARCAAoACgDASIAAhEBAxEB/8QAGQAAAgMBAAAAAAAAAAAAAAAAAAYDBQcE/8QAMBAAAQMDAwMDAgQHAAAAAAAAAQIDBAAFEQYSIQcTMTJBURRhCBYikSNScXKhsdH/xAAZAQACAwEAAAAAAAAAAAAAAAAFBgIDBAf/xAAtEQABAwMBBgQHAQAAAAAAAAABAgMRAAQhMQUSE0FRYQaBocEUFiJCcrHR8P/aAAwDAQACEQMRAD8A3+RYY1unSYzCS0ttZUkAgktn0q5yT7jPyDUC4wdGwycH5U2Kt9ZQ7VI1qw5PkvQy3CSVPpf7aQjuKyFH25xzn3pHn3TVNy01Hl2hyy6YdkSpKsS9sl/6RlI3rRu3dxWd6spwnAGPIJTfl925fcLaoSDHXvyo6i9SlCQrU9wKln3OyWiaDN1RAbW3kKbSd7gPtwMkH/tTWy9afuy1iPfnXMAblITwkE4yf08cn3pSbYt1uts24XH6fUbiLAuY1MWyGkLEmUW0rcCRvUpQ5CtwKQCPgi4S1ZbDe4sd9NntDEe79m3uOBLTr0IR9jzodSMqUpTu9JJ8owD7UTT4ZCfv9PbP7860m+s+HBSrejWRuz2kAxoesGYxTW/Zlpkwo1vkuSly3UgKWQUhHJUvIHsAaKTemF8XE6sWmxyZkiaZrMh1jv8ArQNpUVqB8FW0njHqx4zRVVhsph1KlKk5xQ+7uHmikaSJrQerMByet2IwvtuTLa4xv2k7Rk84H9x/esHv92d01boenLXGcuiWrFIhLlpbcaQ2/JdK3VJCkAq2pAR7Zz7YxWudY9fxNIdQbNGkR5TyX4aisNNpUMFZAzkj4NK0jq9ZpbLr0PSlzkhrlZDaQlP3P8Q4/ap3F87bPucJEkx/hHv60b2TYXLrKN5sramYECSQRk9M6c6zmJ+eb5Hi22M7cnWGIQgFLbX0zSo4PDa1YBcTgDyMjJ/qbGPabH08SJt1Uzc9QqRliGg5QySPKvgc+TyfYDmmTUWpNYz7ctxoQdPQshCktupckDJUPUcJT6DwMq8YyaQ9VL0pCS8zapcq4SVOBZmPDO8/cnknlWcDBwn4NYnPjLkQ+qE9OtOVlYpeVHDCEkkkJyT+SuQzy5Y0ru6Ez511/Efa5s1fdkOtyVurIxgdlQAA9gOKKPwolU7remU5hCGYEgo38KUv9I/0TRTDYJCWQBSF4rIN/CRgAR0iTpVD1j1g/qDqJcJqlKcjB9bcda142MpOEJAzgeMnjyTSyze5KEuNRpDoDvC0oe4X9iAeaKKFK+oya6fbOqYbDTeEiAPKpHdS3gBLYc7RQkp3ApQog+cq8nwPJrljzxnPZbUfnugn/NFFRgEVch9xKsH0H8pg6e3x3T3UC1ajaZITGkJLoS4MKbOUrzz/ACKVRRRVzVwtoQmhG1NkWu0HuI+JI8u/Kv/Z';\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    const raw = JSON.stringify({\n        \"user_app_id\": {\n            \"user_id\": USER_ID,\n            \"app_id\": APP_ID\n        },\n        \"inputs\": [\n            {\n                \"data\": {\n                    \"image\": {\n                        \"base64\": IMAGE_BYTES_STRING\n                    }\n                }\n            }\n        ]\n    });\n\n    const requestOptions = {\n        method: 'POST',\n        headers: {\n            'Accept': 'application/json',\n            'Authorization': 'Key ' + PAT\n        },\n        body: raw\n    };\n\n    // NOTE: MODEL_VERSION_ID is optional, you can also call prediction with the MODEL_ID only\n    // https://api.clarifai.com/v2/models/{YOUR_MODEL_ID}/outputs\n    // this will default to the latest version_id\n\n    fetch(\"https://api.clarifai.com/v2/models/\" + MODEL_ID + \"/versions/\" + MODEL_VERSION_ID + \"/outputs\", requestOptions)\n        .then(response => response.text())\n        .then(result => console.log(result))\n        .catch(error => console.log('error', error));\n<\/script>",m="\x3c!--index.html file--\x3e\n\n<script>\n  ///////////////////////////////////////////////////////////////////////////////////////////////////\n  // In this section, we set the user authentication, user and app ID, model details, and the URL\n  // of the image we want as an input. Change these strings to run your own example.\n  //////////////////////////////////////////////////////////////////////////////////////////////////\n\n  // Your PAT (Personal Access Token) can be found in the Account's Security section\n  const PAT = 'YOUR_PAT_HERE';\n  // Specify the correct user_id/app_id pairings\n  // Since you're making inferences outside your app's scope\n  const USER_ID = 'clarifai';\n  const APP_ID = 'main';\n  // Change these to whatever model and image URL you want to use\n  const MODEL_ID = 'general-image-detection';\n  const MODEL_VERSION_ID = '1580bb1932594c93b7e2e04456af7c6f';\n  const IMAGE_URL = 'https://samples.clarifai.com/metro-north.jpg';\n  // To use image bytes, assign its variable   \n  // const IMAGE_BYTES_STRING = '/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAUDBAQEAwUEBAQFBQUGBwwIBwcHBw8LCwkMEQ8SEhEPERETFhwXExQaFRERGCEYGh0dHx8fExciJCIeJBweHx7/2wBDAQUFBQcGBw4ICA4eFBEUHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh7/wAARCAAoACgDASIAAhEBAxEB/8QAGQAAAgMBAAAAAAAAAAAAAAAAAAYDBQcE/8QAMBAAAQMDAwMDAgQHAAAAAAAAAQIDBAAFEQYSIQcTMTJBURRhCBYikSNScXKhsdH/xAAZAQACAwEAAAAAAAAAAAAAAAAFBgIDBAf/xAAtEQABAwMBBgQHAQAAAAAAAAABAgMRAAQhMQUSE0FRYQaBocEUFiJCcrHR8P/aAAwDAQACEQMRAD8A3+RYY1unSYzCS0ttZUkAgktn0q5yT7jPyDUC4wdGwycH5U2Kt9ZQ7VI1qw5PkvQy3CSVPpf7aQjuKyFH25xzn3pHn3TVNy01Hl2hyy6YdkSpKsS9sl/6RlI3rRu3dxWd6spwnAGPIJTfl925fcLaoSDHXvyo6i9SlCQrU9wKln3OyWiaDN1RAbW3kKbSd7gPtwMkH/tTWy9afuy1iPfnXMAblITwkE4yf08cn3pSbYt1uts24XH6fUbiLAuY1MWyGkLEmUW0rcCRvUpQ5CtwKQCPgi4S1ZbDe4sd9NntDEe79m3uOBLTr0IR9jzodSMqUpTu9JJ8owD7UTT4ZCfv9PbP7860m+s+HBSrejWRuz2kAxoesGYxTW/Zlpkwo1vkuSly3UgKWQUhHJUvIHsAaKTemF8XE6sWmxyZkiaZrMh1jv8ArQNpUVqB8FW0njHqx4zRVVhsph1KlKk5xQ+7uHmikaSJrQerMByet2IwvtuTLa4xv2k7Rk84H9x/esHv92d01boenLXGcuiWrFIhLlpbcaQ2/JdK3VJCkAq2pAR7Zz7YxWudY9fxNIdQbNGkR5TyX4aisNNpUMFZAzkj4NK0jq9ZpbLr0PSlzkhrlZDaQlP3P8Q4/ap3F87bPucJEkx/hHv60b2TYXLrKN5sramYECSQRk9M6c6zmJ+eb5Hi22M7cnWGIQgFLbX0zSo4PDa1YBcTgDyMjJ/qbGPabH08SJt1Uzc9QqRliGg5QySPKvgc+TyfYDmmTUWpNYz7ctxoQdPQshCktupckDJUPUcJT6DwMq8YyaQ9VL0pCS8zapcq4SVOBZmPDO8/cnknlWcDBwn4NYnPjLkQ+qE9OtOVlYpeVHDCEkkkJyT+SuQzy5Y0ru6Ez511/Efa5s1fdkOtyVurIxgdlQAA9gOKKPwolU7remU5hCGYEgo38KUv9I/0TRTDYJCWQBSF4rIN/CRgAR0iTpVD1j1g/qDqJcJqlKcjB9bcda142MpOEJAzgeMnjyTSyze5KEuNRpDoDvC0oe4X9iAeaKKFK+oya6fbOqYbDTeEiAPKpHdS3gBLYc7RQkp3ApQog+cq8nwPJrljzxnPZbUfnugn/NFFRgEVch9xKsH0H8pg6e3x3T3UC1ajaZITGkJLoS4MKbOUrzz/ACKVRRRVzVwtoQmhG1NkWu0HuI+JI8u/Kv/Z';\n\n  ///////////////////////////////////////////////////////////////////////////////////\n  // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n  ///////////////////////////////////////////////////////////////////////////////////\n\n  const raw = JSON.stringify({\n      \"user_app_id\": {\n          \"user_id\": USER_ID,\n          \"app_id\": APP_ID\n      },\n      \"inputs\": [\n          {\n              \"data\": {\n                  \"image\": {\n                      \"url\": IMAGE_URL\n                      // \"base64\": IMAGE_BYTES_STRING\n                  }\n              }\n          }\n      ]\n  });\n\n  const requestOptions = {\n      method: 'POST',\n      headers: {\n          'Accept': 'application/json',\n          'Authorization': 'Key ' + PAT\n      },\n      body: raw\n  };\n\n  // NOTE: MODEL_VERSION_ID is optional, you can also call prediction with the MODEL_ID only\n  // https://api.clarifai.com/v2/models/{YOUR_MODEL_ID}/outputs\n  // this will default to the latest version_id\n\n  fetch(\"https://api.clarifai.com/v2/models/\" + MODEL_ID + \"/versions/\" + MODEL_VERSION_ID + \"/outputs\", requestOptions)\n      .then(response => response.json())\n      .then(result => {\n\n          const regions = result.outputs[0].data.regions;\n\n          regions.forEach(region => {\n              // Accessing and rounding the bounding box values\n              const boundingBox = region.region_info.bounding_box;\n              const topRow = boundingBox.top_row.toFixed(3);\n              const leftCol = boundingBox.left_col.toFixed(3);\n              const bottomRow = boundingBox.bottom_row.toFixed(3);\n              const rightCol = boundingBox.right_col.toFixed(3);\n\n              region.data.concepts.forEach(concept => {\n                  // Accessing and rounding the concept value\n                  const name = concept.name;\n                  const value = concept.value.toFixed(4);\n\n                  console.log(`${name}: ${value} BBox: ${topRow}, ${leftCol}, ${bottomRow}, ${rightCol}`);\n                  \n              });\n          });\n\n      })\n      .catch(error => console.log('error', error));\n<\/script>",f="\x3c!--index.html file--\x3e\n\n<script>\n    ///////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, and the URL\n    // of the image we want as an input. Change these strings to run your own example.\n    //////////////////////////////////////////////////////////////////////////////////////////////////\n\n    // Your PAT (Personal Access Token) can be found in the Account's Security section\n    const PAT = 'YOUR_PAT_HERE';\n    // Specify the correct user_id/app_id pairings\n    // Since you're making inferences outside your app's scope\n    const USER_ID = 'clarifai';\n    const APP_ID = 'main';\n    // Change these to whatever model and image URL you want to use\n    const MODEL_ID = 'image-general-segmentation';\n    const MODEL_VERSION_ID = '1581820110264581908ce024b12b4bfb';\n    const IMAGE_URL = 'https://samples.clarifai.com/metro-north.jpg';\n    // To use image bytes, assign its variable   \n    // const IMAGE_BYTES_STRING = '/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAUDBAQEAwUEBAQFBQUGBwwIBwcHBw8LCwkMEQ8SEhEPERETFhwXExQaFRERGCEYGh0dHx8fExciJCIeJBweHx7/2wBDAQUFBQcGBw4ICA4eFBEUHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh7/wAARCAAoACgDASIAAhEBAxEB/8QAGQAAAgMBAAAAAAAAAAAAAAAAAAYDBQcE/8QAMBAAAQMDAwMDAgQHAAAAAAAAAQIDBAAFEQYSIQcTMTJBURRhCBYikSNScXKhsdH/xAAZAQACAwEAAAAAAAAAAAAAAAAFBgIDBAf/xAAtEQABAwMBBgQHAQAAAAAAAAABAgMRAAQhMQUSE0FRYQaBocEUFiJCcrHR8P/aAAwDAQACEQMRAD8A3+RYY1unSYzCS0ttZUkAgktn0q5yT7jPyDUC4wdGwycH5U2Kt9ZQ7VI1qw5PkvQy3CSVPpf7aQjuKyFH25xzn3pHn3TVNy01Hl2hyy6YdkSpKsS9sl/6RlI3rRu3dxWd6spwnAGPIJTfl925fcLaoSDHXvyo6i9SlCQrU9wKln3OyWiaDN1RAbW3kKbSd7gPtwMkH/tTWy9afuy1iPfnXMAblITwkE4yf08cn3pSbYt1uts24XH6fUbiLAuY1MWyGkLEmUW0rcCRvUpQ5CtwKQCPgi4S1ZbDe4sd9NntDEe79m3uOBLTr0IR9jzodSMqUpTu9JJ8owD7UTT4ZCfv9PbP7860m+s+HBSrejWRuz2kAxoesGYxTW/Zlpkwo1vkuSly3UgKWQUhHJUvIHsAaKTemF8XE6sWmxyZkiaZrMh1jv8ArQNpUVqB8FW0njHqx4zRVVhsph1KlKk5xQ+7uHmikaSJrQerMByet2IwvtuTLa4xv2k7Rk84H9x/esHv92d01boenLXGcuiWrFIhLlpbcaQ2/JdK3VJCkAq2pAR7Zz7YxWudY9fxNIdQbNGkR5TyX4aisNNpUMFZAzkj4NK0jq9ZpbLr0PSlzkhrlZDaQlP3P8Q4/ap3F87bPucJEkx/hHv60b2TYXLrKN5sramYECSQRk9M6c6zmJ+eb5Hi22M7cnWGIQgFLbX0zSo4PDa1YBcTgDyMjJ/qbGPabH08SJt1Uzc9QqRliGg5QySPKvgc+TyfYDmmTUWpNYz7ctxoQdPQshCktupckDJUPUcJT6DwMq8YyaQ9VL0pCS8zapcq4SVOBZmPDO8/cnknlWcDBwn4NYnPjLkQ+qE9OtOVlYpeVHDCEkkkJyT+SuQzy5Y0ru6Ez511/Efa5s1fdkOtyVurIxgdlQAA9gOKKPwolU7remU5hCGYEgo38KUv9I/0TRTDYJCWQBSF4rIN/CRgAR0iTpVD1j1g/qDqJcJqlKcjB9bcda142MpOEJAzgeMnjyTSyze5KEuNRpDoDvC0oe4X9iAeaKKFK+oya6fbOqYbDTeEiAPKpHdS3gBLYc7RQkp3ApQog+cq8nwPJrljzxnPZbUfnugn/NFFRgEVch9xKsH0H8pg6e3x3T3UC1ajaZITGkJLoS4MKbOUrzz/ACKVRRRVzVwtoQmhG1NkWu0HuI+JI8u/Kv/Z';\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    const raw = JSON.stringify({\n        \"user_app_id\": {\n            \"user_id\": USER_ID,\n            \"app_id\": APP_ID\n        },\n        \"inputs\": [\n            {\n                \"data\": {\n                    \"image\": {\n                        \"url\": IMAGE_URL\n                        // \"base64\": IMAGE_BYTES_STRING\n                    }\n                }\n            }\n        ]\n    });\n\n    const requestOptions = {\n        method: 'POST',\n        headers: {\n            'Accept': 'application/json',\n            'Authorization': 'Key ' + PAT\n        },\n        body: raw\n    };\n\n    // NOTE: MODEL_VERSION_ID is optional, you can also call prediction with the MODEL_ID only\n    // https://api.clarifai.com/v2/models/{YOUR_MODEL_ID}/outputs\n    // this will default to the latest version_id\n\n    fetch(\"https://api.clarifai.com/v2/models/\" + MODEL_ID + \"/versions/\" + MODEL_VERSION_ID + \"/outputs\", requestOptions)\n        .then(response => response.json())\n        .then(result => {\n\n            const regions = result.outputs[0].data.regions;\n\n            for (const region of regions) {\n                for (const concept of region.data.concepts) {\n                    // Accessing and rounding the concept's percentage of image covered\n                    const name = concept.name;\n                    const value = concept.value.toFixed(4);\n                    console.log(`${name}: ${value}`);\n                }\n            }\n        })\n        .catch(error => console.log('error', error));\n<\/script>",_='\x3c!--index.html file--\x3e\n\n<script>\n    ///////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, and the URL\n    // of the image we want as an input. Change these strings to run your own example.\n    //////////////////////////////////////////////////////////////////////////////////////////////////\n  \n    // Your PAT (Personal Access Token) can be found in the Account\'s Security section\n    const PAT = "YOUR_PAT_HERE";\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    const USER_ID = "salesforce";\n    const APP_ID = "blip";\n    // Change these to whatever model and image URL you want to use\n    const MODEL_ID = "general-english-image-caption-blip";\n    const MODEL_VERSION_ID = "cdb690f13e62470ea6723642044f95e4";\n    const IMAGE_URL = "https://samples.clarifai.com/metro-north.jpg";\n    // To use image bytes, assign its variable   \n    // const IMAGE_BYTES_STRING = \'/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAUDBAQEAwUEBAQFBQUGBwwIBwcHBw8LCwkMEQ8SEhEPERETFhwXExQaFRERGCEYGh0dHx8fExciJCIeJBweHx7/2wBDAQUFBQcGBw4ICA4eFBEUHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh7/wAARCAAoACgDASIAAhEBAxEB/8QAGQAAAgMBAAAAAAAAAAAAAAAAAAYDBQcE/8QAMBAAAQMDAwMDAgQHAAAAAAAAAQIDBAAFEQYSIQcTMTJBURRhCBYikSNScXKhsdH/xAAZAQACAwEAAAAAAAAAAAAAAAAFBgIDBAf/xAAtEQABAwMBBgQHAQAAAAAAAAABAgMRAAQhMQUSE0FRYQaBocEUFiJCcrHR8P/aAAwDAQACEQMRAD8A3+RYY1unSYzCS0ttZUkAgktn0q5yT7jPyDUC4wdGwycH5U2Kt9ZQ7VI1qw5PkvQy3CSVPpf7aQjuKyFH25xzn3pHn3TVNy01Hl2hyy6YdkSpKsS9sl/6RlI3rRu3dxWd6spwnAGPIJTfl925fcLaoSDHXvyo6i9SlCQrU9wKln3OyWiaDN1RAbW3kKbSd7gPtwMkH/tTWy9afuy1iPfnXMAblITwkE4yf08cn3pSbYt1uts24XH6fUbiLAuY1MWyGkLEmUW0rcCRvUpQ5CtwKQCPgi4S1ZbDe4sd9NntDEe79m3uOBLTr0IR9jzodSMqUpTu9JJ8owD7UTT4ZCfv9PbP7860m+s+HBSrejWRuz2kAxoesGYxTW/Zlpkwo1vkuSly3UgKWQUhHJUvIHsAaKTemF8XE6sWmxyZkiaZrMh1jv8ArQNpUVqB8FW0njHqx4zRVVhsph1KlKk5xQ+7uHmikaSJrQerMByet2IwvtuTLa4xv2k7Rk84H9x/esHv92d01boenLXGcuiWrFIhLlpbcaQ2/JdK3VJCkAq2pAR7Zz7YxWudY9fxNIdQbNGkR5TyX4aisNNpUMFZAzkj4NK0jq9ZpbLr0PSlzkhrlZDaQlP3P8Q4/ap3F87bPucJEkx/hHv60b2TYXLrKN5sramYECSQRk9M6c6zmJ+eb5Hi22M7cnWGIQgFLbX0zSo4PDa1YBcTgDyMjJ/qbGPabH08SJt1Uzc9QqRliGg5QySPKvgc+TyfYDmmTUWpNYz7ctxoQdPQshCktupckDJUPUcJT6DwMq8YyaQ9VL0pCS8zapcq4SVOBZmPDO8/cnknlWcDBwn4NYnPjLkQ+qE9OtOVlYpeVHDCEkkkJyT+SuQzy5Y0ru6Ez511/Efa5s1fdkOtyVurIxgdlQAA9gOKKPwolU7remU5hCGYEgo38KUv9I/0TRTDYJCWQBSF4rIN/CRgAR0iTpVD1j1g/qDqJcJqlKcjB9bcda142MpOEJAzgeMnjyTSyze5KEuNRpDoDvC0oe4X9iAeaKKFK+oya6fbOqYbDTeEiAPKpHdS3gBLYc7RQkp3ApQog+cq8nwPJrljzxnPZbUfnugn/NFFRgEVch9xKsH0H8pg6e3x3T3UC1ajaZITGkJLoS4MKbOUrzz/ACKVRRRVzVwtoQmhG1NkWu0HuI+JI8u/Kv/Z\';\n  \n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n  \n    const raw = JSON.stringify({\n      "user_app_id": {\n        "user_id": USER_ID,\n        "app_id": APP_ID\n      },\n      "inputs": [\n        {\n          "data": {\n            "image": {\n              "url": IMAGE_URL\n              // "base64": IMAGE_BYTES_STRING\n            }\n          }\n        }\n      ]\n    });\n  \n    const requestOptions = {\n      method: "POST",\n      headers: {\n        "Accept": "application/json",\n        "Authorization": "Key " + PAT\n      },\n      body: raw\n    };\n  \n    // NOTE: MODEL_VERSION_ID is optional, you can also call prediction with the MODEL_ID only\n    // https://api.clarifai.com/v2/models/{YOUR_MODEL_ID}/outputs\n    // this will default to the latest version_id\n  \n    fetch("https://api.clarifai.com/v2/models/" + MODEL_ID + "/versions/" + MODEL_VERSION_ID + "/outputs", requestOptions)\n      .then(response => response.text())\n      .then(result => console.log(result))\n      .catch(error => console.log("error", error));\n  <\/script>',I='//index.js file\n\n///////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, and the URL\n// of the image we want as an input. Change these strings to run your own example.\n///////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\nconst PAT = \'YOUR_PAT_HERE\';\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\nconst USER_ID = \'clarifai\';\nconst APP_ID = \'main\';\n// Change these to whatever model and image URL you want to use\nconst MODEL_ID = \'general-image-recognition\';\nconst MODEL_VERSION_ID = \'aa7f35c01e0642fda5cf400f543e7c40\';\nconst IMAGE_URL = \'https://samples.clarifai.com/metro-north.jpg\';\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\nstub.PostModelOutputs(\n    {\n        user_app_id: {\n            "user_id": USER_ID,\n            "app_id": APP_ID\n        },\n        model_id: MODEL_ID,\n        version_id: MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n        inputs: [\n            { data: { image: { url: IMAGE_URL, allow_duplicate_url: true } } }\n        ]\n    },\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            throw new Error("Post model outputs failed, status: " + response.status.description);\n        }\n\n        // Since we have one input, one output will exist here\n        const output = response.outputs[0];\n\n        console.log("Predicted concepts:");\n        for (const concept of output.data.concepts) {\n            console.log(concept.name + " " + concept.value);\n        }\n    }\n\n);',E='//index.js file\n\n////////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, and the location\n// of the image we want as an input. Change these strings to run your own example.\n///////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\nconst PAT = \'YOUR_PAT_HERE\';\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\nconst USER_ID = \'clarifai\';\nconst APP_ID = \'main\';\n// Change these to whatever model and image input you want to use\nconst MODEL_ID = \'general-image-recognition\';\nconst MODEL_VERSION_ID = \'aa7f35c01e0642fda5cf400f543e7c40\';\nconst IMAGE_FILE_LOCATION = \'YOUR_IMAGE_FILE_LOCATION_HERE\';\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\nconst fs = require("fs");\nconst imageBytes = fs.readFileSync(IMAGE_FILE_LOCATION);\n\nstub.PostModelOutputs(\n    {\n        user_app_id: {\n            "user_id": USER_ID,\n            "app_id": APP_ID\n        },\n        model_id: MODEL_ID,\n        version_id: MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n        inputs: [\n            { data: { image: { base64: imageBytes } } }\n        ]\n    },\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            throw new Error("Post model outputs failed, status: " + response.status.description);\n        }\n\n        // Since we have one input, one output will exist here\n        const output = response.outputs[0];\n\n        console.log("Predicted concepts:");\n        for (const concept of output.data.concepts) {\n            console.log(concept.name + " " + concept.value);\n        }\n    }\n);',D="//index.js file\n\n///////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, and the URL\n// of the image we want as an input. Change these strings to run your own example.\n///////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account's Security section\nconst PAT = 'YOUR_PAT_HERE';\n// Specify the correct user_id/app_id pairings\n// Since you're making inferences outside your app's scope\nconst USER_ID = 'clarifai';\nconst APP_ID = 'main';\n// Change these to whatever model and image URL you want to use\nconst MODEL_ID = 'general-image-detection';\nconst MODEL_VERSION_ID = '1580bb1932594c93b7e2e04456af7c6f';\nconst IMAGE_URL = 'https://samples.clarifai.com/metro-north.jpg';\n// To use a local file, assign the location variable\n// const IMAGE_FILE_LOCATION = 'YOUR_IMAGE_FILE_LOCATION_HERE'\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require(\"clarifai-nodejs-grpc\");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set(\"authorization\", \"Key \" + PAT);\n\n// To use a local text file, uncomment the following lines\n// const fs = require(\"fs\");\n// const imageBytes = fs.readFileSync(IMAGE_FILE_LOCATION);\n\nstub.PostModelOutputs(\n    {\n        user_app_id: {\n            \"user_id\": USER_ID,\n            \"app_id\": APP_ID\n        },\n        model_id: MODEL_ID,\n        version_id: MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n        inputs: [\n            {\n                data: {\n                    image: {\n                        url: IMAGE_URL,\n                        // base64: imageBytes,\n                        allow_duplicate_url: true\n                    }\n                }\n            }\n        ]\n    },\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            throw new Error(\"Post model outputs failed, status: \" + response.status.description);\n        }\n\n        const regions = response.outputs[0].data.regions;\n\n        regions.forEach(region => {\n            // Accessing and rounding the bounding box values\n            const boundingBox = region.region_info.bounding_box;\n            const topRow = boundingBox.top_row.toFixed(3);\n            const leftCol = boundingBox.left_col.toFixed(3);\n            const bottomRow = boundingBox.bottom_row.toFixed(3);\n            const rightCol = boundingBox.right_col.toFixed(3);\n\n            region.data.concepts.forEach(concept => {\n                // Accessing and rounding the concept value\n                const name = concept.name;\n                const value = concept.value.toFixed(4);\n\n                console.log(`${name}: ${value} BBox: ${topRow}, ${leftCol}, ${bottomRow}, ${rightCol}`);\n\n            });\n        });\n    }\n\n);",b="//index.js file\n\n///////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, and the URL\n// of the image we want as an input. Change these strings to run your own example.\n///////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account's Security section\nconst PAT = 'YOUR_PAT_HERE';\n// Specify the correct user_id/app_id pairings\n// Since you're making inferences outside your app's scope\nconst USER_ID = 'clarifai';\nconst APP_ID = 'main';\n// Change these to whatever model and image URL you want to use\nconst MODEL_ID = 'image-general-segmentation';\nconst MODEL_VERSION_ID = '1581820110264581908ce024b12b4bfb';\nconst IMAGE_URL = 'https://samples.clarifai.com/metro-north.jpg';\n// To use a local file, assign the location variable\n// const IMAGE_FILE_LOCATION = 'YOUR_IMAGE_FILE_LOCATION_HERE'\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require(\"clarifai-nodejs-grpc\");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set(\"authorization\", \"Key \" + PAT);\n\n// To use a local text file, uncomment the following lines\n// const fs = require(\"fs\");\n// const imageBytes = fs.readFileSync(IMAGE_FILE_LOCATION);\n\nstub.PostModelOutputs(\n    {\n        user_app_id: {\n            \"user_id\": USER_ID,\n            \"app_id\": APP_ID\n        },\n        model_id: MODEL_ID,\n        version_id: MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n        inputs: [\n            {\n                data: {\n                    image: {\n                        url: IMAGE_URL,\n                        // base64: imageBytes,\n                        allow_duplicate_url: true\n                    }\n                }\n            }\n        ]\n    },\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            throw new Error(\"Post model outputs failed, status: \" + response.status.description);\n        }\n\n        const regions = response.outputs[0].data.regions;\n\n        for (const region of regions) {\n            for (const concept of region.data.concepts) {\n                // Accessing and rounding the concept's percentage of image covered\n                const name = concept.name;\n                const value = concept.value.toFixed(4);\n                console.log(`${name}: ${value}`);\n            }\n        }\n    }\n);",O='//index.js file\n\n///////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, and the URL\n// of the image we want as an input. Change these strings to run your own example.\n///////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\nconst PAT = "YOUR_PAT_HERE";\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\nconst USER_ID = "salesforce";\nconst APP_ID = "blip";\n// Change these to whatever model and image URL you want to use\nconst MODEL_ID = "general-english-image-caption-blip";\nconst MODEL_VERSION_ID = "cdb690f13e62470ea6723642044f95e4";\nconst IMAGE_URL = "https://samples.clarifai.com/metro-north.jpg";\n// To use a local file, assign the location variable\n// const IMAGE_FILE_LOCATION = "YOUR_IMAGE_FILE_LOCATION_HERE"\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\n// To use a local text file, uncomment the following lines\n// const fs = require("fs");\n// const imageBytes = fs.readFileSync(IMAGE_FILE_LOCATION);\n\nstub.PostModelOutputs({\n    user_app_id: {\n        "user_id": USER_ID,\n        "app_id": APP_ID\n    },\n    model_id: MODEL_ID,\n    version_id: MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n    inputs: [{\n        data: {\n            image: {\n                url: IMAGE_URL,\n                // base64: imageBytes,\n                allow_duplicate_url: true\n            }\n        }\n    }]\n},\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            throw new Error("Post model outputs failed, status: " + response.status.description);\n        }\n\n        // Since we have one input, one output will exist here\n        const output = response.outputs[0];\n\n        console.log("Image caption::");\n        console.log(output.data.text.raw);\n\n    }\n\n);\n',w='package com.clarifai.example;\n\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.status.StatusCode;\n\npublic class ClarifaiExample {\n\t\n\t////////////////////////////////////////////////////////////////////////////////////////////////////\n\t// In this section, we set the user authentication, user and app ID, model details, and the URL\n\t// of the image we want as an input. Change these strings to run your own example.\n\t///////////////////////////////////////////////////////////////////////////////////////////////////\n\t\n\t// Your PAT (Personal Access Token) can be found in the portal under Authentication\n\tstatic final String PAT = "YOUR_PAT_HERE";\n\t// Specify the correct user_id/app_id pairings\n\t// Since you\'re making inferences outside your app\'s scope\n\tstatic final String USER_ID = "clarifai";\t\n\tstatic final String APP_ID = "main";\n\t// Change these to whatever model and image URL you want to use\n\tstatic final String MODEL_ID = "general-image-recognition";\t\n\tstatic final String MODEL_VERSION_ID = "aa7f35c01e0642fda5cf400f543e7c40";\t\n\tstatic final String IMAGE_URL = "https://samples.clarifai.com/metro-north.jpg";\t\t\t\n\t\n\t///////////////////////////////////////////////////////////////////////////////////\n\t// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n\t///////////////////////////////////////////////////////////////////////////////////\t\n\n\tpublic static void main(String[] args) {\n\t\t\n\t\tV2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n\t\t\t    .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\t\t\n\t\tMultiOutputResponse postModelOutputsResponse = stub.postModelOutputs(\n\t\t    PostModelOutputsRequest.newBuilder()\n\t\t    \t.setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\t\t \t\t     \n\t\t        .setModelId(MODEL_ID)\n\t\t        .setVersionId(MODEL_VERSION_ID)  // This is optional. Defaults to the latest model version.\n\t\t        .addInputs(\n\t\t            Input.newBuilder().setData(\n\t\t                Data.newBuilder().setImage(\n\t\t                    Image.newBuilder().setUrl(IMAGE_URL)\n\t\t                )\n\t\t            )\n\t\t        )\n\t\t        .build()\n\t\t);\n\n\t\tif (postModelOutputsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n\t\t  throw new RuntimeException("Post model outputs failed, status: " + postModelOutputsResponse.getStatus());\n\t\t}\n\n\t\t// Since we have one input, one output will exist here.\n\t\tOutput output = postModelOutputsResponse.getOutputs(0);\n\n\t\tSystem.out.println("Predicted concepts:");\n\t\tfor (Concept concept : output.getData().getConceptsList()) {\n\t\t    System.out.printf("%s %.2f%n", concept.getName(), concept.getValue());\n\t\t}\n\n\t}\n\n}\n',S='package com.clarifai.example;\n\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.status.StatusCode;\nimport com.google.protobuf.ByteString;\nimport java.io.File;\nimport java.io.IOException;\nimport java.nio.file.Files;\n\npublic class ClarifaiExample {\n\n    //////////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, and the location\n    // of the image we want as an input. Change these strings to run your own example.\n    /////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n    //Your PAT (Personal Access Token) can be found in the portal under Authentication\t\n    static final String PAT = "YOUR_PAT_HERE";\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    static final String USER_ID = "clarifai";    \n    static final String APP_ID = "main";\n    // Change these to whatever model and image input you want to use\n    static final String MODEL_ID = "general-image-recognition"; \n    static final String MODEL_VERSION_ID = "aa7f35c01e0642fda5cf400f543e7c40";   \n    static final String IMAGE_FILE_LOCATION = "YOUR_IMAGE_FILE_LOCATION_HERE";\n    \n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\t\n\n    public static void main(String[] args) throws IOException {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n            .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        MultiOutputResponse postModelOutputsResponse = stub.postModelOutputs(\n            PostModelOutputsRequest.newBuilder()\n            .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n            .setModelId(MODEL_ID)\n            .setVersionId(MODEL_VERSION_ID) // This is optional. Defaults to the latest model version\n            .addInputs(\n                Input.newBuilder().setData(\n                    Data.newBuilder().setImage(\n                        Image.newBuilder()\n                        .setBase64(ByteString.copyFrom(Files.readAllBytes(\n                            new File(IMAGE_FILE_LOCATION).toPath()\n                        )))\n                    )\n                )\n            )\n            .build()\n        );\n\n        if (postModelOutputsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("Post model outputs failed, status: " + postModelOutputsResponse.getStatus());\n        }\n\n        // Since we have one input, one output will exist here.\n        Output output = postModelOutputsResponse.getOutputs(0);\n\n        System.out.println("Predicted concepts:");\n        for (Concept concept: output.getData().getConceptsList()) {\n            System.out.printf("%s %.2f%n", concept.getName(), concept.getValue());\n        }\n\n    }\n\n}',T='package com.clarifai.example;\n\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.status.StatusCode; \nimport java.util.*;\n\nimport com.google.protobuf.ByteString;\nimport java.io.File;\nimport java.io.IOException;\nimport java.nio.file.Files;\n\npublic class ClarifaiExample {\n\n    ////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, and the URL\n    // of the image we want as an input. Change these strings to run your own example.\n    ///////////////////////////////////////////////////////////////////////////////////////////////////\n\n    // Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    static final String USER_ID = "clarifai";\n    static final String APP_ID = "main";\n    // Change these to whatever model and image URL you want to use\n    static final String MODEL_ID = "general-image-detection";\n    static final String MODEL_VERSION_ID = "1580bb1932594c93b7e2e04456af7c6f";\n    static final String IMAGE_URL = "https://samples.clarifai.com/metro-north.jpg";\n    // Or, to use a local image file, assign the location variable\n    // static final String IMAGE_FILE_LOCATION = "YOUR_IMAGE_FILE_LOCATION_HERE";\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\t\n\n    public static void main(String[] args) throws IOException {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n                .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        MultiOutputResponse postModelOutputsResponse = stub.postModelOutputs(\n                PostModelOutputsRequest.newBuilder()\n                        .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n                        .setModelId(MODEL_ID)\n                        .setVersionId(MODEL_VERSION_ID) // This is optional. Defaults to the latest model version.\n                        .addInputs(\n                                Input.newBuilder().setData(\n                                        Data.newBuilder().setImage(\n                                                Image.newBuilder().setUrl(IMAGE_URL)\n                                         // Image.newBuilder().setBase64(ByteString.copyFrom(Files.readAllBytes(\n                                         // new File(IMAGE_FILE_LOCATION).toPath()\n                                        // )))\n                                        )\n                                )\n                        )\n                        .build()\n        );\n\n        if (postModelOutputsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("Post model outputs failed, status: " + postModelOutputsResponse.getStatus());\n        }\n               \n        List<Region> regions = postModelOutputsResponse.getOutputs(0).getData().getRegionsList();\n\n        for (Region region : regions) {\n            // Accessing and rounding the bounding box values\n            double topRow = region.getRegionInfo().getBoundingBox().getTopRow();\n            double leftCol = region.getRegionInfo().getBoundingBox().getLeftCol();\n            double bottomRow = region.getRegionInfo().getBoundingBox().getBottomRow();\n            double rightCol = region.getRegionInfo().getBoundingBox().getRightCol();\n\n            for (Concept concept : region.getData().getConceptsList()) {\n                // Accessing and rounding the concept value\n                String name = concept.getName();\n                double value = concept.getValue();\n\n                System.out.println(name + ": " + value + " BBox: " + topRow + ", " + leftCol + ", " + bottomRow + ", " + rightCol);\n            }\n        }\n    }\n\n}\n ',R='package com.clarifai.example;\n\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.status.StatusCode;\nimport java.util.*;\n\nimport com.google.protobuf.ByteString;\nimport java.io.File;\nimport java.io.IOException;\nimport java.nio.file.Files;\n\npublic class ClarifaiExample {\n\n    ////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, and the URL\n    // of the image we want as an input. Change these strings to run your own example.\n    ///////////////////////////////////////////////////////////////////////////////////////////////////\n\n    // Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    static final String USER_ID = "clarifai";\n    static final String APP_ID = "main";\n    // Change these to whatever model and image URL you want to use\n    static final String MODEL_ID = "image-general-segmentation";\n    static final String MODEL_VERSION_ID = "1581820110264581908ce024b12b4bfb";\n    static final String IMAGE_URL = "https://samples.clarifai.com/metro-north.jpg";\n    // Or, to use a local image file, assign the location variable\n    // static final String IMAGE_FILE_LOCATION = "YOUR_IMAGE_FILE_LOCATION_HERE";\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\t\n\n    public static void main(String[] args) throws IOException {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n                .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        MultiOutputResponse postModelOutputsResponse = stub.postModelOutputs(\n                PostModelOutputsRequest.newBuilder()\n                        .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n                        .setModelId(MODEL_ID)\n                        .setVersionId(MODEL_VERSION_ID) // This is optional. Defaults to the latest model version.\n                        .addInputs(\n                                Input.newBuilder().setData(\n                                        Data.newBuilder().setImage(\n                                                Image.newBuilder().setUrl(IMAGE_URL)\n                                        // Image.newBuilder().setBase64(ByteString.copyFrom(Files.readAllBytes(\n                                        // new File(IMAGE_FILE_LOCATION).toPath()\n                                        // )))\n                                        )\n                                )\n                        )\n                        .build()\n        );\n\n        if (postModelOutputsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("Post model outputs failed, status: " + postModelOutputsResponse.getStatus());\n        }\n\n        List<Region> regions = postModelOutputsResponse.getOutputs(0).getData().getRegionsList();\n\n        for (Region region : regions) {\n            for (Concept concept : region.getData().getConceptsList()) {\n                // Accessing and rounding the concept\'s percentage of image covered\n                String name = concept.getName();\n                double value = concept.getValue();\n\n                System.out.println(name + ": " + value);\n            }\n        }\n\n    }\n\n}\n',C='package com.clarifai.example;\n\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.status.StatusCode;\n\nimport com.google.protobuf.ByteString;\nimport java.io.File;\nimport java.io.IOException;\nimport java.nio.file.Files;\n\npublic class ClarifaiExample {\n\n    ////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, and the URL\n    // of the image we want as an input. Change these strings to run your own example.\n    ///////////////////////////////////////////////////////////////////////////////////////////////////\n\n    // Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    static final String USER_ID = "salesforce";\n    static final String APP_ID = "blip";\n    // Change these to whatever model and image URL you want to use\n    static final String MODEL_ID = "general-english-image-caption-blip";\n    static final String MODEL_VERSION_ID = "cdb690f13e62470ea6723642044f95e4";\n    static final String IMAGE_URL = "https://samples.clarifai.com/metro-north.jpg";\n    // Or, to use a local image file, assign the location variable\n    //static final String IMAGE_FILE_LOCATION = "YOUR_IMAGE_FILE_LOCATION_HERE";\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\t\n\n    public static void main(String[] args) throws IOException {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n                .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        MultiOutputResponse postModelOutputsResponse = stub.postModelOutputs(\n                PostModelOutputsRequest.newBuilder()\n                        .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n                        .setModelId(MODEL_ID)\n                        .setVersionId(MODEL_VERSION_ID) // This is optional. Defaults to the latest model version.\n                        .addInputs(\n                                Input.newBuilder().setData(\n                                        Data.newBuilder().setImage(\n                                                Image.newBuilder().setUrl(IMAGE_URL)\n                                        // Image.newBuilder().setBase64(ByteString.copyFrom(Files.readAllBytes(\n                                        // new File(IMAGE_FILE_LOCATION).toPath()\n                                        // )))\n                                        )\n                                )\n                        )\n                        .build()\n        );\n\n        if (postModelOutputsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("Post model outputs failed, status: " + postModelOutputsResponse.getStatus());\n        }\n\n        // Since we have one input, one output will exist here.\n        Output output = postModelOutputsResponse.getOutputs(0);\n\n        System.out.println("Image caption:");\n        System.out.println(output.getData().getText().getRaw());\n\n        // Uncomment this line to print the full Response JSON\n        // System.out.println(output);\n    }\n\n}\n',P="<?php\n\nrequire __DIR__ . '/vendor/autoload.php';\n\n//////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, and the URL\n// of the image we want as an input. Change these strings to run your own example.\n//////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account's Security section\n$PAT = 'YOUR_PAT_HERE';\n// Specify the correct user_id/app_id pairings\n// Since you're making inferences outside your app's scope\n$USER_ID = 'clarifai';\n$APP_ID = 'main';\n// Change these to whatever model and image URL you want to use\n$MODEL_ID = 'general-image-recognition';\n$MODEL_VERSION_ID = 'aa7f35c01e0642fda5cf400f543e7c40';\n$IMAGE_URL = 'https://samples.clarifai.com/metro-north.jpg';\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Image;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\PostModelOutputsRequest;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ['Authorization' => ['Key ' . $PAT ]];\n\n$userDataObject = new UserAppIDSet([\n    'user_id' => $USER_ID, \n    'app_id' => $APP_ID \n]);\n\n// Let's make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client->PostModelOutputs(\n    // The request object carries the request along with the request status and other metadata related to the request itself\n    new PostModelOutputsRequest([\n        'user_app_id' => $userDataObject,\n        'model_id' => $MODEL_ID,  \n        'version_id' => $MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n        'inputs' => [\n            new Input([ // The Input object wraps the Data object in order to meet the API specification                \n                'data' => new Data([ // The Data object is constructed around the Image object. It offers a container that has additional image independent\n                                    // metadata. In this particular use case, no other metadata is needed to be specified\n                    'image' => new Image([ // In the Clarifai platform, an image is defined by a special Image object\n                        'url' => $IMAGE_URL\n                    ])\n                ])\n            ])\n        ]\n    ]),\n    $metadata\n)->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception(\"Error: {$status->details}\");\n}\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure \n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    throw new Exception(\"Failure response: \" . $response->getStatus()->getDescription() . \" \" .\n        $response->getStatus()->getDetails());\n}\n\n// The output of a successful call can be used in many ways. In this example, we loop through all of the predicted concepts \n// and print them out along with their numerical prediction value (confidence)\necho \"Predicted concepts: </br>\";\nforeach ($response->getOutputs()[0]->getData()->getConcepts() as $concept) {\n    echo $concept->getName() . \": \" . number_format($concept->getValue(), 2) . \"</br>\";\n}\n\n?>",y="<?php\n\nrequire __DIR__ . '/vendor/autoload.php';\n\n////////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, and the location\n// of the image we want as an input. Change these strings to run your own example.\n///////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account's Security section\n$PAT = 'YOUR_PAT_HERE';\n// Specify the correct user_id/app_id pairings\n// Since you're making inferences outside your app's scope\n$USER_ID = 'clarifai';\n$APP_ID = 'main';\n// Change these to whatever model and image input you want to use\n$MODEL_ID = 'general-image-recognition';\n$MODEL_VERSION_ID = 'aa7f35c01e0642fda5cf400f543e7c40';\n$IMAGE_FILE_LOCATION = 'YOUR_IMAGE_FILE_LOCATION_HERE';\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Image;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\PostModelOutputsRequest;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ['Authorization' => ['Key ' . $PAT ]];\n\n$userDataObject = new UserAppIDSet([\n    'user_id' => $USER_ID, \n    'app_id' => $APP_ID \n]);\n\n$imageData = file_get_contents($IMAGE_FILE_LOCATION); // Get the image bytes data from the location\n\n// Let's make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client->PostModelOutputs(\n    // The request object carries the request along with the request status and other metadata related to the request itself\n    new PostModelOutputsRequest([\n        'user_app_id' => $userDataObject,\n        'model_id' => $MODEL_ID,  \n        'version_id' => $MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n        'inputs' => [\n            new Input([ // The Input object wraps the Data object in order to meet the API specification                \n                'data' => new Data([ // The Data object is constructed around the Image object. It offers a container that has additional image independent\n                                    // metadata. In this particular use case, no other metadata is needed to be specified\n                    'image' => new Image([ // In the Clarifai platform, an image is defined by a special Image object\n                        'base64' => $imageData\n                    ])\n                ])\n            ])\n        ]\n    ]),\n    $metadata\n)->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception(\"Error: {$status->details}\");\n}\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure \n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    throw new Exception(\"Failure response: \" . $response->getStatus()->getDescription() . \" \" .\n        $response->getStatus()->getDetails());\n}\n\n// The output of a successful call can be used in many ways. In this example, we loop through all of the predicted concepts \n// and print them out along with their numerical prediction value (confidence)\necho \"Predicted concepts: </br>\";\nforeach ($response->getOutputs()[0]->getData()->getConcepts() as $concept) {\n    echo $concept->getName() . \": \" . number_format($concept->getValue(), 2) . \"</br>\";\n}\n\n?>",v='<?php\n\nrequire __DIR__ . "/vendor/autoload.php";\n\n//////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, and the URL\n// of the image we want as an input. Change these strings to run your own example.\n//////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\n$PAT = "YOUR_PAT_HERE";\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\n$USER_ID = "clarifai";\n$APP_ID = "main";\n// Change these to whatever model and image URL you want to use\n$MODEL_ID = "general-image-detection";\n$MODEL_VERSION_ID = "1580bb1932594c93b7e2e04456af7c6f";\n$IMAGE_URL = "https://samples.clarifai.com/metro-north.jpg";\n// To use a local file, assign the location variable\n//$IMAGE_FILE_LOCATION = "YOUR_IMAGE_FILE_LOCATION_HERE";\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Image;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\PostModelOutputsRequest;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ["Authorization" => ["Key " . $PAT]];\n\n$userDataObject = new UserAppIDSet([\n    "user_id" => $USER_ID,\n    "app_id" => $APP_ID,\n]);\n\n// To use a local text file, uncomment the following lines\n// $imageData = file_get_contents($IMAGE_FILE_LOCATION);\n\n// Let\'s make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client\n    ->PostModelOutputs(\n        // The request object carries the request along with the request status and other metadata related to the request itself\n        new PostModelOutputsRequest([\n            "user_app_id" => $userDataObject,\n            "model_id" => $MODEL_ID,\n            "version_id" => $MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n            "inputs" => [\n                new Input([\n                    // The Input object wraps the Data object in order to meet the API specification\n                    "data" => new Data([\n                        // The Data object is constructed around the Image object. It offers a container that has additional image independent\n                        // metadata. In this particular use case, no other metadata is needed to be specified\n                        "image" => new Image([\n                            // In the Clarifai platform, an image is defined by a special Image object\n                            "url" => $IMAGE_URL,\n                            //"base64" => $imageData,\n                        ]),\n                    ]),\n                ]),\n            ],\n        ]),\n        $metadata\n    )\n    ->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception("Error: {$status->details}");\n}\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure\n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    throw new Exception(\n        "Failure response: " .\n            $response->getStatus()->getDescription() .\n            " " .\n            $response->getStatus()->getDetails()\n    );\n}\n\n$regions = $response->getOutputs()[0]->getData()->getRegions();\n\nforeach ($regions as $region) {\n    // Accessing and rounding the bounding box values\n    $topRow = $region->getRegionInfo()->getBoundingBox()->getTopRow();\n    $leftCol = $region->getRegionInfo()->getBoundingBox()->getLeftCol();\n    $bottomRow = $region->getRegionInfo()->getBoundingBox()->getBottomRow();\n    $rightCol = $region->getRegionInfo()->getBoundingBox()->getRightCol();\n\n    foreach ($region->getData()->getConcepts() as $concept) {\n        // Accessing and rounding the concept value\n        $name = $concept->getName();\n        $value = $concept->getValue();\n\n        echo $name . ": " . $value . " BBox: " . $topRow . ", " . $leftCol . ", " . $bottomRow . ", " . $rightCol . "\\n";\n    }\n}\n\n?>\n',x='<?php\n\nrequire __DIR__ . "/vendor/autoload.php";\n\n//////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, and the URL\n// of the image we want as an input. Change these strings to run your own example.\n//////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\n$PAT = "YOUR_PAT_HERE";\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\n$USER_ID = "clarifai";\n$APP_ID = "main";\n// Change these to whatever model and image URL you want to use\n$MODEL_ID = "image-general-segmentation";\n$MODEL_VERSION_ID = "1581820110264581908ce024b12b4bfb";\n$IMAGE_URL = "https://samples.clarifai.com/metro-north.jpg";\n// To use a local file, assign the location variable\n//$IMAGE_FILE_LOCATION = "YOUR_IMAGE_FILE_LOCATION_HERE";\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Image;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\PostModelOutputsRequest;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ["Authorization" => ["Key " . $PAT]];\n\n$userDataObject = new UserAppIDSet([\n    "user_id" => $USER_ID,\n    "app_id" => $APP_ID,\n]);\n\n// To use a local text file, uncomment the following lines\n// $imageData = file_get_contents($IMAGE_FILE_LOCATION);\n\n// Let\'s make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client\n    ->PostModelOutputs(\n        // The request object carries the request along with the request status and other metadata related to the request itself\n        new PostModelOutputsRequest([\n            "user_app_id" => $userDataObject,\n            "model_id" => $MODEL_ID,\n            "version_id" => $MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n            "inputs" => [\n                new Input([\n                    // The Input object wraps the Data object in order to meet the API specification\n                    "data" => new Data([\n                        // The Data object is constructed around the Image object. It offers a container that has additional image independent\n                        // metadata. In this particular use case, no other metadata is needed to be specified\n                        "image" => new Image([\n                            // In the Clarifai platform, an image is defined by a special Image object\n                            "url" => $IMAGE_URL,\n                            //"base64" => $imageData,\n                        ]),\n                    ]),\n                ]),\n            ],\n        ]),\n        $metadata\n    )\n    ->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception("Error: {$status->details}");\n}\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure\n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    throw new Exception(\n        "Failure response: " .\n            $response->getStatus()->getDescription() .\n            " " .\n            $response->getStatus()->getDetails()\n    );\n}\n\n$regions = $response->getOutputs()[0]->getData()->getRegions();\n\nforeach ($regions as $region) {\n    foreach ($region->getData()->getConcepts() as $concept) {\n        // Accessing and rounding the concept\'s percentage of image covered\n        $name = $concept->getName();\n        $value = $concept->getValue();\n\n        echo $name . ": " . $value . "\\n";\n    }\n}\n\n?>\n',L='<?php\n\nrequire __DIR__ . "/vendor/autoload.php";\n\n//////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, and the URL\n// of the image we want as an input. Change these strings to run your own example.\n//////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\n$PAT = "YOUR_PAT_HERE";\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\n$USER_ID = "salesforce";\n$APP_ID = "blip";\n// Change these to whatever model and image URL you want to use\n$MODEL_ID = "general-english-image-caption-blip";\n$MODEL_VERSION_ID = "cdb690f13e62470ea6723642044f95e4";\n$IMAGE_URL = "https://samples.clarifai.com/metro-north.jpg";\n// To use a local file, assign the location variable\n// $IMAGE_FILE_LOCATION = "YOUR_IMAGE_FILE_LOCATION_HERE";\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Image;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\PostModelOutputsRequest;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ["Authorization" => ["Key " . $PAT]];\n\n$userDataObject = new UserAppIDSet([\n    "user_id" => $USER_ID,\n    "app_id" => $APP_ID,\n]);\n\n// To use a local text file, uncomment the following lines\n// $imageData = file_get_contents($IMAGE_FILE_LOCATION);\n\n// Let\'s make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client\n    ->PostModelOutputs(\n        // The request object carries the request along with the request status and other metadata related to the request itself\n        new PostModelOutputsRequest([\n            "user_app_id" => $userDataObject,\n            "model_id" => $MODEL_ID,\n            "version_id" => $MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n            "inputs" => [\n                new Input([\n                    // The Input object wraps the Data object in order to meet the API specification\n                    "data" => new Data([\n                        // The Data object is constructed around the Image object. It offers a container that has additional image independent\n                        // metadata. In this particular use case, no other metadata is needed to be specified\n                        "image" => new Image([\n                            // In the Clarifai platform, an image is defined by a special Image object\n                            "url" => $IMAGE_URL,\n                            // "base64" => $imageData,\n                        ]),\n                    ]),\n                ]),\n            ],\n        ]),\n        $metadata\n    )\n    ->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception("Error: {$status->details}");\n}\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure\n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    throw new Exception(\n        "Failure response: " .\n            $response->getStatus()->getDescription() .\n            " " .\n            $response->getStatus()->getDetails()\n    );\n}\n\n// Since we have one input, one output will exist here\n$output = $response->getOutputs()[0];\n\necho "Image caption:" . "\\n";\necho $output->getData()->getText()->getRaw() . "\\n";\n\n// Uncomment this line to print the full Response JSON\n// echo json_encode($output->serializeToJsonString()) . "\\n";\n\n?>\n',U='curl -X POST "https://api.clarifai.com/v2/users/clarifai/apps/main/models/general-image-recognition/versions/aa7f35c01e0642fda5cf400f543e7c40/outputs" \\\n    -H "Authorization: Key YOUR_PAT_HERE" \\\n    -H "Content-Type: application/json" \\\n    -d \'{\n      "inputs": [\n        {\n          "data": {\n            "image": {\n              "url": "https://samples.clarifai.com/metro-north.jpg"\n            }\n          }\n        }\n      ]\n    }\'\n   ',N='# Smaller files (195 KB or less)\n\ncurl -X POST "https://api.clarifai.com/v2/users/clarifai/apps/main/models/general-image-recognition/versions/aa7f35c01e0642fda5cf400f543e7c40/outputs" \\\n  -H "Authorization: Key YOUR_PAT_HERE" \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "inputs": [\n      {\n        "data": {\n          "image": {\n            "base64": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAUDBAQEAwUEBAQFBQUGBwwIBwcHBw8LCwkMEQ8SEhEPERETFhwXExQaFRERGCEYGh0dHx8fExciJCIeJBweHx7/2wBDAQUFBQcGBw4ICA4eFBEUHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh7/wAARCAAoACgDASIAAhEBAxEB/8QAGQAAAgMBAAAAAAAAAAAAAAAAAAYDBQcE/8QAMBAAAQMDAwMDAgQHAAAAAAAAAQIDBAAFEQYSIQcTMTJBURRhCBYikSNScXKhsdH/xAAZAQACAwEAAAAAAAAAAAAAAAAFBgIDBAf/xAAtEQABAwMBBgQHAQAAAAAAAAABAgMRAAQhMQUSE0FRYQaBocEUFiJCcrHR8P/aAAwDAQACEQMRAD8A3+RYY1unSYzCS0ttZUkAgktn0q5yT7jPyDUC4wdGwycH5U2Kt9ZQ7VI1qw5PkvQy3CSVPpf7aQjuKyFH25xzn3pHn3TVNy01Hl2hyy6YdkSpKsS9sl/6RlI3rRu3dxWd6spwnAGPIJTfl925fcLaoSDHXvyo6i9SlCQrU9wKln3OyWiaDN1RAbW3kKbSd7gPtwMkH/tTWy9afuy1iPfnXMAblITwkE4yf08cn3pSbYt1uts24XH6fUbiLAuY1MWyGkLEmUW0rcCRvUpQ5CtwKQCPgi4S1ZbDe4sd9NntDEe79m3uOBLTr0IR9jzodSMqUpTu9JJ8owD7UTT4ZCfv9PbP7860m+s+HBSrejWRuz2kAxoesGYxTW/Zlpkwo1vkuSly3UgKWQUhHJUvIHsAaKTemF8XE6sWmxyZkiaZrMh1jv8ArQNpUVqB8FW0njHqx4zRVVhsph1KlKk5xQ+7uHmikaSJrQerMByet2IwvtuTLa4xv2k7Rk84H9x/esHv92d01boenLXGcuiWrFIhLlpbcaQ2/JdK3VJCkAq2pAR7Zz7YxWudY9fxNIdQbNGkR5TyX4aisNNpUMFZAzkj4NK0jq9ZpbLr0PSlzkhrlZDaQlP3P8Q4/ap3F87bPucJEkx/hHv60b2TYXLrKN5sramYECSQRk9M6c6zmJ+eb5Hi22M7cnWGIQgFLbX0zSo4PDa1YBcTgDyMjJ/qbGPabH08SJt1Uzc9QqRliGg5QySPKvgc+TyfYDmmTUWpNYz7ctxoQdPQshCktupckDJUPUcJT6DwMq8YyaQ9VL0pCS8zapcq4SVOBZmPDO8/cnknlWcDBwn4NYnPjLkQ+qE9OtOVlYpeVHDCEkkkJyT+SuQzy5Y0ru6Ez511/Efa5s1fdkOtyVurIxgdlQAA9gOKKPwolU7remU5hCGYEgo38KUv9I/0TRTDYJCWQBSF4rIN/CRgAR0iTpVD1j1g/qDqJcJqlKcjB9bcda142MpOEJAzgeMnjyTSyze5KEuNRpDoDvC0oe4X9iAeaKKFK+oya6fbOqYbDTeEiAPKpHdS3gBLYc7RQkp3ApQog+cq8nwPJrljzxnPZbUfnugn/NFFRgEVch9xKsH0H8pg6e3x3T3UC1ajaZITGkJLoS4MKbOUrzz/ACKVRRRVzVwtoQmhG1NkWu0HuI+JI8u/Kv/Z"\n          }\n        }\n      }\n    ]\n  }\'\n\n# Larger Files (Greater than 195 KB)\n\ncurl -X POST "https://api.clarifai.com/v2/users/clarifai/apps/main/models/general-image-recognition/versions/aa7f35c01e0642fda5cf400f543e7c40/outputs" \\\n  -H "Authorization: Key YOUR_PAT_HERE" \\\n  -H "Content-Type: application/json" \\\n  -d @-  << FILEIN\n  {\n    "inputs": [\n      {\n        "data": {\n          "image": {\n            "base64": "$(base64 /home/user/image.png)"\n          }\n        }\n      }\n    ]\n  }\nFILEIN',M='curl -X POST "https://api.clarifai.com/v2/users/clarifai/apps/main/models/general-image-detection/versions/1580bb1932594c93b7e2e04456af7c6f/outputs" \\\n    -H "Authorization: Key YOUR_PAT_HERE" \\\n    -H "Content-Type: application/json" \\\n    -d \'{\n      "inputs": [\n        {\n          "data": {\n            "image": {\n              "url": "https://samples.clarifai.com/metro-north.jpg"\n            }\n          }\n        }\n      ]\n    }\'\n   ',j='curl -X POST "https://api.clarifai.com/v2/users/clarifai/apps/main/models/image-general-segmentation/versions/1581820110264581908ce024b12b4bfb/outputs" \\\n    -H "Authorization: Key YOUR_PAT_HERE" \\\n    -H "Content-Type: application/json" \\\n    -d \'{\n      "inputs": [\n        {\n          "data": {\n            "image": {\n              "url": "https://samples.clarifai.com/metro-north.jpg"\n            }\n          }\n        }\n      ]\n    }\'\n   ',H='curl -X POST "https://api.clarifai.com/v2/users/clarifai/apps/main/models/general-image-recognition/versions/aa7f35c01e0642fda5cf400f543e7c40/outputs" \\\n    -H "Authorization: Key YOUR_PAT_HERE" \\\n    -H "Content-Type: application/json" \\\n    -d \'{\n      "inputs": [\n        {\n          "data": {\n            "image": {\n              "url": "https://samples.clarifai.com/metro-north.jpg"\n            }\n          }\n        },\n        {\n          "data": {\n            "image": {\n              "url": "...any other valid image url..."\n            }\n          }\n        },\n        # ... and so on\n      ]\n    }\'\n   ',B='curl -X POST "https://api.clarifai.com/v2/users/salesforce/apps/blip/models/general-english-image-caption-blip/versions/cdb690f13e62470ea6723642044f95e4/outputs" \\\n    -H "Authorization: Key YOUR_PAT_HERE" \\\n    -H "Content-Type: application/json" \\\n    -d \'{\n      "inputs": [\n        {\n          "data": {\n            "image": {\n              "url": "https://samples.clarifai.com/metro-north.jpg"\n            }\n          }\n        }\n      ]\n    }\'\n   ',k="Predicted concepts:\ntrain 1.00\nrailway 1.00\nsubway system 1.00\nstation 1.00\nlocomotive 1.00\ntransportation system 1.00\ntravel 0.99\ncommuter 0.98\nplatform 0.98\nlight 0.97\ntrain station 0.97\nblur 0.97\ncity 0.96\nroad 0.96\nurban 0.96\ntraffic 0.96\nstreet 0.95\npublic 0.93\ntramway 0.93\nbusiness 0.93",Q='id: "ca9767e3dab44da2b7fa811ce6e382f0"\nstatus {\n  code: SUCCESS\n  description: "Ok"\n}\ncreated_at {\n  seconds: 1701796564\n  nanos: 495388804\n}\nmodel {\n  id: "general-image-recognition"\n  name: "Image Recognition"\n  created_at {\n    seconds: 1457543499\n    nanos: 608845000\n  }\n  modified_at {\n    seconds: 1694180313\n    nanos: 148401000\n  }\n  app_id: "main"\n  model_version {\n    id: "aa7f35c01e0642fda5cf400f543e7c40"\n    created_at {\n      seconds: 1520370624\n      nanos: 454834000\n    }\n    status {\n      code: MODEL_TRAINED\n      description: "Model is trained and ready"\n    }\n    visibility {\n      gettable: PUBLIC\n    }\n    app_id: "main"\n    user_id: "clarifai"\n    metadata {\n    }\n  }\n  user_id: "clarifai"\n  model_type_id: "visual-classifier"\n  visibility {\n    gettable: PUBLIC\n  }\n  workflow_recommended {\n  }\n}\ninput {\n  id: "855b331a54554660adb83d56088da511"\n  data {\n    image {\n      url: "https://samples.clarifai.com/metro-north.jpg"\n    }\n  }\n}\ndata {\n  concepts {\n    id: "ai_HLmqFqBf"\n    name: "train"\n    value: 0.999605358\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_fvlBqXZR"\n    name: "railway"\n    value: 0.999298692\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_SHNDcmJ3"\n    name: "subway system"\n    value: 0.998257935\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_6kTjGfF6"\n    name: "station"\n    value: 0.998012304\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_RRXLczch"\n    name: "locomotive"\n    value: 0.99726069\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_Xxjc3MhT"\n    name: "transportation system"\n    value: 0.996979594\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_VRmbGVWh"\n    name: "travel"\n    value: 0.988970637\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_jlb9q33b"\n    name: "commuter"\n    value: 0.980911732\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_2gkfMDsM"\n    name: "platform"\n    value: 0.980664492\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_n9vjC1jB"\n    name: "light"\n    value: 0.974198043\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_sQQj52KZ"\n    name: "train station"\n    value: 0.968836844\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_l4WckcJN"\n    name: "blur"\n    value: 0.967306197\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_WBQfVV0p"\n    name: "city"\n    value: 0.961521745\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_TZ3C79C6"\n    name: "road"\n    value: 0.961392581\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_CpFBRWzD"\n    name: "urban"\n    value: 0.960395515\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_tr0MBp64"\n    name: "traffic"\n    value: 0.959996164\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_GjVpxXrs"\n    name: "street"\n    value: 0.947550297\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_mcSHVRfS"\n    name: "public"\n    value: 0.934354544\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_J6d1kV8t"\n    name: "tramway"\n    value: 0.932101309\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_6lhccv44"\n    name: "business"\n    value: 0.929465771\n    app_id: "main"\n  }\n}',Y="Building: 0.9396 BBox: 0.216, 0.002, 0.552, 0.25\nPerson: 0.8321 BBox: 0.497, 0.647, 0.669, 0.697\nTree: 0.6975 BBox: 0.392, 0.365, 0.507, 0.511\nBuilding: 0.6604 BBox: 0.003, 0.305, 0.974, 0.999\nTree: 0.5274 BBox: 0.378, 0.932, 0.46, 0.998\nBench: 0.4542 BBox: 0.743, 0.822, 0.987, 0.999\nLand vehicle: 0.4328 BBox: 0.512, 0.61, 0.573, 0.644\nPerson: 0.3903 BBox: 0.522, 0.039, 0.586, 0.058\nTrain: 0.3746 BBox: 0.471, 0.29, 0.543, 0.472\nWaste container: 0.3714 BBox: 0.539, 0.738, 0.849, 0.893\nPerson: 0.3326 BBox: 0.532, 0.072, 0.578, 0.106",G="sky-other: 0.2198\nrailroad: 0.1943\nplatform: 0.1773\nceiling-other: 0.1658\nbuilding-other: 0.1185\ntrain: 0.0939\ntree: 0.0098\nperson: 0.008\nunlabeled: 0.0077\nwall-concrete: 0.0047\nfence: 0.0001",V="Image caption:\na photograph of a train station with a train on the tracks",$='id: "d13769d2c8da461d9c806246ed925047"\nstatus {\n  code: SUCCESS\n  description: "Ok"\n}\ncreated_at {\n  seconds: 1700936312\n  nanos: 267440491\n}\nmodel {\n  id: "general-english-image-caption-blip"\n  name: "Image Captioner"\n  created_at {\n    seconds: 1650312092\n    nanos: 67938000\n  }\n  modified_at {\n    seconds: 1660226508\n    nanos: 93093000\n  }\n  app_id: "blip"\n  model_version {\n    id: "cdb690f13e62470ea6723642044f95e4"\n    created_at {\n      seconds: 1681249232\n      nanos: 444463000\n    }\n    status {\n      code: MODEL_TRAINED\n      description: "Model is trained and ready"\n    }\n    visibility {\n      gettable: PUBLIC\n    }\n    app_id: "blip"\n    user_id: "salesforce"\n    metadata {\n    }\n  }\n  user_id: "salesforce"\n  model_type_id: "image-to-text"\n  visibility {\n    gettable: PUBLIC\n  }\n  workflow_recommended {\n  }\n}\ninput {\n  id: "361e341bdc2541339fcf8b5c7c9e1452"\n  data {\n    image {\n      url: "https://samples.clarifai.com/metro-north.jpg"\n    }\n  }\n}\ndata {\n  text {\n    raw: "a photograph of a train station with a train on the tracks"\n    text_info {\n      encoding: "UnknownTextEnc"\n    }\n  }\n}',F={description:"Make predictions on image inputs",sidebar_position:1},q="Images",K={id:"api-guide/predict/images",title:"Images",description:"Make predictions on image inputs",source:"@site/docs/api-guide/predict/images.md",sourceDirName:"api-guide/predict",slug:"/api-guide/predict/images",permalink:"/api-guide/predict/images",draft:!1,unlisted:!1,editUrl:"https://github.com/Clarifai/docs/blob/main/docs/api-guide/predict/images.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{description:"Make predictions on image inputs",sidebar_position:1},sidebar:"tutorialSidebar",previous:{title:"Making Predictions",permalink:"/api-guide/predict/"},next:{title:"Video",permalink:"/api-guide/predict/video"}},J={},z=[{value:"Visual Classification",id:"visual-classification",level:2},{value:"Predict via URL",id:"predict-via-url",level:3},{value:"Predict via Bytes",id:"predict-via-bytes",level:3},{value:"Predict Multiple Inputs",id:"predict-multiple-inputs",level:3},{value:"Visual Detection",id:"visual-detection",level:2},{value:"Visual Segmentation",id:"visual-segmentation",level:2},{value:"Image-to-Text",id:"image-to-text",level:2}];function W(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",p:"p",strong:"strong",...(0,i.R)(),...e.components},{Details:t}=n;return t||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h1,{id:"images",children:"Images"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Make predictions on image inputs"})}),"\n",(0,a.jsx)("hr",{}),"\n",(0,a.jsx)(n.p,{children:"To get predictions for a given image input, you need to supply the image along with the specific model from which you wish to receive predictions. You can supply the image via a publicly accessible URL or by directly sending bytes."}),"\n",(0,a.jsx)(n.p,{children:"You can send up to 128 images in one API call. Each image input should be limited to 85 megapixels and should not exceed 20MB in size."}),"\n",(0,a.jsxs)(n.p,{children:["You need to specify your choice of ",(0,a.jsx)(n.a,{href:"https://clarifai.com/explore/models?page=1&perPage=24&filterData=%5B%7B%22field%22%3A%22input_fields%22%2C%22value%22%3A%5B%22image%22%5D%7D%5D",children:"model"})," for prediction by utilizing the ",(0,a.jsx)(n.code,{children:"MODEL_ID"})," parameter."]}),"\n",(0,a.jsxs)(n.admonition,{type:"tip",children:[(0,a.jsxs)(n.p,{children:["When you take an image with a digital device (such as a smartphone camera) the image's meta-information (such as the orientation value for how the camera is held) is stored in the image's ",(0,a.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Exif",children:"Exif's data"}),". And when you use a photo viewer to check the image on your computer, the photo viewer will respect that orientation value and automatically rotate the image to present it the way it was viewed. This allows you to see a correctly-oriented image no matter how the camera was held."]}),(0,a.jsx)(n.p,{children:"So, when you want to make predictions from an image taken with a digital device, you need to strip the Exif data from the image. Since the Clarifai platform does not account for the Exif data, removing it allows you to make accurate predictions using images in their desired rotation."})]}),"\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n",(0,a.jsx)(n.h2,{id:"visual-classification",children:"Visual Classification"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Input"}),": Image"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Output"}),": ",(0,a.jsx)(n.a,{href:"https://docs.clarifai.com/portal-guide/concepts/create-get-update-delete",children:"Concepts"})]}),"\n",(0,a.jsx)(n.p,{children:"Visual classification, also known as image classification, is the process of categorizing images into predefined classes based on their visual content. Machine learning models are employed to recognize patterns within images and assign them to the appropriate class."}),"\n",(0,a.jsx)(n.p,{children:"After training, these models can classify new, unseen images by analyzing their visual content and assigning them to predefined categories based on what they've learned during training."}),"\n",(0,a.jsx)(n.h3,{id:"predict-via-url",children:"Predict via URL"}),"\n",(0,a.jsxs)(n.p,{children:["Below is an example of how you would send image URLs and receive predictions from Clarifai's ",(0,a.jsx)(n.a,{href:"https://clarifai.com/clarifai/main/models/general-image-recognition",children:(0,a.jsx)(n.code,{children:"general-image-recognition"})})," model."]}),"\n",(0,a.jsx)(n.admonition,{type:"info",children:(0,a.jsxs)(n.p,{children:["The initialization code used in the following examples is outlined in detail on the ",(0,a.jsx)(n.a,{href:"https://docs.clarifai.com/api-guide/api-overview/api-clients/#client-installation-instructions",children:"client installation page."})]})}),"\n",(0,a.jsxs)(s.A,{children:[(0,a.jsx)(o.A,{value:"python",label:"Python",children:(0,a.jsx)(r.A,{className:"language-python",children:c})}),(0,a.jsx)(o.A,{value:"js_rest",label:"JavaScript (REST)",children:(0,a.jsx)(r.A,{className:"language-javascript",children:g})}),(0,a.jsx)(o.A,{value:"nodejs",label:"NodeJS",children:(0,a.jsx)(r.A,{className:"language-javascript",children:I})}),(0,a.jsx)(o.A,{value:"java",label:"Java",children:(0,a.jsx)(r.A,{className:"language-java",children:w})}),(0,a.jsx)(o.A,{value:"php",label:"PHP",children:(0,a.jsx)(r.A,{className:"language-php",children:P})}),(0,a.jsx)(o.A,{value:"curl",label:"cURL",children:(0,a.jsx)(r.A,{className:"language-bash",children:U})})]}),"\n",(0,a.jsxs)(t,{children:[(0,a.jsx)("summary",{children:"Text Output Example"}),(0,a.jsx)(r.A,{className:"language-text",children:k})]}),"\n",(0,a.jsxs)(t,{children:[(0,a.jsx)("summary",{children:"JSON Output Example"}),(0,a.jsx)(r.A,{className:"language-javascript",children:Q})]}),"\n",(0,a.jsx)(n.h3,{id:"predict-via-bytes",children:"Predict via Bytes"}),"\n",(0,a.jsxs)(n.p,{children:["Below is an example of how you would send the bytes of an image and receive predictions from Clarifai's ",(0,a.jsx)(n.a,{href:"https://clarifai.com/clarifai/main/models/general-image-recognition",children:(0,a.jsx)(n.code,{children:"general-image-recognition"})})," model."]}),"\n",(0,a.jsxs)(s.A,{children:[(0,a.jsx)(o.A,{value:"python",label:"Python",children:(0,a.jsx)(r.A,{className:"language-python",children:u})}),(0,a.jsx)(o.A,{value:"js_rest",label:"JavaScript (REST)",children:(0,a.jsx)(r.A,{className:"language-javascript",children:A})}),(0,a.jsx)(o.A,{value:"nodejs",label:"NodeJS",children:(0,a.jsx)(r.A,{className:"language-javascript",children:E})}),(0,a.jsx)(o.A,{value:"java",label:"Java",children:(0,a.jsx)(r.A,{className:"language-java",children:S})}),(0,a.jsx)(o.A,{value:"php",label:"PHP",children:(0,a.jsx)(r.A,{className:"language-php",children:y})}),(0,a.jsx)(o.A,{value:"curl",label:"cURL",children:(0,a.jsx)(r.A,{className:"language-bash",children:N})})]}),"\n",(0,a.jsx)(n.h3,{id:"predict-multiple-inputs",children:"Predict Multiple Inputs"}),"\n",(0,a.jsx)(n.p,{children:"To predict multiple inputs at once and avoid the need for numerous API calls, you can use the following approach. Note that these examples are provided for cURL and Python, but the same concept is applicable to any supported programming language."}),"\n",(0,a.jsxs)(s.A,{children:[(0,a.jsx)(o.A,{value:"curl",label:"cURL",children:(0,a.jsx)(r.A,{className:"language-bash",children:H})}),(0,a.jsx)(o.A,{value:"python",label:"Python",children:(0,a.jsx)(r.A,{className:"language-python",children:l})})]}),"\n",(0,a.jsx)(n.h2,{id:"visual-detection",children:"Visual Detection"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Input"}),": Image"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Output"}),": Regions[...].data.concepts,regions[...].region_info"]}),"\n",(0,a.jsx)(n.p,{children:"Visual detection, also known as object detection, involves identifying and locating objects or specific regions of interest within images."}),"\n",(0,a.jsx)(n.p,{children:"Unlike image classification, which assigns a single label or category to the entire image, visual detection provides more detailed information by detecting and outlining multiple objects or regions within the image, associating them with specific classes or labels."}),"\n",(0,a.jsx)(n.p,{children:"Visual detection models are trained on labeled datasets with class labels and bounding box coordinates, enabling them to recognize object patterns and positions during inferencing."}),"\n",(0,a.jsxs)(n.p,{children:["Below is an example of how you would perform visual detection using the Clarifai's ",(0,a.jsx)(n.a,{href:"https://clarifai.com/clarifai/main/models/general-image-detection",children:(0,a.jsx)(n.code,{children:"general-image-detection"})})," model."]}),"\n",(0,a.jsxs)(s.A,{children:[(0,a.jsx)(o.A,{value:"python",label:"Python",children:(0,a.jsx)(r.A,{className:"language-python",children:p})}),(0,a.jsx)(o.A,{value:"js_rest",label:"JavaScript (REST)",children:(0,a.jsx)(r.A,{className:"language-javascript",children:m})}),(0,a.jsx)(o.A,{value:"nodejs",label:"NodeJS",children:(0,a.jsx)(r.A,{className:"language-javascript",children:D})}),(0,a.jsx)(o.A,{value:"java",label:"Java",children:(0,a.jsx)(r.A,{className:"language-java",children:T})}),(0,a.jsx)(o.A,{value:"php",label:"PHP",children:(0,a.jsx)(r.A,{className:"language-php",children:v})}),(0,a.jsx)(o.A,{value:"curl",label:"cURL",children:(0,a.jsx)(r.A,{className:"language-bash",children:M})})]}),"\n",(0,a.jsxs)(t,{children:[(0,a.jsx)("summary",{children:"Text Output Example"}),(0,a.jsx)(r.A,{className:"language-text",children:Y})]}),"\n",(0,a.jsx)(n.h2,{id:"visual-segmentation",children:"Visual Segmentation"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Input"}),": Image"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Output"}),": Regions[...].region_info.mask,regions[...].data.con"]}),"\n",(0,a.jsx)(n.p,{children:"Visual segmentation, or image segmentation, involves partitioning an image into distinct regions, each representing a meaningful object or component. Its purpose is to break down an image into meaningful parts, making it easier to analyze and understand."}),"\n",(0,a.jsx)(n.p,{children:"This is achieved by assigning labels to individual pixels based on shared characteristics. Image segmentation is commonly used to locate objects and boundaries in images, resulting in a set of segments that cover the entire image or a set of extracted contours."}),"\n",(0,a.jsxs)(n.p,{children:["Below is an example of how you would perform visual segmentation using the Clarifai's ",(0,a.jsx)(n.a,{href:"https://clarifai.com/clarifai/main/models/image-general-segmentation",children:(0,a.jsx)(n.code,{children:"image-general-segmentation"})})," model."]}),"\n",(0,a.jsxs)(s.A,{children:[(0,a.jsx)(o.A,{value:"python",label:"Python",children:(0,a.jsx)(r.A,{className:"language-python",children:d})}),(0,a.jsx)(o.A,{value:"js_rest",label:"JavaScript (REST)",children:(0,a.jsx)(r.A,{className:"language-javascript",children:f})}),(0,a.jsx)(o.A,{value:"nodejs",label:"NodeJS",children:(0,a.jsx)(r.A,{className:"language-javascript",children:b})}),(0,a.jsx)(o.A,{value:"java",label:"Java",children:(0,a.jsx)(r.A,{className:"language-java",children:R})}),(0,a.jsx)(o.A,{value:"php",label:"PHP",children:(0,a.jsx)(r.A,{className:"language-php",children:x})}),(0,a.jsx)(o.A,{value:"curl",label:"cURL",children:(0,a.jsx)(r.A,{className:"language-bash",children:j})})]}),"\n",(0,a.jsxs)(t,{children:[(0,a.jsx)("summary",{children:"Text Output Example"}),(0,a.jsx)(r.A,{className:"language-text",children:G})]}),"\n",(0,a.jsx)(n.h2,{id:"image-to-text",children:"Image-to-Text"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Input"}),": Image"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Output"}),": Text"]}),"\n",(0,a.jsx)(n.p,{children:"Image-to-text generation, also known as image captioning, refers to the process of generating textual descriptions or captions for images."}),"\n",(0,a.jsx)(n.p,{children:"It involves using a model to analyze the content of an image and then generate a coherent and relevant textual description that describes what is happening in the image\u2014similar to how humans would describe it."}),"\n",(0,a.jsxs)(n.p,{children:["Below is an example of how you would perform image-to-text generation using the ",(0,a.jsx)(n.a,{href:"https://clarifai.com/salesforce/blip/models/general-english-image-caption-blip",children:"general-english-image-caption-blip"})," model."]}),"\n",(0,a.jsxs)(s.A,{children:[(0,a.jsx)(o.A,{value:"python",label:"Python",children:(0,a.jsx)(r.A,{className:"language-python",children:h})}),(0,a.jsx)(o.A,{value:"js_rest",label:"JavaScript (REST)",children:(0,a.jsx)(r.A,{className:"language-javascript",children:_})}),(0,a.jsx)(o.A,{value:"nodejs",label:"NodeJS",children:(0,a.jsx)(r.A,{className:"language-javascript",children:O})}),(0,a.jsx)(o.A,{value:"java",label:"Java",children:(0,a.jsx)(r.A,{className:"language-java",children:C})}),(0,a.jsx)(o.A,{value:"php",label:"PHP",children:(0,a.jsx)(r.A,{className:"language-php",children:L})}),(0,a.jsx)(o.A,{value:"curl",label:"cURL",children:(0,a.jsx)(r.A,{className:"language-bash",children:B})})]}),"\n",(0,a.jsxs)(t,{children:[(0,a.jsx)("summary",{children:"Text Output Example"}),(0,a.jsx)(r.A,{className:"language-text",children:V})]}),"\n",(0,a.jsxs)(t,{children:[(0,a.jsx)("summary",{children:"JSON Output Example"}),(0,a.jsx)(r.A,{className:"language-javascript",children:$})]})]})}function X(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(W,{...e})}):W(e)}},19365:(e,n,t)=>{t.d(n,{A:()=>o});t(96540);var a=t(18215);const i={tabItem:"tabItem_Ymn6"};var s=t(74848);function o(e){let{children:n,hidden:t,className:o}=e;return(0,s.jsx)("div",{role:"tabpanel",className:(0,a.A)(i.tabItem,o),hidden:t,children:n})}},11470:(e,n,t)=>{t.d(n,{A:()=>b});var a=t(96540),i=t(18215),s=t(23104),o=t(56347),r=t(205),c=t(57485),l=t(31682),u=t(70679);function p(e){return a.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,a.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function d(e){const{values:n,children:t}=e;return(0,a.useMemo)((()=>{const e=n??function(e){return p(e).map((e=>{let{props:{value:n,label:t,attributes:a,default:i}}=e;return{value:n,label:t,attributes:a,default:i}}))}(t);return function(e){const n=(0,l.X)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,t])}function h(e){let{value:n,tabValues:t}=e;return t.some((e=>e.value===n))}function g(e){let{queryString:n=!1,groupId:t}=e;const i=(0,o.W6)(),s=function(e){let{queryString:n=!1,groupId:t}=e;if("string"==typeof n)return n;if(!1===n)return null;if(!0===n&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:n,groupId:t});return[(0,c.aZ)(s),(0,a.useCallback)((e=>{if(!s)return;const n=new URLSearchParams(i.location.search);n.set(s,e),i.replace({...i.location,search:n.toString()})}),[s,i])]}function A(e){const{defaultValue:n,queryString:t=!1,groupId:i}=e,s=d(e),[o,c]=(0,a.useState)((()=>function(e){let{defaultValue:n,tabValues:t}=e;if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(n){if(!h({value:n,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${n}" but none of its children has the corresponding value. Available values are: ${t.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return n}const a=t.find((e=>e.default))??t[0];if(!a)throw new Error("Unexpected error: 0 tabValues");return a.value}({defaultValue:n,tabValues:s}))),[l,p]=g({queryString:t,groupId:i}),[A,m]=function(e){let{groupId:n}=e;const t=function(e){return e?`docusaurus.tab.${e}`:null}(n),[i,s]=(0,u.Dv)(t);return[i,(0,a.useCallback)((e=>{t&&s.set(e)}),[t,s])]}({groupId:i}),f=(()=>{const e=l??A;return h({value:e,tabValues:s})?e:null})();(0,r.A)((()=>{f&&c(f)}),[f]);return{selectedValue:o,selectValue:(0,a.useCallback)((e=>{if(!h({value:e,tabValues:s}))throw new Error(`Can't select invalid tab value=${e}`);c(e),p(e),m(e)}),[p,m,s]),tabValues:s}}var m=t(92303);const f={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var _=t(74848);function I(e){let{className:n,block:t,selectedValue:a,selectValue:o,tabValues:r}=e;const c=[],{blockElementScrollPositionUntilNextRender:l}=(0,s.a_)(),u=e=>{const n=e.currentTarget,t=c.indexOf(n),i=r[t].value;i!==a&&(l(n),o(i))},p=e=>{let n=null;switch(e.key){case"Enter":u(e);break;case"ArrowRight":{const t=c.indexOf(e.currentTarget)+1;n=c[t]??c[0];break}case"ArrowLeft":{const t=c.indexOf(e.currentTarget)-1;n=c[t]??c[c.length-1];break}}n?.focus()};return(0,_.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,i.A)("tabs",{"tabs--block":t},n),children:r.map((e=>{let{value:n,label:t,attributes:s}=e;return(0,_.jsx)("li",{role:"tab",tabIndex:a===n?0:-1,"aria-selected":a===n,ref:e=>c.push(e),onKeyDown:p,onClick:u,...s,className:(0,i.A)("tabs__item",f.tabItem,s?.className,{"tabs__item--active":a===n}),children:t??n},n)}))})}function E(e){let{lazy:n,children:t,selectedValue:i}=e;const s=(Array.isArray(t)?t:[t]).filter(Boolean);if(n){const e=s.find((e=>e.props.value===i));return e?(0,a.cloneElement)(e,{className:"margin-top--md"}):null}return(0,_.jsx)("div",{className:"margin-top--md",children:s.map(((e,n)=>(0,a.cloneElement)(e,{key:n,hidden:e.props.value!==i})))})}function D(e){const n=A(e);return(0,_.jsxs)("div",{className:(0,i.A)("tabs-container",f.tabList),children:[(0,_.jsx)(I,{...n,...e}),(0,_.jsx)(E,{...n,...e})]})}function b(e){const n=(0,m.A)();return(0,_.jsx)(D,{...e,children:p(e.children)},String(n))}}}]);
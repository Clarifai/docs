"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[6855],{65537:(e,n,t)=>{t.d(n,{A:()=>N});var a=t(96540),i=t(18215),l=t(65627),r=t(56347),o=t(50372),s=t(30604),p=t(11861),d=t(78749);function _(e){return a.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,a.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function u(e){const{values:n,children:t}=e;return(0,a.useMemo)((()=>{const e=n??function(e){return _(e).map((e=>{let{props:{value:n,label:t,attributes:a,default:i}}=e;return{value:n,label:t,attributes:a,default:i}}))}(t);return function(e){const n=(0,p.XI)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,t])}function m(e){let{value:n,tabValues:t}=e;return t.some((e=>e.value===n))}function c(e){let{queryString:n=!1,groupId:t}=e;const i=(0,r.W6)(),l=function(e){let{queryString:n=!1,groupId:t}=e;if("string"==typeof n)return n;if(!1===n)return null;if(!0===n&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:n,groupId:t});return[(0,s.aZ)(l),(0,a.useCallback)((e=>{if(!l)return;const n=new URLSearchParams(i.location.search);n.set(l,e),i.replace({...i.location,search:n.toString()})}),[l,i])]}function f(e){const{defaultValue:n,queryString:t=!1,groupId:i}=e,l=u(e),[r,s]=(0,a.useState)((()=>function(e){let{defaultValue:n,tabValues:t}=e;if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(n){if(!m({value:n,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${n}" but none of its children has the corresponding value. Available values are: ${t.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return n}const a=t.find((e=>e.default))??t[0];if(!a)throw new Error("Unexpected error: 0 tabValues");return a.value}({defaultValue:n,tabValues:l}))),[p,_]=c({queryString:t,groupId:i}),[f,h]=function(e){let{groupId:n}=e;const t=function(e){return e?`docusaurus.tab.${e}`:null}(n),[i,l]=(0,d.Dv)(t);return[i,(0,a.useCallback)((e=>{t&&l.set(e)}),[t,l])]}({groupId:i}),y=(()=>{const e=p??f;return m({value:e,tabValues:l})?e:null})();(0,o.A)((()=>{y&&s(y)}),[y]);return{selectedValue:r,selectValue:(0,a.useCallback)((e=>{if(!m({value:e,tabValues:l}))throw new Error(`Can't select invalid tab value=${e}`);s(e),_(e),h(e)}),[_,h,l]),tabValues:l}}var h=t(9136);const y={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var g=t(74848);function v(e){let{className:n,block:t,selectedValue:a,selectValue:r,tabValues:o}=e;const s=[],{blockElementScrollPositionUntilNextRender:p}=(0,l.a_)(),d=e=>{const n=e.currentTarget,t=s.indexOf(n),i=o[t].value;i!==a&&(p(n),r(i))},_=e=>{let n=null;switch(e.key){case"Enter":d(e);break;case"ArrowRight":{const t=s.indexOf(e.currentTarget)+1;n=s[t]??s[0];break}case"ArrowLeft":{const t=s.indexOf(e.currentTarget)-1;n=s[t]??s[s.length-1];break}}n?.focus()};return(0,g.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,i.A)("tabs",{"tabs--block":t},n),children:o.map((e=>{let{value:n,label:t,attributes:l}=e;return(0,g.jsx)("li",{role:"tab",tabIndex:a===n?0:-1,"aria-selected":a===n,ref:e=>{s.push(e)},onKeyDown:_,onClick:d,...l,className:(0,i.A)("tabs__item",y.tabItem,l?.className,{"tabs__item--active":a===n}),children:t??n},n)}))})}function b(e){let{lazy:n,children:t,selectedValue:l}=e;const r=(Array.isArray(t)?t:[t]).filter(Boolean);if(n){const e=r.find((e=>e.props.value===l));return e?(0,a.cloneElement)(e,{className:(0,i.A)("margin-top--md",e.props.className)}):null}return(0,g.jsx)("div",{className:"margin-top--md",children:r.map(((e,n)=>(0,a.cloneElement)(e,{key:n,hidden:e.props.value!==l})))})}function E(e){const n=f(e);return(0,g.jsxs)("div",{className:(0,i.A)("tabs-container",y.tabList),children:[(0,g.jsx)(v,{...n,...e}),(0,g.jsx)(b,{...n,...e})]})}function N(e){const n=(0,h.A)();return(0,g.jsx)(E,{...e,children:_(e.children)},String(n))}},78565:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>v,contentTitle:()=>g,default:()=>N,frontMatter:()=>y,metadata:()=>a,toc:()=>b});const a=JSON.parse('{"id":"create/models/templates/README","title":"Training Templates","description":"Learn about our deep fine-tuning template types","source":"@site/docs/create/models/templates/README.mdx","sourceDirName":"create/models/templates","slug":"/create/models/templates/","permalink":"/create/models/templates/","draft":false,"unlisted":false,"editUrl":"https://github.com/Clarifai/docs/blob/main/docs/create/models/templates/README.mdx","tags":[],"version":"current","frontMatter":{"description":"Learn about our deep fine-tuning template types"},"sidebar":"tutorialSidebar","previous":{"title":"Manage Models","permalink":"/create/models/manage"},"next":{"title":"Visual Classification Templates","permalink":"/create/models/templates/visual-classification"}}');var i=t(74848),l=t(28453),r=t(65537),o=t(79329),s=t(58069);const p="#####################################################################################\n# In this section, we set the user authentication, app ID, and model type ID. \n# Change these strings to run your own example.\n####################################################################################\n\nUSER_ID = 'YOUR_USER_ID_HERE'\n# Your PAT (Personal Access Token) can be found in the Account's Security section\nPAT = 'YOUR_PAT_HERE'\nAPP_ID = 'YOUR_APP_ID_HERE'\n# Change this to list the template types of your preferred model \nMODEL_TYPE = 'visual-classifier'\n\n##########################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n##########################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (('authorization', 'Key ' + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\nresponse = stub.ListModelTypes(\n    service_pb2.ListModelTypesRequest(\n        user_app_id=userDataObject\n    ),\n    metadata=metadata\n    )\n\nif response.status.code != status_code_pb2.SUCCESS:\n    print(response.status)\n    raise Exception(\"List models failed, status: \" + response.status.description)\n\nfor model_type in response.model_types:\n   if model_type.id == MODEL_TYPE:\n      for modeltypefield in model_type.model_type_fields:\n        if modeltypefield.path.split('.')[-1] == \"template\":\n          for template in modeltypefield.model_type_enum_options:\n            print(template)",d="\x3c!--index.html file--\x3e\n\n<script>\n    ////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, app ID, and model type ID. \n    // Change these strings to run your own example.\n    ////////////////////////////////////////////////////////////////////////////////////////  \n\n    const USER_ID = 'YOUR_USER_ID_HERE';\n    // Your PAT (Personal Access Token) can be found in the Account's Security section\n    const PAT = 'YOUR_PAT_HERE';\n    const APP_ID = 'YOUR_APP_ID_HERE';\n    // Change this to list the template types of your preferred model \n    const MODEL_TYPE = 'visual-classifier';\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    const requestOptions = {\n        method: 'GET',\n        headers: {\n            'Accept': 'application/json',\n            'Authorization': 'Key ' + PAT\n        }\n    };\n\n    fetch(`https://api.clarifai.com/v2/users/${USER_ID}/apps/${APP_ID}/models/types?per_page=20&page=1`, requestOptions)\n        .then(response => response.json())\n        .then(result => {\n            console.log('Clarifai API Response:', result);\n\n            result.model_types.forEach(modelType => {\n                if (modelType.id === MODEL_TYPE) {\n                    modelType.model_type_fields.forEach(modelTypeField => {\n                        if (modelTypeField.path.split('.').slice(-1)[0] === 'template') {\n                            modelTypeField.model_type_enum_options.forEach(template => {\n                                console.log('Template:', template);\n                            });\n                        }\n                    });\n                }\n            });\n        })\n        .catch(error => console.error('Clarifai API Error:', error));\n\n\n<\/script>",_='//index.js file\n\n///////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, app ID, and model type ID. \n// Change these strings to run your own example.\n//////////////////////////////////////////////////////////////////////////////////////\n\nconst USER_ID = "YOUR_USER_ID_HERE";\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\nconst PAT = "YOUR_PAT_HERE";\nconst APP_ID = "YOUR_APP_ID_HERE";\n// Change this to list the template types of your preferred model \nconst MODEL_TYPE = "visual-classifier";\n\n/////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n/////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\nstub.ListModelTypes(\n    {\n        user_app_id: {\n            "user_id": USER_ID,\n            "app_id": APP_ID\n        },\n        page: 1,\n        per_page: 500\n    },\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            throw new Error("Received status: " + response.status.description + "\\n" + response.status.details);\n        }\n\n        response.model_types.forEach((modelType) => {\n            if (modelType.id === MODEL_TYPE) {\n                modelType.model_type_fields.forEach((modelTypeField) => {\n                    if (modelTypeField.path.split(\'.\').pop() === \'template\') {\n                        modelTypeField.model_type_enum_options.forEach((template) => {\n                            console.log(template);\n                        });\n                    }\n                });\n            }\n        });\n\n    }\n);',u='package com.clarifai.example;\n\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.grpc.api.status.StatusCode;\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\n\npublic class ClarifaiExample {\n\n    //////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, app ID, and model type ID. \n    // Change these strings to run your own example.\n    //////////////////////////////////////////////////////////////////////////////////\n\n    static final String USER_ID = "YOUR_USER_ID_HERE";\n    //Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    static final String APP_ID = "YOUR_APP_ID_HERE";\n    // Change this to list the template types of your preferred model \n    static final String MODEL_TYPE = "visual-classifier";\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    public static void main(String[] args) {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n                .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        MultiModelTypeResponse listModelTypesResponse = stub.listModelTypes(\n                ListModelTypesRequest.newBuilder()\n                        .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n                        .build()\n        );\n\n        if (listModelTypesResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("List models failed, status: " + listModelTypesResponse.getStatus());\n        }\n\n        for (ModelType modelType : listModelTypesResponse.getModelTypesList()) {\n            if (modelType.getId().equals(MODEL_TYPE)) {\n                for (ModelTypeField modelTypeField : modelType.getModelTypeFieldsList()) {\n                    if (modelTypeField.getPath().split("\\\\.")[modelTypeField.getPath().split("\\\\.").length - 1]\n                            .equals("template")) {\n                        for (ModelTypeEnumOption template : modelTypeField.getModelTypeEnumOptionsList()) {\n                            System.out.println(template);\n                        }\n                    }\n                }\n            }\n        }\n\n    }\n\n}\n',m='<?php\n\nrequire __DIR__ . "/vendor/autoload.php";\n\n///////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, app ID, and model type ID. \n// Change these strings to run your own example.\n/////////////////////////////////////////////////////////////////////////////////////\n\n$USER_ID = "YOUR_USER_ID_HERE";\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\n$PAT = "YOUR_PAT_HERE";\n$APP_ID = "YOUR_APP_ID_HERE";\n// Change this to list the template types of your preferred model \n$MODEL_TYPE = "visual-classifier";\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\ListModelTypesRequest;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ["Authorization" => ["Key " . $PAT]];\n\n$userDataObject = new UserAppIDSet([\n    "user_id" => $USER_ID,\n    "app_id" => $APP_ID,\n]);\n\n// Let\'s make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client->ListModelTypes(\n    // The request object carries the request along with the request status and other metadata related to the request itself\n    new ListModelTypesRequest([\n        "user_app_id" => $userDataObject\n    ]),\n    $metadata\n)->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception("Error: {$status->details}");\n}\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure\n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    throw new Exception("Failure response: " . $response->getStatus()->getDescription() . " " . $response->getStatus()->getDetails());\n}\n\nforeach ($response->getModelTypes() as $modelType) {\n    if ($modelType->getId() === $MODEL_TYPE) {\n        foreach ($modelType->getModelTypeFields() as $modelTypeField) {\n            $pathComponents = explode(\'.\', $modelTypeField->getPath());\n            if (end($pathComponents) === \'template\') {\n                foreach ($modelTypeField->getModelTypeEnumOptions() as $template) {\n                    echo $template->serializeToJsonString() . "\\n";\n                }\n            }\n        }\n    }\n}\n',c='model_type_fields {\n  path: "train_info.params.num_gpus"\n  field_type: RANGE\n  default_value {\n    number_value: 1.0\n  }\n  description: "[internal_only] the number of gpus to train with."\n  placeholder: "num_gpus"\n  internal_only: true\n  model_type_range_info {\n    max: 1.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.logreg"\n  field_type: RANGE\n  default_value {\n    number_value: 1.0\n  }\n  description: "Whether to use sigmoid units (logreg=1) or softmax (logreg=0)."\n  placeholder: "logreg"\n  internal_only: true\n  model_type_range_info {\n    max: 1.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.image_size"\n  field_type: RANGE\n  default_value {\n    number_value: 256.0\n  }\n  description: "Input image size (minimum side dimension)."\n  placeholder: "image_size"\n  internal_only: true\n  model_type_range_info {\n    min: 32.0\n    max: 1024.0\n    step: 16.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.batch_size"\n  field_type: RANGE\n  default_value {\n    number_value: 128.0\n  }\n  description: "the batch size to use during training."\n  placeholder: "batch_size"\n  internal_only: true\n  model_type_range_info {\n    min: 1.0\n    max: 128.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.lrate"\n  field_type: NUMBER\n  default_value {\n    number_value: 0.1\n  }\n  description: "the learning rate (per minibatch)"\n  placeholder: "lrate"\n  internal_only: true\n}\nmodel_type_fields {\n  path: "train_info.params.base_gradient_multiplier"\n  field_type: NUMBER\n  default_value {\n    number_value: 0.001\n  }\n  description: "learning rate multipler applied to the pre-initialized backbone model weights"\n  placeholder: "base_gradient_multiplier"\n  internal_only: true\n}\nmodel_type_fields {\n  path: "train_info.params.num_epochs"\n  field_type: RANGE\n  default_value {\n    number_value: 20.0\n  }\n  description: "the total number of epochs to train for."\n  placeholder: "num_epochs"\n  internal_only: true\n  model_type_range_info {\n    min: 1.0\n    max: 200.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.num_items_per_epoch"\n  field_type: NUMBER\n  default_value {\n    number_value: 0.0\n  }\n  description: "number of input images that constitute an \\"epoch\\".  Default is the number of images in the dataset."\n  placeholder: "num_items_per_epoch"\n  internal_only: true\n}\nmodel_type_fields {\n  path: "train_info.params.embeddings_layer"\n  field_type: STRING\n  default_value {\n    string_value: "mod5B.concat"\n  }\n  description: "the embedding layer to use as output from this model."\n  placeholder: "embeddings_layer"\n  internal_only: true\n}\nmodel_type_fields {\n  path: "train_info.params.average_horizontal_flips"\n  field_type: BOOLEAN\n  default_value {\n    bool_value: true\n  }\n  description: "if true then average the embeddings from the image and a horizontal flip of the image to get the final embedding vectors to output."\n  placeholder: "average_horizontal_flips"\n  internal_only: true\n}\ninternal_only: true\n\nid: "classification_basemodel_v1"\ndescription: "A training template that uses Clarifais training implementation. "\nmodel_type_fields {\n  path: "train_info.params.num_gpus"\n  field_type: RANGE\n  default_value {\n    number_value: 1.0\n  }\n  description: "[internal_only] the number of gpus to train with."\n  placeholder: "num_gpus"\n  internal_only: true\n  model_type_range_info {\n    max: 1.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.model_cfg"\n  field_type: STRING\n  default_value {\n    string_value: "resnext"\n  }\n  description: "the underlying model configuration to use."\n  placeholder: "model_cfg"\n  internal_only: true\n}\nmodel_type_fields {\n  path: "train_info.params.preinit"\n  field_type: STRING\n  default_value {\n    string_value: "general-v1.5"\n  }\n  description: "specifies pre-initialized net to use."\n  placeholder: "preinit"\n  internal_only: true\n}\nmodel_type_fields {\n  path: "train_info.params.logreg"\n  field_type: NUMBER\n  default_value {\n    number_value: 1.0\n  }\n  description: "Whether to use sigmoid units (logreg=1) or softmax (logreg=0)."\n  placeholder: "logreg"\n  internal_only: true\n}\nmodel_type_fields {\n  path: "train_info.params.image_size"\n  field_type: RANGE\n  default_value {\n    number_value: 256.0\n  }\n  description: "Input image size (minimum side dimension)."\n  placeholder: "image_size"\n  internal_only: true\n  model_type_range_info {\n    min: 32.0\n    max: 1024.0\n    step: 16.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.batch_size"\n  field_type: RANGE\n  default_value {\n    number_value: 64.0\n  }\n  description: "the batch size to use during training."\n  placeholder: "batch_size"\n  internal_only: true\n  model_type_range_info {\n    min: 1.0\n    max: 128.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.init_epochs"\n  field_type: RANGE\n  default_value {\n    number_value: 25.0\n  }\n  description: "number of epochs to run at the initial learning rate."\n  placeholder: "init_epochs"\n  internal_only: true\n  model_type_range_info {\n    min: 1.0\n    max: 200.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.step_epochs"\n  field_type: RANGE\n  default_value {\n    number_value: 7.0\n  }\n  description: "the number of epochs between learning rate decreases."\n  placeholder: "step_epochs"\n  internal_only: true\n  model_type_range_info {\n    min: 1.0\n    max: 200.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.num_epochs"\n  field_type: RANGE\n  default_value {\n    number_value: 65.0\n  }\n  description: "the total number of epochs to train for."\n  placeholder: "num_epochs"\n  internal_only: true\n  model_type_range_info {\n    min: 1.0\n    max: 200.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.per_item_lrate"\n  field_type: NUMBER\n  default_value {\n    number_value: 7.8125e-05\n  }\n  description: "the initial learning rate per item. The overall learning rate (per step) is set to lrate = batch_size * per_item_lrate"\n  placeholder: "per_item_lrate"\n  internal_only: true\n}\nmodel_type_fields {\n  path: "train_info.params.num_items_per_epoch"\n  field_type: NUMBER\n  default_value {\n    number_value: 0.0\n  }\n  description: "number of input images that constitute an \\"epoch\\".  Default is the number of images in the dataset."\n  placeholder: "num_items_per_epoch"\n  internal_only: true\n}\nmodel_type_fields {\n  path: "train_info.params.inference_crop_type"\n  field_type: STRING\n  default_value {\n    string_value: "sorta2"\n  }\n  description: "the crop type to use for inference (used when evaluating the model)."\n  placeholder: "inference_crop_type"\n  internal_only: true\n}\ninternal_only: true\n\nid: "classification_cifar10_v1"\ndescription: "A runner optimized for cifar10 training. Not to be used in real use cases. "\nmodel_type_fields {\n  path: "train_info.params.num_gpus"\n  field_type: RANGE\n  default_value {\n    number_value: 1.0\n  }\n  description: "the number of gpus to train with."\n  placeholder: "num_gpus"\n  internal_only: true\n  model_type_range_info {\n    max: 1.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.image_size"\n  field_type: RANGE\n  default_value {\n    number_value: 32.0\n  }\n  description: "the image size to train on. This is for the minimum dimension."\n  placeholder: "image_size"\n  internal_only: true\n  model_type_range_info {\n    min: 32.0\n    max: 1024.0\n    step: 16.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.batch_size"\n  field_type: RANGE\n  default_value {\n    number_value: 128.0\n  }\n  description: "the batch size to use during training."\n  placeholder: "batch_size"\n  internal_only: true\n  model_type_range_info {\n    min: 1.0\n    max: 128.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.num_epochs"\n  field_type: RANGE\n  default_value {\n    number_value: 65.0\n  }\n  description: "the total number of epochs to train for."\n  placeholder: "num_epochs"\n  internal_only: true\n  model_type_range_info {\n    min: 1.0\n    max: 200.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.inference_crop_type"\n  field_type: STRING\n  default_value {\n    string_value: "sorta2"\n  }\n  description: "the crop type to use for inference (used when evaluating the model)."\n  placeholder: "inference_crop_type"\n  internal_only: true\n}\ninternal_only: true\n\nid: "Clarifai_InceptionTransferEmbedNorm"\ndescription: "A custom visual classifier template inspired by Inception networks and tuned for speed with\\nother optimizations for transfer learning. "\nmodel_type_fields {\n  path: "train_info.params.logreg"\n  field_type: RANGE\n  default_value {\n    number_value: 1.0\n  }\n  description: "Whether to use sigmoid units (logreg=1) or softmax (logreg=0)."\n  placeholder: "logreg"\n  model_type_range_info {\n    max: 1.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.image_size"\n  field_type: RANGE\n  default_value {\n    number_value: 256.0\n  }\n  description: "Input image size (minimum side dimension)."\n  placeholder: "image_size"\n  model_type_range_info {\n    min: 32.0\n    max: 1024.0\n    step: 16.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.batch_size"\n  field_type: RANGE\n  default_value {\n    number_value: 128.0\n  }\n  description: "the batch size to use during training."\n  placeholder: "batch_size"\n  model_type_range_info {\n    min: 1.0\n    max: 128.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.lrate"\n  field_type: NUMBER\n  default_value {\n    number_value: 0.1\n  }\n  description: "the learning rate (per minibatch)"\n  placeholder: "lrate"\n}\nmodel_type_fields {\n  path: "train_info.params.base_gradient_multiplier"\n  field_type: NUMBER\n  default_value {\n    number_value: 0.001\n  }\n  description: "learning rate multipler applied to the pre-initialized backbone model weights"\n  placeholder: "base_gradient_multiplier"\n}\nmodel_type_fields {\n  path: "train_info.params.num_epochs"\n  field_type: RANGE\n  default_value {\n    number_value: 20.0\n  }\n  description: "the total number of epochs to train for."\n  placeholder: "num_epochs"\n  model_type_range_info {\n    min: 1.0\n    max: 200.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.num_items_per_epoch"\n  field_type: NUMBER\n  default_value {\n    number_value: 0.0\n  }\n  description: "number of input images that constitute an \\"epoch\\".  Default is the number of images in the dataset."\n  placeholder: "num_items_per_epoch"\n}\nmodel_type_fields {\n  path: "train_info.params.average_horizontal_flips"\n  field_type: BOOLEAN\n  default_value {\n    bool_value: true\n  }\n  description: "if true then average the embeddings from the image and a horizontal flip of the image to get the final embedding vectors to output."\n  placeholder: "average_horizontal_flips"\n}\n\nid: "Clarifai_ResNext"\ndescription: "A custom visual classifier template inspired by ResNext networks. "\nmodel_type_fields {\n  path: "train_info.params.logreg"\n  field_type: RANGE\n  default_value {\n    number_value: 1.0\n  }\n  description: "Whether to use sigmoid units (logreg=1) or softmax (logreg=0)."\n  placeholder: "logreg"\n  model_type_range_info {\n    max: 1.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.image_size"\n  field_type: RANGE\n  default_value {\n    number_value: 256.0\n  }\n  description: "Input image size (minimum side dimension)."\n  placeholder: "image_size"\n  model_type_range_info {\n    min: 32.0\n    max: 1024.0\n    step: 16.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.batch_size"\n  field_type: RANGE\n  default_value {\n    number_value: 64.0\n  }\n  description: "the batch size to use during training."\n  placeholder: "batch_size"\n  model_type_range_info {\n    min: 1.0\n    max: 128.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.init_epochs"\n  field_type: RANGE\n  default_value {\n    number_value: 25.0\n  }\n  description: "number of epochs to run at the initial learning rate."\n  placeholder: "init_epochs"\n  model_type_range_info {\n    min: 1.0\n    max: 200.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.step_epochs"\n  field_type: RANGE\n  default_value {\n    number_value: 7.0\n  }\n  description: "the number of epochs between learning rate decreases."\n  placeholder: "step_epochs"\n  model_type_range_info {\n    min: 1.0\n    max: 200.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.num_epochs"\n  field_type: RANGE\n  default_value {\n    number_value: 65.0\n  }\n  description: "the total number of epochs to train for."\n  placeholder: "num_epochs"\n  model_type_range_info {\n    min: 1.0\n    max: 200.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.per_item_lrate"\n  field_type: NUMBER\n  default_value {\n    number_value: 7.8125e-05\n  }\n  description: "the initial learning rate per item. The overall learning rate (per step) is set to lrate = batch_size * per_item_lrate"\n  placeholder: "per_item_lrate"\n}\nmodel_type_fields {\n  path: "train_info.params.num_items_per_epoch"\n  field_type: NUMBER\n  default_value {\n    number_value: 0.0\n  }\n  description: "number of input images that constitute an \\"epoch\\".  Default is the number of images in the dataset."\n  placeholder: "num_items_per_epoch"\n}\n\nid: "Clarifai_InceptionV2"\ndescription: "A custom visual classifier template inspired by Inception-V2 networks. "\nmodel_type_fields {\n  path: "train_info.params.logreg"\n  field_type: RANGE\n  default_value {\n    number_value: 1.0\n  }\n  description: "Whether to use sigmoid units (logreg=1) or softmax (logreg=0)."\n  placeholder: "logreg"\n  model_type_range_info {\n    max: 1.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.image_size"\n  field_type: RANGE\n  default_value {\n    number_value: 256.0\n  }\n  description: "Input image size (minimum side dimension)."\n  placeholder: "image_size"\n  model_type_range_info {\n    min: 32.0\n    max: 1024.0\n    step: 16.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.batch_size"\n  field_type: RANGE\n  default_value {\n    number_value: 64.0\n  }\n  description: "the batch size to use during training."\n  placeholder: "batch_size"\n  model_type_range_info {\n    min: 1.0\n    max: 128.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.init_epochs"\n  field_type: RANGE\n  default_value {\n    number_value: 25.0\n  }\n  description: "number of epochs to run at the initial learning rate."\n  placeholder: "init_epochs"\n  model_type_range_info {\n    min: 1.0\n    max: 200.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.step_epochs"\n  field_type: RANGE\n  default_value {\n    number_value: 7.0\n  }\n  description: "the number of epochs between learning rate decreases."\n  placeholder: "step_epochs"\n  model_type_range_info {\n    min: 1.0\n    max: 200.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.num_epochs"\n  field_type: RANGE\n  default_value {\n    number_value: 65.0\n  }\n  description: "the total number of epochs to train for."\n  placeholder: "num_epochs"\n  model_type_range_info {\n    min: 1.0\n    max: 200.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.per_item_lrate"\n  field_type: NUMBER\n  default_value {\n    number_value: 7.8125e-05\n  }\n  description: "the initial learning rate per item. The overall learning rate (per step) is set to lrate = batch_size * per_item_lrate"\n  placeholder: "per_item_lrate"\n}\nmodel_type_fields {\n  path: "train_info.params.num_items_per_epoch"\n  field_type: NUMBER\n  default_value {\n    number_value: 0.0\n  }\n  description: "number of input images that constitute an \\"epoch\\".  Default is the number of images in the dataset."\n  placeholder: "num_items_per_epoch"\n}\n\nid: "Clarifai_InceptionBatchNorm"\ndescription: "A custom visual classifier template inspired by Inception networks tuned for speed. "\nmodel_type_fields {\n  path: "train_info.params.logreg"\n  field_type: RANGE\n  default_value {\n    number_value: 1.0\n  }\n  description: "Whether to use sigmoid units (logreg=1) or softmax (logreg=0)."\n  placeholder: "logreg"\n  model_type_range_info {\n    max: 1.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.image_size"\n  field_type: RANGE\n  default_value {\n    number_value: 256.0\n  }\n  description: "Input image size (minimum side dimension)."\n  placeholder: "image_size"\n  model_type_range_info {\n    min: 32.0\n    max: 1024.0\n    step: 16.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.batch_size"\n  field_type: RANGE\n  default_value {\n    number_value: 64.0\n  }\n  description: "the batch size to use during training."\n  placeholder: "batch_size"\n  model_type_range_info {\n    min: 1.0\n    max: 128.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.init_epochs"\n  field_type: RANGE\n  default_value {\n    number_value: 25.0\n  }\n  description: "number of epochs to run at the initial learning rate."\n  placeholder: "init_epochs"\n  model_type_range_info {\n    min: 1.0\n    max: 200.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.step_epochs"\n  field_type: RANGE\n  default_value {\n    number_value: 7.0\n  }\n  description: "the number of epochs between learning rate decreases."\n  placeholder: "step_epochs"\n  model_type_range_info {\n    min: 1.0\n    max: 200.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.num_epochs"\n  field_type: RANGE\n  default_value {\n    number_value: 65.0\n  }\n  description: "the total number of epochs to train for."\n  placeholder: "num_epochs"\n  model_type_range_info {\n    min: 1.0\n    max: 200.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.per_item_lrate"\n  field_type: NUMBER\n  default_value {\n    number_value: 7.8125e-05\n  }\n  description: "the initial learning rate per item. The overall learning rate (per step) is set to lrate = batch_size * per_item_lrate"\n  placeholder: "per_item_lrate"\n}\nmodel_type_fields {\n  path: "train_info.params.num_items_per_epoch"\n  field_type: NUMBER\n  default_value {\n    number_value: 0.0\n  }\n  description: "number of input images that constitute an \\"epoch\\".  Default is the number of images in the dataset."\n  placeholder: "num_items_per_epoch"\n}\n\nid: "MMClassification"\ndescription: "A training template that uses the MMClassification toolkit and a custom configuration "\nmodel_type_fields {\n  path: "train_info.params.seed"\n  field_type: NUMBER\n  default_value {\n    number_value: -1.0\n  }\n  description: "[internal_only] the random seed to init training. If seed < 0, it is not set"\n  placeholder: "seed"\n  internal_only: true\n}\nmodel_type_fields {\n  path: "train_info.params.custom_config"\n  field_type: PYTHON_CODE\n  default_value {\n    string_value: "\\n_base_ = \\\'/mmclassification/configs/resnext/resnext101_32x4d_b32x8_imagenet.py\\\'\\nrunner = dict(type=\\\'EpochBasedRunner\\\', max_epochs=60)\\ndata = dict(\\n    train=dict(\\n        data_prefix=\\\'\\\',\\n        ann_file=\\\'\\\',\\n        classes=\\\'\\\'),\\n    val=dict(\\n        data_prefix=\\\'\\\',\\n        ann_file=\\\'\\\',\\n        classes=\\\'\\\'))\\n"\n  }\n  description: "custom mmclassification config, in python config file format. Note that the \\\'_base_\\\' field, if used, should be a config file relative to the parent directory \\\'/mmclassification/\\\', e.g. \\"_base_ = \\\'/mmclassification/configs/efficientnet/efficientnet-b8_8xb32-01norm_in1k.py\\\'\\". The \\\'num_classes\\\' field must be included somewhere in the config. The \\\'data\\\' section should include \\\'train\\\' and \\\'val\\\' sections, each with \\\'ann_file\\\', \\\'data_prefix\\\', and \\\'classes\\\' fields with empty strings as values. These values will be overwritten to be compatible with Clarifai\\\'s system, but must be included in the imported config."\n  placeholder: "custom_config"\n}\nmodel_type_fields {\n  path: "train_info.params.concepts_mutually_exclusive"\n  field_type: BOOLEAN\n  default_value {\n    bool_value: false\n  }\n  description: "whether the concepts are mutually exclusive. If true then each input is expected to only be tagged with a single concept."\n  placeholder: "concepts_mutually_exclusive"\n}\nmodel_type_fields {\n  path: "train_info.params.num_gpus"\n  field_type: RANGE\n  default_value {\n    number_value: 1.0\n  }\n  description: "[internal_only] the number of gpus to train with."\n  placeholder: "num_gpus"\n  internal_only: true\n  model_type_range_info {\n    max: 1.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.image_size"\n  field_type: ARRAY_OF_NUMBERS\n  default_value {\n    list_value {\n      values {\n        number_value: 320.0\n      }\n    }\n  }\n  description: "the image size for inference (the training image size is defined in the mmcv config). If a single value, specifies the size of the min side."\n  placeholder: "image_size"\n}\n\nid: "MMClassification_EfficientNet"\ndescription: "A training template that uses the MMClassification toolkit and EfficientNet-B8 configuration "\nmodel_type_fields {\n  path: "train_info.params.seed"\n  field_type: NUMBER\n  default_value {\n    number_value: -1.0\n  }\n  description: "[internal_only] the random seed to init training. If seed < 0, we will not set it."\n  placeholder: "seed"\n  internal_only: true\n}\nmodel_type_fields {\n  path: "train_info.params.num_gpus"\n  field_type: RANGE\n  default_value {\n    number_value: 1.0\n  }\n  description: "[internal_only] the number of gpus to train with."\n  placeholder: "num_gpus"\n  internal_only: true\n  model_type_range_info {\n    max: 1.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.image_size"\n  field_type: NUMBER\n  default_value {\n    number_value: 336.0\n  }\n  description: "the image size for training and inference. EfficientNet works on square images."\n  placeholder: "image_size"\n  internal_only: true\n}\nmodel_type_fields {\n  path: "train_info.params.batch_size"\n  field_type: RANGE\n  default_value {\n    number_value: 4.0\n  }\n  description: "the batch size to use during training."\n  placeholder: "batch_size"\n  internal_only: true\n  model_type_range_info {\n    min: 1.0\n    max: 256.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.num_epochs"\n  field_type: RANGE\n  default_value {\n    number_value: 30.0\n  }\n  description: "the total number of epochs to train for."\n  placeholder: "num_epochs"\n  internal_only: true\n  model_type_range_info {\n    min: 1.0\n    max: 200.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.per_item_lrate"\n  field_type: NUMBER\n  default_value {\n    number_value: 0.000390625\n  }\n  description: "the initial learning rate per item. The overall learning rate (per step) is set to lrate = batch_size * per_item_lrate"\n  placeholder: "per_item_lrate"\n  internal_only: true\n}\nmodel_type_fields {\n  path: "train_info.params.weight_decay"\n  field_type: RANGE\n  default_value {\n    number_value: 0.0001\n  }\n  description: "the weight decay value"\n  placeholder: "weight_decay"\n  internal_only: true\n  model_type_range_info {\n    max: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.momentum"\n  field_type: RANGE\n  default_value {\n    number_value: 0.9\n  }\n  description: "the momentum value for the SGD optimizer"\n  placeholder: "momentum"\n  internal_only: true\n  model_type_range_info {\n    max: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.pretrained_weights"\n  field_type: ENUM\n  default_value {\n    string_value: "ImageNet-1k"\n  }\n  description: "whether to use pretrained weights."\n  placeholder: "pretrained_weights"\n  model_type_enum_options {\n    id: "None"\n  }\n  model_type_enum_options {\n    id: "ImageNet-1k"\n  }\n  internal_only: true\n}\nmodel_type_fields {\n  path: "train_info.params.flip_probability"\n  field_type: RANGE\n  default_value {\n    number_value: 0.5\n  }\n  description: "the probability an image will be flipped during training"\n  placeholder: "flip_probability"\n  internal_only: true\n  model_type_range_info {\n    max: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.flip_direction"\n  field_type: ENUM\n  default_value {\n    string_value: "horizontal"\n  }\n  description: "the direction to randomly flip during training."\n  placeholder: "flip_direction"\n  model_type_enum_options {\n    id: "horizontal"\n  }\n  model_type_enum_options {\n    id: "vertical"\n  }\n  internal_only: true\n}\nmodel_type_fields {\n  path: "train_info.params.concepts_mutually_exclusive"\n  field_type: BOOLEAN\n  default_value {\n    bool_value: false\n  }\n  description: "whether the concepts are mutually exclusive. If true then each input is expected to only be tagged with a single concept."\n  placeholder: "concepts_mutually_exclusive"\n  internal_only: true\n}\ninternal_only: true\n\nid: "MMClassification_ResNet_50_RSB_A1"\ndescription: "A training template that uses the MMClassification toolkit and ResNet-50 (rsb-a1) configuration "\nmodel_type_fields {\n  path: "train_info.params.seed"\n  field_type: NUMBER\n  default_value {\n    number_value: -1.0\n  }\n  description: "[internal_only] the random seed to init training. If seed < 0, we will not set it."\n  placeholder: "seed"\n  internal_only: true\n}\nmodel_type_fields {\n  path: "train_info.params.num_gpus"\n  field_type: RANGE\n  default_value {\n    number_value: 1.0\n  }\n  description: "[internal_only] the number of gpus to train with."\n  placeholder: "num_gpus"\n  internal_only: true\n  model_type_range_info {\n    max: 1.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.image_size"\n  field_type: NUMBER\n  default_value {\n    number_value: 224.0\n  }\n  description: "the image size for training and inference. ResNet uses square images."\n  placeholder: "image_size"\n}\nmodel_type_fields {\n  path: "train_info.params.batch_size"\n  field_type: RANGE\n  default_value {\n    number_value: 64.0\n  }\n  description: "the batch size to use during training."\n  placeholder: "batch_size"\n  model_type_range_info {\n    min: 1.0\n    max: 256.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.num_epochs"\n  field_type: RANGE\n  default_value {\n    number_value: 60.0\n  }\n  description: "the total number of epochs to train for."\n  placeholder: "num_epochs"\n  model_type_range_info {\n    min: 1.0\n    max: 600.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.per_item_lrate"\n  field_type: NUMBER\n  default_value {\n    number_value: 1.953125e-05\n  }\n  description: "the initial learning rate per item. The overall learning rate (per step) is set to lrate = batch_size * per_item_lrate"\n  placeholder: "per_item_lrate"\n}\nmodel_type_fields {\n  path: "train_info.params.weight_decay"\n  field_type: RANGE\n  default_value {\n    number_value: 0.01\n  }\n  description: "the weight decay value"\n  placeholder: "weight_decay"\n  model_type_range_info {\n    max: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.per_item_min_lrate"\n  field_type: NUMBER\n  default_value {\n    number_value: 1.5625e-08\n  }\n  description: "The minimum learning (per item) at end of training using cosine schedule."\n  placeholder: "per_item_min_lrate"\n}\nmodel_type_fields {\n  path: "train_info.params.warmup_iters"\n  field_type: NUMBER\n  default_value {\n    number_value: 100.0\n  }\n  description: "The number of steps in the warmup phase"\n  placeholder: "warmup_iters"\n}\nmodel_type_fields {\n  path: "train_info.params.warmup_ratio"\n  field_type: NUMBER\n  default_value {\n    number_value: 0.0001\n  }\n  description: " Warmup phase learning rate multiplier"\n  placeholder: "warmup_ratio"\n}\nmodel_type_fields {\n  path: "train_info.params.pretrained_weights"\n  field_type: ENUM\n  default_value {\n    string_value: "ImageNet-1k"\n  }\n  description: "whether to use pretrained weights."\n  placeholder: "pretrained_weights"\n  model_type_enum_options {\n    id: "None"\n  }\n  model_type_enum_options {\n    id: "ImageNet-1k"\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.flip_probability"\n  field_type: RANGE\n  default_value {\n    number_value: 0.5\n  }\n  description: "the probability an image will be flipped during training"\n  placeholder: "flip_probability"\n  model_type_range_info {\n    max: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.flip_direction"\n  field_type: ENUM\n  default_value {\n    string_value: "horizontal"\n  }\n  description: "the direction to randomly flip during training."\n  placeholder: "flip_direction"\n  model_type_enum_options {\n    id: "horizontal"\n  }\n  model_type_enum_options {\n    id: "vertical"\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.concepts_mutually_exclusive"\n  field_type: BOOLEAN\n  default_value {\n    bool_value: false\n  }\n  description: "whether the concepts are mutually exclusive. If true then each input is expected to only be tagged with a single concept."\n  placeholder: "concepts_mutually_exclusive"\n}\nrecommended: true\n\nid: "MMClassification_ResNet_50"\ndescription: "A training template that uses the MMClassification toolkit and ResNet-50 configuration "\nmodel_type_fields {\n  path: "train_info.params.seed"\n  field_type: NUMBER\n  default_value {\n    number_value: -1.0\n  }\n  description: "[internal_only] the random seed to init training. If seed < 0, we will not set it."\n  placeholder: "seed"\n  internal_only: true\n}\nmodel_type_fields {\n  path: "train_info.params.num_gpus"\n  field_type: RANGE\n  default_value {\n    number_value: 1.0\n  }\n  description: "[internal_only] the number of gpus to train with."\n  placeholder: "num_gpus"\n  internal_only: true\n  model_type_range_info {\n    max: 1.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.image_size"\n  field_type: NUMBER\n  default_value {\n    number_value: 224.0\n  }\n  description: "the image size for training and inference. ResNet works on square images."\n  placeholder: "image_size"\n  internal_only: true\n}\nmodel_type_fields {\n  path: "train_info.params.batch_size"\n  field_type: RANGE\n  default_value {\n    number_value: 64.0\n  }\n  description: "the batch size to use per gpu during training."\n  placeholder: "batch_size"\n  internal_only: true\n  model_type_range_info {\n    min: 1.0\n    max: 256.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.num_epochs"\n  field_type: RANGE\n  default_value {\n    number_value: 60.0\n  }\n  description: "the total number of epochs to train for."\n  placeholder: "num_epochs"\n  internal_only: true\n  model_type_range_info {\n    min: 1.0\n    max: 600.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.per_item_lrate"\n  field_type: NUMBER\n  default_value {\n    number_value: 0.000390625\n  }\n  description: "the initial learning rate per item. The overall learning rate (per step) is set to lrate = batch_size * per_item_lrate"\n  placeholder: "per_item_lrate"\n  internal_only: true\n}\nmodel_type_fields {\n  path: "train_info.params.learning_rate_steps"\n  field_type: ARRAY_OF_NUMBERS\n  default_value {\n    list_value {\n      values {\n        number_value: 30.0\n      }\n      values {\n        number_value: 40.0\n      }\n      values {\n        number_value: 50.0\n      }\n    }\n  }\n  description: "epoch schedule for stepping down learning rate"\n  placeholder: "learning_rate_steps"\n  internal_only: true\n}\nmodel_type_fields {\n  path: "train_info.params.weight_decay"\n  field_type: RANGE\n  default_value {\n    number_value: 0.0001\n  }\n  description: "the weight decay value"\n  placeholder: "weight_decay"\n  internal_only: true\n  model_type_range_info {\n    max: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.momentum"\n  field_type: RANGE\n  default_value {\n    number_value: 0.9\n  }\n  description: "the momentum value for the SGD optimizer"\n  placeholder: "momentum"\n  internal_only: true\n  model_type_range_info {\n    max: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.pretrained_weights"\n  field_type: ENUM\n  default_value {\n    string_value: "ImageNet-1k"\n  }\n  description: "whether to use pretrained weights."\n  placeholder: "pretrained_weights"\n  model_type_enum_options {\n    id: "None"\n  }\n  model_type_enum_options {\n    id: "ImageNet-1k"\n  }\n  internal_only: true\n}\nmodel_type_fields {\n  path: "train_info.params.flip_probability"\n  field_type: RANGE\n  default_value {\n    number_value: 0.5\n  }\n  description: "the probability an image will be flipped during training"\n  placeholder: "flip_probability"\n  internal_only: true\n  model_type_range_info {\n    max: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.flip_direction"\n  field_type: ENUM\n  default_value {\n    string_value: "horizontal"\n  }\n  description: "the direction to randomly flip during training."\n  placeholder: "flip_direction"\n  model_type_enum_options {\n    id: "horizontal"\n  }\n  model_type_enum_options {\n    id: "vertical"\n  }\n  internal_only: true\n}\nmodel_type_fields {\n  path: "train_info.params.concepts_mutually_exclusive"\n  field_type: BOOLEAN\n  default_value {\n    bool_value: false\n  }\n  description: "whether the concepts are mutually exclusive. If true then each input is expected to only be tagged with a single concept."\n  placeholder: "concepts_mutually_exclusive"\n  internal_only: true\n}\ninternal_only: true\n';var f=t(99563),h=t(89791);const y={description:"Learn about our deep fine-tuning template types"},g="Training Templates",v={},b=[{value:"List Template Types",id:"list-template-types",level:2}];function E(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",header:"header",p:"p",strong:"strong",...(0,l.R)(),...e.components},{Details:t}=n;return t||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"training-templates",children:"Training Templates"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Learn about our deep fine-tuning template types"})}),"\n",(0,i.jsx)("hr",{}),"\n",(0,i.jsx)(n.p,{children:"You can take advantage of a variety of our pre-configured templates when developing your deep fine-tuned models."}),"\n",(0,i.jsx)(n.p,{children:"Templates give you the control to choose the specific architecture used by your neural network, and also define a set of hyperparameters that you can use to fine-tune the way your model learns."}),"\n",(0,i.jsx)(n.h2,{id:"list-template-types",children:"List Template Types"}),"\n",(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsxs)(n.p,{children:["Before using the ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/additional-resources/api-overview/python-sdk",children:"Python SDK"}),", ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/additional-resources/api-overview/nodejs-sdk",children:"Node.js SDK"}),", or any of our ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/additional-resources/api-overview/grpc-clients",children:"gRPC clients"}),", ensure they are properly installed on your machine. Refer to their respective installation guides for instructions on how to install and initialize them."]})}),"\n","\n","\n",(0,i.jsxs)(n.p,{children:["Below is an example of how you would use the ",(0,i.jsx)(n.code,{children:"ListModelTypes"})," endpoint to list the templates and hyperparameters available in a specific ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/create-manage/models/manage#list-model-types",children:"model type"}),"."]}),"\n",(0,i.jsxs)(r.A,{children:[(0,i.jsx)(o.A,{value:"python",label:"Python (gRPC)",children:(0,i.jsx)(s.A,{className:"language-python",children:p})}),(0,i.jsx)(o.A,{value:"js_rest",label:"JavaScript (REST)",children:(0,i.jsx)(s.A,{className:"language-javascript",children:d})}),(0,i.jsx)(o.A,{value:"nodejs",label:"Node.js (gRPC)",children:(0,i.jsx)(s.A,{className:"language-javascript",children:_})}),(0,i.jsx)(o.A,{value:"java",label:"Java (gRPC)",children:(0,i.jsx)(s.A,{className:"language-java",children:u})}),(0,i.jsx)(o.A,{value:"php",label:"PHP (gRPC)",children:(0,i.jsx)(s.A,{className:"language-php",children:m})})]}),"\n",(0,i.jsxs)(t,{children:[(0,i.jsx)("summary",{children:"Raw Output Example"}),(0,i.jsx)(s.A,{className:"language-text",children:c})]}),"\n","\n",(0,i.jsx)(f.A,{items:(0,h.$S)().items})]})}function N(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(E,{...e})}):E(e)}},79329:(e,n,t)=>{t.d(n,{A:()=>r});t(96540);var a=t(18215);const i={tabItem:"tabItem_Ymn6"};var l=t(74848);function r(e){let{children:n,hidden:t,className:r}=e;return(0,l.jsx)("div",{role:"tabpanel",className:(0,a.A)(i.tabItem,r),hidden:t,children:n})}},81430:(e,n,t)=>{t.d(n,{W:()=>p});var a=t(96540),i=t(40797);const l=["zero","one","two","few","many","other"];function r(e){return l.filter((n=>e.includes(n)))}const o={locale:"en",pluralForms:r(["one","other"]),select:e=>1===e?"one":"other"};function s(){const{i18n:{currentLocale:e}}=(0,i.A)();return(0,a.useMemo)((()=>{try{return function(e){const n=new Intl.PluralRules(e);return{locale:e,pluralForms:r(n.resolvedOptions().pluralCategories),select:e=>n.select(e)}}(e)}catch(n){return console.error(`Failed to use Intl.PluralRules for locale "${e}".\nDocusaurus will fallback to the default (English) implementation.\nError: ${n.message}\n`),o}}),[e])}function p(){const e=s();return{selectMessage:(n,t)=>function(e,n,t){const a=e.split("|");if(1===a.length)return a[0];a.length>t.pluralForms.length&&console.error(`For locale=${t.locale}, a maximum of ${t.pluralForms.length} plural forms are expected (${t.pluralForms.join(",")}), but the message contains ${a.length}: ${e}`);const i=t.select(n),l=t.pluralForms.indexOf(i);return a[Math.min(l,a.length-1)]}(t,n,e)}}},89791:(e,n,t)=>{t.d(n,{$S:()=>a});t(40797);function a(){return t(69493).$S(...arguments)}},99563:(e,n,t)=>{t.d(n,{A:()=>g});t(96540);var a=t(18215),i=t(93751),l=t(56289),r=t(81430),o=t(22887),s=t(50539),p=t(9303);const d={cardContainer:"cardContainer_fWXF",cardTitle:"cardTitle_rnsV",cardDescription:"cardDescription_PWke"};var _=t(74848);function u(e){let{href:n,children:t}=e;return(0,_.jsx)(l.A,{href:n,className:(0,a.A)("card padding--lg",d.cardContainer),children:t})}function m(e){let{href:n,icon:t,title:i,description:l}=e;return(0,_.jsxs)(u,{href:n,children:[(0,_.jsxs)(p.A,{as:"h2",className:(0,a.A)("text--truncate",d.cardTitle),title:i,children:[t," ",i]}),l&&(0,_.jsx)("p",{className:(0,a.A)("text--truncate",d.cardDescription),title:l,children:l})]})}function c(e){let{item:n}=e;const t=(0,i.Nr)(n),a=function(){const{selectMessage:e}=(0,r.W)();return n=>e(n,(0,s.T)({message:"1 item|{count} items",id:"theme.docs.DocCard.categoryDescription.plurals",description:"The default description for a category card in the generated index about how many items this category includes"},{count:n}))}();return t?(0,_.jsx)(m,{href:t,icon:"\ud83d\uddc3\ufe0f",title:n.label,description:n.description??a(n.items.length)}):null}function f(e){let{item:n}=e;const t=(0,o.A)(n.href)?"\ud83d\udcc4\ufe0f":"\ud83d\udd17",a=(0,i.cC)(n.docId??void 0);return(0,_.jsx)(m,{href:n.href,icon:t,title:n.label,description:n.description??a?.description})}function h(e){let{item:n}=e;switch(n.type){case"link":return(0,_.jsx)(f,{item:n});case"category":return(0,_.jsx)(c,{item:n});default:throw new Error(`unknown item type ${JSON.stringify(n)}`)}}function y(e){let{className:n}=e;const t=(0,i.$S)();return(0,_.jsx)(g,{items:t.items,className:n})}function g(e){const{items:n,className:t}=e;if(!n)return(0,_.jsx)(y,{...e});const l=(0,i.d1)(n);return(0,_.jsx)("section",{className:(0,a.A)("row",t),children:l.map(((e,n)=>(0,_.jsx)("article",{className:"col col--6 margin-bottom--lg",children:(0,_.jsx)(h,{item:e})},n)))})}}}]);
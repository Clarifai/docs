"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[3352],{65537:(e,n,t)=>{t.d(n,{A:()=>A});var a=t(96540),i=t(18215),s=t(65627),o=t(56347),r=t(50372),c=t(30604),l=t(11861),p=t(78749);function u(e){return a.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,a.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function d(e){const{values:n,children:t}=e;return(0,a.useMemo)((()=>{const e=n??function(e){return u(e).map((e=>{let{props:{value:n,label:t,attributes:a,default:i}}=e;return{value:n,label:t,attributes:a,default:i}}))}(t);return function(e){const n=(0,l.XI)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,t])}function h(e){let{value:n,tabValues:t}=e;return t.some((e=>e.value===n))}function m(e){let{queryString:n=!1,groupId:t}=e;const i=(0,o.W6)(),s=function(e){let{queryString:n=!1,groupId:t}=e;if("string"==typeof n)return n;if(!1===n)return null;if(!0===n&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:n,groupId:t});return[(0,c.aZ)(s),(0,a.useCallback)((e=>{if(!s)return;const n=new URLSearchParams(i.location.search);n.set(s,e),i.replace({...i.location,search:n.toString()})}),[s,i])]}function f(e){const{defaultValue:n,queryString:t=!1,groupId:i}=e,s=d(e),[o,c]=(0,a.useState)((()=>function(e){let{defaultValue:n,tabValues:t}=e;if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(n){if(!h({value:n,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${n}" but none of its children has the corresponding value. Available values are: ${t.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return n}const a=t.find((e=>e.default))??t[0];if(!a)throw new Error("Unexpected error: 0 tabValues");return a.value}({defaultValue:n,tabValues:s}))),[l,u]=m({queryString:t,groupId:i}),[f,g]=function(e){let{groupId:n}=e;const t=function(e){return e?`docusaurus.tab.${e}`:null}(n),[i,s]=(0,p.Dv)(t);return[i,(0,a.useCallback)((e=>{t&&s.set(e)}),[t,s])]}({groupId:i}),_=(()=>{const e=l??f;return h({value:e,tabValues:s})?e:null})();(0,r.A)((()=>{_&&c(_)}),[_]);return{selectedValue:o,selectValue:(0,a.useCallback)((e=>{if(!h({value:e,tabValues:s}))throw new Error(`Can't select invalid tab value=${e}`);c(e),u(e),g(e)}),[u,g,s]),tabValues:s}}var g=t(9136);const _={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var I=t(74848);function E(e){let{className:n,block:t,selectedValue:a,selectValue:o,tabValues:r}=e;const c=[],{blockElementScrollPositionUntilNextRender:l}=(0,s.a_)(),p=e=>{const n=e.currentTarget,t=c.indexOf(n),i=r[t].value;i!==a&&(l(n),o(i))},u=e=>{let n=null;switch(e.key){case"Enter":p(e);break;case"ArrowRight":{const t=c.indexOf(e.currentTarget)+1;n=c[t]??c[0];break}case"ArrowLeft":{const t=c.indexOf(e.currentTarget)-1;n=c[t]??c[c.length-1];break}}n?.focus()};return(0,I.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,i.A)("tabs",{"tabs--block":t},n),children:r.map((e=>{let{value:n,label:t,attributes:s}=e;return(0,I.jsx)("li",{role:"tab",tabIndex:a===n?0:-1,"aria-selected":a===n,ref:e=>{c.push(e)},onKeyDown:u,onClick:p,...s,className:(0,i.A)("tabs__item",_.tabItem,s?.className,{"tabs__item--active":a===n}),children:t??n},n)}))})}function w(e){let{lazy:n,children:t,selectedValue:s}=e;const o=(Array.isArray(t)?t:[t]).filter(Boolean);if(n){const e=o.find((e=>e.props.value===s));return e?(0,a.cloneElement)(e,{className:(0,i.A)("margin-top--md",e.props.className)}):null}return(0,I.jsx)("div",{className:"margin-top--md",children:o.map(((e,n)=>(0,a.cloneElement)(e,{key:n,hidden:e.props.value!==s})))})}function b(e){const n=f(e);return(0,I.jsxs)("div",{className:(0,i.A)("tabs-container",_.tabList),children:[(0,I.jsx)(E,{...n,...e}),(0,I.jsx)(w,{...n,...e})]})}function A(e){const n=(0,g.A)();return(0,I.jsx)(b,{...e,children:u(e.children)},String(n))}},79329:(e,n,t)=>{t.d(n,{A:()=>o});t(96540);var a=t(18215);const i={tabItem:"tabItem_Ymn6"};var s=t(74848);function o(e){let{children:n,hidden:t,className:o}=e;return(0,s.jsx)("div",{role:"tabpanel",className:(0,a.A)(i.tabItem,o),hidden:t,children:n})}},97151:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>H,contentTitle:()=>$,default:()=>G,frontMatter:()=>k,metadata:()=>a,toc:()=>V});const a=JSON.parse('{"id":"compute/inference/advanced","title":"Advanced Inference Options","description":"Learn how to use our advanced inference operations","source":"@site/docs/compute/inference/advanced.md","sourceDirName":"compute/inference","slug":"/compute/inference/advanced","permalink":"/compute/inference/advanced","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"description":"Learn how to use our advanced inference operations","sidebar_position":5},"sidebar":"tutorialSidebar","previous":{"title":"Vercel AI SDK","permalink":"/compute/inference/vercel"},"next":{"title":"Build and Upload Models","permalink":"/compute/upload/"}}');var i=t(74848),s=t(28453),o=t(65537),r=t(79329),c=t(58069);const l='from clarifai.client.input import Inputs\nfrom clarifai.client.model import Model\n\nmodel_url = "https://clarifai.com/openai/chat-completion/models/gpt-4o-mini"\nprompt = "What\'s the future of AI?"\n\n# here is an example of creating an input proto list of size 16\nproto_list=[]\nfor i in range(16):\n    proto_list.append(Inputs.get_input_from_bytes(input_id = f\'demo_{i}\', text_bytes=prompt.encode()))\n\n# pass the input proto as paramater to the predict function\nmodel_prediction = Model(url=model_url, pat="YOUR_PAT").predict(\n    proto_list\n)\n\n# Check the length of predictions to see if all inputs were passed successfully\n\nprint(len(model_prediction.outputs))',p="import { Inputs } from 'clarifai-client';\nimport { Model } from 'clarifai-client';\n\nconst modelUrl: string = \"https://clarifai.com/openai/chat-completion/models/gpt-4o-mini\";\nconst prompt: string = \"What's the future of AI?\";\n\n// here is an example of creating an input proto list of size 16\nconst protoList: any[] = [];\nfor (let i = 0; i < 16; i++) {\n    protoList.push(Inputs.getInputFromBytes({ inputId: `demo_${i}`, textBytes: Buffer.from(prompt) }));\n}\n\n// passthe input proto as parameter to the predict function\nconst modelPrediction = new Model({ url: modelUrl }).predict(protoList);\n\n// Check the length of predictions to see if all inputs were passed successfully\nconsole.log(modelPrediction.outputs.length);",u='from clarifai.client.model import Model\n\n# Your PAT (Personal Access Token) can be found in the Account\'s Security section\n# Specify the correct user_id/app_id pairings\n# Since you\'re making inferences outside your app\'s scope\n#USER_ID = "cohere"\n#APP_ID = "embed"\n\n# You can set the model using model URL or model ID.\n# Change these to whatever model you want to use\n# eg : MODEL_ID = \'cohere-embed-english-v3_0\'\n# You can also set a particular model version by specifying the  version ID\n# eg: MODEL_VERSION_ID = \'model_version\'\n#  Model class objects can be inititalised by providing its URL or also by defining respective user_id, app_id and model_id\n\n# eg : model = Model(user_id="clarifai", app_id="main", model_id=MODEL_ID)\n\ninput_text = """In India Green Revolution commenced in the early 1960s that led to an increase in food grain production, especially in Punjab, Haryana, and Uttar Pradesh. Major milestones in this undertaking were the development of high-yielding varieties of wheat. The Green revolution is revolutionary in character due to the introduction of new technology, new ideas, the new application of inputs like HYV seeds, fertilizers, irrigation water, pesticides, etc. As all these were brought suddenly and spread quickly to attain dramatic results thus it is termed as a revolution in green agriculture.\n"""\n# The predict API gives the flexibility to generate predictions for data provided through URL, Filepath and bytes format.\n\n# Example for prediction through URL:\n# model_prediction = Model(model_url).predict_by_url(URL ,input_type="text")\n\n# Example for prediction through Filepath:\n# model_prediction = Model(model_url).predict_by_filepath(image_filepath, input_type="text")\n\nmodel_url = "https://clarifai.com/cohere/embed/models/cohere-embed-english-v3_0"\n\n# You can pass the new base url as paramater while initializing the Model object\nmodel_prediction = Model(url=model_url, pat="YOUR_PAT",base_url="New Base URL").predict_by_bytes(\n    input_text, "text"\n)\n\nembeddings = model_prediction.outputs[0].data.embeddings[0].vector\n\nnum_dimensions = model_prediction.outputs[0].data.embeddings[0].num_dimensions\n\nprint(embeddings[:10])\n',d='from clarifai.client.model import Model\n\n# Your PAT (Personal Access Token) can be found in the Account\'s Security section\n# Specify the correct user_id/app_id pairings\n# Since you\'re making inferences outside your app\'s scope\n#USER_ID = "clarifai"\n#APP_ID = "main"\n\n# You can set the model using model URL or model ID.\n# Change these to whatever model you want to use\n# eg : MODEL_ID = "general-image-recognition"\n# You can also set a particular model version by specifying the  version ID\n# eg: MODEL_VERSION_ID = "aa7f35c01e0642fda5cf400f543e7c40"\n#  Model class objects can be inititalised by providing its URL or also by defining respective user_id, app_id and model_id\n\n# eg : model = Model(user_id="clarifai", app_id="main", model_id=MODEL_ID)\n\nmodel_url = "https://clarifai.com/clarifai/main/models/general-image-recognition"\nimage_url = "https://samples.clarifai.com/metro-north.jpg"\n\n# The predict API gives flexibility to generate predictions for data provided through URL,Filepath and bytes format.\n\n# Example for prediction through Bytes:\n# model_prediction = model.predict_by_bytes(input_bytes, input_type="image")\n\n# Example for prediction through Filepath:\n# model_prediction = Model(model_url).predict_by_filepath(filepath, input_type="image")\n\nmodel_prediction = Model(url=model_url, pat="YOUR_PAT",root_certificates_path="PATH_TO_ROOT_CERTIFICATE").predict_by_url(\n    image_url, input_type="image"\n)\n\n# Get the output\nprint(model_prediction.outputs[0].data)',h="#######################################################################################################\n# In this section, we set the user authentication, user and app ID, model ID, model version ID, and\n# URL of the image we want as an input. Change these strings to run your own example.\n#######################################################################################################\n\n# Your PAT (Personal Access Token) can be found in the Account's Security section\nPAT = 'YOUR_PAT_HERE'\n# Specify the correct user_id/app_id pairings\n# Since you're making inferences outside your app's scope\nUSER_ID = 'clarifai'\nAPP_ID = 'main'\n# Change these to whatever you want to process\nMODEL_ID = 'general-image-recognition'\nMODEL_VERSION_ID = 'aa7f35c01e0642fda5cf400f543e7c40'\nIMAGE_URL = 'https://samples.clarifai.com/metro-north.jpg'\n\n############################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n############################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (('authorization', 'Key ' + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\npost_model_outputs_response = stub.PostModelOutputs(\n    service_pb2.PostModelOutputsRequest(\n        user_app_id=userDataObject,  # The userDataObject is created in the overview and is required when using a PAT\n        model_id=MODEL_ID,\n        version_id=MODEL_VERSION_ID,  \n        inputs=[\n            \n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    image=resources_pb2.Image(\n                        url=IMAGE_URL\n                    )\n                )\n            )\n        ]\n    ),\n    metadata=metadata\n)\nif post_model_outputs_response.status.code != status_code_pb2.SUCCESS:\n    print(post_model_outputs_response.status)\n    raise Exception(\"Post model outputs failed, status: \" + post_model_outputs_response.status.description)\n\n# Since we have one input, one output will exist here\noutput = post_model_outputs_response.outputs[0]\n\nprint(\"Predicted concepts:\")\nfor concept in output.data.concepts:\n    print(\"%s %.2f\" % (concept.name, concept.value))\n\n# Uncomment this line to print the raw output\n#print(output)",m="#######################################################################################################\n# In this section, we set the user authentication, user and app ID, model details, URL of the image\n# we want as an input, and max concepts. Change these strings to run your own example.\n#######################################################################################################\n\n# Your PAT (Personal Access Token) can be found in the Account's Security section\nPAT = 'YOUR_PAT_HERE'\n# Specify the correct user_id/app_id pairings\n# Since you're making inferences outside your app's scope\nUSER_ID = 'clarifai'\nAPP_ID = 'main'\n# Change these to whatever you want to process\nMODEL_ID = 'general-image-recognition'\nMODEL_VERSION_ID = 'aa7f35c01e0642fda5cf400f543e7c40'\nIMAGE_URL = 'https://samples.clarifai.com/metro-north.jpg'\nMAX_CONCEPTS = 3\n\n############################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n############################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (('authorization', 'Key ' + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\npost_model_outputs_response = stub.PostModelOutputs(\n    service_pb2.PostModelOutputsRequest(\n        user_app_id=userDataObject,  # The userDataObject is created in the overview and is required when using a PAT\n        model_id=MODEL_ID,\n        version_id=MODEL_VERSION_ID,  # This is optional. Defaults to the latest model version  \n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    image=resources_pb2.Image(\n                        url=IMAGE_URL\n                    )\n                )\n            )\n        ],\n        model=resources_pb2.Model(\n            output_info=resources_pb2.OutputInfo(\n                output_config=resources_pb2.OutputConfig(\n                    max_concepts=MAX_CONCEPTS\n                )\n            )\n        )\n    ),\n    metadata=metadata\n)\nif post_model_outputs_response.status.code != status_code_pb2.SUCCESS:\n    print(post_model_outputs_response.status)\n    raise Exception(\"Post model outputs failed, status: \" + post_model_outputs_response.status.description)\n\n# Since we have one input, one output will exist here.\noutput = post_model_outputs_response.outputs[0]\n\nprint(\"Predicted concepts:\")\nfor concept in output.data.concepts:\n    print(\"%s %.2f\" % (concept.name, concept.value))\n\n# Uncomment this line to print the raw output\n#print(output)",f="#########################################################################################################\n# In this section, we set the user authentication, user and app ID, model details, URL of the image\n# we want as an input, and minimum value. Change these strings to run your own example.\n#########################################################################################################\n\n# Your PAT (Personal Access Token) can be found in the Account's Security section\nPAT = 'YOUR_PAT_HERE'\n# Specify the correct user_id/app_id pairings\n# Since you're making inferences outside your app's scope\nUSER_ID = 'clarifai'\nAPP_ID = 'main'\n# Change these to whatever you want to process\nMODEL_ID = 'general-image-recognition'\nMODEL_VERSION_ID = 'aa7f35c01e0642fda5cf400f543e7c40'\nIMAGE_URL = 'https://samples.clarifai.com/metro-north.jpg'\nMINIMUM_VALUE = 0.95\n\n############################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n############################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (('authorization', 'Key ' + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\npost_model_outputs_response = stub.PostModelOutputs(\n    service_pb2.PostModelOutputsRequest(\n        user_app_id=userDataObject,  # The userDataObject is created in the overview and is required when using a PAT\n        model_id=MODEL_ID,  \n        version_id=MODEL_VERSION_ID,  # This is optional. Defaults to the latest model version\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    image=resources_pb2.Image(\n                        url=IMAGE_URL\n                    )\n                )\n            )\n        ],\n        model=resources_pb2.Model(\n            output_info=resources_pb2.OutputInfo(\n                output_config=resources_pb2.OutputConfig(\n                    min_value=MINIMUM_VALUE\n                )\n            )\n        )\n    ),\n    metadata=metadata\n)\nif post_model_outputs_response.status.code != status_code_pb2.SUCCESS:\n    print(post_model_outputs_response.status)\n    raise Exception(\"Post model outputs failed, status: \" + post_model_outputs_response.status.description)\n\n# Since we have one input, one output will exist here.\noutput = post_model_outputs_response.outputs[0]\n\nprint(\"Predicted concepts:\")\nfor concept in output.data.concepts:\n    print(\"%s %.2f\" % (concept.name, concept.value))\n\n# Uncomment this line to print the raw output\n#print(output)",g="#########################################################################################################\n# In this section, we set the user authentication, user and app ID, model details, URL of the image\n# we want as an input, and concept name and ID. Change these strings to run your own example.\n########################################################################################################\n\n# Your PAT (Personal Access Token) can be found in the Account's Security section\nPAT = 'YOUR_PAT_HERE'\n# Specify the correct user_id/app_id pairings\n# Since you're making inferences outside your app's scope\nUSER_ID = 'clarifai'\nAPP_ID = 'main'\n# Change these to whatever you want to process\nMODEL_ID = 'general-image-recognition'\nMODEL_VERSION_ID = 'aa7f35c01e0642fda5cf400f543e7c40'\nIMAGE_URL = 'https://samples.clarifai.com/metro-north.jpg'\nCONCEPT_NAME = \"train\"\nCONCEPT_ID = \"ai_6kTjGfF6\"\n\n############################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n############################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (('authorization', 'Key ' + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\npost_model_outputs_response = stub.PostModelOutputs(\n    service_pb2.PostModelOutputsRequest(\n        user_app_id=userDataObject,  # The userDataObject is created in the overview and is required when using a PAT\n        model_id=MODEL_ID,  \n        version_id=MODEL_VERSION_ID,  # This is optional. Defaults to the latest model version\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    image=resources_pb2.Image(\n                        url=IMAGE_URL\n                    )\n                )\n            )\n        ],\n        model=resources_pb2.Model(\n            output_info=resources_pb2.OutputInfo(\n                output_config=resources_pb2.OutputConfig(\n                    select_concepts=[\n                        # When selecting concepts, value is ignored, so no need to specify it\n                        resources_pb2.Concept(name=CONCEPT_NAME),\n                        resources_pb2.Concept(id=CONCEPT_ID)\n                    ]\n                )\n            )\n        )\n    ),\n    metadata=metadata\n)\nif post_model_outputs_response.status.code != status_code_pb2.SUCCESS:\n    print(post_model_outputs_response.status)\n    raise Exception(\"Post model outputs failed, status: \" + post_model_outputs_response.status.description)\n\n# Since we have one input, one output will exist here.\noutput = post_model_outputs_response.outputs[0]\n\nprint(\"Predicted concepts:\")\nfor concept in output.data.concepts:\n    print(\"%s %.2f\" % (concept.name, concept.value))\n\n# Uncomment this line to print the raw output\n#print(output)",_="\x3c!--index.html file--\x3e\n\n<script>\n    ///////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model ID, model version ID, and\n    // URL of the image we want as an input. Change these strings to run your own example.\n    //////////////////////////////////////////////////////////////////////////////////////////////////////\n\n    // Your PAT (Personal Access Token) can be found in the Account's Security section\n    const PAT = 'YOUR_PAT_HERE';\n    // Specify the correct user_id/app_id pairings\n    // Since you're making inferences outside your app's scope\n    const USER_ID = 'clarifai';    \n    const APP_ID = 'main';\n    // Change these to whatever you want to process\n    const MODEL_ID = 'general-image-recognition';\n    const MODEL_VERSION_ID = 'aa7f35c01e0642fda5cf400f543e7c40';\n    const IMAGE_URL = 'https://samples.clarifai.com/metro-north.jpg';    \n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    const raw = JSON.stringify({\n        \"user_app_id\": {\n            \"user_id\": USER_ID,\n            \"app_id\": APP_ID\n        },\n        \"inputs\": [\n            {\n                \"data\": {\n                    \"image\": {\n                        \"url\": IMAGE_URL\n                    }\n                }\n            }\n        ]\n    });\n\n    const requestOptions = {\n        method: 'POST',\n        headers: {\n            'Accept': 'application/json',\n            'Authorization': 'Key ' + PAT\n        },\n        body: raw\n    };\n\n    // NOTE: MODEL_VERSION_ID is optional, you can also call prediction with the MODEL_ID only\n    // https://api.clarifai.com/v2/models/{YOUR_MODEL_ID}/outputs\n    // this will default to the latest version_id\n\n    fetch(\"https://api.clarifai.com/v2/models/\" + MODEL_ID + \"/versions/\" + MODEL_VERSION_ID + \"/outputs\", requestOptions)\n        .then(response => response.text())\n        .then(result => console.log(result))\n        .catch(error => console.log('error', error));\n<\/script>",I='\x3c!--index.html file--\x3e\n\n<script>\n    ////////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, URL of the image\n    // we want as an input, and max concepts. Change these strings to run your own example.\n    ///////////////////////////////////////////////////////////////////////////////////////////////////////\n\n    // Your PAT (Personal Access Token) can be found in the Account\'s Security section\n    const PAT = \'YOUR_PAT_HERE\';\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    const USER_ID = \'clarifai\';    \n    const APP_ID = \'main\';\n    // Change these to whatever you want to process\n    const MODEL_ID = \'general-image-recognition\';\n    const MODEL_VERSION_ID = \'aa7f35c01e0642fda5cf400f543e7c40\';\n    const IMAGE_URL = \'https://samples.clarifai.com/metro-north.jpg\';\n    const MAX_CONCEPTS = 3;\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    const raw = JSON.stringify({\n        "user_app_id": {\n            "user_id": USER_ID,\n            "app_id": APP_ID\n        },\n        "inputs": [\n            {\n                "data": {\n                    "image": {\n                        "url": IMAGE_URL\n                    }\n                }\n            }\n        ],\n        "model": {\n            "output_info": {\n                "output_config": {\n                    "max_concepts": MAX_CONCEPTS\n                }\n            }\n        }\n    });\n\n    const requestOptions = {\n        method: \'POST\',\n        headers: {\n            \'Accept\': \'application/json\',\n            \'Authorization\': \'Key \' + PAT\n        },\n        body: raw\n    };\n\n    // NOTE: MODEL_VERSION_ID is optional, you can also call prediction with the MODEL_ID only\n    // https://api.clarifai.com/v2/models/{YOUR_MODEL_ID}/outputs\n    // this will default to the latest version_id\n\n    fetch("https://api.clarifai.com/v2/models/" + MODEL_ID + "/versions/" + MODEL_VERSION_ID + "/outputs", requestOptions)\n        .then(response => response.text())\n        .then(result => console.log(result))\n        .catch(error => console.log(\'error\', error));\n<\/script>',E='\x3c!--index.html file--\x3e\n\n<script>\n    /////////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, URL of the image\n    // we want as an input, and minimum value. Change these strings to run your own example.\n    ////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n    // Your PAT (Personal Access Token) can be found in the Account\'s Security section\n    const PAT = \'YOUR_PAT_HERE\';\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    const USER_ID = \'clarifai\';    \n    const APP_ID = \'main\';\n    // Change these to whatever you want to process\n    const MODEL_ID = \'general-image-recognition\';   \n    const MODEL_VERSION_ID = \'aa7f35c01e0642fda5cf400f543e7c40\';\n    const IMAGE_URL = \'https://samples.clarifai.com/metro-north.jpg\';\n    const MINIMUM_VALUE = 0.95;\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    const raw = JSON.stringify({\n        "user_app_id": {\n            "user_id": USER_ID,\n            "app_id": APP_ID\n        },\n        "inputs": [\n            {\n                "data": {\n                    "image": {\n                        "url": IMAGE_URL\n                    }\n                }\n            }\n        ],\n        "model": {\n            "output_info": {\n                "output_config": {\n                    "min_value": MINIMUM_VALUE\n                }\n            }\n        }\n    });\n\n    const requestOptions = {\n        method: \'POST\',\n        headers: {\n            \'Accept\': \'application/json\',\n            \'Authorization\': \'Key \' + PAT\n        },\n        body: raw\n    };\n\n    // NOTE: MODEL_VERSION_ID is optional, you can also call prediction with the MODEL_ID only\n    // https://api.clarifai.com/v2/models/{YOUR_MODEL_ID}/outputs\n    // this will default to the latest version_id\n\n    fetch("https://api.clarifai.com/v2/models/" + MODEL_ID + "/versions/" + MODEL_VERSION_ID + "/outputs", requestOptions)\n        .then(response => response.text())\n        .then(result => console.log(result))\n        .catch(error => console.log(\'error\', error));\n<\/script>',w='\x3c!--index.html file--\x3e\n\n<script>\n    /////////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, URL of the image\n    // we want as an input, and concept name and ID. Change these strings to run your own example.\n    ////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n    // Your PAT (Personal Access Token) can be found in the Account\'s Security section\n    const PAT = \'YOUR_PAT_HERE\';\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    const USER_ID = \'clarifai\';    \n    const APP_ID = \'main\';\n    // Change these to whatever you want to process\n    const MODEL_ID = \'general-image-recognition\';   \n    const MODEL_VERSION_ID = \'aa7f35c01e0642fda5cf400f543e7c40\'; \n    const IMAGE_URL = \'https://samples.clarifai.com/metro-north.jpg\';\n    const CONCEPT_NAME = "train";\n    const CONCEPT_ID = "ai_6kTjGfF6";\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    const raw = JSON.stringify({\n        "user_app_id": {\n            "user_id": USER_ID,\n            "app_id": APP_ID\n        },\n        "inputs": [\n            {\n                "data": {\n                    "image": {\n                        "url": IMAGE_URL\n                    }\n                }\n            }\n        ],\n        "model": {\n            "output_info": {\n                "output_config": {\n                    "select_concepts": [\n                        { "name": CONCEPT_NAME },\n                        { "id": CONCEPT_ID }\n                    ]\n                }\n            }\n        }\n    });\n\n    const requestOptions = {\n        method: \'POST\',\n        headers: {\n            \'Accept\': \'application/json\',\n            \'Authorization\': \'Key \' + PAT\n        },\n        body: raw\n    };\n\n    // NOTE: MODEL_VERSION_ID is optional, you can also call prediction with the MODEL_ID only\n    // https://api.clarifai.com/v2/models/{YOUR_MODEL_ID}/outputs\n    // this will default to the latest version_id\n\n    fetch("https://api.clarifai.com/v2/models/" + MODEL_ID + "/versions/" + MODEL_VERSION_ID + "/outputs", requestOptions)\n        .then(response => response.text())\n        .then(result => console.log(result))\n        .catch(error => console.log(\'error\', error));\n<\/script>',b='//index.js file\n\n////////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model ID, model version ID, and \n// URL of the image we want as an input. Change these strings to run your own example.\n//////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\nconst PAT = \'YOUR_PAT_HERE\';\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\nconst USER_ID = \'clarifai\';\nconst APP_ID = \'main\';\n// Change these to whatever you want to process\nconst MODEL_ID = \'general-image-recognition\';\nconst MODEL_VERSION_ID = \'aa7f35c01e0642fda5cf400f543e7c40\';\nconst IMAGE_URL = \'https://samples.clarifai.com/metro-north.jpg\';\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\nstub.PostModelOutputs(\n    {\n        user_app_id: {\n            "user_id": USER_ID,\n            "app_id": APP_ID\n        },\n        model_id: MODEL_ID, \n        version_id: MODEL_VERSION_ID,\n        inputs: [\n            { data: { image: { url: IMAGE_URL, allow_duplicate_url: true } } }\n        ]\n    },\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            throw new Error("Post model outputs failed, status: " + response.status.description);\n        }\n\n        // Since we have one input, one output will exist here.\n        const output = response.outputs[0];\n\n        console.log("Predicted concepts:");\n        for (const concept of output.data.concepts) {\n            console.log(concept.name + " " + concept.value);\n        }\n    }\n\n);',A='//index.js file\n\n///////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, URL of the image\n// we want as an input, and max concepts. Change these strings to run your own example.\n//////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\nconst PAT = \'YOUR_PAT_HERE\';\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\nconst USER_ID = \'clarifai\';\nconst APP_ID = \'main\';\n// Change these to whatever you want to process\nconst MODEL_ID = \'general-image-recognition\';\nconst MODEL_VERSION_ID = \'aa7f35c01e0642fda5cf400f543e7c40\'; \nconst IMAGE_URL = \'https://samples.clarifai.com/metro-north.jpg\';\nconst MAX_CONCEPTS = 3;\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\nstub.PostModelOutputs(\n    {\n        user_app_id: {\n            "user_id": USER_ID,\n            "app_id": APP_ID\n        },\n        model_id: MODEL_ID, \n        version_id: MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n        inputs: [\n            { data: { image: { url: IMAGE_URL, allow_duplicate_url: true } } }\n        ],\n        model: { output_info: { output_config: { max_concepts: MAX_CONCEPTS } } }\n    },\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            throw new Error("Post model outputs failed, status: " + response.status.description);\n        }\n\n        // Since we have one input, one output will exist here.\n        const output = response.outputs[0];\n\n        console.log("Predicted concepts:");\n        for (const concept of output.data.concepts) {\n            console.log(concept.name + " " + concept.value);\n        }\n    }\n\n);',y='//index.js file\n\n/////////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, URL of the image\n// we want as an input, and minimum value. Change these strings to run your own example.\n////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\nconst PAT = \'YOUR_PAT_HERE\';\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\nconst USER_ID = \'clarifai\';\nconst APP_ID = \'main\';\n// Change these to whatever you want to process\nconst MODEL_ID = \'general-image-recognition\';\nconst MODEL_VERSION_ID = \'aa7f35c01e0642fda5cf400f543e7c40\'; \nconst IMAGE_URL = \'https://samples.clarifai.com/metro-north.jpg\';\nconst MINIMUM_VALUE = 0.95;\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\nstub.PostModelOutputs(\n    {\n        user_app_id: {\n            "user_id": USER_ID,\n            "app_id": APP_ID\n        },\n        model_id: MODEL_ID, \n        version_id: MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n        inputs: [\n            { data: { image: { url: IMAGE_URL, allow_duplicate_url: true } } }\n        ],\n        model: { output_info: { output_config: { min_value: MINIMUM_VALUE } } }\n    },\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            throw new Error("Post model outputs failed, status: " + response.status.description);\n        }\n\n        // Since we have one input, one output will exist here.\n        const output = response.outputs[0];\n\n        console.log("Predicted concepts:");\n        for (const concept of output.data.concepts) {\n            console.log(concept.name + " " + concept.value);\n        }\n    }\n\n);',D='//index.js file\n\n////////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, URL of the image\n// we want as an input, and concept name and ID. Change these strings to run your own example.\n///////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\nconst PAT = \'YOUR_PAT_HERE\';\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\nconst USER_ID = \'clarifai\';\nconst APP_ID = \'main\';\n// Change these to whatever you want to process\nconst MODEL_ID = \'general-image-recognition\';\nconst MODEL_VERSION_ID = \'aa7f35c01e0642fda5cf400f543e7c40\'; \nconst IMAGE_URL = \'https://samples.clarifai.com/metro-north.jpg\';\nconst CONCEPT_NAME = "train";\nconst CONCEPT_ID = "ai_6kTjGfF6";\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\nstub.PostModelOutputs(\n    {\n        user_app_id: {\n            "user_id": USER_ID,\n            "app_id": APP_ID\n        },\n        model_id: MODEL_ID, \n        version_id: MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n        inputs: [\n            { data: { image: { url: IMAGE_URL, allow_duplicate_url: true } } }\n        ],\n        // When selecting concepts, value is ignored, so no need to specify it.\n        model: { output_info: { output_config: { select_concepts: [{ name: CONCEPT_NAME }, { id: CONCEPT_ID }] } } }\n    },\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            throw new Error("Post model outputs failed, status: " + response.status.description);\n        }\n\n        // Since we have one input, one output will exist here.\n        const output = response.outputs[0];\n\n        console.log("Predicted concepts:");\n        for (const concept of output.data.concepts) {\n            console.log(concept.name + " " + concept.value);\n        }\n    }\n\n);',O='package com.clarifai.example;\n\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.status.StatusCode;\n\npublic class ClarifaiExample {\n\n    /////////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model ID, model version ID, and\n    // URL of the image we want as an input. Change these strings to run your own example.\n    /////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n    // Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    static final String USER_ID = "clarifai";   \n    static final String APP_ID = "main";\n    // Change these to whatever you want to process\n    static final String MODEL_ID = "general-image-recognition";\n    static final String MODEL_VERSION_ID = "aa7f35c01e0642fda5cf400f543e7c40";\n    static final String IMAGE_URL = "https://samples.clarifai.com/metro-north.jpg";\n    \n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\t\n\n    public static void main(String[] args) {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n            .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        MultiOutputResponse postModelOutputsResponse = stub.postModelOutputs(\n            PostModelOutputsRequest.newBuilder()\n            .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n            .setModelId(MODEL_ID) \n            .setVersionId(MODEL_VERSION_ID) // This is optional. Defaults to the latest model version\n            .addInputs(\n                Input.newBuilder().setData(\n                    Data.newBuilder().setImage(\n                        Image.newBuilder().setUrl(IMAGE_URL)\n                    )\n                )\n            )\n            .build()\n        );\n\n        if (postModelOutputsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("Post model outputs failed, status: " + postModelOutputsResponse.getStatus());\n        }\n\n        // Since we have one input, one output will exist here.\n        Output output = postModelOutputsResponse.getOutputs(0);\n\n        System.out.println("Predicted concepts:");\n        for (Concept concept: output.getData().getConceptsList()) {\n            System.out.printf("%s %.2f%n", concept.getName(), concept.getValue());\n        }\n\n    }\n\n}',x='package com.clarifai.example;\n\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.status.StatusCode;\n\npublic class ClarifaiExample {\n\n    /////////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, URL of the image\n    // we want as an input, and max concepts. Change these strings to run your own example.\n    ////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n    // Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    static final String USER_ID = "clarifai";    \n    static final String APP_ID = "main";\n    // Change these to whatever you want to process\n    static final String MODEL_ID = "general-image-recognition";\n    static final String MODEL_VERSION_ID = "aa7f35c01e0642fda5cf400f543e7c40";\n    static final String IMAGE_URL = "https://samples.clarifai.com/metro-north.jpg";\n    static final int MAX_CONCEPTS = 3;\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\t\n\n    public static void main(String[] args) {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n            .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        MultiOutputResponse postModelOutputsResponse = stub.postModelOutputs(\n            PostModelOutputsRequest.newBuilder()\n            .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n            .setModelId(MODEL_ID)\n            .setVersionId(MODEL_VERSION_ID)  // This is optional. Defaults to the latest model version\n            .addInputs(\n                Input.newBuilder().setData(\n                    Data.newBuilder().setImage(\n                        Image.newBuilder().setUrl(IMAGE_URL)\n                    )\n                )\n            )\n            .setModel(\n                Model.newBuilder().setOutputInfo(\n                    OutputInfo.newBuilder().setOutputConfig(\n                        OutputConfig.newBuilder().setMaxConcepts(MAX_CONCEPTS)\n                    )\n                )\n            )\n            .build()\n        );\n\n        if (postModelOutputsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("Post model outputs failed, status: " + postModelOutputsResponse.getStatus());\n        }\n\n        // Since we have one input, one output will exist here\n        Output output = postModelOutputsResponse.getOutputs(0);\n\n        System.out.println("Predicted concepts:");\n        for (Concept concept: output.getData().getConceptsList()) {\n            System.out.printf("%s %.2f%n", concept.getName(), concept.getValue());\n        }\n\n    }\n\n}',v='package com.clarifai.example;\n\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.status.StatusCode;\n\npublic class ClarifaiExample {\n\n    /////////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, URL of the image\n    // we want as an input, and minimum value. Change these strings to run your own example.\n    ////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n    // Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    static final String USER_ID = "clarifai";    \n    static final String APP_ID = "main";\n    // Change these to whatever you want to process\n    static final String MODEL_ID = "general-image-recognition";\n    static final String MODEL_VERSION_ID = "aa7f35c01e0642fda5cf400f543e7c40";\n    static final String IMAGE_URL = "https://samples.clarifai.com/metro-north.jpg";\n    static final float MINIMUM_VALUE = 0.95f;\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\t\n\n    public static void main(String[] args) {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n            .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        MultiOutputResponse postModelOutputsResponse = stub.postModelOutputs(\n            PostModelOutputsRequest.newBuilder()\n            .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n            .setModelId(MODEL_ID) \n            .setVersionId(MODEL_VERSION_ID)  // This is optional. Defaults to the latest model version\n            .addInputs(\n                Input.newBuilder().setData(\n                    Data.newBuilder().setImage(\n                        Image.newBuilder().setUrl(IMAGE_URL)\n                    )\n                )\n            )\n            .setModel(\n                Model.newBuilder().setOutputInfo(\n                    OutputInfo.newBuilder().setOutputConfig(\n                        OutputConfig.newBuilder().setMinValue(MINIMUM_VALUE)\n                    )\n                )\n            )\n            .build()\n        );\n\n        if (postModelOutputsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("Post model outputs failed, status: " + postModelOutputsResponse.getStatus());\n        }\n\n        // Since we have one input, one output will exist here\n        Output output = postModelOutputsResponse.getOutputs(0);\n\n        System.out.println("Predicted concepts:");\n        for (Concept concept: output.getData().getConceptsList()) {\n            System.out.printf("%s %.2f%n", concept.getName(), concept.getValue());\n        }\n\n    }\n\n}',S='package com.clarifai.example;\n\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.status.StatusCode;\n\npublic class ClarifaiExample {\n\n    ////////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, URL of the image\n    // we want as an input, and concept name and ID. Change these strings to run your own example.\n    ///////////////////////////////////////////////////////////////////////////////////////////////////////\n\n    //Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    static final String USER_ID = "clarifai";    \n    static final String APP_ID = "main";\n    // Change these to whatever you want to process\n    static final String MODEL_ID = "general-image-recognition";\n    static final String MODEL_VERSION_ID = "aa7f35c01e0642fda5cf400f543e7c40";\n    static final String IMAGE_URL = "https://samples.clarifai.com/metro-north.jpg";\n    static final String CONCEPT_NAME = "train";\n    static final String CONCEPT_ID = "ai_6kTjGfF6";\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\t\n\n    public static void main(String[] args) {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n            .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        MultiOutputResponse postModelOutputsResponse = stub.postModelOutputs(\n            PostModelOutputsRequest.newBuilder()\n            .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n            .setModelId(MODEL_ID) \n            .setVersionId(MODEL_VERSION_ID)  // This is optional. Defaults to the latest model version\n            .addInputs(\n                Input.newBuilder().setData(\n                    Data.newBuilder().setImage(\n                        Image.newBuilder().setUrl(IMAGE_URL)\n                    )\n                )\n            )\n            .setModel(\n                Model.newBuilder().setOutputInfo(\n                    OutputInfo.newBuilder().setOutputConfig(\n                        OutputConfig.newBuilder()\n                        // When selecting concepts, value is ignored, so no need to specify it\n                        .addSelectConcepts(Concept.newBuilder().setName(CONCEPT_NAME))\n                        .addSelectConcepts(Concept.newBuilder().setId(CONCEPT_ID))\n                    )\n                )\n            )\n            .build()\n        );\n\n        if (postModelOutputsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("Post model outputs failed, status: " + postModelOutputsResponse.getStatus());\n        }\n\n        // Since we have one input, one output will exist here\n        Output output = postModelOutputsResponse.getOutputs(0);\n\n        System.out.println("Predicted concepts:");\n        for (Concept concept: output.getData().getConceptsList()) {\n            System.out.printf("%s %.2f%n", concept.getName(), concept.getValue());\n        }\n\n    }\n\n}',j="<?php\n\nrequire __DIR__ . '/vendor/autoload.php';\n\n////////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model ID, model version ID, and \n// URL of the image we want as an input. Change these strings to run your own example.\n///////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account's Security section\n$PAT = 'YOUR_PAT_HERE';\n// Specify the correct user_id/app_id pairings\n// Since you're making inferences outside your app's scope\n$USER_ID = 'clarifai';\n$APP_ID = 'main';\n// Change these to whatever you want to process\n$MODEL_ID = 'general-image-recognition';\n$MODEL_VERSION_ID = 'aa7f35c01e0642fda5cf400f543e7c40';\n$IMAGE_URL = 'https://samples.clarifai.com/metro-north.jpg';\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Image;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\PostModelOutputsRequest;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ['Authorization' => ['Key ' . $PAT ]];\n\n$userDataObject = new UserAppIDSet([\n    'user_id' => $USER_ID, \n    'app_id' => $APP_ID \n]);\n\n// Let's make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client->PostModelOutputs(\n    // The request object carries the request along with the request status and other metadata related to the request itself\n    new PostModelOutputsRequest([\n        'user_app_id' => $userDataObject,\n        'model_id' => $MODEL_ID, \n        'version_id' => $MODEL_VERSION_ID,\n        'inputs' => [\n            new Input([ // The Input object wraps the Data object in order to meet the API specification                \n                'data' => new Data([ // The Data object is constructed around the Image object. It offers a container that has additional image independent\n                                    // metadata. In this particular use case, no other metadata is needed to be specified\n                    'image' => new Image([ // In the Clarifai platform, an image is defined by a special Image object\n                        'url' => $IMAGE_URL\n                    ])\n                ])\n            ])\n        ]       \n    ]),\n    $metadata\n)->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception(\"Error: {$status->details}\");\n}\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure \n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    throw new Exception(\"Failure response: \" . $response->getStatus()->getDescription() . \" \" .\n        $response->getStatus()->getDetails());\n}\n\n// The output of a successful call can be used in many ways. In this example, we loop through all of the predicted concepts \n// and print them out along with their numerical prediction value (confidence)\necho \"Predicted concepts: </br>\";\nforeach ($response->getOutputs()[0]->getData()->getConcepts() as $concept) {\n    echo $concept->getName() . \": \" . number_format($concept->getValue(), 2) . \"</br>\";\n}\n\n?>",C="<?php\n\nrequire __DIR__ . '/vendor/autoload.php';\n\n///////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, URL of the image\n// we want as an input, and max concepts. Change these strings to run your own example.\n//////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account's Security section\n$PAT = \"YOUR_PAT_HERE\";\n// Specify the correct user_id/app_id pairings\n// Since you're making inferences outside your app's scope\n$USER_ID = \"clarifai\";\n$APP_ID = \"main\";\n// Change these to whatever you want to process\n$MODEL_ID = 'general-image-recognition';\n$MODEL_VERSION_ID = 'aa7f35c01e0642fda5cf400f543e7c40';  \n$IMAGE_URL = 'https://samples.clarifai.com/metro-north.jpg';\n$MAX_CONCEPTS = 3;\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Image;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\Model;\nuse Clarifai\\Api\\OutputInfo;\nuse Clarifai\\Api\\OutputConfig;\nuse Clarifai\\Api\\PostModelOutputsRequest;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ['Authorization' => ['Key ' . $PAT ]];\n\n$userDataObject = new UserAppIDSet([\n    'user_id' => $USER_ID, \n    'app_id' => $APP_ID \n]);\n\n// Let's make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client->PostModelOutputs(\n    // The request object carries the request along with the request status and other metadata related to the request itself\n    new PostModelOutputsRequest([\n        'user_app_id' => $userDataObject,\n        'model_id' => $MODEL_ID, \n        'version_id' => $MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n        'inputs' => [\n            new Input([ // The Input object wraps the Data object in order to meet the API specification                \n                'data' => new Data([ // The Data object is constructed around the Image object. It offers a container that has additional image independent\n                                    // metadata. In this particular use case, no other metadata is needed to be specified\n                    'image' => new Image([ // In the Clarifai platform, an image is defined by a special Image object\n                        'url' => $IMAGE_URL\n                    ])\n                ])\n            ])\n        ],\n        // The model object is a wrapper around the OutputInfo object\n        \"model\" => new Model([\n            //The OutputInfo object is a wrapper around the OutputConfig object\n            \"output_info\" => new OutputInfo([\n                // Output configuration can be specified by the OutputConfig object. Here we specify the max number of concepts \n                // to return at 3.\n                \"output_config\" => new OutputConfig([\n                   \"max_concepts\" => $MAX_CONCEPTS\n                ])\n            ])\n        ])\n\n    ]),\n    $metadata\n)->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception(\"Error: {$status->details}\");\n}\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure \n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    throw new Exception(\"Failure response: \" . $response->getStatus()->getDescription() . \" \" .\n        $response->getStatus()->getDetails());\n}\n\n// The output of a successful call can be used in many ways. In this example, we loop through all of the predicted concepts \n// and print them out along with their numerical prediction value (confidence)\necho \"Predicted concepts: </br>\";\nforeach ($response->getOutputs()[0]->getData()->getConcepts() as $concept) {\n    echo $concept->getName() . \": \" . number_format($concept->getValue(), 2) . \"</br>\";\n}\n\n?>",T="<?php\n\nrequire __DIR__ . '/vendor/autoload.php';\n\n////////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, URL of the image\n// we want as an input, and minimum value. Change these strings to run your own example.\n////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account's Security section\n$PAT = \"YOUR_PAT_HERE\";\n// Specify the correct user_id/app_id pairings\n// Since you're making inferences outside your app's scope\n$USER_ID = \"clarifai\";\n$APP_ID = \"main\";\n// Change these to whatever you want to process\n$MODEL_ID = 'general-image-recognition';\n$MODEL_VERSION_ID = 'aa7f35c01e0642fda5cf400f543e7c40';\n$IMAGE_URL = 'https://samples.clarifai.com/metro-north.jpg';\n$MINIMUM_VALUE = 0.95;\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Image;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\Model;\nuse Clarifai\\Api\\OutputInfo;\nuse Clarifai\\Api\\OutputConfig;\nuse Clarifai\\Api\\PostModelOutputsRequest;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ['Authorization' => ['Key ' . $PAT ]];\n\n$userDataObject = new UserAppIDSet([\n    'user_id' => $USER_ID, \n    'app_id' => $APP_ID \n]);\n\n// Let's make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client->PostModelOutputs(\n    // The request object carries the request along with the request status and other metadata related to the request itself\n    new PostModelOutputsRequest([\n        'user_app_id' => $userDataObject,\n        'model_id' => $MODEL_ID, \n        'version_id' => $MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n        'inputs' => [\n            new Input([ // The Input object wraps the Data object in order to meet the API specification                \n                'data' => new Data([ // The Data object is constructed around the Image object. It offers a container that has additional image independent\n                                    // metadata. In this particular use case, no other metadata is needed to be specified\n                    'image' => new Image([ // In the Clarifai platform, an image is defined by a special Image object\n                        'url' => $IMAGE_URL\n                    ])\n                ])\n            ])\n        ],\n        // The model object is a wrapper around the OutputInfo object\n        \"model\" => new Model([\n            //The OutputInfo object is a wrapper around the OutputConfig object\n            \"output_info\" => new OutputInfo([\n                // Output configuration can be specified by the OutputConfig object. Here we specify the minimum \n                // threshold value to 0.95.\n                \"output_config\" => new OutputConfig([\n                   \"min_value\" => $MINIMUM_VALUE\n                ])\n            ])\n        ])\n\n    ]),\n    $metadata\n)->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception(\"Error: {$status->details}\");\n}\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure \n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    throw new Exception(\"Failure response: \" . $response->getStatus()->getDescription() . \" \" .\n        $response->getStatus()->getDetails());\n}\n\n// The output of a successful call can be used in many ways. In this example, we loop through all of the predicted concepts \n// and print them out along with their numerical prediction value (confidence)\necho \"Predicted concepts: </br>\";\nforeach ($response->getOutputs()[0]->getData()->getConcepts() as $concept) {\n    echo $concept->getName() . \": \" . number_format($concept->getValue(), 2) . \"</br>\";\n}\n\n?>",P="<?php\n\nrequire __DIR__ . '/vendor/autoload.php';\n\n///////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, URL of the image\n// we want as an input, and concept name and ID. Change these strings to run your own example.\n/////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account's Security section\n$PAT = \"YOUR_PAT_HERE\";\n// Specify the correct user_id/app_id pairings\n// Since you're making inferences outside your app's scope\n$USER_ID = \"clarifai\";\n$APP_ID = \"main\";\n// Change these to whatever you want to process\n$MODEL_ID = 'general-image-recognition'; \n$MODEL_VERSION_ID = 'aa7f35c01e0642fda5cf400f543e7c40';   \n$IMAGE_URL = 'https://samples.clarifai.com/metro-north.jpg';\n$CONCEPT_NAME = \"train\";\n$CONCEPT_ID = \"ai_6kTjGfF6\";\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\Api\\Concept;\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Image;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\Model;\nuse Clarifai\\Api\\OutputInfo;\nuse Clarifai\\Api\\OutputConfig;\nuse Clarifai\\Api\\PostModelOutputsRequest;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ['Authorization' => ['Key ' . $PAT ]];\n\n$userDataObject = new UserAppIDSet([\n    'user_id' => $USER_ID, \n    'app_id' => $APP_ID \n]);\n\n// Let's make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client->PostModelOutputs(\n    // The request object carries the request along with the request status and other metadata related to the request itself\n    new PostModelOutputsRequest([\n        'user_app_id' => $userDataObject,\n        'model_id' => $MODEL_ID, \n        'version_id' => $MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n        'inputs' => [\n            new Input([ // The Input object wraps the Data object in order to meet the API specification                \n                'data' => new Data([ // The Data object is constructed around the Image object. It offers a container that has additional image independent\n                                    // metadata. In this particular use case, no other metadata is needed to be specified\n                    'image' => new Image([ // In the Clarifai platform, an image is defined by a special Image object\n                        'url' => $IMAGE_URL\n                    ])\n                ])\n            ])\n        ],\n        // The model object is a wrapper around the OutputInfo object\n        \"model\" => new Model([\n            //The OutputInfo object is a wrapper around the OutputConfig object\n            \"output_info\" => new OutputInfo([\n                // Output configuration can be specified by the OutputConfig object. Here we specify a concept by both the name and the id \n                // for what we want to narrow down to in the results.\n                \"output_config\" => new OutputConfig([\n                    \"select_concepts\" => [\n                        // When selecting concepts, value is ignored, so no need to specify it\n                        new Concept([\n                            \"name\" => $CONCEPT_NAME\n                        ]),\n                        new Concept([\n                            \"id\" => $CONCEPT_ID\n                        ])\n                    ]\n                ])\n            ])\n        ])\n\n    ]),\n    $metadata\n)->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception(\"Error: {$status->details}\");\n}\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure \n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    throw new Exception(\"Failure response: \" . $response->getStatus()->getDescription() . \" \" .\n        $response->getStatus()->getDetails());\n}\n\n// The output of a successful call can be used in many ways. In this example, we loop through all of the predicted concepts \n// and print them out along with their numerical prediction value (confidence)\necho \"Predicted concepts: </br>\";\nforeach ($response->getOutputs()[0]->getData()->getConcepts() as $concept) {\n    echo $concept->getName() . \": \" . number_format($concept->getValue(), 2) . \"</br>\";\n}\n\n?>",R='# Version ID is optional. It defaults to the latest model version, if omitted.\n\ncurl -X POST "https://api.clarifai.com/v2/users/clarifai/apps/main/models/general-image-recognition/versions/aa7f35c01e0642fda5cf400f543e7c40/outputs" \\\n  -H "Authorization: Key YOUR_PAT_HERE" \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "inputs": [\n      {\n        "data": {\n          "image": {\n            "url": "https://samples.clarifai.com/metro-north.jpg"\n          }\n        }\n      }\n    ]\n  }\'\n  \n',M='curl -X POST "https://api.clarifai.com/v2/users/clarifai/apps/main/models/general-image-recognition/outputs" \\\n  -H "Authorization: Key YOUR_PAT_HERE" \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "inputs": [\n      {\n        "data": {\n          "image": {\n            "url": "https://samples.clarifai.com/metro-north.jpg"\n          }\n        }\n      }\n    ],\n    "model":{\n      "output_info":{\n        "output_config":{\n          "max_concepts": 3\n        }\n      }\n    }\n  }\'',N='curl -X POST "https://api.clarifai.com/v2/users/clarifai/apps/main/models/general-image-recognition/outputs" \\\n  -H "Authorization: Key YOUR_PAT_HERE" \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "inputs": [\n      {\n        "data": {\n          "image": {\n            "url": "https://samples.clarifai.com/metro-north.jpg"\n          }\n        }\n      }\n    ],\n    "model":{\n      "output_info":{\n        "output_config":{\n          "min_value": 0.95\n        }\n      }\n    }\n  }\'',L='curl -X POST "https://api.clarifai.com/v2/users/clarifai/apps/main/models/general-image-recognition/outputs" \\\n  -H "authorization: Key YOUR_PAT_HERE" \\\n  -H "content-Type: application/json" \\\n  -d \'{\n  "inputs": [\n    {\n      "data": {\n        "image": {\n          "url": "https://samples.clarifai.com/metro-north.jpg"\n        }\n      }\n    }\n  ],\n  "model": {\n    "output_info": {\n      "output_config": {\n        "select_concepts": [\n          {"name": "train"},\n          {"id": "ai_6kTjGfF6"}\n        ]\n      }\n    }\n  }\n}\'\n\n',U="Predicted concepts:\ntrain 1.00\nrailway 1.00\nsubway system 1.00\nstation 1.00\nlocomotive 1.00\ntransportation system 1.00\ntravel 0.99\ncommuter 0.98\nplatform 0.98\nlight 0.97\ntrain station 0.97\nblur 0.97\ncity 0.96\nroad 0.96\nurban 0.96\ntraffic 0.96\nstreet 0.95\npublic 0.93\ntramway 0.93\nbusiness 0.93",k={description:"Learn how to use our advanced inference operations",sidebar_position:5},$="Advanced Inference Options",H={},V=[{value:"Perform Batch Predictions",id:"perform-batch-predictions",level:2},{value:"Customize Base_URL",id:"customize-base_url",level:2},{value:"Add Root Certificate",id:"add-root-certificate",level:2},{value:"Prompt Types",id:"prompt-types",level:2},{value:"Question Answering",id:"question-answering",level:3},{value:"Grammar Correction",id:"grammar-correction",level:3},{value:"Summarize",id:"summarize",level:3},{value:"Translation",id:"translation",level:3},{value:"Types of Inference Parameters",id:"types-of-inference-parameters",level:2},{value:"Max Tokens (or Max Length)",id:"max-tokens-or-max-length",level:3},{value:"Minimum Prediction Value",id:"minimum-prediction-value",level:3},{value:"Maximum Concepts",id:"maximum-concepts",level:3},{value:"Select Concepts",id:"select-concepts",level:3},{value:"Temperature",id:"temperature",level:3},{value:"Top-p (Nucleus)",id:"top-p-nucleus",level:3},{value:"Top-k",id:"top-k",level:3},{value:"Reasoning Effort",id:"reasoning-effort",level:3},{value:"Number of Beams",id:"number-of-beams",level:3},{value:"Do Sample",id:"do-sample",level:3},{value:"Return Full Text",id:"return-full-text",level:3},{value:"System Prompt",id:"system-prompt",level:3},{value:"Prompt Template",id:"prompt-template",level:3},{value:"Predict By Model Version ID",id:"predict-by-model-version-id",level:2}];function Y(e){const n={a:"a",admonition:"admonition",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components},{Details:t}=n;return t||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"advanced-inference-options",children:"Advanced Inference Options"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Learn how to use our advanced inference operations"})}),"\n",(0,i.jsx)("hr",{}),"\n",(0,i.jsx)(n.admonition,{title:"note",type:"warning",children:(0,i.jsxs)(n.p,{children:["To find out which advanced inference parameters a model supports, you can review its description and notes on the Clarifai Community platform, or run the snippet shown ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/compute/inference/clarifai/#get-method-signature",children:"here"})," to inspect the model\u2019s signature."]})}),"\n",(0,i.jsx)(n.p,{children:"The advanced inference operations allow you to fine-tune how outputs are generated, giving you greater control to manipulate results according to their specific tasks and requirements."}),"\n",(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsxs)(n.p,{children:["Before using the ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/additional-resources/api-overview/python-sdk",children:"Python SDK"}),", ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/additional-resources/api-overview/cli",children:"CLI"}),", ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/additional-resources/api-overview/nodejs-sdk",children:"Node.js SDK"}),", or any of our ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/additional-resources/api-overview/grpc-clients",children:"gRPC clients"}),", ensure they are properly installed on your machine. Refer to their respective installation guides for instructions on how to install and initialize them."]})}),"\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n",(0,i.jsx)(n.h2,{id:"perform-batch-predictions",children:"Perform Batch Predictions"}),"\n",(0,i.jsx)(n.p,{children:"You can process multiple inputs in a single request, streamlining the prediction workflow and saving both time and resources."}),"\n",(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsxs)(n.p,{children:["The batch size should not exceed 128. Learn more ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/create-manage/inputs/upload/#upload-limits",children:"here"}),"."]})}),"\n",(0,i.jsxs)(o.A,{groupId:"code",children:[(0,i.jsx)(r.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(c.A,{className:"language-python",children:l})}),(0,i.jsx)(r.A,{value:"typescript",label:"Node.js SDK",children:(0,i.jsx)(c.A,{className:"language-typescript",children:p})})]}),"\n",(0,i.jsx)(n.h2,{id:"customize-base_url",children:"Customize Base_URL"}),"\n",(0,i.jsxs)(n.p,{children:["You can obtain model predictions by customizing the ",(0,i.jsx)(n.code,{children:"base_url"}),". This allows you to easily adapt your endpoint to different environments, providing a flexible and seamless way to access model predictions."]}),"\n",(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsxs)(n.p,{children:["This feature is particularly useful for enterprises using on-premises deployments, allowing the ",(0,i.jsx)(n.code,{children:"base_url"})," to be configured to point to their respective servers."]})}),"\n",(0,i.jsx)(o.A,{groupId:"code",children:(0,i.jsx)(r.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(c.A,{className:"language-python",children:u})})}),"\n",(0,i.jsx)(n.h2,{id:"add-root-certificate",children:"Add Root Certificate"}),"\n",(0,i.jsx)(n.p,{children:"A root certificate provides an additional layer of security when communicating through APIs. As a self-signed certificate that verifies the legitimacy of other certificates, it establishes a chain of trust \u2014 ensuring that you are connecting to authentic APIs and that your data remains encrypted."}),"\n",(0,i.jsx)(n.p,{children:"You can add your own root certificates to further strengthen data security and protect user privacy."}),"\n",(0,i.jsx)(o.A,{groupId:"code",children:(0,i.jsx)(r.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(c.A,{className:"language-python",children:d})})}),"\n",(0,i.jsx)(n.h2,{id:"prompt-types",children:"Prompt Types"}),"\n",(0,i.jsx)(n.p,{children:"A prompt is a piece of text or set of instructions that you provide to generative AI models, such as Large Language Models (LLMs),  to generate a specific response or action."}),"\n",(0,i.jsx)(n.p,{children:"Generative AI models are a type of artificial intelligence system that are designed to create new content, such as text, images, audio, or even videos, based on patterns learned from existing data."}),"\n",(0,i.jsx)(n.p,{children:"There are several prompting techniques you can use to communicate with generative AI models. For example, zero-shot prompting leverages a model\u2019s inherent language understanding capabilities to generate responses without any specific preparation or examples."}),"\n",(0,i.jsxs)(n.p,{children:["You can learn about other prompting techniques ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/portal-guide/agent-system-operators/prompter/",children:"here"}),"."]}),"\n",(0,i.jsx)(n.p,{children:"Here are some examples of prompts."}),"\n",(0,i.jsx)(n.h3,{id:"question-answering",children:"Question Answering"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",children:"Prompt: \nWho was president of the United States in 1955?\n"})}),"\n",(0,i.jsx)(n.h3,{id:"grammar-correction",children:"Grammar Correction"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",children:"Prompt: \nCorrect this to standard English: She no went to the market.\n\nSample Response: \nShe did not go to the market.\n"})}),"\n",(0,i.jsx)(n.h3,{id:"summarize",children:"Summarize"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",children:"Prompt: \nSummarize this: Jupiter is the fifth planet from the Sun and the largest in the Solar System. It is a \ngas giant with a mass one-thousandth that of the Sun, but two-and-a-half times\nthat of all the other planets in the Solar System combined. Jupiter is one of the \nbrightest objects visible to the naked eye in the night sky, and has been known to\nancient civilizations since before recorded history. It is named after the Roman \ngod Jupiter.[19] When viewed from Earth, Jupiter can be bright enough for its \nreflected light to cast visible shadows,[20] and is on average the third-brightest \nnatural object in the night sky after the Moon and Venus.\n\nSample Response: \nJupiter is the fifth planet from the Sun and is very big and bright. It can be seen\nwith our eyes in the night sky and it has been known since ancient times. Its \nname comes from the Roman god Jupiter. It is usually the third brightest object in \nthe night sky after the Moon and Venus.\n"})}),"\n",(0,i.jsx)(n.h3,{id:"translation",children:"Translation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",children:"Prompt: \nTranslate this into 1. French, 2. Spanish, and 3. Japanese: What rooms do you have available?`\n\nSample Response: \nQuels sont les chambres que vous avez disponibles?\n2. \xbfQu\xe9 habitaciones tienes disponibles?\n3. \u3069\u306e\u90e8\u5c4b\u304c\u5229\u7528\u53ef\u80fd\u3067\u3059\u304b\uff1f\n"})}),"\n",(0,i.jsx)(n.admonition,{type:"tip",children:(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.a,{href:"https://platform.openai.com/docs/examples",children:"Click here"})," for more prompting examples."]})}),"\n",(0,i.jsx)(n.h2,{id:"types-of-inference-parameters",children:"Types of Inference Parameters"}),"\n",(0,i.jsx)(n.p,{children:"When making predictions using the models on our platform, some of them offer the ability to specify various inference parameters to influence their output."}),"\n",(0,i.jsx)(n.p,{children:"These parameters control the behavior of the model during the prediction process, affecting aspects like creativity, coherence, and the diversity of the output."}),"\n",(0,i.jsx)(n.p,{children:"Let\u2019s talk about them."}),"\n",(0,i.jsx)(n.h3,{id:"max-tokens-or-max-length",children:"Max Tokens (or Max Length)"}),"\n",(0,i.jsx)(n.p,{children:"Max Tokens specifies the maximum number of tokens (words or characters) the model is allowed to generate in a single response. It limits the length of the output, preventing the model from generating overly long responses. As such, shorter token lengths will provide faster performance."}),"\n",(0,i.jsx)(n.p,{children:"This inference parameter helps in controlling the verbosity of the output, especially in applications where concise responses are required."}),"\n",(0,i.jsx)(n.p,{children:"Here is a usage example:"}),"\n",(0,i.jsx)(o.A,{groupId:"code",children:(0,i.jsx)(r.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(c.A,{className:"language-python",children:"inference_params = dict(max_tokens=100)\nModel(model_url).predict(inputs,inference_params=inference_params)"})})}),"\n",(0,i.jsx)(n.h3,{id:"minimum-prediction-value",children:"Minimum Prediction Value"}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"min_value"})," specifies the minimum prediction confidence required to include a result in the output. For example if you want to see all concepts with a probability score of ",(0,i.jsx)(n.code,{children:".95"})," or higher, this parameter will allow you to accomplish that."]}),"\n",(0,i.jsxs)(n.p,{children:["Also note that if you don't specify the number of ",(0,i.jsx)(n.code,{children:"max_concepts"}),", you will only see the top 20. If your result can contain more values you will have to increase the number of maximum concepts as well."]}),"\n",(0,i.jsx)(n.p,{children:"Here is a usage example:"}),"\n",(0,i.jsxs)(o.A,{groupId:"code",children:[(0,i.jsx)(r.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(c.A,{className:"language-python",children:"output_config = dict(min_value=0.6)\nModel(model_url).predict(inputs,output_config=output_config)"})}),(0,i.jsx)(r.A,{value:"python2",label:"Python (gRPC)",children:(0,i.jsx)(c.A,{className:"language-python",children:f})}),(0,i.jsx)(r.A,{value:"js_rest",label:"JavaScript (REST)",children:(0,i.jsx)(c.A,{className:"language-javascript",children:E})}),(0,i.jsx)(r.A,{value:"nodejs",label:"Node.js (gRPC)",children:(0,i.jsx)(c.A,{className:"language-javascript",children:y})}),(0,i.jsx)(r.A,{value:"java",label:"Java (gRPC)",children:(0,i.jsx)(c.A,{className:"language-java",children:v})}),(0,i.jsx)(r.A,{value:"php",label:"PHP (gRPC)",children:(0,i.jsx)(c.A,{className:"language-php",children:T})}),(0,i.jsx)(r.A,{value:"curl",label:"cURL",children:(0,i.jsx)(c.A,{className:"language-bash",children:N})})]}),"\n",(0,i.jsx)(n.h3,{id:"maximum-concepts",children:"Maximum Concepts"}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"max_concepts"})," parameter specifies how many concepts and their associated probability scores the Predict endpoint should return. If not set, the endpoint defaults to returning the top 20 concepts."]}),"\n",(0,i.jsxs)(n.p,{children:["You can currently set ",(0,i.jsx)(n.code,{children:"max_concepts"})," to any value between 1 and 200."]}),"\n",(0,i.jsxs)(n.p,{children:["If your use case requires more than 200 concepts, please reach out to our ",(0,i.jsx)(n.a,{href:"mailto:support@clarifai.com",children:"support team"})," for assistance."]}),"\n",(0,i.jsxs)(o.A,{groupId:"code",children:[(0,i.jsx)(r.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(c.A,{className:"language-python",children:"output_config = dict(max_concepts=3)\nModel(model_url).predict(inputs,output_config=output_config)"})}),(0,i.jsx)(r.A,{value:"python2",label:"Python (gRPC)",children:(0,i.jsx)(c.A,{className:"language-python",children:m})}),(0,i.jsx)(r.A,{value:"js_rest",label:"JavaScript (REST)",children:(0,i.jsx)(c.A,{className:"language-javascript",children:I})}),(0,i.jsx)(r.A,{value:"nodejs",label:"Node.js (gRPC)",children:(0,i.jsx)(c.A,{className:"language-javascript",children:A})}),(0,i.jsx)(r.A,{value:"java",label:"Java (gRPC)",children:(0,i.jsx)(c.A,{className:"language-java",children:x})}),(0,i.jsx)(r.A,{value:"php",label:"PHP (gRPC)",children:(0,i.jsx)(c.A,{className:"language-php",children:C})}),(0,i.jsx)(r.A,{value:"curl",label:"cURL",children:(0,i.jsx)(c.A,{className:"language-bash",children:M})})]}),"\n",(0,i.jsx)(n.h3,{id:"select-concepts",children:"Select Concepts"}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"select_concepts"})," specifies the concepts to include in the prediction results. By putting this additional parameter on your predict calls, you can receive predict value(s) for ",(0,i.jsx)(n.strong,{children:"only"})," the concepts that you want to. You can specify particular concepts by either their id and/or their name."]}),"\n",(0,i.jsx)(n.p,{children:"The concept names and ids are case sensitive; and so, these must be exact matches."}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Note"}),": You can use the ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/api-guide/model/create-get-update-and-delete#get-model-output-info-by-id",children:(0,i.jsx)(n.code,{children:"GetModelOutputInfo"})})," endpoint to retrieve an entire list of concepts from a given model, and get their ids and names."]}),"\n"]}),"\n",(0,i.jsx)(n.admonition,{type:"caution",children:(0,i.jsx)(n.p,{children:"If you submit a request with not an exact match of the concept id or name, you will receive an invalid model argument error. However, if one or more matches while one or more do not, the API will respond with a Mixed Success."})}),"\n",(0,i.jsxs)(o.A,{groupId:"code",children:[(0,i.jsx)(r.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(c.A,{className:"language-python",children:'output_config = dict(select_concepts=["concept_name"])\nModel(model_url).predict(inputs,output_config=output_config)'})}),(0,i.jsx)(r.A,{value:"python2",label:"Python (gRPC)",children:(0,i.jsx)(c.A,{className:"language-python",children:g})}),(0,i.jsx)(r.A,{value:"js_rest",label:"JavaScript (REST)",children:(0,i.jsx)(c.A,{className:"language-javascript",children:w})}),(0,i.jsx)(r.A,{value:"nodejs",label:"Node.js (gRPC)",children:(0,i.jsx)(c.A,{className:"language-javascript",children:D})}),(0,i.jsx)(r.A,{value:"java",label:"Java (gRPC)",children:(0,i.jsx)(c.A,{className:"language-java",children:S})}),(0,i.jsx)(r.A,{value:"php",label:"PHP (gRPC)",children:(0,i.jsx)(c.A,{className:"language-php",children:P})}),(0,i.jsx)(r.A,{value:"curl",label:"cURL",children:(0,i.jsx)(c.A,{className:"language-bash",children:L})})]}),"\n",(0,i.jsx)(n.h3,{id:"temperature",children:"Temperature"}),"\n",(0,i.jsx)(n.p,{children:"Temperature is a decimal number (between 0 and 1) that controls the degree of randomness in the response."}),"\n",(0,i.jsx)(n.p,{children:"A low temperature (e.g., 0.2) makes the model more deterministic, leading to a more conservative and predictable output. A high temperature (e.g., 0.8) increases the randomness, allowing for more creative and varied responses."}),"\n",(0,i.jsx)(n.p,{children:"Adjusting temperature is useful when you want to balance between creative responses and focused, precise answers."}),"\n",(0,i.jsx)(n.p,{children:"Here is a usage example:"}),"\n",(0,i.jsx)(o.A,{groupId:"code",children:(0,i.jsx)(r.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(c.A,{className:"language-python",children:"inference_params = dict(temperature=0.2)\nModel(model_url).predict(inputs,inference_params=inference_params)"})})}),"\n",(0,i.jsx)(n.h3,{id:"top-p-nucleus",children:"Top-p (Nucleus)"}),"\n",(0,i.jsx)(n.p,{children:"Top-p sampling is an alternative to temperature sampling that controls output diversity by considering the smallest set of tokens whose cumulative probability is greater than or equal to a specified threshold p (e.g., 0.9)."}),"\n",(0,i.jsx)(n.p,{children:"Rather than restricting the model to a fixed number of top tokens, this method dynamically adjusts the selection based on token probabilities, ensuring that the most likely tokens are always included while maintaining flexibility in the number of tokens considered."}),"\n",(0,i.jsx)(n.p,{children:"It\u2019s useful when you want to dynamically control the diversity of the generated output without setting a fixed limit on the number of tokens."}),"\n",(0,i.jsx)(n.h3,{id:"top-k",children:"Top-k"}),"\n",(0,i.jsx)(n.p,{children:"Top-k sampling limits the model to only consider the top k most probable tokens when generating the next word, ignoring all others."}),"\n",(0,i.jsx)(n.p,{children:"A low k (e.g., 10) reduces diversity by restricting the choice of tokens, leading to more focused outputs. A high k increases diversity by allowing a broader range of possible tokens, leading to more varied outputs."}),"\n",(0,i.jsx)(n.p,{children:"It\u2019s useful when you want to prevent the model from choosing rare, less likely words, but still allow for some diversity."}),"\n",(0,i.jsx)(n.h3,{id:"reasoning-effort",children:"Reasoning Effort"}),"\n",(0,i.jsx)(n.p,{children:"The Reasoning Effort parameter controls how much internal reasoning the model performs before generating a response."}),"\n",(0,i.jsx)(n.p,{children:"You can set it to:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Low"})," \u2013 Prioritizes faster responses and minimal token usage."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Medium"})," \u2013 Strikes a balance between response time and depth of reasoning."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"High"})," \u2013 Emphasizes thorough reasoning, which may lead to slower but more detailed answers."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"You can adjust this setting based on your needs \u2014 whether you value speed, detail, or a balance of both."}),"\n",(0,i.jsx)(n.h3,{id:"number-of-beams",children:"Number of Beams"}),"\n",(0,i.jsx)(n.p,{children:"The Number of Beams inference parameter is integral to a method called beam search. Beam search is a search algorithm that keeps track of the top n (beam width) sequences at each step of generation, considering multiple possibilities before selecting the best one."}),"\n",(0,i.jsx)(n.p,{children:"It helps produce more coherent and optimized outputs by exploring multiple potential sequences. This parameter is particularly useful in tasks where the quality and diversity of the entire sequence is crucial, such as translation or summarization."}),"\n",(0,i.jsx)(n.h3,{id:"do-sample",children:"Do Sample"}),"\n",(0,i.jsx)(n.p,{children:"This parameter determines whether the model should sample from the probability distribution of the next token or select the token with the highest probability."}),"\n",(0,i.jsx)(n.p,{children:"If set to true, the model samples from the probability distribution, introducing randomness and allowing for more creative and diverse outputs. If set to false, the model selects the token with the highest probability, leading to more deterministic and predictable responses."}),"\n",(0,i.jsx)(n.p,{children:"Sampling is typically enabled (set to true) when you want the model to generate varied and creative text. When precision and consistency are more important, sampling may be disabled (set to false)."}),"\n",(0,i.jsx)(n.h3,{id:"return-full-text",children:"Return Full Text"}),"\n",(0,i.jsx)(n.p,{children:"This parameter determines whether the entire generated text should be returned or just a portion of it."}),"\n",(0,i.jsx)(n.p,{children:"If set to true, the model returns the full text, including both the prompt (if provided) and the generated continuation. If set to false, the model returns only the newly generated text, excluding the prompt."}),"\n",(0,i.jsx)(n.p,{children:"It\u2019s useful when you need the complete context, including the prompt, in the output. This can be important for understanding the generated response in the context of the input."}),"\n",(0,i.jsx)(n.h3,{id:"system-prompt",children:"System Prompt"}),"\n",(0,i.jsx)(n.p,{children:"A system prompt is a special input prompt provided to guide the model's behavior throughout the conversation or task. It sets the tone, style, or context for the model\u2019s responses."}),"\n",(0,i.jsx)(n.p,{children:"It influences how the model generates responses by setting expectations or providing instructions that the model follows."}),"\n",(0,i.jsx)(n.p,{children:"It\u2019s often used in conversational AI to define the role the model should play (e.g., a helpful assistant, a friendly chatbot) or in specialized tasks where specific behavior or output style is desired."}),"\n",(0,i.jsx)(n.p,{children:"It helps steer the model's responses in a consistent and contextually appropriate direction."}),"\n",(0,i.jsx)(n.h3,{id:"prompt-template",children:"Prompt Template"}),"\n",(0,i.jsx)(n.p,{children:"A prompt template serves as a pre-configured piece of text used to instruct an LLM. It acts as a structured query or input that guides the model in generating the desired response. You can use a template to tailor your prompts for different use cases."}),"\n",(0,i.jsxs)(n.p,{children:["Many LLMs require prompts to follow a specific template format. To streamline this process, we provide the ",(0,i.jsx)(n.code,{children:"prompt_template"})," inference parameter, which automatically applies the correct template format for the LLM. This means that you do not need to manually format your prompts when using an LLM through our UI or ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/sdk/Inference-from-AI-Models/Text-as-Input#text-generation-using-llm#set-inference-parameters",children:"SDK"}),"."]}),"\n",(0,i.jsxs)(n.p,{children:["By default, the ",(0,i.jsx)(n.code,{children:"prompt_template"})," is set to the LLM's standard template, allowing you to simply enter your prompts without worrying about formatting. The prompts will be automatically adjusted to fit the required template."]}),"\n",(0,i.jsxs)(n.p,{children:["If you need more flexibility, you can customize the ",(0,i.jsx)(n.code,{children:"prompt_template"})," parameter. When modifying this variable, make sure it includes the placeholder ",(0,i.jsx)(n.code,{children:"{prompt}"}),", which will be replaced with the user's prompt input."]}),"\n",(0,i.jsxs)(n.p,{children:["For example, the ",(0,i.jsx)(n.a,{href:"https://clarifai.com/openchat/openchat/models/openchat-3_6-8b-20240522",children:"Openchat-3.6-8b"})," model supports the following chat template format:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",children:"GPT4 Correct User: {prompt}<|end_of_turn|>GPT4 Correct Assistant:\n"})}),"\n",(0,i.jsx)(n.p,{children:"Let\u2019s break down the meaning of the template:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"GPT4 Correct User"}),":  \u2014 This delimiter indicates the start of a user's input."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"{prompt}"}),": \u2014 This substring will be replaced by the actual input or question from the user. It must be included in the prompt template. It works just like the ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/portal-guide/agent-system-operators/prompter#zero-shot-prompting",children:"prompter node"})," in a workflow builder, which must contain the ",(0,i.jsx)(n.code,{children:"{data.raw.text}"})," substring. When your text data is inputted at inference time, all occurrences of the ",(0,i.jsx)(n.code,{children:"{prompt}"})," variable within the template will be replaced with the prompt text."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"<|end_of_turn|>"}),":\u2014 This delimiter indicates the end of a user's input."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"GPT4 Correct Assistant:"})," \u2014 This indicates the start of the assistant's (or the language model's) response, which should be a corrected or refined version of the user's input or an appropriate answer to the user's question."]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["You can also add the ",(0,i.jsx)(n.code,{children:"<|start_of_turn|>"})," delimiter, which specifically indicates the start of a turn; in this case, a user\u2019s input."]}),"\n",(0,i.jsx)(n.p,{children:"Here is an example:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",children:"GPT4 Correct User: <|start_of_turn|> {prompt}<|end_of_turn|>GPT4 Correct Assistant:\n"})}),"\n",(0,i.jsxs)(n.p,{children:["Another example is the ",(0,i.jsx)(n.a,{href:"https://clarifai.com/meta/Llama-3/models/llama-3_1-8b-instruct",children:"Llama 3.1-8b-Instruct"})," model, which supports the following chat template format:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",children:"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\n{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n"})}),"\n",(0,i.jsx)(n.p,{children:"The main purpose of this format is to clearly delineate the roles and contributions of different participants in the conversation: system, user, and assistant."}),"\n",(0,i.jsx)(n.p,{children:"Let\u2019s break down its meaning:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"<|begin_of_text|>"})," \u2014 This delimiter marks the beginning of the text content."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"<|start_header_id|>system<|end_header_id|>"})," \u2014 This indicates the beginning of a system-level instruction or context."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"{system_prompt}"})," \u2014 This placeholder is for the actual system-level instruction or context."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"<|eot_id|>"})," \u2014 This indicates the end of a text unit; in this case, the system prompt."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"<|start_header_id|>user<|end_header_id|>"})," \u2014 This marks the beginning of a user's input."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"{prompt}"})," \u2014 As earlier described, this placeholder represents the actual prompt or query from the user."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"<|eot_id|>"})," \u2014 This marks the end of a text unit; in this case, the user's input."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"<|start_header_id|>assistant<|end_header_id|>"})," \u2014  This indicates the beginning of the assistant's response."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"predict-by-model-version-id",children:"Predict By Model Version ID"}),"\n",(0,i.jsxs)(n.p,{children:["Every time you train a custom model, it creates a new model version. By specifying ",(0,i.jsx)(n.code,{children:"version_id"})," in your predict call, you can continue to predict on a previous version, for consistent prediction results. Clarifai also updates its pre-built models on a regular basis."]}),"\n",(0,i.jsxs)(n.p,{children:["If you are looking for consistent results from your predict calls, use ",(0,i.jsx)(n.code,{children:"version_id"}),". If the model ",(0,i.jsx)(n.code,{children:"version_id"})," is not specified, predict will default to the most current model."]}),"\n",(0,i.jsxs)(n.p,{children:["Below is an example of how you would set a model version ID and receive predictions from Clarifai's ",(0,i.jsx)(n.a,{href:"https://clarifai.com/clarifai/main/models/general-image-recognition",children:(0,i.jsx)(n.code,{children:"general-image-recognition"})})," model."]}),"\n",(0,i.jsxs)(o.A,{groupId:"code",children:[(0,i.jsx)(r.A,{value:"python",label:"Python (gRPC)",children:(0,i.jsx)(c.A,{className:"language-python",children:h})}),(0,i.jsx)(r.A,{value:"js_rest",label:"JavaScript (REST)",children:(0,i.jsx)(c.A,{className:"language-javascript",children:_})}),(0,i.jsx)(r.A,{value:"nodejs",label:"Node.js (gRPC)",children:(0,i.jsx)(c.A,{className:"language-javascript",children:b})}),(0,i.jsx)(r.A,{value:"java",label:"Java (gRPC)",children:(0,i.jsx)(c.A,{className:"language-java",children:O})}),(0,i.jsx)(r.A,{value:"php",label:"PHP (gRPC)",children:(0,i.jsx)(c.A,{className:"language-php",children:j})}),(0,i.jsx)(r.A,{value:"curl",label:"cURL",children:(0,i.jsx)(c.A,{className:"language-bash",children:R})})]}),"\n",(0,i.jsxs)(t,{children:[(0,i.jsx)("summary",{children:"Output Example"}),(0,i.jsx)(c.A,{className:"language-text",children:U})]})]})}function G(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(Y,{...e})}):Y(e)}}}]);
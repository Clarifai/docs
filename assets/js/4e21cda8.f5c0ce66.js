"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[9463],{19811:(e,t,r)=>{r.r(t),r.d(t,{assets:()=>o,contentTitle:()=>n,default:()=>l,frontMatter:()=>d,metadata:()=>c,toc:()=>a});var s=r(74848),i=r(28453);const d={description:"Learn about our tracker operators",sidebar_position:8},n="Tracker",c={id:"portal-guide/agent-system-operators/tracker",title:"Tracker",description:"Learn about our tracker operators",source:"@site/docs/portal-guide/agent-system-operators/tracker.md",sourceDirName:"portal-guide/agent-system-operators",slug:"/portal-guide/agent-system-operators/tracker",permalink:"/portal-guide/agent-system-operators/tracker",draft:!1,unlisted:!1,editUrl:"https://github.com/Clarifai/docs/blob/main/docs/portal-guide/agent-system-operators/tracker.md",tags:[],version:"current",sidebarPosition:8,frontMatter:{description:"Learn about our tracker operators",sidebar_position:8},sidebar:"tutorialSidebar",previous:{title:"Algorithmic Predict",permalink:"/portal-guide/agent-system-operators/algorithmic-predict"},next:{title:"Embed",permalink:"/portal-guide/agent-system-operators/embed"}},o={},a=[{value:"BYTE Tracker",id:"byte-tracker",level:2},{value:"Centroid Tracker",id:"centroid-tracker",level:2},{value:"Neural Tracker",id:"neural-tracker",level:2},{value:"Kalman Filter Hungarian Tracker",id:"kalman-filter-hungarian-tracker",level:2},{value:"Kalman Reid Tracker",id:"kalman-reid-tracker",level:2},{value:"Neural Lite Tracker",id:"neural-lite-tracker",level:2},{value:"Tracker Operators Parameters",id:"tracker-operators-parameters",level:2}];function h(e){const t={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",img:"img",li:"li",ol:"ol",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.h1,{id:"tracker",children:"Tracker"}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.strong,{children:"Learn about our tracker operators"})}),"\n",(0,s.jsx)("hr",{}),"\n",(0,s.jsx)(t.p,{children:"Tracker operators are a specific type of agent system operators that are designed for object tracking in computer vision. Object tracking involves following the movement of objects in a sequence of images or frames in a video. Tracker models are trained using machine learning techniques to learn patterns and features that help them identify and track objects over time."}),"\n",(0,s.jsx)(t.p,{children:"The goal of object tracking is to maintain the identity of the object(s) over time, despite changes in position, scale, orientation, and lighting conditions."}),"\n",(0,s.jsx)(t.admonition,{type:"tip",children:(0,s.jsxs)(t.p,{children:['Since the tracker operators can be "chained" together with models to automate tasks in a workflow, you can learn how to create workflows ',(0,s.jsx)(t.a,{href:"https://docs.clarifai.com/portal-guide/workflows/input-nodes#create-your-workflow",children:"here"}),"."]})}),"\n",(0,s.jsx)(t.h2,{id:"byte-tracker",children:"BYTE Tracker"}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Input"}),": ",(0,s.jsx)(t.code,{children:"frames[\u2026].data.regions[\u2026].data.concepts"}),", ",(0,s.jsx)(t.code,{children:"frames[\u2026].data.regions[\u2026].region_info.bounding_box"})]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Output"}),": ",(0,s.jsx)(t.code,{children:"frames[\u2026].data.regions[\u2026].track_id"})]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.a,{href:"https://arxiv.org/abs/2110.06864",children:"BYTE Tracker"})," is a multi-object tracking by-detection model built upon the ",(0,s.jsx)(t.a,{href:"https://arxiv.org/abs/1602.00763",children:"Simple Online and Real-time Tracking"})," (SORT) principles. Multi-object tracking aims to predict the bounding boxes and identities of objects within video sequences."]}),"\n",(0,s.jsx)(t.p,{children:"Most tracking techniques retrieve identities by associating detection boxes whose scores are higher than a threshold. Unlike simpler trackers that ditch detections with low confidence scores, BYTE Tracker considers them, too, making it better at handling situations like temporary occlusions or lighting changes."}),"\n",(0,s.jsx)(t.p,{children:"Typically, it works in two stages:"}),"\n",(0,s.jsxs)(t.ol,{children:["\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"High Confidence Matches"}),": First, BYTE Tracker focuses on high-scoring detections (bounding boxes around objects). It uses a combination of motion similarity (how much the object moved between frames) and appearance similarity (features extracted from the object) to match these detections with existing tracks (tracklets). A motion prediction technique is then used to predict the position of these tracks in the next frame."]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Low Confidence Recovery"}),": Here's where BYTE Tracker differs. It revisits the low confidence detections (discarded by simpler trackers) and unmatched tracklets from the previous stage. Using the same motion similarity metric, BYTE Tracker tries to re-associate these with each other, potentially recovering tracks that were lost due to occlusions or low initial confidence."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(t.p,{children:["With this powerful operator, you can seamlessly integrate object tracking into your detect-track workflows and unlock advanced capabilities. Let's demonstrate how you can use the BYTE Tracker, alongside ",(0,s.jsx)(t.a,{href:"https://docs.clarifai.com/portal-guide/model/model-types/visual-detector",children:"a detection model"}),", to efficiently track objects in videos."]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"1."})," Go to the workflow builder page. Search for the ",(0,s.jsx)(t.strong,{children:"visual-detector"})," option in the left-hand sidebar and drag it onto the empty workspace. Then, use the pop-up that appears on the right-hand sidebar to search for a detection model, such as ",(0,s.jsx)(t.a,{href:"https://clarifai.com/clarifai/main/models/general-image-detection",children:"general-image-detection"}),", and select its version. You can also set the other configuration options \u2014 including selecting the concepts you want to filter."]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"2."})," Search for the ",(0,s.jsx)(t.strong,{children:"byte-tracker"})," option in the left-hand sidebar and drag it onto the workspace. You can set up its output configuration parameters, which are outlined below."]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"3."})," Connect the ",(0,s.jsx)(t.strong,{children:"visual-detector"})," model with the ",(0,s.jsx)(t.strong,{children:"byte-tracker"})," operator and save your workflow."]}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{src:r(2312).A+"",width:"1920",height:"838"})}),"\n",(0,s.jsxs)(t.p,{children:["To observe it in action, navigate to the workflow's individual page and click the ",(0,s.jsx)(t.strong,{children:"+"})," button to input your video. For this example, let's provide ",(0,s.jsx)(t.a,{href:"https://samples.clarifai.com/kep_CLIP_20180509-114253.mp4",children:"this video"}),"."]}),"\n",(0,s.jsx)(t.p,{children:"The workflow will analyze the video and identify objects consistently throughout its duration."}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{src:r(39171).A+"",width:"1616",height:"915"})}),"\n",(0,s.jsx)(t.h2,{id:"centroid-tracker",children:"Centroid Tracker"}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Input"}),": ",(0,s.jsx)(t.code,{children:"frames[\u2026].data.regions[\u2026].data.concepts"}),", ",(0,s.jsx)(t.code,{children:"frames[\u2026].data.regions[\u2026].region_info.bounding_box"})]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Output"}),": ",(0,s.jsx)(t.code,{children:"frames[\u2026].data.regions[\u2026].track_id"})]}),"\n",(0,s.jsx)(t.p,{children:"Centroid trackers rely on the Euclidean distance between centroids of regions in different video frames to assign the same track ID to detections of the same object."}),"\n",(0,s.jsx)(t.p,{children:"Here's a breakdown of how they operate:"}),"\n",(0,s.jsxs)(t.ol,{children:["\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Object Detection"}),": In the first step, an object detector or a segmentation model (not part of the centroid tracker itself) identifies objects in each frame of a video. The detector outputs bounding boxes around the identified objects."]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Centroid Calculation"}),":  For each bounding box, the centroid tracker calculates its centroid. The centroid is simply the center point of the box, typically represented by its X and Y coordinates."]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Distance Comparison"}),": The tracker then compares the centroids of objects detected in the current frame with the centroids of objects from the previous frame. It calculates the Euclidean distance, which is a straight-line distance between two points in space."]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Track Assignment"}),": Based on a predefined threshold value, the tracker assigns track IDs. Objects in the current frame whose centroids are within a certain distance of a centroid in the previous frame are considered to be the same object and are assigned the same track ID. Objects with centroids exceeding the threshold distance are assumed to be new objects and assigned new track IDs."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(t.p,{children:["Let's demonstrate how you can use the centroid tracker, alongside ",(0,s.jsx)(t.a,{href:"https://docs.clarifai.com/portal-guide/model/model-types/visual-detector",children:"a detection model"}),", to efficiently track objects in videos."]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"1."})," Go to the workflow builder page. Search for the ",(0,s.jsx)(t.strong,{children:"visual-detector"})," option in the left-hand sidebar and drag it onto the empty workspace. Then, use the pop-up that appears on the right-hand sidebar to search for a detection model, such as ",(0,s.jsx)(t.a,{href:"https://clarifai.com/clarifai/main/models/general-image-detection",children:"general-image-detection"}),", and select its version. You can also set the other configuration options \u2014 including selecting the concepts you want to filter."]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"2."})," Search for the ",(0,s.jsx)(t.strong,{children:"centroid-tracker"})," option in the left-hand sidebar and drag it onto the workspace. You can set up its output configuration parameters, which are outlined below."]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"3."})," Connect the ",(0,s.jsx)(t.strong,{children:"visual-detector"})," model with the ",(0,s.jsx)(t.strong,{children:"centroid-tracker"})," operator and save your workflow."]}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{src:r(24545).A+"",width:"1911",height:"836"})}),"\n",(0,s.jsxs)(t.p,{children:["To observe it in action, navigate to the workflow's individual page and click the ",(0,s.jsx)(t.strong,{children:"+"})," button to input your video. For this example, let's provide ",(0,s.jsx)(t.a,{href:"https://samples.clarifai.com/kep_CLIP_20180509-114253.mp4",children:"this video"}),"."]}),"\n",(0,s.jsx)(t.p,{children:"The workflow will analyze the video and identify objects consistently throughout its duration."}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{src:r(75562).A+"",width:"1890",height:"866"})}),"\n",(0,s.jsx)(t.h2,{id:"neural-tracker",children:"Neural Tracker"}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Output"}),": Regions"]}),"\n",(0,s.jsx)(t.p,{children:"Neural tracker uses neural probabilistic models to perform filtering and association."}),"\n",(0,s.jsx)(t.h2,{id:"kalman-filter-hungarian-tracker",children:"Kalman Filter Hungarian Tracker"}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Output"}),": Regions"]}),"\n",(0,s.jsx)(t.p,{children:"Kalman filter trackers rely on the Kalman filter algorithm to estimate the next position of an object based on its position and velocity in previous frames. Then detections are matched to predictions by using the Hungarian algorithm."}),"\n",(0,s.jsx)(t.h2,{id:"kalman-reid-tracker",children:"Kalman Reid Tracker"}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Output"}),": Regions"]}),"\n",(0,s.jsx)(t.p,{children:"Kalman reid tracker is a Kalman filter tracker that expects the embedding proto field to be populated for detections, and reassigns track IDs based off of the embedding distance."}),"\n",(0,s.jsx)(t.h2,{id:"neural-lite-tracker",children:"Neural Lite Tracker"}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Output"}),": Regions"]}),"\n",(0,s.jsx)(t.p,{children:"Neural lite tracker uses lightweight trainable graphical models to infer states of tracks and perform associations using the hybrid similarity of IoU and centroid distance."}),"\n",(0,s.jsx)(t.h2,{id:"tracker-operators-parameters",children:"Tracker Operators Parameters"}),"\n",(0,s.jsx)(t.p,{children:"Here is a table outlining the various output configuration parameters you can configure for each operator (the \u2713 symbol represents the operator that supports the parameter)."}),"\n",(0,s.jsxs)(t.table,{children:[(0,s.jsx)(t.thead,{children:(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.th,{children:"Parameter"}),(0,s.jsx)(t.th,{children:"Description"}),(0,s.jsx)(t.th,{children:"BYTE Tracker"}),(0,s.jsx)(t.th,{children:"Centroid Tracker"}),(0,s.jsx)(t.th,{children:"Neural Tracker"}),(0,s.jsx)(t.th,{children:"Kalman Filter Hungarian Tracker"}),(0,s.jsx)(t.th,{children:"Kalman Reid Tracker"}),(0,s.jsx)(t.th,{children:"Neural Lite Tracker"})]})}),(0,s.jsxs)(t.tbody,{children:[(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"min_confidence"})}),(0,s.jsx)(t.td,{children:"This is the minimum confidence score for detections to be considered for tracking"}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{children:"\u2713"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"min_visible_frames"})}),(0,s.jsx)(t.td,{children:"Only return tracks with minimum visible frames > min_visible_frames"}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{children:"\u2713"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"track_id_prefix"})}),(0,s.jsx)(t.td,{children:"Prefix to add on to track and eliminate conflicts"}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{children:"\u2713"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"max_disappeared"})}),(0,s.jsx)(t.td,{children:"This is the number of maximum consecutive frames a given object is allowed to be marked as \u201cdisappeared\u201d until we need to deregister the object from tracking"}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{children:"\u2713"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"new_track_confidence_thresh"})}),(0,s.jsx)(t.td,{children:"Initialize a new track if the confidence score of the new detection is greater than the setting"}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"confidence_thresh"})}),(0,s.jsx)(t.td,{children:"This is used to categorize high score detections for the first association if their scores are greater, and the second association if not"}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"high_confidence_match_thresh"})}),(0,s.jsx)(t.td,{children:"The distance threshold for high-score detection"}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"low_confidence_match_thresh"})}),(0,s.jsx)(t.td,{children:"The distance threshold for low-score detection"}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"unconfirmed_match_thresh"})}),(0,s.jsxs)(t.td,{children:["The distance threshold for unconfirmed tracks, usually tracks with only one beginning frame. ",(0,s.jsx)(t.code,{children:"{\u201cmin\u201d: 0, \u201cmax\u201d: 1}"})]}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"max_distance"})}),(0,s.jsx)(t.td,{children:"Associate tracks with detections only when their distance is below max_distance"}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{children:"\u2713"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"filtered_probability"})}),(0,s.jsx)(t.td,{children:"If false, return original detection probability; if true, return processed probability from the tracker"}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"max_detection"})}),(0,s.jsx)(t.td,{children:"Maximum detection per frame"}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"has_probability"})}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"has_embedding"})}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"association_confidence"})}),(0,s.jsx)(t.td,{children:"The list of association confidences to perform for each round"}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"covariance_error"})}),(0,s.jsx)(t.td,{children:"Magnitude of the uncertainty on the initial state"}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"observation_error"})}),(0,s.jsx)(t.td,{children:"Magnitude of the uncertainty on detection coordinates"}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"distance_metric"})}),(0,s.jsx)(t.td,{children:"Distance metric for Hungarian matching"}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"initialization_confidence"})}),(0,s.jsx)(t.td,{children:"Confidence for starting a new track. Must be > min_confidence to have an effect"}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"project_track"})}),(0,s.jsx)(t.td,{children:"How many frames in total to the project box when detection isn\u2019t recorded for track"}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"use_detect_box"})}),(0,s.jsx)(t.td,{children:"How many frames to project the last detection box, should be less than project_track_frames (1 is the current frame)"}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"project_without_detect"})}),(0,s.jsx)(t.td,{children:"Whether to keep projecting the box forward if no detect is matched"}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"project_fix_box_size"})}),(0,s.jsx)(t.td,{children:"Whether to fix the box size when the track is in a project state"}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"detect_box_fall_back"})}),(0,s.jsx)(t.td,{children:"Rely on the detect box if the association error is above this value"}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"keep_track_in_image"})}),(0,s.jsx)(t.td,{children:"If this is 1, then push the tracker predict to stay inside image boundaries"}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"match_limit_ratio"})}),(0,s.jsx)(t.td,{children:"Multiplier to constrain association (< 1 is ignored) based on other associations"}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"match_limit_min_matches"})}),(0,s.jsx)(t.td,{children:"Minimum number of matched tracks needed to invoke match limit"}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"optimal_assignment"})}),(0,s.jsx)(t.td,{children:"If True, rule out pairs with distance > max_distance before assignment"}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"max_emb_distance"})}),(0,s.jsx)(t.td,{children:"Maximum embedding distance to be considered a re-identification"}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"max_dead"})}),(0,s.jsx)(t.td,{children:"Maximum number of frames for track to be dead before we re-assign the ID"}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"var_tracker"})}),(0,s.jsx)(t.td,{children:"String that determines how embeddings from multiple timestamps are aggregated, defaults to \u201cna\u201d (most recent embedding overwrites past embeddings)"}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"reid_model_path"})}),(0,s.jsx)(t.td,{children:"The path to the linker"}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"iou_dist_ratio"})}),(0,s.jsx)(t.td,{children:"If 1.0 purely IoU similarity, if 0.0 purely centroid distance similarity"}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{children:"\u2713"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"mortal_th"})}),(0,s.jsx)(t.td,{children:"Mortality threshold"}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{children:"\u2713"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"min_box_area"})}),(0,s.jsx)(t.td,{children:"Minimum area of a valid box"}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{children:"\u2713"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"min_activity"})}),(0,s.jsx)(t.td,{children:"Returns only tracks with activities  above min_activity"}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{children:"\u2713"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"nms_iou_th"})}),(0,s.jsx)(t.td,{children:"NMS IoU threshold"}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{children:"\u2713"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"shrink_factor"})}),(0,s.jsxs)(t.td,{children:["Change box size by ",(0,s.jsx)(t.code,{children:"shrink_factor"})]}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{children:"\u2713"})]})]})]})]})}function l(e={}){const{wrapper:t}={...(0,i.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}},2312:(e,t,r)=>{r.d(t,{A:()=>s});const s=r.p+"assets/images/byte_tracker_1-8d0c656fb858f1bde8c776fa6b8a9d39.png"},39171:(e,t,r)=>{r.d(t,{A:()=>s});const s=r.p+"assets/images/byte_tracker_2-5c5e7944344f0dcdbfb9387ffc0bec46.png"},24545:(e,t,r)=>{r.d(t,{A:()=>s});const s=r.p+"assets/images/tracker-1-4b9eb361354a463a852fadcc3fca3d2e.png"},75562:(e,t,r)=>{r.d(t,{A:()=>s});const s=r.p+"assets/images/tracker-2-4264b84c1f76369edccfcf9b313dac8f.png"},28453:(e,t,r)=>{r.d(t,{R:()=>n,x:()=>c});var s=r(96540);const i={},d=s.createContext(i);function n(e){const t=s.useContext(d);return s.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function c(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:n(e.components),s.createElement(d.Provider,{value:t},e.children)}}}]);
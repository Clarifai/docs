"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[771],{3905:(e,t,n)=>{n.d(t,{Zo:()=>p,kt:()=>g});var r=n(67294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},i=Object.keys(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var l=r.createContext({}),m=function(e){var t=r.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},p=function(e){var t=m(e.components);return r.createElement(l.Provider,{value:t},e.children)},u="mdxType",c={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},d=r.forwardRef((function(e,t){var n=e.components,a=e.mdxType,i=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),u=m(n),d=a,g=u["".concat(l,".").concat(d)]||u[d]||c[d]||i;return n?r.createElement(g,o(o({ref:t},p),{},{components:n})):r.createElement(g,o({ref:t},p))}));function g(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var i=n.length,o=new Array(i);o[0]=d;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[u]="string"==typeof e?e:a,o[1]=s;for(var m=2;m<i;m++)o[m]=n[m];return r.createElement.apply(null,o)}return r.createElement.apply(null,n)}d.displayName="MDXCreateElement"},65141:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>c,frontMatter:()=>i,metadata:()=>s,toc:()=>m});var r=n(87462),a=(n(67294),n(3905));const i={description:"Learn about our visual segmenter templates",sidebar_position:4},o="Visual Segmenter Templates",s={unversionedId:"portal-guide/model/deep-training/visual-segmenter-templates",id:"portal-guide/model/deep-training/visual-segmenter-templates",title:"Visual Segmenter Templates",description:"Learn about our visual segmenter templates",source:"@site/docs/portal-guide/model/deep-training/visual-segmenter-templates.md",sourceDirName:"portal-guide/model/deep-training",slug:"/portal-guide/model/deep-training/visual-segmenter-templates",permalink:"/portal-guide/model/deep-training/visual-segmenter-templates",draft:!1,tags:[],version:"current",sidebarPosition:4,frontMatter:{description:"Learn about our visual segmenter templates",sidebar_position:4},sidebar:"tutorialSidebar",previous:{title:"Visual Embedding Templates",permalink:"/portal-guide/model/deep-training/visual-embedding-templates"},next:{title:"Model Training FAQs",permalink:"/portal-guide/model/training-faqs"}},l={},m=[{value:"MMSegmentation_SegFormer",id:"mmsegmentation_segformer",level:2}],p={toc:m},u="wrapper";function c(e){let{components:t,...n}=e;return(0,a.kt)(u,(0,r.Z)({},p,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"visual-segmenter-templates"},"Visual Segmenter Templates"),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"Learn about our visual segmenter templates")),(0,a.kt)("hr",null),(0,a.kt)("p",null,"Visual segmenter templates are predefined models or patterns used in computer vision tasks to perform image segmentation. Image segmentation involves dividing an image into different regions or segments based on their visual characteristics, such as color, texture, or object boundaries."),(0,a.kt)("h2",{id:"mmsegmentation_segformer"},"MMSegmentation_SegFormer"),(0,a.kt)("p",null,"This is a deep learning visual segmenter template that has configurations, datasets, and training and evaluation pipelines for various visual segmentation tasks, including semantic segmentation, instance segmentation, panoptic segmentation, and more. It is designed to be highly modular and flexible, allowing researchers and practitioners to easily experiment with different models and datasets."),(0,a.kt)("p",null,"Visual segmentation, also known as semantic segmentation, is a computer vision task that involves dividing an image into meaningful regions or segments and assigning each pixel within the image to a particular class or category."),(0,a.kt)("p",null,"MMSegmentation is a visual segmentation toolbox based on the OpenMMLab ecosystem. OpenMMLab is an open-source project that aims to provide a comprehensive set of modularized and state-of-the-art computer vision algorithms and tools. MMSegmentation is one of the sub-projects within OpenMMLab, specifically focused on visual segmentation tasks."),(0,a.kt)("p",null,"SegFormer is a deep learning model that has been proposed for visual segmentation. It is based on the Transformer architecture, which is originally designed for natural language processing tasks but has been successfully adapted to computer vision tasks as well. SegFormer utilizes the Transformer encoder-decoder architecture to perform pixel-wise segmentation on images. By leveraging self-attention mechanisms, the model can capture global contextual information while maintaining fine-grained spatial details."),(0,a.kt)("p",null,"With the MSegmentation_SegFormer template, you can carry out a wide range of visual segmentation tasks. It provides you with a rich set of resources, tools, and workflows to facilitate research and development in the field of computer vision."))}c.isMDXComponent=!0}}]);
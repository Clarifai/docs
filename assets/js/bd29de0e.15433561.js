"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[7638],{65270:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>r,contentTitle:()=>s,default:()=>u,frontMatter:()=>d,metadata:()=>o,toc:()=>l});var t=n(74848),a=n(28453);const d={description:"Learn about our visual embedder model type",sidebar_position:6,keywords:["visual embedder","image embedding","visual embedding models","AI image embedding","image feature extraction","machine learning image embedder","computer vision embedding","visual embedding AI","deep learning embedding models"]},s="Visual Embedder",o={id:"portal-guide/model/model-types/visual-embedder",title:"Visual Embedder",description:"Learn about our visual embedder model type",source:"@site/docs/portal-guide/model/model-types/visual-embedder.md",sourceDirName:"portal-guide/model/model-types",slug:"/portal-guide/model/model-types/visual-embedder",permalink:"/portal-guide/model/model-types/visual-embedder",draft:!1,unlisted:!1,editUrl:"https://github.com/Clarifai/docs/blob/main/docs/portal-guide/model/model-types/visual-embedder.md",tags:[],version:"current",sidebarPosition:6,frontMatter:{description:"Learn about our visual embedder model type",sidebar_position:6,keywords:["visual embedder","image embedding","visual embedding models","AI image embedding","image feature extraction","machine learning image embedder","computer vision embedding","visual embedding AI","deep learning embedding models"]},sidebar:"tutorialSidebar",previous:{title:"Visual Anomaly",permalink:"/portal-guide/model/model-types/visual-anomaly"},next:{title:"Clusterer",permalink:"/portal-guide/model/model-types/clusterer"}},r={},l=[];function m(e){const i={a:"a",admonition:"admonition",h1:"h1",li:"li",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(i.h1,{id:"visual-embedder",children:"Visual Embedder"}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.strong,{children:"Learn about our visual embedder model type"})}),"\n",(0,t.jsx)("hr",{}),"\n",(0,t.jsxs)(i.p,{children:[(0,t.jsx)(i.strong,{children:"Input"}),": Images and videos"]}),"\n",(0,t.jsxs)(i.p,{children:[(0,t.jsx)(i.strong,{children:"Output"}),": Embeddings"]}),"\n",(0,t.jsx)(i.p,{children:"Visual embedder, also known as visual embedding, is a type of deep fine-tuned model specifically designed to generate meaningful numerical representations (embeddings) from images and video frames."}),"\n",(0,t.jsx)(i.p,{children:"The primary goal of a visual embedder model is to transform the raw pixel values of images or video frames into a compact and high-dimensional vector. These vectors capture essential features and patterns in the visual content, enabling the model to understand and process the data in a more structured and interpretable way."}),"\n",(0,t.jsx)(i.p,{children:"These vectors can then be used for a variety of tasks, such as:"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"Visual search"}),": This is the task of finding images or videos that are similar to a given query image or video. The visual embedder model can be used to create a similarity metric between images or videos, which can then be used to search for similar visual content in a vector database."]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"Training on top of them"}),": The visual embedder model can also be used as a starting point for training other machine learning models. For example, a model that can classify images or videos can be trained on top of the visual embedder model."]}),"\n"]}),"\n",(0,t.jsx)(i.admonition,{type:"info",children:(0,t.jsxs)(i.p,{children:["The visual embedder model type also comes with various ",(0,t.jsx)(i.a,{href:"https://docs.clarifai.com/portal-guide/model/deep-training/visual-embedding-templates",children:"templates"})," that give you the control to choose the specific architecture used by your neural network, as well as define a set of hyperparameters you can use to fine-tune the way your model learns."]})}),"\n",(0,t.jsx)(i.p,{children:"You may choose a visual embedder model type in cases where:"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"You need a model that can accurately represent images and video frames as vectors. Once the model is trained, you can use it to embed new images or videos into vectors."}),"\n",(0,t.jsxs)(i.li,{children:['You need an embedding model to learn new features not recognized by the existing Clarifai models. In that case, you may need to "deep fine-tune" your custom model and integrate it directly within your ',(0,t.jsx)(i.a,{href:"https://docs.clarifai.com/portal-guide/workflows/",children:"workflows"}),"."]}),"\n",(0,t.jsx)(i.li,{children:"You have a custom-tailored dataset, accurate labels, and the expertise and time to fine-tune models."}),"\n"]})]})}function u(e={}){const{wrapper:i}={...(0,a.R)(),...e.components};return i?(0,t.jsx)(i,{...e,children:(0,t.jsx)(m,{...e})}):m(e)}},28453:(e,i,n)=>{n.d(i,{R:()=>s,x:()=>o});var t=n(96540);const a={},d=t.createContext(a);function s(e){const i=t.useContext(d);return t.useMemo((function(){return"function"==typeof e?e(i):{...i,...e}}),[i,e])}function o(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),t.createElement(d.Provider,{value:i},e.children)}}}]);
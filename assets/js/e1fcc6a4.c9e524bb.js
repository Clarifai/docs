"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[924],{28175:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/files/old_model_upload_method-9985c40b5c542a77c426504974559818.pdf"},65537:(e,n,t)=>{t.d(n,{A:()=>j});var o=t(96540),i=t(18215),r=t(65627),s=t(56347),a=t(50372),l=t(30604),d=t(11861),c=t(78749);function u(e){return o.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,o.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function p(e){const{values:n,children:t}=e;return(0,o.useMemo)((()=>{const e=n??function(e){return u(e).map((e=>{let{props:{value:n,label:t,attributes:o,default:i}}=e;return{value:n,label:t,attributes:o,default:i}}))}(t);return function(e){const n=(0,d.XI)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,t])}function m(e){let{value:n,tabValues:t}=e;return t.some((e=>e.value===n))}function h(e){let{queryString:n=!1,groupId:t}=e;const i=(0,s.W6)(),r=function(e){let{queryString:n=!1,groupId:t}=e;if("string"==typeof n)return n;if(!1===n)return null;if(!0===n&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:n,groupId:t});return[(0,l.aZ)(r),(0,o.useCallback)((e=>{if(!r)return;const n=new URLSearchParams(i.location.search);n.set(r,e),i.replace({...i.location,search:n.toString()})}),[r,i])]}function f(e){const{defaultValue:n,queryString:t=!1,groupId:i}=e,r=p(e),[s,l]=(0,o.useState)((()=>function(e){let{defaultValue:n,tabValues:t}=e;if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(n){if(!m({value:n,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${n}" but none of its children has the corresponding value. Available values are: ${t.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return n}const o=t.find((e=>e.default))??t[0];if(!o)throw new Error("Unexpected error: 0 tabValues");return o.value}({defaultValue:n,tabValues:r}))),[d,u]=h({queryString:t,groupId:i}),[f,_]=function(e){let{groupId:n}=e;const t=function(e){return e?`docusaurus.tab.${e}`:null}(n),[i,r]=(0,c.Dv)(t);return[i,(0,o.useCallback)((e=>{t&&r.set(e)}),[t,r])]}({groupId:i}),g=(()=>{const e=d??f;return m({value:e,tabValues:r})?e:null})();(0,a.A)((()=>{g&&l(g)}),[g]);return{selectedValue:s,selectValue:(0,o.useCallback)((e=>{if(!m({value:e,tabValues:r}))throw new Error(`Can't select invalid tab value=${e}`);l(e),u(e),_(e)}),[u,_,r]),tabValues:r}}var _=t(9136);const g={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var x=t(74848);function y(e){let{className:n,block:t,selectedValue:o,selectValue:s,tabValues:a}=e;const l=[],{blockElementScrollPositionUntilNextRender:d}=(0,r.a_)(),c=e=>{const n=e.currentTarget,t=l.indexOf(n),i=a[t].value;i!==o&&(d(n),s(i))},u=e=>{let n=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const t=l.indexOf(e.currentTarget)+1;n=l[t]??l[0];break}case"ArrowLeft":{const t=l.indexOf(e.currentTarget)-1;n=l[t]??l[l.length-1];break}}n?.focus()};return(0,x.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,i.A)("tabs",{"tabs--block":t},n),children:a.map((e=>{let{value:n,label:t,attributes:r}=e;return(0,x.jsx)("li",{role:"tab",tabIndex:o===n?0:-1,"aria-selected":o===n,ref:e=>{l.push(e)},onKeyDown:u,onClick:c,...r,className:(0,i.A)("tabs__item",g.tabItem,r?.className,{"tabs__item--active":o===n}),children:t??n},n)}))})}function b(e){let{lazy:n,children:t,selectedValue:r}=e;const s=(Array.isArray(t)?t:[t]).filter(Boolean);if(n){const e=s.find((e=>e.props.value===r));return e?(0,o.cloneElement)(e,{className:(0,i.A)("margin-top--md",e.props.className)}):null}return(0,x.jsx)("div",{className:"margin-top--md",children:s.map(((e,n)=>(0,o.cloneElement)(e,{key:n,hidden:e.props.value!==r})))})}function v(e){const n=f(e);return(0,x.jsxs)("div",{className:(0,i.A)("tabs-container",g.tabList),children:[(0,x.jsx)(y,{...n,...e}),(0,x.jsx)(b,{...n,...e})]})}function j(e){const n=(0,_.A)();return(0,x.jsx)(v,{...e,children:u(e.children)},String(n))}},79329:(e,n,t)=>{t.d(n,{A:()=>s});t(96540);var o=t(18215);const i={tabItem:"tabItem_Ymn6"};var r=t(74848);function s(e){let{children:n,hidden:t,className:s}=e;return(0,r.jsx)("div",{role:"tabpanel",className:(0,o.A)(i.tabItem,s),hidden:t,children:n})}},98324:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>O,contentTitle:()=>M,default:()=>N,frontMatter:()=>q,metadata:()=>o,toc:()=>T});const o=JSON.parse('{"id":"compute/models/model-upload/README","title":"Model Uploading","description":"Import custom models, including from external sources like Hugging Face and OpenAI","source":"@site/docs/compute/models/model-upload/README.mdx","sourceDirName":"compute/models/model-upload","slug":"/compute/models/model-upload/","permalink":"/compute/models/model-upload/","draft":false,"unlisted":false,"editUrl":"https://github.com/Clarifai/docs/blob/main/docs/compute/models/model-upload/README.mdx","tags":[],"version":"current","frontMatter":{"description":"Import custom models, including from external sources like Hugging Face and OpenAI","toc_min_heading_level":2,"toc_max_heading_level":5},"sidebar":"tutorialSidebar","previous":{"title":"Model Inference","permalink":"/compute/models/model-inference"},"next":{"title":"Test Models Locally","permalink":"/compute/models/model-upload/test-models-locally"}}');var i=t(74848),r=t(28453),s=t(65537),a=t(79329),l=t(58069);const d="from clarifai.runners.models.model_class import ModelClass\n\nclass YourCustomModel(ModelClass):\n    def load_model(self):\n        # Initialize and load the model here\n        pass\n\n    def predict(self, request):\n        # Handle input and return the model's predictions\n        return output_data\n\n    def generate(self, request):\n        # Handle streaming output (if applicable)\n        pass\n\n    def stream(self, request):\n        # Handle both streaming input and output\n        pass",c='model:\n  id: "model_id"\n  user_id: "user_id"\n  app_id: "app_id"\n  model_type_id: "text-to-text" # Change this based on your model type (e.g., image-classifier, text-to-text)',u='build_info:\n  python_version: "3.11"',p='inference_compute_info:\n  cpu_limit: "2"\n  cpu_memory: "13Gi"\n  num_accelerators: 1\n  accelerator_type: ["NVIDIA-A10G"] # Specify the GPU type if needed\n  accelerator_memory: "15Gi"',m='checkpoints:\n  type: "huggingface"\n  repo_id: "meta-llama/Meta-Llama-3-8B-Instruct"\n  when: "runtime"\n  hf_token: "your_hf_token" # Required for private models',h="concepts:\n  - id: '0'\n    name: bus\n  - id: '1'\n    name: person\n  - id: '2'\n    name: bicycle\n  - id: '3'\n    name: car",f='# Model to be uploaded: https://huggingface.co/Falconsai/nsfw_image_detection\n\nimport os\nimport tempfile\nfrom io import BytesIO\nfrom typing import Iterator\n\nimport cv2\nimport torch\nfrom clarifai.runners.models.model_class import ModelClass\nfrom clarifai.runners.models.model_builder import ModelBuilder\nfrom clarifai.utils.logging import logger\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2\nfrom clarifai_grpc.grpc.api.status import status_code_pb2, status_pb2\nfrom PIL import Image\nfrom transformers import AutoModelForImageClassification, ViTImageProcessor\n\n\ndef preprocess_image(image_bytes):\n  """Fetch and preprocess image data from bytes"""\n  return Image.open(BytesIO(image_bytes)).convert("RGB")\n\n\ndef video_to_frames(video_bytes):\n  """Convert video bytes to frames"""\n  # Write video bytes to a temporary file\n  with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as temp_video_file:\n    temp_video_file.write(video_bytes)\n    temp_video_path = temp_video_file.name\n    logger.info(f"temp_video_path: {temp_video_path}")\n\n    video = cv2.VideoCapture(temp_video_path)\n    print("video opened")\n    logger.info(f"video opened: {video.isOpened()}")\n    while video.isOpened():\n      ret, frame = video.read()\n      if not ret:\n        break\n      # Convert the frame to byte format\n      frame_bytes = cv2.imencode(\'.jpg\', frame)[1].tobytes()\n      yield frame_bytes\n    video.release()\n\n\ndef classify_image(images, model, processor, device):\n  """Classify an image using the model and processor."""\n  inputs = processor(images=images, return_tensors="pt")\n  inputs = {name: tensor.to(device) for name, tensor in inputs.items()}\n  logits = model(**inputs).logits\n  return logits\n\n\ndef process_concepts(logits, images, concept_protos):\n  """Process the logits and return the concepts."""\n  outputs = []\n  for i, logit in enumerate(logits):\n    output_concepts = []\n    probs = torch.softmax(logit, dim=-1)\n    sorted_indices = torch.argsort(probs, dim=-1, descending=True)\n    for idx in sorted_indices:\n      concept_protos[idx.item()].value = probs[idx].item()\n      output_concepts.append(concept_protos[idx.item()])\n    output = resources_pb2.Output()\n    output.data.image.base64 = images[i].tobytes()\n    output.data.concepts.extend(output_concepts)\n    output.status.code = status_code_pb2.SUCCESS\n    outputs.append(output)\n  return outputs\n\n\nclass MyModel(ModelClass):\n  """A custom runner that loads the model and classifies images using it.\n  """\n\n  def load_model(self):\n    """Load the model here."""\n\n    model_path = os.path.dirname(os.path.dirname(__file__))\n    builder = ModelBuilder(model_path, download_validation_only=True)\n    checkpoints = builder.download_checkpoints(stage="runtime")\n\n    self.device = \'cuda\' if torch.cuda.is_available() else \'cpu\'\n    logger.info(f"Running on device: {self.device}")\n\n    self.model = AutoModelForImageClassification.from_pretrained(checkpoints,).to(self.device)\n    self.processor = ViTImageProcessor.from_pretrained(checkpoints)\n    logger.info("Done loading!")\n\n  def predict(self, request: service_pb2.PostModelOutputsRequest\n             ) -> Iterator[service_pb2.MultiOutputResponse]:\n    """This is the method that will be called when the runner is run. It takes in an input and\n    returns an output.\n    """\n\n    outputs = []\n    images = []\n    concept_protos = request.model.model_version.output_info.data.concepts\n    for input in request.inputs:\n      input_data = input.data\n      image = preprocess_image(image_bytes=input_data.image.base64)\n      images.append(image)\n\n    with torch.no_grad():\n      logits = classify_image(images, self.model, self.processor, self.device)\n      outputs = process_concepts(logits, images, concept_protos)\n\n    return service_pb2.MultiOutputResponse(\n        outputs=outputs, status=status_pb2.Status(code=status_code_pb2.SUCCESS))\n\n  def generate(self, request: service_pb2.PostModelOutputsRequest\n              ) -> Iterator[service_pb2.MultiOutputResponse]:\n\n    if len(request.inputs) != 1:\n      raise ValueError("Only one input is allowed for image models for this method.")\n    concept_protos = request.model.model_version.output_info.data.concepts\n    for input in request.inputs:\n      input_data = input.data\n      video_bytes = None\n      if input_data.video.base64:\n        video_bytes = input_data.video.base64\n      if video_bytes:\n        frame_generator = video_to_frames(video_bytes)\n        for frame in frame_generator:\n          image = preprocess_image(frame)\n          images = [image]\n\n          with torch.no_grad():\n            logits = classify_image(images, self.model, self.processor, self.device)\n            outputs = process_concepts(logits, images, concept_protos)\n            yield service_pb2.MultiOutputResponse(\n                outputs=outputs, status=status_pb2.Status(code=status_code_pb2.SUCCESS))\n      else:\n        raise ValueError("Only video input is allowed for this method.")\n\n  def stream(self, request_iterator: Iterator[service_pb2.PostModelOutputsRequest]\n            ) -> Iterator[service_pb2.MultiOutputResponse]:\n    for request in request_iterator:\n      if request.inputs[0].data.video.base64:\n        for output in self.generate(request):\n          yield output\n      elif request.inputs[0].data.image.base64:\n        yield self.predict(request)\n        ',_="torch==2.5.1\ntokenizers>=0.21.0\ntransformers>=4.47.0\npillow==10.4.0\nrequests==2.32.3\ntimm==1.0.12\nopencv-python-headless==4.10.0.84\nnumpy\naiohttp\nclarifai",g='# This is the sample config file for the image-classifier model.\n\nmodel:\n  id: "model_id"\n  user_id: "user_id"\n  app_id: "app_id"\n  model_type_id: "visual-classifier"\n\nbuild_info:\n  python_version: "3.11"\n\ninference_compute_info:\n  cpu_limit: "2"\n  cpu_memory: "2Gi"\n  num_accelerators: 1\n  accelerator_type: ["NVIDIA*"]\n  accelerator_memory: "3Gi"\n\ncheckpoints:\n  type: "huggingface"\n  repo_id: "Falconsai/nsfw_image_detection"\n  hf_token: "hf_token"',x='# Model to be uploaded: https://huggingface.co/facebook/detr-resnet-50\n\nimport os\nimport tempfile\nfrom io import BytesIO\nfrom typing import Iterator\nimport time\n\nimport cv2\nimport torch\nfrom clarifai.runners.models.model_class import ModelClass\nfrom clarifai.runners.models.model_builder import ModelBuilder\nfrom clarifai.utils.logging import logger\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2\nfrom clarifai_grpc.grpc.api.status import status_code_pb2, status_pb2\nfrom PIL import Image\nfrom transformers import DetrForObjectDetection, DetrImageProcessor\n\nimport os\nimport tempfile\nfrom io import BytesIO\nfrom typing import Iterator\n\nimport cv2\nimport torch\nfrom clarifai.utils.logging import logger\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2\nfrom clarifai_grpc.grpc.api.status import status_code_pb2, status_pb2\nfrom PIL import Image\nfrom transformers import DetrForObjectDetection, DetrImageProcessor\n\n\ndef preprocess_image(image_bytes):\n  """Fetch and preprocess image data from bytes"""\n  return Image.open(BytesIO(image_bytes)).convert("RGB")\n\n\ndef video_to_frames(video_bytes):\n  """Convert video bytes to frames"""\n  # Write video bytes to a temporary file\n  with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as temp_video_file:\n    temp_video_file.write(video_bytes)\n    temp_video_path = temp_video_file.name\n    logger.info(f"temp_video_path: {temp_video_path}")\n\n    video = cv2.VideoCapture(temp_video_path)\n    print("video opened")\n    logger.info(f"video opened: {video.isOpened()}")\n    while video.isOpened():\n      ret, frame = video.read()\n      if not ret:\n        break\n      # Convert the frame to byte format\n      frame_bytes = cv2.imencode(\'.jpg\', frame)[1].tobytes()\n      yield frame_bytes\n    video.release()\n\n\ndef detect_objects(images, model, processor, device):\n  model_inputs = processor(images=images, return_tensors="pt").to(device)\n  model_inputs = {name: tensor.to(device) for name, tensor in model_inputs.items()}\n  model_output = model(**model_inputs)\n  results = processor.post_process_object_detection(model_output)\n  return results\n\n\ndef process_bounding_boxes(results, images, concept_protos, threshold, model_labels):\n  outputs = []\n  for i, result in enumerate(results):\n    image = images[i]\n    width, height = image.size\n    output_regions = []\n    for score, label_idx, box in zip(result["scores"], result["labels"], result["boxes"]):\n      if score > threshold:\n        xmin, ymin, xmax, ymax = box\n        xmin, ymin, xmax, ymax = xmin, ymin, xmax, ymax\n        output_region = resources_pb2.Region(region_info=resources_pb2.RegionInfo(\n            bounding_box=resources_pb2.BoundingBox(\n                top_row=ymin,\n                left_col=xmin,\n                bottom_row=ymax,\n                right_col=xmax,\n            ),))\n        label = model_labels[label_idx.item()]\n        concept_protos[label_idx.item()].value = score.item()\n        concept_protos[label_idx.item()].name = label\n        output_region.data.concepts.append(concept_protos[label_idx.item()])\n        output_regions.append(output_region)\n    output = resources_pb2.Output()\n    output.data.regions.extend(output_regions)\n    output.status.code = status_code_pb2.SUCCESS\n    outputs.append(output)\n  return outputs\n\n\nclass MyModel(ModelClass):\n  """A custom runner that adds "Hello World" to the end of the text and replaces the domain of the\n  image URL as an example.\n  """\n\n  def load_model(self):\n    """Load the model here."""\n    model_path = os.path.dirname(os.path.dirname(__file__))\n    builder = ModelBuilder(model_path, download_validation_only=True)\n    checkpoint_path = builder.download_checkpoints(stage="runtime")\n    \n    self.device = \'cuda\' if torch.cuda.is_available() else \'cpu\'\n    logger.info(f"Running on device: {self.device}")\n\n    self.model = DetrForObjectDetection.from_pretrained(checkpoint_path,).to(self.device)\n    self.processor = DetrImageProcessor.from_pretrained(checkpoint_path,)\n    self.model.eval()\n    self.threshold = 0.9\n    self.model_labels = self.model.config.id2label\n    self.concept_protos = None\n\n    logger.info("Done loading!")\n\n  def predict(self, request: service_pb2.PostModelOutputsRequest\n             ) -> Iterator[service_pb2.MultiOutputResponse]:\n    """This is the method that will be called when the runner is run. It takes in an input and\n    returns an output.\n    """\n    outputs = []\n    images = []\n    if not self.concept_protos:\n      self.concept_protos = request.model.model_version.output_info.data.concepts\n    for input in request.inputs:\n      input_data = input.data\n\n      image_bytes = input_data.image.base64\n      image = preprocess_image(image_bytes=image_bytes)\n      images.append(image)\n\n    with torch.no_grad():\n      results = detect_objects(images, self.model, self.processor, self.device)\n\n      # convert outputs (bounding boxes and class logits) to COCO API\n      # let\'s only keep detections with score > 0.7 (You can set it to any other value)\n      outputs = process_bounding_boxes(results, images, self.concept_protos, self.threshold,\n                                       self.model_labels)\n      for oi, out in enumerate(outputs):\n        out.input.id = request.inputs[oi].id\n      return service_pb2.MultiOutputResponse(\n          outputs=outputs, status=status_pb2.Status(code=status_code_pb2.SUCCESS))\n\n  def generate(self, request: service_pb2.PostModelOutputsRequest\n              ) -> Iterator[service_pb2.MultiOutputResponse]:\n    if len(request.inputs) != 1:\n      raise ValueError("Only one input is allowed for image models for this method.")\n    if not self.concept_protos:\n      self.concept_protos = request.model.model_version.output_info.data.concepts\n    for input in request.inputs:\n      input_data = input.data\n      video_bytes = None\n      if input_data.video.base64:\n        video_bytes = input_data.video.base64\n      if video_bytes:\n        frame_generator = video_to_frames(video_bytes)\n        for frame in frame_generator:\n          image = preprocess_image(frame)\n          images = [image]\n          with torch.no_grad():\n            results = detect_objects(images, self.model, self.processor, self.device)\n            outputs = process_bounding_boxes(results, images, self.concept_protos, self.threshold,\n                                             self.model_labels)\n            for out in outputs:\n              out.input.id = input_id\n            yield service_pb2.MultiOutputResponse(\n                outputs=outputs, status=status_pb2.Status(code=status_code_pb2.SUCCESS))\n      else:\n        raise ValueError("Only video input is allowed for this method.")\n\n  def stream(self, request_iterator: Iterator[service_pb2.PostModelOutputsRequest]\n            ) -> Iterator[service_pb2.MultiOutputResponse]:\n    last_t = time.time()\n    for request in request_iterator:\n      if request.inputs[0].data.video.base64:\n        for output in self.generate(request):\n          yield output\n      elif request.inputs[0].data.image.base64:\n        yield self.predict(request)\n        duration = time.time() - last_t\n        logger.info(f"Time taken for one frame: {duration}")\n      last_t = time.time()',y="torch==2.5.1\ntokenizers>=0.21.0\ntransformers>=4.47.0\npillow==10.4.0\nrequests==2.32.3\ntimm==1.0.12\nopencv-python-headless==4.10.0.84\nnumpy\naiohttp\nclarifai",b='# This is the sample config file for the image-detection model.\n\nmodel:\n  id: "model_id"\n  user_id: "user_id"\n  app_id: "app_id"\n  model_type_id: "visual-detector"\n\nbuild_info:\n  python_version: "3.11"\n\ninference_compute_info:\n  cpu_limit: "4"\n  cpu_memory: "2Gi"\n  num_accelerators: 1\n  accelerator_type: ["NVIDIA*"]\n  accelerator_memory: "5Gi"\n\n\ncheckpoints:\n  type: "huggingface"\n  repo_id: "facebook/detr-resnet-50"\n  hf_token: "hf_token"',v='# Model to be uploaded: https://huggingface.co/casperhansen/llama-3-8b-instruct-awq\n\nimport os\nfrom threading import Thread\nfrom typing import Iterator, List, Optional\n\nimport torch\nfrom clarifai.runners.models.model_class import ModelClass\nfrom clarifai.runners.models.model_builder import ModelBuilder\nfrom clarifai.utils.logging import logger\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2\nfrom clarifai_grpc.grpc.api.status import status_code_pb2, status_pb2\nfrom google.protobuf import json_format\nfrom transformers import (AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer)\n\n\n# Custom streamer for batched text generation\nclass BatchTextIteratorStreamer(TextIteratorStreamer):\n  """A custom streamer that handles batched text generation."""\n\n  def __init__(self,\n               batch_size: int,\n               tokenizer: "AutoTokenizer",\n               skip_prompt: bool = False,\n               timeout: Optional[float] = None,\n               **decode_kwargs):\n    super().__init__(tokenizer, skip_prompt, timeout, **decode_kwargs)\n    self.batch_size = batch_size\n    self.token_cache = [[] for _ in range(batch_size)]\n    self.print_len = [0 for _ in range(batch_size)]\n    self.generate_exception = None\n\n  def put(self, value):\n    if len(value.shape) != 2:\n      value = torch.reshape(value, (self.batch_size, value.shape[0] // self.batch_size))\n\n    if self.skip_prompt and self.next_tokens_are_prompt:\n      self.next_tokens_are_prompt = False\n      return\n\n    printable_texts = list()\n    for idx in range(self.batch_size):\n      self.token_cache[idx].extend(value[idx].tolist())\n      text = self.tokenizer.decode(self.token_cache[idx], **self.decode_kwargs)\n\n      if text.endswith("\\n"):\n        printable_text = text[self.print_len[idx]:]\n        self.token_cache[idx] = []\n        self.print_len[idx] = 0\n        # If the last token is a CJK character, we print the characters.\n      elif len(text) > 0 and self._is_chinese_char(ord(text[-1])):\n        printable_text = text[self.print_len[idx]:]\n        self.print_len[idx] += len(printable_text)\n      else:\n        printable_text = text[self.print_len[idx]:text.rfind(" ") + 1]\n        self.print_len[idx] += len(printable_text)\n      printable_texts.append(printable_text)\n\n    self.on_finalized_text(printable_texts)\n\n  def end(self):\n    printable_texts = list()\n    for idx in range(self.batch_size):\n      if len(self.token_cache[idx]) > 0:\n        text = self.tokenizer.decode(self.token_cache[idx], **self.decode_kwargs)\n        printable_text = text[self.print_len[idx]:]\n        self.token_cache[idx] = []\n        self.print_len[idx] = 0\n      else:\n        printable_text = ""\n      printable_texts.append(printable_text)\n\n    self.next_tokens_are_prompt = True\n    self.on_finalized_text(printable_texts, stream_end=True)\n\n  def on_finalized_text(self, texts: List[str], stream_end: bool = False):\n    self.text_queue.put(texts, timeout=self.timeout)\n    if stream_end:\n      self.text_queue.put(self.stop_signal, timeout=self.timeout)\n\n\n# Helper function to create an output\ndef create_output(text="", code=status_code_pb2.SUCCESS):\n  return resources_pb2.Output(\n      data=resources_pb2.Data(text=resources_pb2.Text(raw=text)),\n      status=status_pb2.Status(code=code))\n\n\n# Helper function to get the inference params\ndef get_inference_params(request) -> dict:\n  """Get the inference params from the request."""\n  inference_params = {}\n  if request.model.model_version.id != "":\n    output_info = request.model.model_version.output_info\n    output_info = json_format.MessageToDict(output_info, preserving_proto_field_name=True)\n    if "params" in output_info:\n      inference_params = output_info["params"]\n  return inference_params\n\n\n# Helper function to parse the inference params\ndef parse_inference_params(request):\n  default_params = {\n      "temperature": 0.7,\n      "max_tokens": 100,\n      "top_k": 50,\n      "top_p": 1.0,\n      "do_sample": True,\n  }\n  inference_params = get_inference_params(request)\n  return {\n      "temperature": inference_params.get("temperature", default_params["temperature"]),\n      "max_tokens": int(inference_params.get("max_tokens", default_params["max_tokens"])),\n      "top_k": int(inference_params.get("top_k", default_params["top_k"])),\n      "top_p": inference_params.get("top_p", default_params["top_p"]),\n      "do_sample": inference_params.get("do_sample", default_params["do_sample"]),\n  }\n\n\nclass MyModel(ModelClass):\n  """A custom runner that loads the model and generates text using batched inference."""\n\n  def load_model(self):\n    """Load the model here."""\n    self.device = \'cuda\' if torch.cuda.is_available() else \'cpu\'\n    logger.info(f"Running on device: {self.device}")\n\n    # Load checkpoints\n    model_path = os.path.dirname(os.path.dirname(__file__))\n    builder = ModelBuilder(model_path, download_validation_only=True)\n    checkpoints = builder.download_checkpoints(stage="runtime")\n    \n    # Load model and tokenizer\n    self.tokenizer = AutoTokenizer.from_pretrained(checkpoints,)\n    self.tokenizer.pad_token = self.tokenizer.eos_token\n    self.model = AutoModelForCausalLM.from_pretrained(\n        checkpoints,\n        low_cpu_mem_usage=True,\n        device_map=self.device,\n        torch_dtype=torch.float16,\n    )\n    logger.info("Done loading!")\n\n  def predict(self,\n              request: service_pb2.PostModelOutputsRequest) -> service_pb2.MultiOutputResponse:\n    """This method generates outputs text for the given inputs using the model."""\n\n    inference_params = parse_inference_params(request)\n\n    prompts = [inp.data.text.raw for inp in request.inputs]\n    inputs = self.tokenizer(prompts, return_tensors="pt", padding=True).to(self.device)\n\n    output_tokens = self.model.generate(\n        **inputs,\n        max_new_tokens=inference_params["max_tokens"],\n        do_sample=inference_params["do_sample"],\n        temperature=inference_params["temperature"],\n        top_k=inference_params["top_k"],\n        top_p=inference_params["top_p"],\n        eos_token_id=self.tokenizer.eos_token_id,\n    )\n\n    outputs_text = self.tokenizer.batch_decode(\n        output_tokens[:, inputs[\'input_ids\'].shape[1]:], skip_special_tokens=True)\n\n    outputs = []\n    for text in outputs_text:\n      outputs.append(create_output(text=text, code=status_code_pb2.SUCCESS))\n\n    return service_pb2.MultiOutputResponse(\n        outputs=outputs, status=status_pb2.Status(code=status_code_pb2.SUCCESS))\n\n  def generate(self, request: service_pb2.PostModelOutputsRequest\n              ) -> Iterator[service_pb2.MultiOutputResponse]:\n    """This method generates stream of outputs for the given batch of inputs using the model."""\n    inference_params = parse_inference_params(request)\n\n    prompts = [inp.data.text.raw for inp in request.inputs]\n    batch_size = len(prompts)\n\n    # Initialize the custom streamer\n    streamer = BatchTextIteratorStreamer(\n        batch_size=batch_size,\n        tokenizer=self.tokenizer,\n        skip_prompt=True,\n        decode_kwargs={\n            "skip_special_tokens": True\n        })\n\n    # Tokenize the inputs\n    inputs = self.tokenizer(prompts, return_tensors="pt", padding=True).to(self.device)\n\n    generation_kwargs = {\n        "input_ids": inputs.input_ids,\n        "attention_mask": inputs.attention_mask,\n        "max_new_tokens": inference_params["max_tokens"],\n        "do_sample": inference_params["do_sample"],\n        "temperature": inference_params["temperature"],\n        "top_k": inference_params["top_k"],\n        "top_p": inference_params["top_p"],\n        "eos_token_id": self.tokenizer.eos_token_id,\n        "streamer": streamer,\n    }\n\n    # Start generation in a separate thread\n    thread = Thread(target=self.model.generate, kwargs=generation_kwargs)\n    thread.start()\n\n    # Initialize outputs\n    outputs = [create_output() for _ in range(batch_size)]\n\n    try:\n      for streamed_texts in streamer:  # Iterate over new texts generated\n        for idx, text in enumerate(streamed_texts):  # Iterate over each batch\n          outputs[idx].data.text.raw = text  # Append new text to each output\n          outputs[idx].status.code = status_code_pb2.SUCCESS\n        # Yield the current outputs\n        yield service_pb2.MultiOutputResponse(\n            outputs=outputs, status=status_pb2.Status(code=status_code_pb2.SUCCESS))\n    finally:\n      thread.join()\n\n  def stream(self, request_iterator: Iterator[service_pb2.PostModelOutputsRequest]\n            ) -> Iterator[service_pb2.MultiOutputResponse]:\n    raise NotImplementedError("Stream method is not implemented for the models.")\n  ',j="torch==2.5.1\ntokenizers>=0.21.0\ntransformers>=4.47.0\naccelerate>=1.2.0\nscipy==1.10.1\noptimum>=1.23.3\nxformers==0.0.28.post3\nprotobuf==5.27.3\neinops>=0.8.0\nrequests==2.32.3\nautoawq==0.2.7.post3\nclarifai",k='# This is the sample config file for the llama model.\n\nmodel:\n  id: "llama-3-8b-instruct"\n  user_id: "user_id"\n  app_id: "app_id"\n  model_type_id: "text-to-text"\n\nbuild_info:\n  python_version: "3.11"\n\ninference_compute_info:\n  cpu_limit: "1"\n  cpu_memory: "8Gi"\n  num_accelerators: 1\n  accelerator_type: ["NVIDIA*"]\n  accelerator_memory: "12Gi"\n\ncheckpoints:\n  type: "huggingface"\n  repo_id: "casperhansen/llama-3-8b-instruct-awq"\n  hf_token: "hf_token"',w='# Model to be uploaded: https://platform.openai.com/docs/guides/speech-to-text/quickstart\n\nimport io\nimport itertools\nimport wave\nfrom typing import Iterator\n\nfrom clarifai.runners.models.model_class import ModelClass\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\nfrom google.protobuf import json_format\nfrom openai import OpenAI\n\nOPENAI_API_KEY = "OPENAI_API_KEY"\n\n\ndef bytes_to_audio_file(audio_bytes):\n  """Convert bytes data into a file-like object."""\n  if not audio_bytes:\n    raise ValueError("Audio bytes cannot be empty.")\n  audio_file = io.BytesIO(audio_bytes)\n  audio_file.name = "audio.wav"  # This name is used for the API\n  return audio_file\n\n\ndef preprocess_audio(audio_bytes=None, chunk_duration=1.0, stream=False):\n  """\n  Fetch and preprocess audio data from a URL or bytes.\n\n  Parameters:\n    bytes (bytes): Audio data in bytes (if provided).\n    chunk_duration (float): Duration of each audio chunk in seconds.\n    stream (bool): Whether to stream the audio in chunks.\n\n  Returns:\n    Audio data in bytes or a generator of audio chunks.\n  """\n\n  if audio_bytes:\n    if stream:\n      # Read the original audio bytes\n      audio_bytes = io.BytesIO(audio_bytes)\n      with wave.open(audio_bytes, "rb") as wave_file:\n        params = wave_file.getparams()\n        sample_rate = params.framerate\n        channels = params.nchannels\n        sample_width = params.sampwidth\n\n        # Calculate number of frames per chunk\n        frames_per_chunk = int(sample_rate * chunk_duration)\n\n        # Stream the audio in chunks (generator)\n        def audio_stream_generator():\n          while True:\n            frames = wave_file.readframes(frames_per_chunk)\n            if not frames:\n              break\n            chunk_buffer = io.BytesIO()\n            with wave.open(chunk_buffer, "wb") as chunk_wav:\n              chunk_wav.setnchannels(channels)\n              chunk_wav.setsampwidth(sample_width)\n              chunk_wav.setframerate(sample_rate)\n              chunk_wav.writeframes(frames)\n            yield chunk_buffer.getvalue()\n\n        return audio_stream_generator()\n    else:\n      # Return a single chunk of audio\n      return audio_bytes\n  else:\n    raise ValueError("\'audio_bytes\' must be provided")\n\n\ndef get_inference_params(request) -> dict:\n  """Get the inference params from the request."""\n  inference_params = {}\n  if request.model.model_version.id != "":\n    output_info = request.model.model_version.output_info\n    output_info = json_format.MessageToDict(output_info, preserving_proto_field_name=True)\n\n    if "params" in output_info:\n      inference_params = output_info["params"]\n  return inference_params\n\n\nclass MyModel(ModelClass):\n  """A custom runner that used for transcribing audio."""\n\n  def load_model(self):\n    """Load the model here."""\n    self.client = OpenAI(api_key=OPENAI_API_KEY)\n    self.model = "whisper-1"\n\n  def predict(self,\n              request: service_pb2.PostModelOutputsRequest) -> service_pb2.MultiOutputResponse:\n    """Predict the output for the given audio data."""\n    inference_params = get_inference_params(request)\n    language = inference_params.get("language", None)\n    task = inference_params.get("task", "transcription")\n    outputs = []\n    # TODO: parallelize this over inputs in a single request.\n    for input in request.inputs:\n      output = resources_pb2.Output()\n\n      input_data = input.data\n      audio_bytes = preprocess_audio(audio_bytes=input_data.audio.base64, stream=False)\n\n      if task == "transcription":\n        # Send audio bytes to Whisper for transcription\n        whisper_output = self.client.audio.transcriptions.create(\n            model=self.model, language=language, file=bytes_to_audio_file(audio_bytes))\n      elif task == "translation":\n        # Send audio bytes to Whisper for translation\n        whisper_output = self.client.audio.translations.create(\n            model=self.model, file=bytes_to_audio_file(audio_bytes))\n\n      # Set the output data\n      output.data.text.raw = whisper_output.text\n      output.status.code = status_code_pb2.SUCCESS\n      outputs.append(output)\n    return service_pb2.MultiOutputResponse(outputs=outputs,)\n\n  def generate(self, request: service_pb2.PostModelOutputsRequest\n              ) -> Iterator[service_pb2.MultiOutputResponse]:\n    """Generate the output in a streaming fashion for large audio files."""\n    inference_params = get_inference_params(request)\n    language = inference_params.get("language", None)\n    task = inference_params.get("task", "transcription")\n    batch_audio_streams = []\n    for input in request.inputs:\n      output = resources_pb2.Output()\n\n      input_data = input.data\n\n      audio_bytes = input_data.audio.base64\n      chunk_duration = 3.0\n\n      audio_stream = preprocess_audio(\n          audio_bytes=audio_bytes, stream=True, chunk_duration=chunk_duration)\n      batch_audio_streams.append(audio_stream)\n\n    for audio_stream in itertools.zip_longest(*batch_audio_streams, fillvalue=None):\n      resp = service_pb2.MultiOutputResponse()\n\n      for audio_bytes in audio_stream:\n        output = resp.outputs.add()\n        if task == "transcription":\n          # Send audio bytes to Whisper for transcription\n          whisper_output = self.client.audio.transcriptions.create(\n              model=self.model, language=language, file=bytes_to_audio_file(audio_bytes))\n        elif task == "translation":\n          # Send audio bytes to Whisper for translation\n          whisper_output = self.client.audio.translations.create(\n              model=self.model, file=bytes_to_audio_file(audio_bytes))\n        output.data.text.raw = whisper_output.text\n        output.status.code = status_code_pb2.SUCCESS\n      yield resp\n\n  def stream(self, request_iterator: Iterator[service_pb2.PostModelOutputsRequest]\n            ) -> Iterator[service_pb2.MultiOutputResponse]:\n    """Stream the output in a streaming fashion"""\n    for request in request_iterator:\n      for response in self.generate(request):\n        yield response',I="openai==1.55.3\nrequests\nclarifai",A='# This is the sample config file for the Openai Whisper model.\n\nmodel:\n  id: "model_id"\n  user_id: "user_id"\n  app_id: "app_id"\n  model_type_id: "audio-to-text"\n\nbuild_info:\n  python_version: "3.12"\n\ninference_compute_info:\n  cpu_limit: "1"\n  cpu_memory: "500m"\n  num_accelerators: 0',C='2025-04-03 13:02:32.242005 INFO     Validating folder: C:\\Users\\Alfrick\\Desktop\\upload-model\\       model_builder.py:122\n2025-04-03 13:02:40.138937 INFO     Skipping downloading checkpoints for stage upload since         model_builder.py:497\n                                    config.yaml says to download them at stage runtime\n2025-04-03 13:02:40.143926 INFO     Using Python version 3.11 from the config file to build the     model_builder.py:389\n                                    Dockerfile\n2025-04-03 13:02:40.154304 INFO     Using Torch version 2.5.1 base image to build the Docker image  model_builder.py:415\n2025-04-03 13:02:40.548090 INFO     New model will be created at                                    model_builder.py:779\n                                    https://clarifai.com/alfrick/my-models/models/Llama-3-8b-Instru\n                                    ct with it\'s first version.\nPress Enter to continue...\n2025-04-03 13:02:49.958171 INFO     Uploading file...                                               model_builder.py:677\n2025-04-03 13:02:49.965368 INFO     Upload complete!                                                model_builder.py:700\n   Status: Upload in progress, Progress: 0% - Starting upload.  request_id:\n   Status: Upload done, Progress: 0% - Completed upload of files, initiating model version image build..  request_id:\n   Status: Model image is currently being built., Progress: 0% - Model version image is being built.  request_id:\n2025-04-03 13:02:50.640853 INFO     Created Model Version ID: af882c92324149ae8eeedc1e1f48f28a      model_builder.py:662\n2025-04-03 13:02:50.646760 INFO     Full url to that version is:                                    model_builder.py:663\n                                    https://clarifai.com/alfrick/my-models/models/Llama-3-8b-Instru\n                                    ct\n2025-04-03 13:02:58.050743 INFO     2025-04-03 10:02:49.968987 INFO: Downloading uploaded model     model_builder.py:744\n                                    from storage...\n\n                                    2025-04-03 10:02:50.734386 INFO: Done downloading model\n\n                                    2025-04-03 10:02:50.737053 INFO: Extracting upload...\n\n                                    2025-04-03 10:02:50.740340 INFO: Done extracting upload\n\n                                    2025-04-03 10:02:50.742230 INFO: Parsing requirements file for\n                                    model version ID ****dc1e1f48f28a\n\n                                    2025-04-03 10:02:50.765409 INFO: Dockerfile found at\n                                    /shared/context/Dockerfile\n\n                                    2025-04-03 10:02:51.827142 INFO: Setting up credentials\n\n                                    amazon-ecr-credential-helper\n\n                                    Version:    0.8.0\n\n                                    Git commit: ********\n\n                                    2025-04-03 10:02:51.830820 INFO: Building image...\n\n                                    #1 \\[internal] load build definition from Dockerfile\n\n                                    #1 transferring dockerfile: 2.62kB done\n\n                                    #1 DONE 0.0s\n\n\n\n                                    #2 resolve image config for\n                                    docker-image://docker.io/docker/dockerfile:1.13-labs\n\n                                    #2 DONE 0.1s\n\n\n\n                                    #3\n                                    docker-image://docker.io/docker/dockerfile:1.13-labs@sha256:***\n                                    *********18b8\n\n                                    #3 resolve\n                                    docker.io/docker/dockerfile:1.13-labs@sha256:************18b8\n                                    done\n\n                                    #3 CACHED\n\n\n\n                                    #4 \\[internal] load metadata for\n                                    public.ecr.aws/clarifai-models/torch:2.5.1-py3.11-cuda124-*****\n                                    ***\n\n                                    #4 DONE 0.1s\n\n\n\n                                    #5 \\[internal] load .dockerignore\n\n                                    #5 transferring context: 2B done\n\n                                    #5 DONE 0.0s\n\n\n\n                                    #6 \\[final 1/8] FROM\n                                    public.ecr.aws/clarifai-models/torch:2.5.1-py3.11-cuda124-*****\n                                    ***@sha256:************c40c\n\n                                    #6 resolve\n                                    public.ecr.aws/clarifai-models/torch:2.5.1-py3.11-cuda124-*****\n                                    ***@sha256:************c40c done\n\n                                    #6 DONE 0.0s\n\n\n\n                                    #7 \\[internal] load build context\n\n                                    #7 transferring context: 10.86kB done\n\n                                    #7 DONE 0.0s\n\n\n\n                                    #8 \\[final 2/8] COPY --link requirements.txt\n                                    /home/nonroot/requirements.txt\n\n                                    #8 CACHED\n\n\n\n                                    #9 \\[final 4/8] RUN ["pip", "show", "clarifai"]\n\n                                    #9 CACHED\n\n\n\n                                    #10 \\[final 5/8] COPY --chown=nonroot:nonroot\n                                    downloader/unused.yaml\n                                    /home/nonroot/main/1/checkpoints/.cache/unused.yaml\n\n                                    #10 CACHED\n\n\n\n                                    #11 \\[final 3/8] RUN ["pip", "install", "--no-cache-dir", "-r",\n                                    "/home/nonroot/requirements.txt"]\n\n                                    #11 CACHED\n\n\n\n                                    #12 \\[final 6/8] RUN  ["python", "-m", "clarifai.cli", "model",\n                                    "download-checkpoints", "/home/nonroot/main", "--out_path",\n                                    "/home/nonroot/main/1/checkpoints", "--stage", "build"]\n\n                                    #12 CACHED\n\n\n\n                                    #13 \\[final 7/8] COPY --link=true 1 /home/nonroot/main/1\n\n                                    #13 CACHED\n\n\n\n                                    #14 \\[final 8/8] COPY --link=true requirements.txt config.yaml\n                                    /home/nonroot/main/\n\n                                    #14 DONE 0.0s\n\n\n\n                                    #15 \\[auth] sharing credentials for\n                                    891377382885.dkr.ecr.us-east-1.amazonaws.com\n\n                                    #15 DONE 0.0s\n\n\n\n                                    #16 exporting to image\n\n                                    #16 exporting layers done\n\n                                    #16 exporting manifest sha256:************7a9b done\n\n                                    #16 exporting config sha256:************cdca done\n\n                                    #16 pushing layers\n\n                                    #16 pushing layers 0.9s done\n\n                                    #16 pushing manifest for\n                                    ****/prod/pytorch:****dc1e1f48f28a@sha256:************7a9b\n\n                                    #16 pushing manifest for\n                                    ****/prod/pytorch:****dc1e1f48f28a@sha256:************7a9b 0.4s\n                                    done\n\n                                    #16 DONE 1.3s\n\n                                    2025-04-03 10:02:53.435557 INFO: Done building image!!!\n2025-04-03 13:03:00.834023 INFO     Model build complete!                                           model_builder.py:751\n2025-04-03 13:03:00.837028 INFO     Build time elapsed 10.2s)                                       model_builder.py:752\n2025-04-03 13:03:00.841022 INFO     Check out the model at                                          model_builder.py:753\n                                    https://clarifai.com/alfrick/my-models/models/Llama-3-8b-Instru\n                                    ct version: af882c92324149ae8eeedc1e1f48f28a',q={description:"Import custom models, including from external sources like Hugging Face and OpenAI",toc_min_heading_level:2,toc_max_heading_level:5},M="Model Uploading",O={},T=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Set up Docker or a Virtual Environment",id:"set-up-docker-or-a-virtual-environment",level:3},{value:"Install Clarifai Package",id:"install-clarifai-package",level:3},{value:"Set a PAT Key",id:"set-a-pat-key",level:3},{value:"Create Project Directory",id:"create-project-directory",level:3},{value:"How to Upload a Model",id:"how-to-upload-a-model",level:2},{value:"Step 1: Prepare the <code>config.yaml</code> File",id:"step-1-prepare-the-configyaml-file",level:3},{value:"Model Info",id:"model-info",level:4},{value:"Build Info",id:"build-info",level:4},{value:"Compute Resources",id:"compute-resources",level:4},{value:"Hugging Face Model Checkpoints",id:"hugging-face-model-checkpoints",level:4},{value:"Model Concepts or Labels",id:"model-concepts-or-labels",level:4},{value:"Step 2: Define Dependencies in <code>requirements.txt</code>",id:"step-2-define-dependencies-in-requirementstxt",level:3},{value:"Step 3: Prepare the <code>model.py</code> File",id:"step-3-prepare-the-modelpy-file",level:3},{value:"a. <code>load_model</code> Method",id:"a-load_model-method",level:4},{value:"b. Prediction Methods",id:"b-prediction-methods",level:4},{value:"Step 4: Test the Model Locally",id:"step-4-test-the-model-locally",level:3},{value:"Step 5: Upload the Model to Clarifai",id:"step-5-upload-the-model-to-clarifai",level:3},{value:"Step 6: Predict With Model",id:"step-6-predict-with-model",level:3},{value:"Examples",id:"examples",level:2},{value:"Image Classifier",id:"image-classifier",level:3},{value:"<code>model.py</code>",id:"modelpy",level:4},{value:"<code>requirements.txt</code>",id:"requirementstxt",level:4},{value:"<code>config.yaml</code>",id:"configyaml",level:4},{value:"Image Detector",id:"image-detector",level:3},{value:"<code>model.py</code>",id:"modelpy-1",level:4},{value:"<code>requirements.txt</code>",id:"requirementstxt-1",level:4},{value:"<code>config.yaml</code>",id:"configyaml-1",level:4},{value:"Large Language Models (LLMs)",id:"large-language-models-llms",level:3},{value:"<code>model.py</code>",id:"modelpy-2",level:4},{value:"<code>requirements.txt</code>",id:"requirementstxt-2",level:4},{value:"<code>config.yaml</code>",id:"configyaml-2",level:4},{value:"Speech Recognition Model",id:"speech-recognition-model",level:3},{value:"<code>model.py</code>",id:"modelpy-3",level:4},{value:"<code>requirements.txt</code>",id:"requirementstxt-3",level:4},{value:"<code>config.yaml</code>",id:"configyaml-3",level:4}];function S(e){const n={a:"a",admonition:"admonition",blockquote:"blockquote",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",mdxAdmonitionTitle:"mdxAdmonitionTitle",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components},{Details:o}=n;return o||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"model-uploading",children:"Model Uploading"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Import custom models, including from external sources like Hugging Face and OpenAI"})}),"\n",(0,i.jsx)("hr",{}),"\n",(0,i.jsx)("div",{style:{position:"relative",width:"100%",overflow:"hidden","padding-top":"56.25%"},children:(0,i.jsx)("iframe",{width:"900",height:"500",style:{position:"absolute",top:"0",left:"0",bottom:"0",right:"0",width:"100%",height:"100%"},src:"https://www.youtube.com/embed/SpIDmDtf7UE",title:"Upload Custom Models to Clarifai Platform Using Python SDK",frameborder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share",allowfullscreen:!0})}),"\n",(0,i.jsx)("br",{}),"\n",(0,i.jsx)("br",{}),"\n",(0,i.jsx)(n.p,{children:"The Clarifai Python SDK allows you to upload custom models easily. Whether you're working with a pre-trained model from an external source like Hugging Face or OpenAI, or one you've built from scratch, Clarifai allows seamless integration of your models, enabling you to take advantage of the platform\u2019s powerful capabilities."}),"\n",(0,i.jsx)(n.p,{children:"Once imported to our platform, your model can be utilized alongside Clarifai's vast suite of AI tools. It will be automatically deployed and ready to be evaluated, combined with other models and agent operators in a workflow, or used to serve inference requests as it is."}),"\n",(0,i.jsx)(n.p,{children:"Let\u2019s demonstrate how you can successfully upload different types of models to the Clarifai platform."}),"\n",(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["This new upload experience is compatible with the latest ",(0,i.jsx)(n.a,{href:"https://github.com/Clarifai/clarifai-python",children:(0,i.jsx)(n.code,{children:"clarifai"})})," Python package, starting from version 10.9.2. If you prefer the previous upload method, which is supported up to version 10.8.4, you can refer to the documentation ",(0,i.jsx)(n.a,{target:"_blank","data-noBrokenLinkCheck":!0,href:t(28175).A+"",children:"here"}),"."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["You can run the following command to clone the ",(0,i.jsx)(n.a,{href:"https://github.com/Clarifai/examples/tree/main",children:"repository"})," containing examples of how to upload various model types and follow along with this tutorial:\n",(0,i.jsx)(n.code,{children:"git clone https://github.com/Clarifai/examples.git"}),". After cloning it, go to the ",(0,i.jsx)(n.code,{children:"models/model_upload"})," directory."]}),"\n"]}),"\n"]})}),"\n","\n","\n","\n","\n","\n","\n","\n",(0,i.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsx)(n.h3,{id:"set-up-docker-or-a-virtual-environment",children:"Set up Docker or a Virtual Environment"}),"\n",(0,i.jsx)(n.p,{children:"To test, run, and upload your model, you need to set up either a Docker container or a Python virtual environment. This ensures proper dependency management and prevents conflicts in your project."}),"\n",(0,i.jsxs)(n.p,{children:["Both options allow you to work with different Python versions. For example, you can use Python 3.11 for uploading one model and Python 3.12 for another \u2014 configured via the ",(0,i.jsx)(n.a,{href:"#build-info",children:(0,i.jsx)(n.code,{children:"config.yaml"})})," file."]}),"\n",(0,i.jsx)(n.p,{children:"If Docker is installed on your system, it is highly recommended to use it for running the model. Docker provides better isolation and a fully portable environment, including for Python and system libraries."}),"\n",(0,i.jsxs)(n.p,{children:["You should ensure your local environment has sufficient memory and compute resources to handle model loading and execution, especially during ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/sdk/compute-orchestration/test-models-locally",children:"testing"}),"."]}),"\n",(0,i.jsx)(n.h3,{id:"install-clarifai-package",children:"Install Clarifai Package"}),"\n",(0,i.jsxs)(n.p,{children:["Install the latest version of the ",(0,i.jsx)(n.code,{children:"clarifai"})," Python package. This will also install the Clarifai ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/sdk/cli",children:"Command Line Interface"})," (CLI), which we'll use for testing and uploading the model."]}),"\n",(0,i.jsx)(s.A,{children:(0,i.jsx)(a.A,{value:"bash",label:"Bash",children:(0,i.jsx)(l.A,{className:"language-bash",children:" pip install --upgrade clarifai "})})}),"\n",(0,i.jsx)(n.h3,{id:"set-a-pat-key",children:"Set a PAT Key"}),"\n",(0,i.jsxs)(n.p,{children:["You need to set the ",(0,i.jsx)(n.code,{children:"CLARIFAI_PAT"})," (Personal Access Token) as an environment variable. You can generate the PAT key in your personal settings page by navigating to the ",(0,i.jsx)(n.a,{href:"https://clarifai.com/settings/security",children:"Security section"}),"."]}),"\n",(0,i.jsx)(n.p,{children:"This token is essential for authenticating your connection to the Clarifai platform."}),"\n",(0,i.jsxs)(s.A,{children:[(0,i.jsx)(a.A,{value:"bash",label:"Unix-Like Systems",children:(0,i.jsx)(l.A,{className:"language-bash",children:" export CLARIFAI_PAT=YOUR_PERSONAL_ACCESS_TOKEN_HERE "})}),(0,i.jsx)(a.A,{value:"bash2",label:"Windows",children:(0,i.jsx)(l.A,{className:"language-bash",children:" set CLARIFAI_PAT=YOUR_PERSONAL_ACCESS_TOKEN_HERE "})})]}),"\n",(0,i.jsx)(n.h3,{id:"create-project-directory",children:"Create Project Directory"}),"\n",(0,i.jsx)(n.p,{children:"Create a project directory and organize your files as indicated below to fit the requirements of uploading models to the Clarifai platform."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",children:"your_model_directory/\n\u251c\u2500\u2500 1/\n\u2502   \u2514\u2500\u2500 model.py\n\u251c\u2500\u2500 requirements.txt\n\u2514\u2500\u2500 config.yaml\n"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"your_model_directory/"})," \u2013 The main directory containing your model files.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"1/"})," \u2013 A subdirectory that holds the model file (",(0,i.jsxs)(n.em,{children:["Note that the folder is named as ",(0,i.jsx)(n.strong,{children:"1"})]}),").","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"model.py"})," \u2013 Contains the code that defines your model, including loading the model and running inference."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"requirements.txt"})," \u2013 Lists the Python libraries and dependencies required to run your model."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"config.yaml"})," \u2013 Contains model metadata and configuration details necessary for building the Docker image, defining compute resources, and uploading the model to Clarifai."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"how-to-upload-a-model",children:"How to Upload a Model"}),"\n",(0,i.jsxs)(n.p,{children:["Let's talk about the general steps you'd follow to upload any type of model to the Clarifai platform. You can refer to the ",(0,i.jsx)(n.a,{href:"#examples",children:"examples below"})," to help you configure your files correctly."]}),"\n",(0,i.jsxs)(n.h3,{id:"step-1-prepare-the-configyaml-file",children:["Step 1: Prepare the ",(0,i.jsx)(n.code,{children:"config.yaml"})," File"]}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"config.yaml"})," file is essential for specifying the model\u2019s metadata, compute resource requirements, and model checkpoints."]}),"\n",(0,i.jsx)(n.p,{children:"Here\u2019s a breakdown of the key sections in the file."}),"\n",(0,i.jsx)(n.h4,{id:"model-info",children:"Model Info"}),"\n",(0,i.jsx)(n.p,{children:"This section defines your model ID, Clarifai user ID, and Clarifai app ID, which will determine where the model is uploaded on the Clarifai platform."}),"\n",(0,i.jsx)(s.A,{children:(0,i.jsx)(a.A,{value:"yaml",label:"YAML",children:(0,i.jsx)(l.A,{className:"language-yaml",children:c})})}),"\n",(0,i.jsx)(n.h4,{id:"build-info",children:"Build Info"}),"\n",(0,i.jsxs)(n.p,{children:["This section specifies details about the environment used to build or run the model. You can include the ",(0,i.jsx)(n.code,{children:"python_version"}),", which is useful for ensuring compatibility between the model and its runtime environment, as different Python versions may have varying dependencies, library support, and performance characteristics."]}),"\n",(0,i.jsx)(n.admonition,{type:"note",children:(0,i.jsx)(n.p,{children:"We currently support Python 3.11 and Python 3.12 (default)."})}),"\n",(0,i.jsx)(s.A,{children:(0,i.jsx)(a.A,{value:"yaml",label:"YAML",children:(0,i.jsx)(l.A,{className:"language-yaml",children:u})})}),"\n",(0,i.jsx)(n.h4,{id:"compute-resources",children:"Compute Resources"}),"\n",(0,i.jsx)(n.p,{children:"Here, you define the minimum compute resources required for running your model, including CPU, memory, and optional GPU specifications."}),"\n",(0,i.jsx)(s.A,{children:(0,i.jsx)(a.A,{value:"yaml",label:"YAML",children:(0,i.jsx)(l.A,{className:"language-yaml",children:p})})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"cpu_limit"})}),' \u2013 Number of CPUs allocated for the model (follows Kubernetes notation, e.g., "1", "2").']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"cpu_memory"})}),' \u2013 Minimum memory required for the CPU (uses Kubernetes notation, e.g., "1Gi", "1500Mi", "3Gi").']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"num_accelerators"})})," \u2013 Number of GPUs or TPUs to use for inference."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"accelerator_type"})})," \u2013 Specifies the type of hardware ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/compute/deployments/cloud-instances/",children:"accelerators"}),' (e.g., GPU or TPU) supported by the model (e.g., "NVIDIA-A10G"). ',(0,i.jsxs)(n.em,{children:["Note that instead of specifying an exact accelerator type, you can use a wildcard ",(0,i.jsx)(n.code,{children:"(*)"})," to automatically match all available accelerators that fit your use case. For example, using ",(0,i.jsx)(n.code,{children:"[NVIDIA*]"})," will enable the system to choose from all NVIDIA options compatible with your model."]})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"accelerator_memory"})})," \u2013 Minimum memory required for the GPU or TPU."]}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"hugging-face-model-checkpoints",children:"Hugging Face Model Checkpoints"}),"\n",(0,i.jsx)(n.p,{children:"If you're using a model from Hugging Face, you can automatically download its checkpoints by specifying the appropriate configuration in this section. For private or restricted Hugging Face repositories, include an access token."}),"\n",(0,i.jsx)(s.A,{children:(0,i.jsx)(a.A,{value:"yaml",label:"YAML",children:(0,i.jsx)(l.A,{className:"language-yaml",children:m})})}),"\n",(0,i.jsxs)(n.admonition,{type:"note",children:[(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"when"})," parameter in the ",(0,i.jsx)(n.code,{children:"checkpoints"})," section determines when model checkpoints should be downloaded and stored. It must be set to one of the following options:"]}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"runtime"})," (",(0,i.jsx)(n.em,{children:"default"}),") \u2013 Downloads checkpoints when loading the model in the ",(0,i.jsx)(n.code,{children:"load_model"})," method."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"build"})," \u2013 Downloads checkpoints during the image build process."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"upload"})," \u2013 Downloads checkpoints before uploading the model."]}),"\n"]}),(0,i.jsxs)(n.p,{children:["For larger models, we highly recommend downloading checkpoints at ",(0,i.jsx)(n.code,{children:"runtime"}),". Doing so prevents unnecessary increases in Docker image size, which has some advantages:"]}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Smaller image sizes"}),"\n",(0,i.jsx)(n.li,{children:"Faster build times"}),"\n",(0,i.jsx)(n.li,{children:"Quicker uploads and inference on the Clarifai platform"}),"\n"]}),(0,i.jsxs)(n.p,{children:["Downloading checkpoints at ",(0,i.jsx)(n.code,{children:"build"})," or ",(0,i.jsx)(n.code,{children:"upload"})," time can significantly increase image size, resulting in longer upload times and increased cold start latency."]})]}),"\n",(0,i.jsx)(n.h4,{id:"model-concepts-or-labels",children:"Model Concepts or Labels"}),"\n",(0,i.jsx)(n.admonition,{type:"important",children:(0,i.jsx)(n.p,{children:"This section is required if your model outputs concepts or labels and is not being directly loaded from Hugging Face."})}),"\n",(0,i.jsxs)(n.p,{children:["For models that output concepts or labels, you must define a ",(0,i.jsx)(n.code,{children:"concepts"})," section in the ",(0,i.jsx)(n.code,{children:"config.yaml"})," file."]}),"\n",(0,i.jsx)(n.p,{children:"The following model types output concepts or labels:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"visual-classifier"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"visual-detector"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"visual-segmenter"})}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"text-classifier"}),"."]}),"\n"]}),"\n",(0,i.jsx)(s.A,{children:(0,i.jsx)(a.A,{value:"yaml",label:"YAML",children:(0,i.jsx)(l.A,{className:"language-yaml",children:h})})}),"\n",(0,i.jsxs)(n.admonition,{type:"note",children:[(0,i.jsx)(n.mdxAdmonitionTitle,{}),(0,i.jsxs)(n.p,{children:["If you're using a model from Hugging Face and the ",(0,i.jsx)(n.code,{children:"checkpoints"})," section is defined, the Clarifai platform will automatically infer concepts. In this case, you don\u2019t need to manually specify them."]})]}),"\n",(0,i.jsxs)(n.h3,{id:"step-2-define-dependencies-in-requirementstxt",children:["Step 2: Define Dependencies in ",(0,i.jsx)(n.code,{children:"requirements.txt"})]}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"requirements.txt"})," file lists all the Python dependencies your model needs. If your model requires Torch, we provide optimized pre-built Torch images as the base for machine learning and inference tasks."]}),"\n",(0,i.jsx)(n.p,{children:"These images include all necessary dependencies, ensuring efficient execution. The available pre-built Torch images are:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"2.4.1-py3.11-cuda124"})," \u2014 Based on PyTorch 2.4.1, Python 3.11, and CUDA 12.4."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"2.5.1-py3.11-cuda124"})," \u2014 Based on PyTorch 2.5.1, Python 3.11, and CUDA 12.4."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"2.4.1-py3.12-cuda124"})," \u2014 Based on PyTorch 2.4.1, Python 3.12, and CUDA 12.4."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"2.5.1-py3.12-cuda124"})," \u2014 Based on PyTorch 2.5.1, Python 3.12, and CUDA 12.4."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"To use a specific Torch version, define it in your requirements.txt file like this:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",children:"torch==2.5.1\n"})}),"\n",(0,i.jsx)(n.p,{children:"This ensures the correct pre-built image is pulled from Clarifai's container registry, ensuring the correct environment is used. This minimizes cold start times and speeds up model uploads and runtime execution \u2014 avoiding the overhead of building images from scratch or pulling and configuring them from external sources."}),"\n",(0,i.jsxs)(n.p,{children:["We recommend using either ",(0,i.jsx)(n.code,{children:"torch==2.5.1"})," or ",(0,i.jsx)(n.code,{children:"torch==2.4.1"}),". If your model requires a different Torch version, you can specify it in requirements.txt, but this may slightly increase the model upload time."]}),"\n",(0,i.jsxs)(n.h3,{id:"step-3-prepare-the-modelpy-file",children:["Step 3: Prepare the ",(0,i.jsx)(n.code,{children:"model.py"})," File"]}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"model.py"})," file contains the core logic for your model, including how the model is loaded and how predictions are made. This file must define a custom class that inherits from ",(0,i.jsx)(n.code,{children:"ModelClass"})," and implements the required methods."]}),"\n",(0,i.jsxs)(n.p,{children:["To define a custom model, create a class that inherits from ",(0,i.jsx)(n.code,{children:"ModelClass"})," and implements the following:"]}),"\n",(0,i.jsxs)(n.h4,{id:"a-load_model-method",children:["a. ",(0,i.jsx)(n.code,{children:"load_model"})," Method"]}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"load_model"})," method is optional but recommended, as it prepares the model for inference by handling resource-heavy initializations. It is particularly useful for:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"One-time setup of heavy resources, such as loading trained models or initializing data transformations."}),"\n",(0,i.jsx)(n.li,{children:"Executing tasks during model container startup to reduce runtime latency."}),"\n",(0,i.jsx)(n.li,{children:"Loading essential components like tokenizers, pipelines, and other model-related assets."}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Here is an example:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def load_model(self):\n  self.tokenizer = AutoTokenizer.from_pretrained("model/")\n  self.pipeline = transformers.pipeline(...)\n'})}),"\n",(0,i.jsx)(n.h4,{id:"b-prediction-methods",children:"b. Prediction Methods"}),"\n",(0,i.jsxs)(n.p,{children:["You need to include at least one method decorated with ",(0,i.jsx)(n.code,{children:"@ModelClass.method"})," to define ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/compute/models/model-inference",children:"prediction endpoints"}),"."]}),"\n",(0,i.jsx)(n.p,{children:"The following three method types are supported based on type hints:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Unary-Unary (Standard request-response)\ndef predict(self, input: Image) -> Text\n\n# Unary-Stream (Server-side streaming)\ndef generate(self, prompt: Text) -> Stream[Text]\n\n# Stream-Stream (Bidirectional streaming)\ndef analyze_video(self, frames: Stream[Image]) -> Stream[str]\n"})}),"\n",(0,i.jsx)(n.admonition,{title:"Supported Input and Output Data Types",type:"warning",children:(0,i.jsxs)(n.p,{children:["Each parameter in the class methods must be annotated with a type, and the return type must also be specified. Clarifai's model framework supports rich data typing for both inputs and outputs. Supported types include ",(0,i.jsx)(n.code,{children:"Text"}),", ",(0,i.jsx)(n.code,{children:"Image"}),", ",(0,i.jsx)(n.code,{children:"Audio"}),", ",(0,i.jsx)(n.code,{children:"Video"}),", and more as illustrated ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/compute/models/model-upload/data-types",children:"here"}),"."]})}),"\n",(0,i.jsxs)(n.p,{children:["Here is an example of a ",(0,i.jsx)(n.code,{children:"model.py"})," file."]}),"\n",(0,i.jsx)(s.A,{children:(0,i.jsx)(a.A,{value:"python",label:"Python",children:(0,i.jsx)(l.A,{className:"language-python",children:d})})}),"\n",(0,i.jsx)(n.h3,{id:"step-4-test-the-model-locally",children:"Step 4: Test the Model Locally"}),"\n",(0,i.jsx)(n.p,{children:"Before uploading your model to the Clarifai platform, it's important to test it locally to catch any typos or misconfigurations in the code."}),"\n",(0,i.jsxs)(n.p,{children:["Learn how to test your models locally ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/compute/models/model-upload/test-models-locally/",children:"here"}),"."]}),"\n",(0,i.jsx)(n.h3,{id:"step-5-upload-the-model-to-clarifai",children:"Step 5: Upload the Model to Clarifai"}),"\n",(0,i.jsxs)(n.p,{children:["Once your model is ready, you can upload it to the platform using Clarifai CLI. ",(0,i.jsxs)(n.em,{children:["Note that to use the CLI to upload your model, you'll need to first use the tool to ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/sdk/cli/#login",children:"log in"})," to your account"]}),"."]}),"\n",(0,i.jsx)(n.p,{children:"To upload your model, run the following command in your terminal:"}),"\n",(0,i.jsx)(s.A,{children:(0,i.jsx)(a.A,{value:"bash",label:"Bash",children:(0,i.jsx)(l.A,{className:"language-bash",children:" clarifai model upload ./your/model/path/here "})})}),"\n",(0,i.jsx)(n.p,{children:"Alternatively, navigate to the directory containing your custom model and run the command without specifying the directory path:"}),"\n",(0,i.jsx)(s.A,{children:(0,i.jsx)(a.A,{value:"bash",label:"Bash",children:(0,i.jsx)(l.A,{className:"language-bash",children:" clarifai model upload "})})}),"\n",(0,i.jsx)(n.p,{children:"This command builds the model\u2019s Docker image using the defined compute resources and uploads it to Clarifai, where it can be served in production.\nThe build logs will be displayed in your terminal, which helps you troubleshoot any upload issues."}),"\n",(0,i.jsxs)(o,{children:[(0,i.jsx)("summary",{children:"Build Logs Example"}),(0,i.jsx)(l.A,{className:"language-text",children:C})]}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsx)(n.p,{children:"Note that if you make any changes to your model and upload it again to the Clarifai platform, a new version of the model will be created automatically."}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"step-6-predict-with-model",children:"Step 6: Predict With Model"}),"\n",(0,i.jsxs)(n.p,{children:["Once the model is successfully uploaded to Clarifai, you can start making predictions with it. Note that before making a prediction request with our Compute Orchestration capabilities, you need to first ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/compute/deployments/deploy-model",children:"deploy"})," it into a cluster and nodepool you've created."]}),"\n",(0,i.jsx)(n.h2,{id:"examples",children:"Examples"}),"\n",(0,i.jsx)(n.admonition,{type:"tip",children:(0,i.jsxs)(n.p,{children:["You can find various up-to-date model upload examples ",(0,i.jsx)(n.a,{href:"https://github.com/Clarifai/examples/tree/main/models/model_upload",children:"here"}),", which demonstrate different use cases and optimizations."]})}),"\n",(0,i.jsx)(n.h3,{id:"image-classifier",children:"Image Classifier"}),"\n",(0,i.jsx)(n.h4,{id:"modelpy",children:(0,i.jsx)(n.code,{children:"model.py"})}),"\n",(0,i.jsx)(s.A,{children:(0,i.jsx)(a.A,{value:"python",label:"Python",children:(0,i.jsx)(l.A,{className:"language-python",children:f})})}),"\n",(0,i.jsx)(n.h4,{id:"requirementstxt",children:(0,i.jsx)(n.code,{children:"requirements.txt"})}),"\n",(0,i.jsx)(s.A,{children:(0,i.jsx)(a.A,{value:"text",label:"Text",children:(0,i.jsx)(l.A,{className:"language-text",children:_})})}),"\n",(0,i.jsx)(n.h4,{id:"configyaml",children:(0,i.jsx)(n.code,{children:"config.yaml"})}),"\n",(0,i.jsx)(s.A,{children:(0,i.jsx)(a.A,{value:"yaml",label:"YAML",children:(0,i.jsx)(l.A,{className:"language-yaml",children:g})})}),"\n",(0,i.jsx)(n.h3,{id:"image-detector",children:"Image Detector"}),"\n",(0,i.jsx)(n.h4,{id:"modelpy-1",children:(0,i.jsx)(n.code,{children:"model.py"})}),"\n",(0,i.jsx)(s.A,{children:(0,i.jsx)(a.A,{value:"python",label:"Python",children:(0,i.jsx)(l.A,{className:"language-python",children:x})})}),"\n",(0,i.jsx)(n.h4,{id:"requirementstxt-1",children:(0,i.jsx)(n.code,{children:"requirements.txt"})}),"\n",(0,i.jsx)(s.A,{children:(0,i.jsx)(a.A,{value:"text",label:"Text",children:(0,i.jsx)(l.A,{className:"language-text",children:y})})}),"\n",(0,i.jsx)(n.h4,{id:"configyaml-1",children:(0,i.jsx)(n.code,{children:"config.yaml"})}),"\n",(0,i.jsx)(s.A,{children:(0,i.jsx)(a.A,{value:"yaml",label:"YAML",children:(0,i.jsx)(l.A,{className:"language-yaml",children:b})})}),"\n",(0,i.jsx)(n.h3,{id:"large-language-models-llms",children:"Large Language Models (LLMs)"}),"\n",(0,i.jsx)(n.h4,{id:"modelpy-2",children:(0,i.jsx)(n.code,{children:"model.py"})}),"\n",(0,i.jsx)(s.A,{children:(0,i.jsx)(a.A,{value:"python",label:"Python",children:(0,i.jsx)(l.A,{className:"language-python",children:v})})}),"\n",(0,i.jsx)(n.h4,{id:"requirementstxt-2",children:(0,i.jsx)(n.code,{children:"requirements.txt"})}),"\n",(0,i.jsx)(s.A,{children:(0,i.jsx)(a.A,{value:"text",label:"Text",children:(0,i.jsx)(l.A,{className:"language-text",children:j})})}),"\n",(0,i.jsx)(n.h4,{id:"configyaml-2",children:(0,i.jsx)(n.code,{children:"config.yaml"})}),"\n",(0,i.jsx)(s.A,{children:(0,i.jsx)(a.A,{value:"yaml",label:"YAML",children:(0,i.jsx)(l.A,{className:"language-yaml",children:k})})}),"\n",(0,i.jsx)(n.admonition,{type:"tip",children:(0,i.jsxs)(n.p,{children:["You can refer to ",(0,i.jsx)(n.a,{href:"https://github.com/Clarifai/examples/tree/main/models/model_upload/llms",children:"this examples repository"})," for additional examples of uploading other types of LLMs."]})}),"\n",(0,i.jsx)(n.h3,{id:"speech-recognition-model",children:"Speech Recognition Model"}),"\n",(0,i.jsx)(n.h4,{id:"modelpy-3",children:(0,i.jsx)(n.code,{children:"model.py"})}),"\n",(0,i.jsx)(s.A,{children:(0,i.jsx)(a.A,{value:"python",label:"Python",children:(0,i.jsx)(l.A,{className:"language-python",children:w})})}),"\n",(0,i.jsx)(n.h4,{id:"requirementstxt-3",children:(0,i.jsx)(n.code,{children:"requirements.txt"})}),"\n",(0,i.jsx)(s.A,{children:(0,i.jsx)(a.A,{value:"text",label:"Text",children:(0,i.jsx)(l.A,{className:"language-text",children:I})})}),"\n",(0,i.jsx)(n.h4,{id:"configyaml-3",children:(0,i.jsx)(n.code,{children:"config.yaml"})}),"\n",(0,i.jsx)(s.A,{children:(0,i.jsx)(a.A,{value:"yaml",label:"YAML",children:(0,i.jsx)(l.A,{className:"language-yaml",children:A})})})]})}function N(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(S,{...e})}):S(e)}}}]);
"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[3263],{85162:(e,t,n)=>{n.d(t,{Z:()=>s});var a=n(67294),o=n(86010);const i={tabItem:"tabItem_Ymn6"};function s(e){let{children:t,hidden:n,className:s}=e;return a.createElement("div",{role:"tabpanel",className:(0,o.Z)(i.tabItem,s),hidden:n},t)}},74866:(e,t,n)=>{n.d(t,{Z:()=>g});var a=n(87462),o=n(67294),i=n(86010),s=n(12466),r=n(16550),l=n(91980),u=n(67392),p=n(50012);function c(e){return function(e){return o.Children.map(e,(e=>{if(!e||(0,o.isValidElement)(e)&&function(e){const{props:t}=e;return!!t&&"object"==typeof t&&"value"in t}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}(e).map((e=>{let{props:{value:t,label:n,attributes:a,default:o}}=e;return{value:t,label:n,attributes:a,default:o}}))}function d(e){const{values:t,children:n}=e;return(0,o.useMemo)((()=>{const e=t??c(n);return function(e){const t=(0,u.l)(e,((e,t)=>e.value===t.value));if(t.length>0)throw new Error(`Docusaurus error: Duplicate values "${t.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[t,n])}function m(e){let{value:t,tabValues:n}=e;return n.some((e=>e.value===t))}function _(e){let{queryString:t=!1,groupId:n}=e;const a=(0,r.k6)(),i=function(e){let{queryString:t=!1,groupId:n}=e;if("string"==typeof t)return t;if(!1===t)return null;if(!0===t&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:t,groupId:n});return[(0,l._X)(i),(0,o.useCallback)((e=>{if(!i)return;const t=new URLSearchParams(a.location.search);t.set(i,e),a.replace({...a.location,search:t.toString()})}),[i,a])]}function h(e){const{defaultValue:t,queryString:n=!1,groupId:a}=e,i=d(e),[s,r]=(0,o.useState)((()=>function(e){let{defaultValue:t,tabValues:n}=e;if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(t){if(!m({value:t,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${t}" but none of its children has the corresponding value. Available values are: ${n.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return t}const a=n.find((e=>e.default))??n[0];if(!a)throw new Error("Unexpected error: 0 tabValues");return a.value}({defaultValue:t,tabValues:i}))),[l,u]=_({queryString:n,groupId:a}),[c,h]=function(e){let{groupId:t}=e;const n=function(e){return e?`docusaurus.tab.${e}`:null}(t),[a,i]=(0,p.Nk)(n);return[a,(0,o.useCallback)((e=>{n&&i.set(e)}),[n,i])]}({groupId:a}),f=(()=>{const e=l??c;return m({value:e,tabValues:i})?e:null})();(0,o.useLayoutEffect)((()=>{f&&r(f)}),[f]);return{selectedValue:s,selectValue:(0,o.useCallback)((e=>{if(!m({value:e,tabValues:i}))throw new Error(`Can't select invalid tab value=${e}`);r(e),u(e),h(e)}),[u,h,i]),tabValues:i}}var f=n(72389);const A={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};function E(e){let{className:t,block:n,selectedValue:r,selectValue:l,tabValues:u}=e;const p=[],{blockElementScrollPositionUntilNextRender:c}=(0,s.o5)(),d=e=>{const t=e.currentTarget,n=p.indexOf(t),a=u[n].value;a!==r&&(c(t),l(a))},m=e=>{let t=null;switch(e.key){case"Enter":d(e);break;case"ArrowRight":{const n=p.indexOf(e.currentTarget)+1;t=p[n]??p[0];break}case"ArrowLeft":{const n=p.indexOf(e.currentTarget)-1;t=p[n]??p[p.length-1];break}}t?.focus()};return o.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,i.Z)("tabs",{"tabs--block":n},t)},u.map((e=>{let{value:t,label:n,attributes:s}=e;return o.createElement("li",(0,a.Z)({role:"tab",tabIndex:r===t?0:-1,"aria-selected":r===t,key:t,ref:e=>p.push(e),onKeyDown:m,onClick:d},s,{className:(0,i.Z)("tabs__item",A.tabItem,s?.className,{"tabs__item--active":r===t})}),n??t)})))}function T(e){let{lazy:t,children:n,selectedValue:a}=e;const i=(Array.isArray(n)?n:[n]).filter(Boolean);if(t){const e=i.find((e=>e.props.value===a));return e?(0,o.cloneElement)(e,{className:"margin-top--md"}):null}return o.createElement("div",{className:"margin-top--md"},i.map(((e,t)=>(0,o.cloneElement)(e,{key:t,hidden:e.props.value!==a}))))}function I(e){const t=h(e);return o.createElement("div",{className:(0,i.Z)("tabs-container",A.tabList)},o.createElement(E,(0,a.Z)({},e,t)),o.createElement(T,(0,a.Z)({},e,t)))}function g(e){const t=(0,f.Z)();return o.createElement(I,(0,a.Z)({key:String(t)},e))}},58105:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>E,contentTitle:()=>f,default:()=>b,frontMatter:()=>h,metadata:()=>A,toc:()=>T});var a=n(87462),o=(n(67294),n(3905)),i=n(74866),s=n(85162),r=n(90814);const l='##############################################################################################################\n# In this section, we set the user authentication, user and app ID, model details, and input details.\n# Change these values to run your own example.\n##############################################################################################################\n\n# Your PAT (Personal Access Token) can be found in the Account\'s Security section\nPAT = "YOUR_PAT_HERE"\n# Specify the correct user_id/app_id pairings\n# Since you\'re making inferences outside your app\'s scope\nUSER_ID = "openai"\nAPP_ID = "chat-completion"\n# Change these to whatever model and inputs you want to use\nMODEL_ID = "openai-gpt-4-vision"\nMODEL_VERSION_ID = "266df29bc09843e0aee9b7bf723c03c2"\nRAW_TEXT = "Write a caption for the image"\n# To use a hosted text file, assign the URL variable\n# TEXT_FILE_URL = "https://samples.clarifai.com/negative_sentence_12.txt"\n# Or, to use a local text file, assign the location variable\n# TEXT_FILE_LOCATION = "YOUR_TEXT_FILE_LOCATION_HERE"\nIMAGE_URL = "https://samples.clarifai.com/metro-north.jpg"\n# To use a local file, assign the location variable\n# IMAGE_FILE_LOCATION = "YOUR_IMAGE_FILE_LOCATION_HERE"\n\n############################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n############################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\nfrom google.protobuf.struct_pb2 import Struct\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (("authorization", "Key " + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\n# To use a local text file, uncomment the following lines\n# with open(TEXT_FILE_LOCATION, "rb") as f:\n#    file_bytes = f.read()\n\n# To use a local image file, uncomment the following lines\n# with open(IMAGE_FILE_LOCATION, "rb") as f:\n#     file_bytes = f.read()\n\nparams = Struct()\nparams.update(\n    {\n        "temperature": 0.5,\n        "max_tokens": 2048,\n        "top_p": 0.95,\n        # "api_key": "ADD_THIRD_PARTY_KEY_HERE"\n    }\n)\n\npost_model_outputs_response = stub.PostModelOutputs(\n    service_pb2.PostModelOutputsRequest(\n        user_app_id=userDataObject,  # The userDataObject is created in the overview and is required when using a PAT\n        model_id=MODEL_ID,\n        version_id=MODEL_VERSION_ID,  # This is optional. Defaults to the latest model version\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    text=resources_pb2.Text(\n                        raw=RAW_TEXT\n                        # url=TEXT_FILE_URL\n                        # raw=file_bytes\n                    ),\n                    image=resources_pb2.Image(\n                        url=IMAGE_URL\n                        # base64=file_bytes\n                    ),\n                )\n            )\n        ],\n        model=resources_pb2.Model(\n            model_version=resources_pb2.ModelVersion(\n                output_info=resources_pb2.OutputInfo(params=params)\n            )\n        ),\n    ),\n    metadata=metadata,\n)\nif post_model_outputs_response.status.code != status_code_pb2.SUCCESS:\n    print(post_model_outputs_response.status)\n    raise Exception(f"Post model outputs failed, status: {post_model_outputs_response.status.description}")\n\n# Since we have one input, one output will exist here\noutput = post_model_outputs_response.outputs[0]\n\nprint(output.data.text.raw)\n',u='\x3c!--index.html file--\x3e\n\n<script>\n    /////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, and input details.\n    // Change these values to run your own example.\n    /////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n    // Your PAT (Personal Access Token) can be found in the Account\'s Security section\n    const PAT = "YOUR_PAT_HERE";\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    const USER_ID = "openai";\n    const APP_ID = "chat-completion";\n    // Change these to whatever model and inputs you want to use\n    const MODEL_ID = "openai-gpt-4-vision";\n    const MODEL_VERSION_ID = "266df29bc09843e0aee9b7bf723c03c2";\n    const RAW_TEXT = "Write a caption for the image";\n    // To use a hosted text file, assign the URL variable\n    // const TEXT_FILE_URL = "https://samples.clarifai.com/negative_sentence_12.txt";\n    const IMAGE_URL = "https://samples.clarifai.com/metro-north.jpg";\n    // To use image bytes, assign its variable   \n    // const IMAGE_BYTES_STRING = "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAUDBAQEAwUEBAQFBQUGBwwIBwcHBw8LCwkMEQ8SEhEPERETFhwXExQaFRERGCEYGh0dHx8fExciJCIeJBweHx7/2wBDAQUFBQcGBw4ICA4eFBEUHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh7/wAARCAAoACgDASIAAhEBAxEB/8QAGQAAAgMBAAAAAAAAAAAAAAAAAAYDBQcE/8QAMBAAAQMDAwMDAgQHAAAAAAAAAQIDBAAFEQYSIQcTMTJBURRhCBYikSNScXKhsdH/xAAZAQACAwEAAAAAAAAAAAAAAAAFBgIDBAf/xAAtEQABAwMBBgQHAQAAAAAAAAABAgMRAAQhMQUSE0FRYQaBocEUFiJCcrHR8P/aAAwDAQACEQMRAD8A3+RYY1unSYzCS0ttZUkAgktn0q5yT7jPyDUC4wdGwycH5U2Kt9ZQ7VI1qw5PkvQy3CSVPpf7aQjuKyFH25xzn3pHn3TVNy01Hl2hyy6YdkSpKsS9sl/6RlI3rRu3dxWd6spwnAGPIJTfl925fcLaoSDHXvyo6i9SlCQrU9wKln3OyWiaDN1RAbW3kKbSd7gPtwMkH/tTWy9afuy1iPfnXMAblITwkE4yf08cn3pSbYt1uts24XH6fUbiLAuY1MWyGkLEmUW0rcCRvUpQ5CtwKQCPgi4S1ZbDe4sd9NntDEe79m3uOBLTr0IR9jzodSMqUpTu9JJ8owD7UTT4ZCfv9PbP7860m+s+HBSrejWRuz2kAxoesGYxTW/Zlpkwo1vkuSly3UgKWQUhHJUvIHsAaKTemF8XE6sWmxyZkiaZrMh1jv8ArQNpUVqB8FW0njHqx4zRVVhsph1KlKk5xQ+7uHmikaSJrQerMByet2IwvtuTLa4xv2k7Rk84H9x/esHv92d01boenLXGcuiWrFIhLlpbcaQ2/JdK3VJCkAq2pAR7Zz7YxWudY9fxNIdQbNGkR5TyX4aisNNpUMFZAzkj4NK0jq9ZpbLr0PSlzkhrlZDaQlP3P8Q4/ap3F87bPucJEkx/hHv60b2TYXLrKN5sramYECSQRk9M6c6zmJ+eb5Hi22M7cnWGIQgFLbX0zSo4PDa1YBcTgDyMjJ/qbGPabH08SJt1Uzc9QqRliGg5QySPKvgc+TyfYDmmTUWpNYz7ctxoQdPQshCktupckDJUPUcJT6DwMq8YyaQ9VL0pCS8zapcq4SVOBZmPDO8/cnknlWcDBwn4NYnPjLkQ+qE9OtOVlYpeVHDCEkkkJyT+SuQzy5Y0ru6Ez511/Efa5s1fdkOtyVurIxgdlQAA9gOKKPwolU7remU5hCGYEgo38KUv9I/0TRTDYJCWQBSF4rIN/CRgAR0iTpVD1j1g/qDqJcJqlKcjB9bcda142MpOEJAzgeMnjyTSyze5KEuNRpDoDvC0oe4X9iAeaKKFK+oya6fbOqYbDTeEiAPKpHdS3gBLYc7RQkp3ApQog+cq8nwPJrljzxnPZbUfnugn/NFFRgEVch9xKsH0H8pg6e3x3T3UC1ajaZITGkJLoS4MKbOUrzz/ACKVRRRVzVwtoQmhG1NkWu0HuI+JI8u/Kv/Z";\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    const raw = JSON.stringify({\n        "inputs": [\n            {\n                "data": {\n                    "text": {\n                        "raw": RAW_TEXT\n                        // "url": TEXT_FILE_URL\n                    },\n                    "image": {\n                        "url": IMAGE_URL\n                        // "base64": IMAGE_BYTES_STRING\n                    }\n                }\n            }\n        ],\n        "model": {\n            "model_version": {\n                "output_info": {\n                    "params": {\n                        "temperature": 0.5,\n                        "max_tokens": 2048,\n                        "top_p": 0.95,\n                        // "api_key": "ADD_THIRD_PARTY_KEY_HERE"\n                    }\n                }\n            }\n        }\n    });\n\n    const requestOptions = {\n        method: "POST",\n        headers: {\n            "Accept": "application/json",\n            "Authorization": "Key " + PAT\n        },\n        body: raw\n    };\n\n    fetch(`https://api.clarifai.com/v2/users/${USER_ID}/apps/${APP_ID}/models/${MODEL_ID}/versions/${MODEL_VERSION_ID}/outputs`, requestOptions)\n        .then(response => response.text())\n        .then(result => console.log(result))\n        .catch(error => console.log(\'error\', error));\n\n<\/script>',p='//index.js file\n\n////////////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, and input details.\n// Change these values to run your own example.\n////////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\nconst PAT = "YOUR_PAT_HERE";\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\nconst USER_ID = "openai";\nconst APP_ID = "chat-completion";\n// Change these to whatever model and inputs you want to use\nconst MODEL_ID = "openai-gpt-4-vision";\nconst MODEL_VERSION_ID = "266df29bc09843e0aee9b7bf723c03c2";\nconst RAW_TEXT = "Write a caption for the image";\n// To use a hosted text file, assign the URL variable\n// const TEXT_FILE_URL = "https://samples.clarifai.com/negative_sentence_12.txt";\n// Or, to use a local text file, assign the location variable\n// TEXT_FILE_LOCATION = "YOUR_TEXT_FILE_LOCATION_HERE";\nconst IMAGE_URL = "https://samples.clarifai.com/metro-north.jpg";\n// To use a local file, assign the location variable;\n// const IMAGE_FILE_LOCATION = "YOUR_IMAGE_FILE_LOCATION_HERE";\n\n/////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n/////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\n// To use a local text file, uncomment the following lines\n// const fs = require("fs");\n// const fileBytes = fs.readFileSync(TEXT_FILE_LOCATION);\n\n// To use a local image file, uncomment the following lines\n// const fs = require("fs");\n// const imageBytes = fs.readFileSync(IMAGE_FILE_LOCATION);\n\nstub.PostModelOutputs(\n    {\n        user_app_id: {\n            "user_id": USER_ID,\n            "app_id": APP_ID\n        },\n        model_id: MODEL_ID,\n        version_id: MODEL_VERSION_ID,  // This is optional. Defaults to the latest model version\n        inputs: [\n            {\n                "data": {\n                    "text": {\n                        "raw": RAW_TEXT,\n                        // url: TEXT_FILE_URL,\n                        // raw: fileBytes\n                    },\n                    "image": {\n                        "url": IMAGE_URL,\n                        // base64: imageBytes                      \n                    }\n                }\n            }\n        ],\n        model: {\n            "model_version": {\n                "output_info": {\n                    "params": {\n                        "temperature": 0.5,\n                        "max_tokens": 2048,\n                        "top_p": 0.95\n                        // "api_key": "ADD_THIRD_PARTY_KEY_HERE"\n                    }\n                }\n            }\n        }\n    },\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            throw new Error("Post models failed, status: " + response.status.description);\n        }\n        // Since we have one input, one output will exist here.\n        const output = response.outputs[0];\n\n        console.log(output.data.text.raw);\n    }\n);\n',c='package com.clarifai.example;\n\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.status.StatusCode;\nimport com.google.protobuf.Struct;\nimport com.google.protobuf.Value;\n\nimport com.google.protobuf.ByteString;\nimport java.io.File;\nimport java.io.IOException;\nimport java.nio.file.Files;\n\npublic class ClarifaiExample {\n\n    /////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, and input details.\n    // Change these values to run your own example.\n    /////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n    // Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    static final String USER_ID = "openai";\n    static final String APP_ID = "chat-completion";\n    // Change these to whatever model and inputs you want to use\n    static final String MODEL_ID = "openai-gpt-4-vision";\n    static final String MODEL_VERSION_ID = "266df29bc09843e0aee9b7bf723c03c2";\n    static final String RAW_TEXT = "Write a caption for the image";\n    // To use a hosted text file, assign the URL variable\n    // static final String TEXT_FILE_URL = "https://samples.clarifai.com/negative_sentence_12.txt";\n    // Or, to use a local text file, assign the location variable\n    // static final String TEXT_FILE_LOCATION = "YOUR_TEXT_FILE_LOCATION_HERE";\n    static final String IMAGE_URL = "https://samples.clarifai.com/metro-north.jpg";\n    // Or, to use a local image file, assign the location variable\n    //static final String IMAGE_FILE_LOCATION = "YOUR_IMAGE_FILE_LOCATION_HERE";\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\t\n\n    public static void main(String[] args) throws IOException {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n                .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        Struct.Builder params = Struct.newBuilder()\n                .putFields("temperature", Value.newBuilder().setNumberValue(0.5).build())\n                .putFields("max_tokens", Value.newBuilder().setNumberValue(2048).build())\n                .putFields("top_p", Value.newBuilder().setNumberValue(0.95).build());\n               // .putFields("api_key", Value.newBuilder().setNumberValue("ADD_THIRD_PARTY_KEY_HERE").build());\n\n        MultiOutputResponse postModelOutputsResponse = stub.postModelOutputs(\n                PostModelOutputsRequest.newBuilder()\n                        .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n                        .setModelId(MODEL_ID)\n                        .setVersionId(MODEL_VERSION_ID) // This is optional. Defaults to the latest model version\n                        .addInputs(\n                                Input.newBuilder().setData(\n                                        Data.newBuilder().setText(\n                                                Text.newBuilder().setRaw(RAW_TEXT)\n                                        // Text.newBuilder().setUrl(TEXT_FILE_URL)\n                                        // Text.newBuilder().setRawBytes(ByteString.copyFrom(Files.readAllBytes(\n                                        // new File(TEXT_FILE_LOCATION).toPath()\n                                        // )))\n                                        )\n                                                .setImage(\n                                                        Image.newBuilder().setUrl(IMAGE_URL)\n                                                // Image.newBuilder().setBase64(ByteString.copyFrom(Files.readAllBytes(\n                                                // new File(IMAGE_FILE_LOCATION).toPath()\n                                                // )))\n                                                )\n                                )\n                        )\n                        .setModel(Model.newBuilder()\n                                .setModelVersion(ModelVersion.newBuilder()\n                                        .setOutputInfo(OutputInfo.newBuilder().setParams(params))\n                                )\n                        )\n                        .build()\n        );\n\n        if (postModelOutputsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("Post model outputs failed, status: " + postModelOutputsResponse.getStatus());\n        }\n\n        // Since we have one input, one output will exist here\n        Output output = postModelOutputsResponse.getOutputs(0);\n\n        System.out.println(output.getData().getText().getRaw());\n\n    }\n}\n',d='<?php\n\nrequire __DIR__ . "/vendor/autoload.php";\n\n///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, and input details.\n// Change these values to run your own example.\n///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\n$PAT = "YOUR_PAT_HERE";\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\n$USER_ID = "openai";\n$APP_ID = "chat-completion";\n// Change these to whatever model and inputs you want to use\n$MODEL_ID = "openai-gpt-4-vision";\n$MODEL_VERSION_ID = "266df29bc09843e0aee9b7bf723c03c2";\n$RAW_TEXT = "Write a caption for the image";\n// To use a hosted text file, assign the URL variable\n// $TEXT_FILE_URL = "https://samples.clarifai.com/negative_sentence_12.txt";\n// Or, to use a local text file, assign the location variable\n// $TEXT_FILE_LOCATION = "YOUR_TEXT_FILE_LOCATION_HERE";\n$IMAGE_URL = "https://samples.clarifai.com/metro-north.jpg";\n// To use a local file, assign the location variable\n// $IMAGE_FILE_LOCATION = "YOUR_IMAGE_FILE_LOCATION_HERE";\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Text;\nuse Clarifai\\Api\\Image;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\Model;\nuse Clarifai\\Api\\ModelVersion;\nuse Clarifai\\Api\\OutputInfo;\nuse Clarifai\\Api\\PostModelOutputsRequest;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\nuse Google\\Protobuf\\Struct;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ["Authorization" => ["Key " . $PAT]];\n\n$userDataObject = new UserAppIDSet([\n    "user_id" => $USER_ID,\n    "app_id" => $APP_ID,\n]);\n\n// Create Struct instance\n$params = new Struct();\n$params->temperature = 0.5;\n$params->max_tokens = 2048;\n$params->top_p = 0.95;\n// $params->api_key = "ADD_THIRD_PARTY_KEY_HERE";\n\n// To use a local text file, uncomment the following lines\n// $textData = file_get_contents($TEXT_FILE_LOCATION); // Get the text bytes data from the location\n\n// To use a local image file, uncomment the following lines\n// $imageData = file_get_contents($IMAGE_FILE_LOCATION);\n\n// Let\'s make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client->PostModelOutputs(\n    // The request object carries the request along with the request status and other metadata related to the request itself\n    new PostModelOutputsRequest([\n        "user_app_id" => $userDataObject,\n        "model_id" => $MODEL_ID,\n        "version_id" => $MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n        "inputs" => [\n            new Input([\n                // The Input object wraps the Data object in order to meet the API specification\n                "data" => new Data([\n                    // The Data object is constructed around the Text object. It offers a container that has additional text independent\n                    // metadata. In this particular use case, no other metadata is needed to be specified\n                    "text" => new Text([\n                        // In the Clarifai platform, a text is defined by a special Text object\n                        "raw" => $RAW_TEXT,\n                        // "url" => $TEXT_FILE_URL\n                        // "raw" => $textData\n                    ]),\n                    "image" => new Image([\n                        // In the Clarifai platform, an image is defined by a special Image object\n                        "url" => $IMAGE_URL,\n                        // "base64" => $imageData,\n                    ]),\n                ]),\n            ]),\n        ],\n        "model" => new Model([\n            "model_version" => new ModelVersion([\n                "output_info" => new OutputInfo(["params" => $params]),\n            ]),\n        ]),\n    ]),\n    $metadata\n)->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception("Error: {$status->details}");\n}\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure\n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    throw new Exception("Failure response: " . $response->getStatus()->getDescription() . " " . $response->getStatus()->getDetails() );\n}\n\n# Since we have one input, one output will exist here\necho $response->getOutputs()[0]->getData()->getText()->getRaw();\n\n?>',m='curl -X POST "https://api.clarifai.com/v2/users/openai/apps/chat-completion/models/openai-gpt-4-vision/versions/266df29bc09843e0aee9b7bf723c03c2/outputs" \\\n    -H "Authorization: Key YOUR_PAT_HERE" \\\n    -H "Content-Type: application/json" \\\n    -d \'{\n    "inputs": [\n        {\n            "data": {\n                "text": {\n                    "raw": "Write a caption for the image"\n                },\n                "image": {\n                    "url": "https://samples.clarifai.com/metro-north.jpg"\n                }\n            }\n        }\n    ],\n    "model": {\n        "model_version": {\n            "output_info": {\n                "params": {\n                    "temperature": 0.5,\n                    "max_tokens": 2048,\n                    "top_p": 0.95,\n                    "api_key": "ADD_THIRD_PARTY_KEY_HERE"\n                }\n            }\n        }\n    }\n}\'\n',_='"Early morning solitude: A lone traveler waits on a snowy platform as dawn breaks."',h={description:"Make multimodal-to-text predictions",sidebar_position:6},f="Multimodal-to-Text",A={unversionedId:"api-guide/predict/multimodal-to-text",id:"api-guide/predict/multimodal-to-text",title:"Multimodal-to-Text",description:"Make multimodal-to-text predictions",source:"@site/docs/api-guide/predict/multimodal-to-text.md",sourceDirName:"api-guide/predict",slug:"/api-guide/predict/multimodal-to-text",permalink:"/api-guide/predict/multimodal-to-text",draft:!1,editUrl:"https://github.com/Clarifai/docs/blob/main/docs/api-guide/predict/multimodal-to-text.md",tags:[],version:"current",sidebarPosition:6,frontMatter:{description:"Make multimodal-to-text predictions",sidebar_position:6},sidebar:"tutorialSidebar",previous:{title:"Audio",permalink:"/api-guide/predict/audio"},next:{title:"Embeddings",permalink:"/api-guide/predict/embeddings"}},E={},T=[],I={toc:T},g="wrapper";function b(e){let{components:t,...n}=e;return(0,o.kt)(g,(0,a.Z)({},I,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"multimodal-to-text"},"Multimodal-to-Text"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Make multimodal-to-text predictions")),(0,o.kt)("hr",null),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Input"),": Text and images, etc"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Output"),": Text"),(0,o.kt)("p",null,'Multimodal-to-text models allow you to generate textual descriptions or responses from multimodal inputs. "Multimodal" refers to the integration of information from multiple modalities, such as text, images, and/or other types of data.'),(0,o.kt)("p",null,"A multimodal-to-text model might take as input a combination of textual data and images and generate a descriptive text that captures the content of both modalities. It can comprehend and generate a human-like response that encompasses multiple types of information. "),(0,o.kt)("admonition",{type:"info"},(0,o.kt)("p",{parentName:"admonition"},"The initialization code used in the following examples is outlined in detail on the ",(0,o.kt)("a",{parentName:"p",href:"https://docs.clarifai.com/api-guide/api-overview/api-clients/#client-installation-instructions"},"client installation page."))),(0,o.kt)("admonition",{type:"note"},(0,o.kt)("p",{parentName:"admonition"},"You can use ",(0,o.kt)("a",{parentName:"p",href:"https://docs.clarifai.com/api-guide/predict/llms#use-hyperparameters-to-customize-llms"},"hyperparameters")," to customize the behavior of the models. You can also utilize the ",(0,o.kt)("a",{parentName:"p",href:"https://docs.clarifai.com/api-guide/predict/text#use-third-party-api-keys"},"API keys")," from a third-party model provider as an option\u2014in addition to using the default Clarifai keys. ")),(0,o.kt)("p",null,"Below is an example of how you would make a multimodal-to-text prediction using the ",(0,o.kt)("a",{parentName:"p",href:"https://clarifai.com/openai/chat-completion/models/openai-gpt-4-vision"},"GPT-4 Vision")," model. "),(0,o.kt)(i.Z,{mdxType:"Tabs"},(0,o.kt)(s.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,o.kt)(r.Z,{className:"language-python",mdxType:"CodeBlock"},l)),(0,o.kt)(s.Z,{value:"js_rest",label:"JavaScript (REST)",mdxType:"TabItem"},(0,o.kt)(r.Z,{className:"language-javascript",mdxType:"CodeBlock"},u)),(0,o.kt)(s.Z,{value:"nodejs",label:"NodeJS",mdxType:"TabItem"},(0,o.kt)(r.Z,{className:"language-javascript",mdxType:"CodeBlock"},p)),(0,o.kt)(s.Z,{value:"java",label:"Java",mdxType:"TabItem"},(0,o.kt)(r.Z,{className:"language-java",mdxType:"CodeBlock"},c)),(0,o.kt)(s.Z,{value:"php",label:"PHP",mdxType:"TabItem"},(0,o.kt)(r.Z,{className:"language-php",mdxType:"CodeBlock"},d)),(0,o.kt)(s.Z,{value:"curl",label:"cURL",mdxType:"TabItem"},(0,o.kt)(r.Z,{className:"language-bash",mdxType:"CodeBlock"},m))),(0,o.kt)("details",null,(0,o.kt)("summary",null,"Text Output Example"),(0,o.kt)(r.Z,{className:"language-text",mdxType:"CodeBlock"},_)))}b.isMDXComponent=!0}}]);
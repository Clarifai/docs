"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[6257],{38233:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>f,contentTitle:()=>m,default:()=>x,frontMatter:()=>p,metadata:()=>r,toc:()=>h});const r=JSON.parse('{"id":"getting-started/upload-model","title":"Upload Your First Model","description":"Learn how to upload your model to the Clarifai platform","source":"@site/docs/getting-started/upload-model.md","sourceDirName":"getting-started","slug":"/getting-started/upload-model","permalink":"/getting-started/upload-model","draft":false,"unlisted":false,"editUrl":"https://github.com/Clarifai/docs/blob/main/docs/getting-started/upload-model.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"description":"Learn how to upload your model to the Clarifai platform","sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Deploy Your First Model","permalink":"/getting-started/first-deployment"},"next":{"title":"Compute Orchestration","permalink":"/compute/overview"}}');var a=n(74848),s=n(28453),o=n(65537),i=n(79329),l=n(58069);const d='import os\nfrom threading import Thread\nfrom typing import Iterator, List, Optional\n\nimport torch\nfrom clarifai.runners.models.model_class import ModelClass\nfrom clarifai.runners.models.model_builder import ModelBuilder\nfrom clarifai.utils.logging import logger\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2\nfrom clarifai_grpc.grpc.api.status import status_code_pb2, status_pb2\nfrom google.protobuf import json_format\nfrom transformers import (AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer)\n\n\n# Custom streamer for batched text generation\nclass BatchTextIteratorStreamer(TextIteratorStreamer):\n  """A custom streamer that handles batched text generation."""\n\n  def __init__(self,\n               batch_size: int,\n               tokenizer: "AutoTokenizer",\n               skip_prompt: bool = False,\n               timeout: Optional[float] = None,\n               **decode_kwargs):\n    super().__init__(tokenizer, skip_prompt, timeout, **decode_kwargs)\n    self.batch_size = batch_size\n    self.token_cache = [[] for _ in range(batch_size)]\n    self.print_len = [0 for _ in range(batch_size)]\n    self.generate_exception = None\n\n  def put(self, value):\n    if len(value.shape) != 2:\n      value = torch.reshape(value, (self.batch_size, value.shape[0] // self.batch_size))\n\n    if self.skip_prompt and self.next_tokens_are_prompt:\n      self.next_tokens_are_prompt = False\n      return\n\n    printable_texts = list()\n    for idx in range(self.batch_size):\n      self.token_cache[idx].extend(value[idx].tolist())\n      text = self.tokenizer.decode(self.token_cache[idx], **self.decode_kwargs)\n\n      if text.endswith("\\n"):\n        printable_text = text[self.print_len[idx]:]\n        self.token_cache[idx] = []\n        self.print_len[idx] = 0\n        # If the last token is a CJK character, we print the characters.\n      elif len(text) > 0 and self._is_chinese_char(ord(text[-1])):\n        printable_text = text[self.print_len[idx]:]\n        self.print_len[idx] += len(printable_text)\n      else:\n        printable_text = text[self.print_len[idx]:text.rfind(" ") + 1]\n        self.print_len[idx] += len(printable_text)\n      printable_texts.append(printable_text)\n\n    self.on_finalized_text(printable_texts)\n\n  def end(self):\n    printable_texts = list()\n    for idx in range(self.batch_size):\n      if len(self.token_cache[idx]) > 0:\n        text = self.tokenizer.decode(self.token_cache[idx], **self.decode_kwargs)\n        printable_text = text[self.print_len[idx]:]\n        self.token_cache[idx] = []\n        self.print_len[idx] = 0\n      else:\n        printable_text = ""\n      printable_texts.append(printable_text)\n\n    self.next_tokens_are_prompt = True\n    self.on_finalized_text(printable_texts, stream_end=True)\n\n  def on_finalized_text(self, texts: List[str], stream_end: bool = False):\n    self.text_queue.put(texts, timeout=self.timeout)\n    if stream_end:\n      self.text_queue.put(self.stop_signal, timeout=self.timeout)\n\n\n# Helper function to create an output\ndef create_output(text="", code=status_code_pb2.SUCCESS):\n  return resources_pb2.Output(\n      data=resources_pb2.Data(text=resources_pb2.Text(raw=text)),\n      status=status_pb2.Status(code=code))\n\n\n# Helper function to get the inference params\ndef get_inference_params(request) -> dict:\n  """Get the inference params from the request."""\n  inference_params = {}\n  if request.model.model_version.id != "":\n    output_info = request.model.model_version.output_info\n    output_info = json_format.MessageToDict(output_info, preserving_proto_field_name=True)\n    if "params" in output_info:\n      inference_params = output_info["params"]\n  return inference_params\n\n\n# Helper function to parse the inference params\ndef parse_inference_params(request):\n  default_params = {\n      "temperature": 0.7,\n      "max_tokens": 100,\n      "top_k": 50,\n      "top_p": 1.0,\n      "do_sample": True,\n  }\n  inference_params = get_inference_params(request)\n  return {\n      "temperature": inference_params.get("temperature", default_params["temperature"]),\n      "max_tokens": int(inference_params.get("max_tokens", default_params["max_tokens"])),\n      "top_k": int(inference_params.get("top_k", default_params["top_k"])),\n      "top_p": inference_params.get("top_p", default_params["top_p"]),\n      "do_sample": inference_params.get("do_sample", default_params["do_sample"]),\n  }\n\n\nclass MyModel(ModelClass):\n  """A custom runner that loads the model and generates text using batched inference."""\n\n  def load_model(self):\n    """Load the model here."""\n    self.device = \'cuda\' if torch.cuda.is_available() else \'cpu\'\n    logger.info(f"Running on device: {self.device}")\n\n    # Load checkpoints\n    model_path = os.path.dirname(os.path.dirname(__file__))\n    builder = ModelBuilder(model_path, download_validation_only=True)\n    checkpoints = builder.download_checkpoints(stage="runtime")\n    \n    # Load model and tokenizer\n    self.tokenizer = AutoTokenizer.from_pretrained(checkpoints,)\n    self.tokenizer.pad_token = self.tokenizer.eos_token  # Set pad token to eos token\n    self.model = AutoModelForCausalLM.from_pretrained(\n        checkpoints,\n        low_cpu_mem_usage=True,\n        device_map=self.device,\n        torch_dtype=torch.bfloat16,\n    )\n    logger.info("Done loading!")\n\n  def predict(self,\n              request: service_pb2.PostModelOutputsRequest) -> service_pb2.MultiOutputResponse:\n    """This method generates outputs text for the given inputs using the model."""\n\n    inference_params = parse_inference_params(request)\n\n    prompts = [inp.data.text.raw for inp in request.inputs]\n    inputs = self.tokenizer(prompts, return_tensors="pt", padding=True).to(self.device)\n\n    output_tokens = self.model.generate(\n        **inputs,\n        max_new_tokens=inference_params["max_tokens"],\n        do_sample=inference_params["do_sample"],\n        temperature=inference_params["temperature"],\n        top_k=inference_params["top_k"],\n        top_p=inference_params["top_p"],\n        eos_token_id=self.tokenizer.eos_token_id,\n    )\n\n    outputs_text = self.tokenizer.batch_decode(\n        output_tokens[:, inputs[\'input_ids\'].shape[1]:], skip_special_tokens=True)\n\n    outputs = []\n    for text in outputs_text:\n      outputs.append(create_output(text=text, code=status_code_pb2.SUCCESS))\n\n    return service_pb2.MultiOutputResponse(\n        outputs=outputs, status=status_pb2.Status(code=status_code_pb2.SUCCESS))\n\n  def generate(self, request: service_pb2.PostModelOutputsRequest\n              ) -> Iterator[service_pb2.MultiOutputResponse]:\n    """This method generates stream of outputs for the given batch of inputs using the model."""\n    inference_params = parse_inference_params(request)\n\n    prompts = [inp.data.text.raw for inp in request.inputs]\n    batch_size = len(prompts)\n\n    # Initialize the custom streamer\n    streamer = BatchTextIteratorStreamer(\n        batch_size=batch_size,\n        tokenizer=self.tokenizer,\n        skip_prompt=True,\n        decode_kwargs={\n            "skip_special_tokens": True\n        })\n\n    # Tokenize the inputs\n    inputs = self.tokenizer(prompts, return_tensors="pt", padding=True).to(self.device)\n\n    generation_kwargs = {\n        "input_ids": inputs.input_ids,\n        "attention_mask": inputs.attention_mask,\n        "max_new_tokens": inference_params["max_tokens"],\n        "do_sample": inference_params["do_sample"],\n        "temperature": inference_params["temperature"],\n        "top_k": inference_params["top_k"],\n        "top_p": inference_params["top_p"],\n        "eos_token_id": self.tokenizer.eos_token_id,\n        "streamer": streamer,\n    }\n\n    # Start generation in a separate thread\n    thread = Thread(target=self.model.generate, kwargs=generation_kwargs)\n    thread.start()\n\n    # Initialize outputs\n    outputs = [create_output() for _ in range(batch_size)]\n\n    try:\n      for streamed_texts in streamer:  # Iterate over new texts generated\n        for idx, text in enumerate(streamed_texts):  # Iterate over each batch\n          outputs[idx].data.text.raw = text  # Append new text to each output\n          outputs[idx].status.code = status_code_pb2.SUCCESS\n        # Yield the current outputs\n        yield service_pb2.MultiOutputResponse(\n            outputs=outputs, status=status_pb2.Status(code=status_code_pb2.SUCCESS))\n    finally:\n      thread.join()\n\n  def stream(self, request_iterator: Iterator[service_pb2.PostModelOutputsRequest]\n            ) -> Iterator[service_pb2.MultiOutputResponse]:\n    raise NotImplementedError("Stream method is not implemented for the models.")',u='# This is the sample config file for the llama model.\n\nmodel:\n  id: "llama_3_2_1b_instruct"\n  user_id: "user_id"\n  app_id: "app_id"\n  model_type_id: "text-to-text"\n\nbuild_info:\n  python_version: "3.11"\n\ninference_compute_info:\n  cpu_limit: "1"\n  cpu_memory: "13Gi"\n  num_accelerators: 1\n  accelerator_type: ["NVIDIA*"]\n  accelerator_memory: "18Gi"\n\ncheckpoints:\n  type: "huggingface"\n  repo_id: "meta-llama/Llama-3.2-1B-Instruct"\n  when: "runtime"\n  hf_token: "hf_token"\n',c="torch==2.5.1\ntokenizers>=0.21.0\ntransformers>=4.47.0\naccelerate>=1.2.0\nscipy==1.10.1\noptimum>=1.23.3\nxformers==0.0.28.post3\nprotobuf==5.27.3\neinops>=0.8.0\nrequests==2.32.3\nclarifai",p={description:"Learn how to upload your model to the Clarifai platform",sidebar_position:3},m="Upload Your First Model",f={},h=[{value:"Step 1: Perform Prerequisites",id:"step-1-perform-prerequisites",level:2},{value:"Install Clarifai Package",id:"install-clarifai-package",level:3},{value:"Set a PAT Key",id:"set-a-pat-key",level:3},{value:"Step 2: Create Files",id:"step-2-create-files",level:2},{value:"<code>model.py</code>",id:"modelpy",level:3},{value:"<code>requirements.txt</code>",id:"requirementstxt",level:3},{value:"<code>config.yaml</code>",id:"configyaml",level:3},{value:"Step 3: Upload the Model",id:"step-3-upload-the-model",level:2}];function _(e){const t={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.header,{children:(0,a.jsx)(t.h1,{id:"upload-your-first-model",children:"Upload Your First Model"})}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.strong,{children:"Learn how to upload your model to the Clarifai platform"})}),"\n",(0,a.jsx)("hr",{}),"\n",(0,a.jsx)(t.p,{children:"The Clarifai platform allows you to upload custom models for a wide range of use cases. With just a few simple steps, you can get your models up and running and leverage the platform\u2019s powerful capabilities."}),"\n",(0,a.jsxs)(t.p,{children:["Let's demonstrate how you can upload the ",(0,a.jsx)(t.a,{href:"https://github.com/Clarifai/examples/tree/main/models/model_upload/llms/llama-3_2-1b-instruct",children:"Llama-3_2-1B-Instruct"})," model from Hugging Face to the Clarifai platform."]}),"\n",(0,a.jsx)(t.admonition,{type:"tip",children:(0,a.jsxs)(t.p,{children:["To learn more about how to upload different types of models, check out other comprehensive guides ",(0,a.jsx)(t.a,{href:"https://docs.clarifai.com/compute/models/model-upload/",children:"here"})," and ",(0,a.jsx)(t.a,{href:"https://github.com/Clarifai/examples/tree/main/models/model_upload",children:"here"}),"."]})}),"\n","\n","\n",(0,a.jsx)(t.h2,{id:"step-1-perform-prerequisites",children:"Step 1: Perform Prerequisites"}),"\n",(0,a.jsx)(t.h3,{id:"install-clarifai-package",children:"Install Clarifai Package"}),"\n",(0,a.jsxs)(t.p,{children:["Install the latest version of the ",(0,a.jsx)(t.code,{children:"clarifai"})," Python SDK. This also installs the Clarifai ",(0,a.jsx)(t.a,{href:"https://docs.clarifai.com/additional-resources/api-overview/cli",children:"Command Line Interface (CLI)"}),", which we'll use for uploading the model."]}),"\n",(0,a.jsx)(o.A,{children:(0,a.jsx)(i.A,{value:"bash",label:"Bash",children:(0,a.jsx)(l.A,{className:"language-bash",children:" pip install --upgrade clarifai "})})}),"\n",(0,a.jsx)(t.h3,{id:"set-a-pat-key",children:"Set a PAT Key"}),"\n",(0,a.jsxs)(t.p,{children:["You need to set the ",(0,a.jsx)(t.code,{children:"CLARIFAI_PAT"})," (Personal Access Token) as an environment variable. You can generate the PAT key in your personal settings page by navigating to the ",(0,a.jsx)(t.a,{href:"https://clarifai.com/settings/security",children:"Security section"}),"."]}),"\n",(0,a.jsx)(t.p,{children:"This token is essential for authenticating your connection to the Clarifai platform."}),"\n",(0,a.jsx)(o.A,{children:(0,a.jsx)(i.A,{value:"bash",label:"Bash",children:(0,a.jsx)(l.A,{className:"language-bash",children:" export CLARIFAI_PAT=YOUR_PERSONAL_ACCESS_TOKEN_HERE "})})}),"\n",(0,a.jsx)(t.h2,{id:"step-2-create-files",children:"Step 2: Create Files"}),"\n",(0,a.jsx)(t.p,{children:"Create a project directory and organize your files as indicated below to fit the requirements of uploading models to the Clarifai platform."}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-text",children:"your_model_directory/\n\u251c\u2500\u2500 1/\n\u2502   \u2514\u2500\u2500 model.py\n\u251c\u2500\u2500 requirements.txt\n\u2514\u2500\u2500 config.yaml\n"})}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"your_model_directory/"})," \u2013 The main directory containing your model files.","\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"1/"})," \u2013 A subdirectory that holds the model file (",(0,a.jsxs)(t.em,{children:["Note that the folder is named as ",(0,a.jsx)(t.strong,{children:"1"})]}),").","\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"model.py"})," \u2013 Contains the code that defines your model, including loading the model and running inference."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"requirements.txt"})," \u2013 Lists the Python libraries and dependencies required to run your model."]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"config.yaml"})," \u2013 Contains model metadata and configuration details necessary for building the Docker image, defining compute resources, and uploading the model to Clarifai."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(t.p,{children:"Add the following snippets to each of the respective files."}),"\n",(0,a.jsx)(t.h3,{id:"modelpy",children:(0,a.jsx)(t.code,{children:"model.py"})}),"\n",(0,a.jsx)(o.A,{children:(0,a.jsx)(i.A,{value:"python",label:"Python",children:(0,a.jsx)(l.A,{className:"language-python",children:d})})}),"\n",(0,a.jsx)(t.h3,{id:"requirementstxt",children:(0,a.jsx)(t.code,{children:"requirements.txt"})}),"\n",(0,a.jsx)(o.A,{children:(0,a.jsx)(i.A,{value:"text",label:"Text",children:(0,a.jsx)(l.A,{className:"language-text",children:c})})}),"\n",(0,a.jsx)(t.h3,{id:"configyaml",children:(0,a.jsx)(t.code,{children:"config.yaml"})}),"\n",(0,a.jsx)(t.admonition,{type:"note",children:(0,a.jsxs)(t.p,{children:["In the ",(0,a.jsx)(t.code,{children:"model"})," section of the ",(0,a.jsx)(t.code,{children:"config.yaml"})," file, specify your model ID, Clarifai user ID, and Clarifai app ID. These will define where your model will be uploaded on the Clarifai platform. Also, specify ",(0,a.jsx)(t.a,{href:"https://huggingface.co/docs/hub/en/security-tokens",children:(0,a.jsx)(t.code,{children:"hf_token"})})," to authenticate your connection to Hugging Face services."]})}),"\n",(0,a.jsx)(o.A,{children:(0,a.jsx)(i.A,{value:"yaml",label:"YAML",children:(0,a.jsx)(l.A,{className:"language-yaml",children:u})})}),"\n",(0,a.jsx)(t.h2,{id:"step-3-upload-the-model",children:"Step 3: Upload the Model"}),"\n",(0,a.jsx)(t.p,{children:"Once your custom model is ready, upload it to the Clarifai platform by navigating to its directory and running the following command:"}),"\n",(0,a.jsx)(o.A,{children:(0,a.jsx)(i.A,{value:"bash",label:"CLI",children:(0,a.jsx)(l.A,{className:"language-bash",children:" clarifai model upload "})})}),"\n",(0,a.jsx)(t.p,{children:"Congratulations \u2014 you've just uploaded your first model to the Clarifai platform!"}),"\n",(0,a.jsxs)(t.p,{children:["Now, you can ",(0,a.jsx)(t.a,{href:"https://docs.clarifai.com/compute/deployments/deploy-model",children:"deploy"})," the model to a cluster and nodepool. This allows you to cost-efficiently and scalably make ",(0,a.jsx)(t.a,{href:"https://docs.clarifai.com/compute/models/model-inference",children:"inferences"})," with it."]})]})}function x(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(_,{...e})}):_(e)}},65537:(e,t,n)=>{n.d(t,{A:()=>y});var r=n(96540),a=n(18215),s=n(65627),o=n(56347),i=n(50372),l=n(30604),d=n(11861),u=n(78749);function c(e){return r.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,r.isValidElement)(e)&&function(e){const{props:t}=e;return!!t&&"object"==typeof t&&"value"in t}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function p(e){const{values:t,children:n}=e;return(0,r.useMemo)((()=>{const e=t??function(e){return c(e).map((e=>{let{props:{value:t,label:n,attributes:r,default:a}}=e;return{value:t,label:n,attributes:r,default:a}}))}(n);return function(e){const t=(0,d.XI)(e,((e,t)=>e.value===t.value));if(t.length>0)throw new Error(`Docusaurus error: Duplicate values "${t.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[t,n])}function m(e){let{value:t,tabValues:n}=e;return n.some((e=>e.value===t))}function f(e){let{queryString:t=!1,groupId:n}=e;const a=(0,o.W6)(),s=function(e){let{queryString:t=!1,groupId:n}=e;if("string"==typeof t)return t;if(!1===t)return null;if(!0===t&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:t,groupId:n});return[(0,l.aZ)(s),(0,r.useCallback)((e=>{if(!s)return;const t=new URLSearchParams(a.location.search);t.set(s,e),a.replace({...a.location,search:t.toString()})}),[s,a])]}function h(e){const{defaultValue:t,queryString:n=!1,groupId:a}=e,s=p(e),[o,l]=(0,r.useState)((()=>function(e){let{defaultValue:t,tabValues:n}=e;if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(t){if(!m({value:t,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${t}" but none of its children has the corresponding value. Available values are: ${n.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return t}const r=n.find((e=>e.default))??n[0];if(!r)throw new Error("Unexpected error: 0 tabValues");return r.value}({defaultValue:t,tabValues:s}))),[d,c]=f({queryString:n,groupId:a}),[h,_]=function(e){let{groupId:t}=e;const n=function(e){return e?`docusaurus.tab.${e}`:null}(t),[a,s]=(0,u.Dv)(n);return[a,(0,r.useCallback)((e=>{n&&s.set(e)}),[n,s])]}({groupId:a}),x=(()=>{const e=d??h;return m({value:e,tabValues:s})?e:null})();(0,i.A)((()=>{x&&l(x)}),[x]);return{selectedValue:o,selectValue:(0,r.useCallback)((e=>{if(!m({value:e,tabValues:s}))throw new Error(`Can't select invalid tab value=${e}`);l(e),c(e),_(e)}),[c,_,s]),tabValues:s}}var _=n(9136);const x={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var g=n(74848);function b(e){let{className:t,block:n,selectedValue:r,selectValue:o,tabValues:i}=e;const l=[],{blockElementScrollPositionUntilNextRender:d}=(0,s.a_)(),u=e=>{const t=e.currentTarget,n=l.indexOf(t),a=i[n].value;a!==r&&(d(t),o(a))},c=e=>{let t=null;switch(e.key){case"Enter":u(e);break;case"ArrowRight":{const n=l.indexOf(e.currentTarget)+1;t=l[n]??l[0];break}case"ArrowLeft":{const n=l.indexOf(e.currentTarget)-1;t=l[n]??l[l.length-1];break}}t?.focus()};return(0,g.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,a.A)("tabs",{"tabs--block":n},t),children:i.map((e=>{let{value:t,label:n,attributes:s}=e;return(0,g.jsx)("li",{role:"tab",tabIndex:r===t?0:-1,"aria-selected":r===t,ref:e=>{l.push(e)},onKeyDown:c,onClick:u,...s,className:(0,a.A)("tabs__item",x.tabItem,s?.className,{"tabs__item--active":r===t}),children:n??t},t)}))})}function k(e){let{lazy:t,children:n,selectedValue:s}=e;const o=(Array.isArray(n)?n:[n]).filter(Boolean);if(t){const e=o.find((e=>e.props.value===s));return e?(0,r.cloneElement)(e,{className:(0,a.A)("margin-top--md",e.props.className)}):null}return(0,g.jsx)("div",{className:"margin-top--md",children:o.map(((e,t)=>(0,r.cloneElement)(e,{key:t,hidden:e.props.value!==s})))})}function v(e){const t=h(e);return(0,g.jsxs)("div",{className:(0,a.A)("tabs-container",x.tabList),children:[(0,g.jsx)(b,{...t,...e}),(0,g.jsx)(k,{...t,...e})]})}function y(e){const t=(0,_.A)();return(0,g.jsx)(v,{...e,children:c(e.children)},String(t))}},79329:(e,t,n)=>{n.d(t,{A:()=>o});n(96540);var r=n(18215);const a={tabItem:"tabItem_Ymn6"};var s=n(74848);function o(e){let{children:t,hidden:n,className:o}=e;return(0,s.jsx)("div",{role:"tabpanel",className:(0,r.A)(a.tabItem,o),hidden:n,children:t})}}}]);
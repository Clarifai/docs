"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[3410],{11470:(e,n,t)=>{t.d(n,{A:()=>I});var s=t(96540),r=t(18215),i=t(17559),o=t(23104),a=t(56347),l=t(205),c=t(57485),d=t(31682),h=t(70679);function u(e){return s.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,s.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function p(e){const{values:n,children:t}=e;return(0,s.useMemo)(()=>{const e=n??function(e){return u(e).map(({props:{value:e,label:n,attributes:t,default:s}})=>({value:e,label:n,attributes:t,default:s}))}(t);return function(e){const n=(0,d.XI)(e,(e,n)=>e.value===n.value);if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[n,t])}function m({value:e,tabValues:n}){return n.some(n=>n.value===e)}function f({queryString:e=!1,groupId:n}){const t=(0,a.W6)(),r=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,c.aZ)(r),(0,s.useCallback)(e=>{if(!r)return;const n=new URLSearchParams(t.location.search);n.set(r,e),t.replace({...t.location,search:n.toString()})},[r,t])]}function g(e){const{defaultValue:n,queryString:t=!1,groupId:r}=e,i=p(e),[o,a]=(0,s.useState)(()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!m({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const t=n.find(e=>e.default)??n[0];if(!t)throw new Error("Unexpected error: 0 tabValues");return t.value}({defaultValue:n,tabValues:i})),[c,d]=f({queryString:t,groupId:r}),[u,g]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[t,r]=(0,h.Dv)(n);return[t,(0,s.useCallback)(e=>{n&&r.set(e)},[n,r])]}({groupId:r}),x=(()=>{const e=c??u;return m({value:e,tabValues:i})?e:null})();(0,l.A)(()=>{x&&a(x)},[x]);return{selectedValue:o,selectValue:(0,s.useCallback)(e=>{if(!m({value:e,tabValues:i}))throw new Error(`Can't select invalid tab value=${e}`);a(e),d(e),g(e)},[d,g,i]),tabValues:i}}var x=t(92303);const _={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var y=t(74848);function v({className:e,block:n,selectedValue:t,selectValue:s,tabValues:i}){const a=[],{blockElementScrollPositionUntilNextRender:l}=(0,o.a_)(),c=e=>{const n=e.currentTarget,r=a.indexOf(n),o=i[r].value;o!==t&&(l(n),s(o))},d=e=>{let n=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const t=a.indexOf(e.currentTarget)+1;n=a[t]??a[0];break}case"ArrowLeft":{const t=a.indexOf(e.currentTarget)-1;n=a[t]??a[a.length-1];break}}n?.focus()};return(0,y.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,r.A)("tabs",{"tabs--block":n},e),children:i.map(({value:e,label:n,attributes:s})=>(0,y.jsx)("li",{role:"tab",tabIndex:t===e?0:-1,"aria-selected":t===e,ref:e=>{a.push(e)},onKeyDown:d,onClick:c,...s,className:(0,r.A)("tabs__item",_.tabItem,s?.className,{"tabs__item--active":t===e}),children:n??e},e))})}function b({lazy:e,children:n,selectedValue:t}){const i=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=i.find(e=>e.props.value===t);return e?(0,s.cloneElement)(e,{className:(0,r.A)("margin-top--md",e.props.className)}):null}return(0,y.jsx)("div",{className:"margin-top--md",children:i.map((e,n)=>(0,s.cloneElement)(e,{key:n,hidden:e.props.value!==t}))})}function j(e){const n=g(e);return(0,y.jsxs)("div",{className:(0,r.A)(i.G.tabs.container,"tabs-container",_.tabList),children:[(0,y.jsx)(v,{...n,...e}),(0,y.jsx)(b,{...n,...e})]})}function I(e){const n=(0,x.A)();return(0,y.jsx)(j,{...e,children:u(e.children)},String(n))}},19365:(e,n,t)=>{t.d(n,{A:()=>o});t(96540);var s=t(18215);const r={tabItem:"tabItem_Ymn6"};var i=t(74848);function o({children:e,hidden:n,className:t}){return(0,i.jsx)("div",{role:"tabpanel",className:(0,s.A)(r.tabItem,t),hidden:n,children:e})}},81971:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>v,contentTitle:()=>y,default:()=>I,frontMatter:()=>_,metadata:()=>s,toc:()=>b});const s=JSON.parse('{"id":"compute/toolkits/sglang","title":"SGLang","description":"Run models using the SGLang runtime format and make them available via a public API","source":"@site/docs/compute/toolkits/sglang.md","sourceDirName":"compute/toolkits","slug":"/compute/toolkits/sglang","permalink":"/compute/toolkits/sglang","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"description":"Run models using the SGLang runtime format and make them available via a public API","sidebar_position":6},"sidebar":"tutorialSidebar","previous":{"title":"vLLM","permalink":"/compute/toolkits/vllm"},"next":{"title":"Agents","permalink":"/compute/agents/"}}');var r=t(74848),i=t(28453),o=t(11470),a=t(19365),l=t(88149);const c='import os\nimport sys\n\nsys.path.append(os.path.dirname(__file__))\nfrom typing import Iterator, List\n\nfrom clarifai.runners.models.model_builder import ModelBuilder\nfrom clarifai.runners.models.openai_class import OpenAIModelClass\nfrom clarifai.runners.utils.data_utils import Param\nfrom clarifai.runners.utils.openai_convertor import build_openai_messages\nfrom clarifai.utils.logging import logger\nfrom openai import OpenAI\nfrom openai_server_starter import OpenAI_APIServer\n##################\n\n\n\nclass SglangModel(OpenAIModelClass):\n    """\n    A custom runner that integrates with the Clarifai platform and uses Server inference\n    to process inputs, including text.\n    """\n\n    client = True  # This will be set in load_model method\n    model = True  # This will be set in load_model method\n\n    def load_model(self):\n        """Load the model here and start the  server."""\n        os.path.join(os.path.dirname(__file__))\n    \n        # Use downloaded checkpoints.\n        # Or if you intend to download checkpoint at runtime, set hf id instead. For example:\n        # checkpoints = "Qwen/Qwen2-7B-Instruct"\n\n        # server args were generated by `upload` module\n        server_args = {\n            \'dtype\': \'auto\',\n            \'kv_cache_dtype\': \'auto\',\n            \'tp_size\': 1,\n            \'load_format\': \'auto\',\n            \'context_length\': None,\n            \'device\': \'cuda\',\n            \'port\': 23333,\n            \'host\': \'0.0.0.0\',\n            \'mem_fraction_static\': 0.9,\n            \'max_total_tokens\': \'8192\',\n            \'max_prefill_tokens\': None,\n            \'schedule_policy\': \'fcfs\',\n            \'schedule_conservativeness\': 1.0,\n            \'checkpoints\': \'runtime\',\n        }\n\n        # if checkpoints == "checkpoints" => assign to checkpoints var aka local checkpoints path\n        stage = server_args.get("checkpoints")\n        if stage in ["build", "runtime"]:\n            # checkpoints = os.path.join(os.path.dirname(__file__), "checkpoints")\n            config_path = os.path.dirname(os.path.dirname(__file__))\n            builder = ModelBuilder(config_path, download_validation_only=True)\n            checkpoints = builder.download_checkpoints(stage=stage)\n            server_args.update({"checkpoints": checkpoints})\n\n        if server_args.get("additional_list_args") == [\'\']:\n            server_args.pop("additional_list_args")\n\n        # Start server\n        # This line were generated by `upload` module\n        self.server = OpenAI_APIServer.from_sglang_backend(**server_args)\n\n        # Create client\n        self.client = OpenAI(\n            api_key="notset", base_url=SglangModel.make_api_url(self.server.host, self.server.port)\n        )\n        self.model = self._get_model()\n\n        logger.info(f"OpenAI {self.model} model loaded successfully!")\n\n    def _get_model(self):\n        try:\n            return self.client.models.list().data[0].id\n        except Exception as e:\n            raise ConnectionError("Failed to retrieve model ID from API") from e\n\n    @staticmethod\n    def make_api_url(host: str, port: int, version: str = "v1") -> str:\n        return f"http://{host}:{port}/{version}"\n\n    @OpenAIModelClass.method\n    def predict(\n        self,\n        prompt: str,\n        chat_history: List[dict] = None,\n        max_tokens: int = Param(\n            default=512,\n            description="The maximum number of tokens to generate. Shorter token lengths will provide faster performance.",\n        ),\n        temperature: float = Param(\n            default=0.7,\n            description="A decimal number that determines the degree of randomness in the response",\n        ),\n        top_p: float = Param(\n            default=0.8,\n            description="An alternative to sampling with temperature, where the model considers the results of the tokens with top_p probability mass.",\n        ),\n    ) -> str:\n        """This is the method that will be called when the runner is run. It takes in an input and\n        returns an output.\n        """\n        openai_messages = build_openai_messages(prompt=prompt, messages=chat_history)\n        response = self.client.chat.completions.create(\n            model=self.model,\n            messages=openai_messages,\n            max_completion_tokens=max_tokens,\n            temperature=temperature,\n            top_p=top_p,\n        )\n        if response.usage and response.usage.prompt_tokens and response.usage.completion_tokens:\n            self.set_output_context(\n                prompt_tokens=response.usage.prompt_tokens,\n                completion_tokens=response.usage.completion_tokens,\n            )\n        return response.choices[0].message.content\n\n    @OpenAIModelClass.method\n    def generate(\n        self,\n        prompt: str,\n        chat_history: List[dict] = None,\n        max_tokens: int = Param(\n            default=512,\n            description="The maximum number of tokens to generate. Shorter token lengths will provide faster performance.",\n        ),\n        temperature: float = Param(\n            default=0.7,\n            description="A decimal number that determines the degree of randomness in the response",\n        ),\n        top_p: float = Param(\n            default=0.8,\n            description="An alternative to sampling with temperature, where the model considers the results of the tokens with top_p probability mass.",\n        ),\n    ) -> Iterator[str]:\n        """Example yielding a whole batch of streamed stuff back."""\n        openai_messages = build_openai_messages(prompt=prompt, messages=chat_history)\n        for chunk in self.client.chat.completions.create(\n            model=self.model,\n            messages=openai_messages,\n            max_completion_tokens=max_tokens,\n            temperature=temperature,\n            top_p=top_p,\n            stream=True,\n        ):\n            if chunk.choices:\n                text = (\n                    chunk.choices[0].delta.content\n                    if (chunk and chunk.choices[0].delta.content) is not None\n                    else \'\'\n                )\n                yield text\n\n    # This method is needed to test the model with the test-locally CLI command.\n    def test(self):\n        """Test the model here."""\n        try:\n            print("Testing predict...")\n            # Test predict\n            print(\n                self.predict(\n                    prompt="Hello, how are you?",\n                )\n            )\n        except Exception as e:\n            print("Error in predict", e)\n\n        try:\n            print("Testing generate...")\n            # Test generate\n            for each in self.generate(\n                prompt="Hello, how are you?",\n            ):\n                print(each, end=" ")\n        except Exception as e:\n            print("Error in generate", e)\n',d="model:\n  id: SmolLM2-135M-Instruct\n  user_id: YOUR_USER_ID\n  app_id: YOUR_APP_ID\n  model_type_id: text-to-text\nbuild_info:\n  python_version: '3.11'\ninference_compute_info:\n  cpu_limit: '3'\n  cpu_memory: 14Gi\n  num_accelerators: 1\n  accelerator_type:\n  - NVIDIA-L40S\n  accelerator_memory: 42Gi\ncheckpoints:\n  repo_id: HuggingFaceTB/SmolLM2-135M-Instruct\n  type: huggingface\n  when: runtime\n",h="clarifai login\nEnter your Clarifai user ID: user-id\n> To authenticate, you'll need a Personal Access Token (PAT).\n> You can create one from your account settings: https://clarifai.com/user-id/settings/security\n\nEnter your Personal Access Token (PAT) value (or type \"ENVVAR\" to use an environment variable): XXXXXXXXXX\n\n> Verifying token...\n[INFO] 12:10:55.558733 Validating the Context Credentials... |  thread=8729403584 \n[INFO] 12:10:56.693295 \u2705 Context is valid |  thread=8729403584 \n\n> Let's save these credentials to a new context.\n> You can have multiple contexts to easily switch between accounts or projects.\n\nEnter a name for this context [default]: \n\u2705 Success! You are now logged in.\nCredentials saved to the 'default' context.\n\n\ud83d\udca1 To switch contexts later, use `clarifai config use-context <name>`.\n[INFO] 12:10:59.177368 Login successful for user 'alfrick' in context 'default' |  thread=8729403584 \n",u="clarifai\nopenai",p="clarifai model init --toolkit sglang\n[INFO] 20:14:19.494294 Parsed GitHub repository: owner=Clarifai, repo=runners-examples, branch=sglang, folder_path= |  thread=8729403584 \n[INFO] 20:14:20.762093 Files to be downloaded are:\n1. 1/model.py\n2. 1/openai_server_starter.py\n3. Dockerfile\n4. README.md\n5. config.yaml\n6. requirements.txt |  thread=8729403584 \nPress Enter to continue...\n[INFO] 20:14:24.640395 Initializing model from GitHub repository: https://github.com/Clarifai/runners-examples |  thread=8729403584 \n[INFO] 20:14:33.997825 Successfully cloned repository from https://github.com/Clarifai/runners-examples (branch: sglang) |  thread=8729403584 \n[INFO] 20:14:34.006824 Updated Hugging Face model repo_id to: None |  thread=8729403584 \n[INFO] 20:14:34.006878 Model initialization complete with GitHub repository |  thread=8729403584 \n[INFO] 20:14:34.006909 Next steps: |  thread=8729403584 \n[INFO] 20:14:34.006929 1. Review the model configuration |  thread=8729403584 \n[INFO] 20:14:34.006946 2. Install any required dependencies manually |  thread=8729403584 \n[INFO] 20:14:34.006966 3. Test the model locally using 'clarifai model local-test' |  thread=8729403584 ",m="",f='import os\nfrom openai import OpenAI\n\n# Initialize the OpenAI client, pointing to Clarifai\'s API\nclient = OpenAI(     \n    base_url="https://api.clarifai.com/v2/ext/openai/v1",  # Clarifai\'s OpenAI-compatible API endpoint\n    api_key=os.environ["CLARIFAI_PAT"]  # Ensure CLARIFAI_PAT is set as an environment variable\n)\n\n# Make a chat completion request to a Clarifai-hosted model\nresponse = client.chat.completions.create(    \n    model="https://clarifai.com/<user-id>/local-runner-app/models/local-runner-model",    \n    messages=[\n        {"role": "system", "content": "You are a helpful assistant."},\n        {"role": "user", "content": "What is the future of AI?"}\n    ],  \n)\n\n# Print the model\'s response\nprint(response.choices[0].message.content)',g='import os\nimport signal\nimport subprocess\nimport sys\nimport threading\nfrom typing import List\n\nimport psutil\nfrom clarifai.utils.logging import logger\n\nPYTHON_EXEC = sys.executable\n\n\ndef kill_process_tree(parent_pid, include_parent: bool = True, skip_pid: int = None):\n    """Kill the process and all its child processes."""\n    if parent_pid is None:\n        parent_pid = os.getpid()\n        include_parent = False\n\n    try:\n        itself = psutil.Process(parent_pid)\n    except psutil.NoSuchProcess:\n        return\n\n    children = itself.children(recursive=True)\n    for child in children:\n        if child.pid == skip_pid:\n            continue\n        try:\n            child.kill()\n        except psutil.NoSuchProcess:\n            pass\n\n    if include_parent:\n        try:\n            itself.kill()\n\n            # Sometime processes cannot be killed with SIGKILL (e.g, PID=1 launched by kubernetes),\n            # so we send an additional signal to kill them.\n            itself.send_signal(signal.SIGQUIT)\n        except psutil.NoSuchProcess:\n            pass\n\n\nclass OpenAI_APIServer:\n    def __init__(self, **kwargs):\n        self.server_started_event = threading.Event()\n        self.process = None\n        self.backend = None\n        self.server_thread = None\n\n    def __del__(self, *exc):\n        # This is important\n        # close the server when exit the program\n        self.close()\n\n    def close(self):\n        if self.process:\n            try:\n                kill_process_tree(self.process.pid)\n            except:\n                self.process.terminate()\n        if self.server_thread:\n            self.server_thread.join()\n\n    def wait_for_startup(self):\n        self.server_started_event.wait()\n\n    def validate_if_server_start(self, line: str):\n        line_lower = line.lower()\n        if self.backend in ["vllm", "sglang", "lmdeploy"]:\n            if self.backend == "vllm":\n                return (\n                    "application startup complete" in line_lower\n                    or "vllm api server on" in line_lower\n                )\n            else:\n                return f" running on http://{self.host}:" in line.strip()\n        elif self.backend == "llamacpp":\n            return "waiting for new tasks" in line_lower\n        elif self.backend == "tgi":\n            return "Connected" in line.strip()\n\n    def _start_server(self, cmds):\n        try:\n            env = os.environ.copy()\n            env["VLLM_USAGE_SOURCE"] = "production-docker-image"\n            self.process = subprocess.Popen(\n                cmds,\n                stdout=subprocess.PIPE,\n                stderr=subprocess.STDOUT,\n                text=True,\n            )\n            for line in self.process.stdout:\n                logger.info("Server Log:  " + line.strip())\n                if self.validate_if_server_start(line):\n                    self.server_started_event.set()\n                    # break\n        except Exception as e:\n            if self.process:\n                self.process.terminate()\n            raise RuntimeError(f"Failed to start Server server: {e}")\n\n    def start_server_thread(self, cmds: str):\n        try:\n            # Start the  server in a separate thread\n            self.server_thread = threading.Thread(\n                target=self._start_server, args=(cmds,), daemon=None\n            )\n            self.server_thread.start()\n\n            # Wait for the server to start\n            self.wait_for_startup()\n        except Exception as e:\n            raise Exception(e)\n\n    @classmethod\n    def from_sglang_backend(\n        cls,\n        checkpoints,\n        dtype: str = "auto",\n        kv_cache_dtype: str = "auto",\n        tp_size: int = 1,\n        quantization: str = None,\n        load_format: str = "auto",\n        context_length: str = None,\n        device: str = "cuda",\n        port=23333,\n        host="0.0.0.0",\n        chat_template: str = None,\n        mem_fraction_static: float = 0.8,\n        max_running_requests: int = None,\n        max_total_tokens: int = None,\n        max_prefill_tokens: int = None,\n        schedule_policy: str = "fcfs",\n        schedule_conservativeness: float = 1.0,\n        cpu_offload_gb: int = 0,\n        additional_list_args: List[str] = [],\n    ):\n        """Start SGlang OpenAI compatible server.\n\n        Args:\n            checkpoints (str): model id or path.\n            dtype (str, optional): Dtype used for the model {"auto", "half", "float16", "bfloat16", "float", "float32"}. Defaults to "auto".\n            kv_cache_dtype (str, optional): Dtype of the kv cache, defaults to the dtype. Defaults to "auto".\n            tp_size (int, optional): The number of GPUs the model weights get sharded over. Mainly for saving memory rather than for high throughput. Defaults to 1.\n            quantization (str, optional): Quantization format {"awq","fp8","gptq","marlin","gptq_marlin","awq_marlin","bitsandbytes","gguf","modelopt","w8a8_int8"}. Defaults to None.\n            load_format (str, optional): The format of the model weights to load:\\n* `auto`: will try to load the weights in the safetensors format and fall back to the pytorch bin format if safetensors format is not available.\\n* `pt`: will load the weights in the pytorch bin format. \\n* `safetensors`: will load the weights in the safetensors format. \\n* `npcache`: will load the weights in pytorch format and store a numpy cache to speed up the loading. \\n* `dummy`: will initialize the weights with random values, which is mainly for profiling.\\n* `gguf`: will load the weights in the gguf format. \\n* `bitsandbytes`: will load the weights using bitsandbytes quantization."\\n* `layered`: loads weights layer by layer so that one can quantize a layer before loading another to make the peak memory envelope smaller.\\n. Defaults to "auto".\\n\n            context_length (str, optional): The model\'s maximum context length. Defaults to None (will use the value from the model\'s config.json instead). Defaults to None.\n            device (str, optional): The device type {"cuda", "xpu", "hpu", "cpu"}. Defaults to "cuda".\n            port (int, optional): Port number. Defaults to 23333.\n            host (str, optional): Host name. Defaults to "0.0.0.0".\n            chat_template (str, optional): The buliltin chat template name or the path of the chat template file. This is only used for OpenAI-compatible API server.. Defaults to None.\n            mem_fraction_static (float, optional): The fraction of the memory used for static allocation (model weights and KV cache memory pool). Use a smaller value if you see out-of-memory errors. Defaults to 0.8.\n            max_running_requests (int, optional): The maximum number of running requests.. Defaults to None.\n            max_total_tokens (int, optional): The maximum number of tokens in the memory pool. If not specified, it will be automatically calculated based on the memory usage fraction. This option is typically used for development and debugging purposes.. Defaults to None.\n            max_prefill_tokens (int, optional): The maximum number of tokens in a prefill batch. The real bound will be the maximum of this value and the model\'s maximum context length. Defaults to None.\n            schedule_policy (str, optional): The scheduling policy of the requests {"lpm", "random", "fcfs", "dfs-weight"}. Defaults to "fcfs".\n            schedule_conservativeness (float, optional): How conservative the schedule policy is. A larger value means more conservative scheduling. Use a larger value if you see requests being retracted frequently. Defaults to 1.0.\n            cpu_offload_gb (int, optional): How many GBs of RAM to reserve for CPU offloading. Defaults to 0.\n            additional_list_args (List[str], optional): additional args to run subprocess cmd e.g. ["--arg-name", "arg value"]. See more at [github](https://github.com/sgl-project/sglang/blob/1baa9e6cf90b30aaa7dae51c01baa25229e8f7d5/python/sglang/srt/server_args.py#L298). Defaults to [].\n\n        Returns:\n            _type_: _description_\n        """\n\n        from clarifai.runners.utils.model_utils import execute_shell_command, wait_for_server\n\n        cmds = [\n            PYTHON_EXEC,\n            "-m",\n            "sglang.launch_server",\n            "--model-path",\n            checkpoints,\n            "--dtype",\n            str(dtype),\n            "--device",\n            str(device),\n            "--kv-cache-dtype",\n            str(kv_cache_dtype),\n            "--tp-size",\n            str(tp_size),\n            "--load-format",\n            str(load_format),\n            "--mem-fraction-static",\n            str(mem_fraction_static),\n            "--schedule-policy",\n            str(schedule_policy),\n            "--schedule-conservativeness",\n            str(schedule_conservativeness),\n            "--port",\n            str(port),\n            "--host",\n            host,\n            "--trust-remote-code",\n        ]\n        if chat_template:\n            cmds += ["--chat-template", chat_template]\n        if quantization:\n            cmds += [\n                "--quantization",\n                quantization,\n            ]\n        if context_length:\n            cmds += [\n                "--context-length",\n                context_length,\n            ]\n        if max_running_requests:\n            cmds += [\n                "--max-running-requests",\n                max_running_requests,\n            ]\n        if max_total_tokens:\n            cmds += [\n                "--max-total-tokens",\n                max_total_tokens,\n            ]\n        if max_prefill_tokens:\n            cmds += [\n                "--max-prefill-tokens",\n                max_prefill_tokens,\n            ]\n\n        if additional_list_args:\n            cmds += additional_list_args\n\n        print("CMDS to run `sglang` server: ", " ".join(cmds), "\\n")\n        _self = cls()\n\n        _self.host = host\n        _self.port = port\n        _self.backend = "sglang"\n        # _self.start_server_thread(cmds)\n        # new_path = os.environ["PATH"] + ":/sbin"\n        # _self.process = subprocess.Popen(cmds, text=True, stderr=subprocess.STDOUT, env={**os.environ, "PATH": new_path})\n        _self.process = execute_shell_command(" ".join(cmds))\n\n        logger.info("Waiting for " + f"http://{_self.host}:{_self.port}")\n        wait_for_server(f"http://{_self.host}:{_self.port}")\n        logger.info("Done")\n\n        return _self\n',x='# syntax=docker/dockerfile:1.13-labs\nFROM --platform=$TARGETPLATFORM lmsysorg/sglang:v0.5.3-cu129 as final\nCOPY --link requirements.txt /home/nonroot/requirements.txt\n\n# Update clarifai package so we always have latest protocol to the API. Everything should land in /venv\nRUN ["pip", "install", "--no-cache-dir", "-r", "/home/nonroot/requirements.txt"]\nRUN ["pip", "show", "--no-cache-dir", "clarifai"]\n\n# Set the NUMBA cache dir to /tmp\n# Set the TORCHINDUCTOR cache dir to /tmp\n# The CLARIFAI* will be set by the templaing system.\nENV NUMBA_CACHE_DIR=/tmp/numba_cache \\\n    TORCHINDUCTOR_CACHE_DIR=/tmp/torchinductor_cache \\\n    HOME=/tmp \\\n    DEBIAN_FRONTEND=noninteractive\n\n#####\n# Copy the files needed to download\n#####\n# This creates the directory that HF downloader will populate and with nonroot:nonroot permissions up.\nCOPY --chown=nonroot:nonroot downloader/unused.yaml /home/nonroot/main/1/checkpoints/.cache/unused.yaml\n\n#####\n# Download checkpoints if config.yaml has checkpoints.when = "build"\nCOPY --link=true config.yaml /home/nonroot/main/\nRUN ["python", "-m", "clarifai.cli", "model", "download-checkpoints", "/home/nonroot/main", "--out_path", "/home/nonroot/main/1/checkpoints", "--stage", "build"]\n#####\n\n# Copy in the actual files like config.yaml, requirements.txt, and most importantly 1/model.py\n# for the actual model.\n# If checkpoints aren\'t downloaded since a checkpoints: block is not provided, then they will\n# be in the build context and copied here as well.\nCOPY --link=true 1 /home/nonroot/main/1\n# At this point we only need these for validation in the SDK.\nCOPY --link=true requirements.txt config.yaml /home/nonroot/main/\n\n# Add the model directory to the python path.\nENV PYTHONPATH=${PYTHONPATH}:/home/nonroot/main \\\n    CLARIFAI_PAT=${CLARIFAI_PAT} \\\n    CLARIFAI_USER_ID=${CLARIFAI_USER_ID} \\\n    CLARIFAI_RUNNER_ID=${CLARIFAI_RUNNER_ID} \\\n    CLARIFAI_NODEPOOL_ID=${CLARIFAI_NODEPOOL_ID} \\\n    CLARIFAI_COMPUTE_CLUSTER_ID=${CLARIFAI_COMPUTE_CLUSTER_ID} \\\n    CLARIFAI_API_BASE=${CLARIFAI_API_BASE:-https://api.clarifai.com}\n\nUSER root\nRUN echo "nonroot:x:65532:65532:nonroot user:/home/nonroot:/sbin/nologin" >> /etc/passwd\nUSER nonroot\n    \n\n# Finally run the clarifai entrypoint to start the runner loop and local runner server.\n# Note(zeiler): we may want to make this a clarifai CLI call.\nENTRYPOINT ["python", "-m", "clarifai.runners.server"]\nCMD ["--model_path", "/home/nonroot/main"]\n#############################\n',_={description:"Run models using the SGLang runtime format and make them available via a public API",sidebar_position:6},y="SGLang",v={},b=[{value:"Step 1: Perform Prerequisites",id:"step-1-perform-prerequisites",level:2},{value:"Sign Up or Log In",id:"sign-up-or-log-in",level:3},{value:"Install Clarifai CLI",id:"install-clarifai-cli",level:3},{value:"Install SGLang",id:"install-sglang",level:3},{value:"Install OpenAI",id:"install-openai",level:3},{value:"Get Hugging Face Token",id:"get-hugging-face-token",level:3},{value:"Step 2: Initialize a Model",id:"step-2-initialize-a-model",level:2},{value:"<code>model.py</code>",id:"modelpy",level:3},{value:"<code>openai_server_starter.py</code>",id:"openai_server_starterpy",level:3},{value:"<code>Dockerfile</code>",id:"dockerfile",level:3},{value:"<code>config.yaml</code>",id:"configyaml",level:3},{value:"<code>requirements.txt</code>",id:"requirementstxt",level:3},{value:"Step 3: Log In to Clarifai",id:"step-3-log-in-to-clarifai",level:2},{value:"Step 4: Start Your Local Runner",id:"step-4-start-your-local-runner",level:2},{value:"Step 5: Test Your Runner",id:"step-5-test-your-runner",level:2}];function j(e){const n={a:"a",admonition:"admonition",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components},{Details:t}=n;return t||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"sglang",children:"SGLang"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Run models using the SGLang runtime format and make them available via a public API"})}),"\n",(0,r.jsx)("hr",{}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"https://github.com/sgl-project/sglang",children:"SGLang"})," is an open-source runtime and programming framework designed for structured generation and high-performance inference of large language models (LLMs) and vision-language models."]}),"\n",(0,r.jsx)(n.p,{children:"It provides a flexible way to execute models with advanced capabilities like multi-step prompting, structured outputs, and multimodal reasoning \u2014 all while maximizing throughput and minimizing latency."}),"\n",(0,r.jsxs)(n.p,{children:["With Clarifai\u2019s ",(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/compute/local-runners/",children:"Local Runners"}),", you can download and run these models on your own machine using the SGLang runtime format, expose them securely via a public URL, and tap into Clarifai\u2019s powerful platform  \u2014 all while retaining the privacy, performance, and control of local execution."]}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Note:"})," The SGLang toolkit specifies a runtime format to run models sourced from external sources like Hugging Face. After initializing a model using the toolkit, you can ",(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/compute/upload/#step-4-upload-the-model-to-clarifai",children:"upload"})," it to Clarifai to leverage the platform\u2019s capabilities."]}),"\n"]}),"\n","\n","\n",(0,r.jsx)(n.h2,{id:"step-1-perform-prerequisites",children:"Step 1: Perform Prerequisites"}),"\n",(0,r.jsx)(n.h3,{id:"sign-up-or-log-in",children:"Sign Up or Log In"}),"\n",(0,r.jsxs)(n.p,{children:["Log in to your existing Clarifai account or ",(0,r.jsx)(n.a,{href:"https://clarifai.com/signup",children:"sign up"})," for a new one. Once you\u2019re logged in, gather the following credentials required for setup:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"App ID"})," \u2013 Go to the application you want to use to run the model. In the collapsible left sidebar, select ",(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/create/applications/manage/#app-overview",children:"Overview"})})," and copy the app ID displayed there."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"User ID"})," \u2013 In the collapsible left sidebar, open ",(0,r.jsx)(n.strong,{children:"Settings"}),", then choose ",(0,r.jsx)(n.strong,{children:"Account"})," from the dropdown list to locate your user ID."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Personal Access Token (PAT)"})," \u2013 From the same ",(0,r.jsx)(n.strong,{children:"Settings"})," menu, select ",(0,r.jsx)(n.strong,{children:"Secrets"})," to create or copy your ",(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/control/authentication/pat",children:"PAT"}),". This token is used to authenticate your connection with the Clarifai platform."]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Then, set your PAT as an environment variable:"}),"\n",(0,r.jsxs)(o.A,{groupId:"code",children:[(0,r.jsx)(a.A,{value:"bash",label:"Unix-Like Systems",children:(0,r.jsx)(l.A,{className:"language-bash",children:"export CLARIFAI_PAT=YOUR_PERSONAL_ACCESS_TOKEN_HERE"})}),(0,r.jsx)(a.A,{value:"bash2",label:"Windows",children:(0,r.jsx)(l.A,{className:"language-bash",children:"set CLARIFAI_PAT=YOUR_PERSONAL_ACCESS_TOKEN_HERE"})})]}),"\n",(0,r.jsx)(n.h3,{id:"install-clarifai-cli",children:"Install Clarifai CLI"}),"\n",(0,r.jsxs)(n.p,{children:["Install the latest ",(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/sdk/cli",children:"Clarifai CLI"})," which includes built-in support for Local Runners:"]}),"\n",(0,r.jsx)(o.A,{groupId:"code",children:(0,r.jsx)(a.A,{value:"bash",label:"Bash",children:(0,r.jsx)(l.A,{className:"language-bash",children:"pip install --upgrade clarifai"})})}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Note:"})," The Local Runners require ",(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/resources/api-overview/python-sdk#python-requirements",children:"Python 3.11 or 3.12"})}),"."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"install-sglang",children:"Install SGLang"}),"\n",(0,r.jsx)(n.p,{children:"Install SGLang to enable its runtime execution environment."}),"\n",(0,r.jsx)(o.A,{groupId:"code",children:(0,r.jsx)(a.A,{value:"bash",label:"Bash",children:(0,r.jsx)(l.A,{className:"language-bash",children:"pip install sglang"})})}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Tip:"})," GPU acceleration (CUDA) is highly recommended for optimal performance."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"install-openai",children:"Install OpenAI"}),"\n",(0,r.jsxs)(n.p,{children:["Install the ",(0,r.jsx)(n.code,{children:"openai"})," package, which is needed to perform inference with models that use the ",(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/compute/inference/#predict-with-openai-compatible-format",children:"OpenAI-compatible format"}),"."]}),"\n",(0,r.jsx)(o.A,{groupId:"code",children:(0,r.jsx)(a.A,{value:"bash",label:"Bash",children:(0,r.jsx)(l.A,{className:"language-bash",children:"pip install openai"})})}),"\n",(0,r.jsx)(n.h3,{id:"get-hugging-face-token",children:"Get Hugging Face Token"}),"\n",(0,r.jsx)(n.p,{children:"If you want to initialize a Hugging Face model for use with SGLang, you\u2019ll need a Hugging Face access token to authenticate with Hugging Face services \u2014 especially when accessing private or restricted repositories."}),"\n",(0,r.jsxs)(n.p,{children:["You can create one by following ",(0,r.jsx)(n.a,{href:"https://huggingface.co/docs/hub/en/security-tokens",children:"these instructions"}),". Once you have the token, include it either in your model\u2019s ",(0,r.jsx)(n.code,{children:"config.yaml"})," file (as described ",(0,r.jsx)(n.a,{href:"#configyaml",children:"below"}),") or set it as an environment variable."]}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Note:"})," If ",(0,r.jsx)(n.code,{children:"hf_token"})," is not specified in the ",(0,r.jsx)(n.code,{children:"config.yaml"})," file, the CLI will automatically use the ",(0,r.jsx)(n.code,{children:"HF_TOKEN"})," environment variable for authentication with Hugging Face."]}),"\n"]}),"\n",(0,r.jsxs)(o.A,{groupId:"code",children:[(0,r.jsx)(a.A,{value:"bash",label:"Unix-Like Systems",children:(0,r.jsx)(l.A,{className:"language-bash",children:'export HF_TOKEN="YOUR_HF_ACCESS_TOKEN_HERE"'})}),(0,r.jsx)(a.A,{value:"bash2",label:"Windows",children:(0,r.jsx)(l.A,{className:"language-bash",children:'set HF_TOKEN="YOUR_HF_ACCESS_TOKEN_HERE"'})})]}),"\n",(0,r.jsx)(n.h2,{id:"step-2-initialize-a-model",children:"Step 2: Initialize a Model"}),"\n",(0,r.jsx)(n.p,{children:"With the Clarifai CLI, you can initialize a model configured to run using the SGLang runtime format. It sets up a Clarifai-compatible project directory with the appropriate files."}),"\n",(0,r.jsxs)(n.p,{children:["You can customize or optimize the model by editing the generated files as needed. For example, the command below initializes a default Hugging Face model (",(0,r.jsx)(n.a,{href:"https://huggingface.co/HuggingFaceTB/SmolLM2-135M-Instruct",children:"HuggingFaceTB/SmolLM2-135M-Instruct"}),") in your current directory."]}),"\n",(0,r.jsx)(o.A,{groupId:"code",children:(0,r.jsx)(a.A,{value:"bash",label:"Bash",children:(0,r.jsx)(l.A,{className:"language-bash",children:"clarifai model init --toolkit sglang"})})}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Note:"})," You can initialize a model in a specific location by passing a ",(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/resources/api-overview/cli#clarifai-model-init",children:(0,r.jsx)(n.code,{children:"MODEL_PATH"})}),"."]}),"\n"]}),"\n",(0,r.jsxs)(t,{children:[(0,r.jsx)("summary",{children:"Example Output"}),(0,r.jsx)(l.A,{className:"language-text",children:p})]}),"\n",(0,r.jsxs)(n.admonition,{type:"tip",children:[(0,r.jsxs)(n.p,{children:["You can use the ",(0,r.jsx)(n.code,{children:"--model-name"})," parameter to initialize any supported Hugging Face model. This sets the model\u2019s ",(0,r.jsx)(n.code,{children:"repo_id"}),", specifying which Hugging Face repository to initialize from."]}),(0,r.jsx)(o.A,{groupId:"code",children:(0,r.jsx)(a.A,{value:"bash",label:"Bash",children:(0,r.jsx)(l.A,{className:"language-bash",children:"clarifai model init --toolkit sglang --model-name unsloth/Llama-3.2-1B-Instruct"})})})]}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Note:"})," Large models require significant GPU memory. Ensure your machine has enough compute capacity to run them efficiently."]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"The generated structure includes:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"\n\u251c\u2500\u2500 1/\n\u2502   \u2514\u2500\u2500 model.py\n|   \u2514\u2500\u2500 openai_server_starter.py\n\u251c\u2500\u2500 Dockerfile\n\u2514\u2500\u2500 README.md\n\u251c\u2500\u2500 config.yaml\n\u2514\u2500\u2500 requirements.txt\n\n"})}),"\n",(0,r.jsx)(n.h3,{id:"modelpy",children:(0,r.jsx)(n.code,{children:"model.py"})}),"\n",(0,r.jsxs)(t,{children:[(0,r.jsx)("summary",{children:"Example: 1/model.py"}),(0,r.jsx)(l.A,{className:"language-text",children:c})]}),"\n",(0,r.jsxs)(n.p,{children:["This is the ",(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/compute/upload/#prepare-modelpy",children:"main runner"})," script that defines how your model loads, runs, and handles inference."]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["It subclasses ",(0,r.jsx)(n.code,{children:"OpenAIModelClass"}),", meaning it exposes OpenAI-compatible endpoints for inference."]}),"\n",(0,r.jsxs)(n.li,{children:["The ",(0,r.jsx)(n.code,{children:"load_model()"})," method spins up a local SGLang backend server (via ",(0,r.jsx)(n.code,{children:"OpenAI_APIServer.from_sglang_backend"}),") and initializes the model checkpoint."]}),"\n",(0,r.jsxs)(n.li,{children:["The ",(0,r.jsx)(n.code,{children:"predict()"})," and ",(0,r.jsx)(n.code,{children:"generate()"})," methods define how text generation requests are processed \u2014 supporting both standard predictions and streaming outputs."]}),"\n",(0,r.jsxs)(n.li,{children:["The ",(0,r.jsx)(n.code,{children:"test()"})," method lets you verify locally that everything is working before deployment."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"openai_server_starterpy",children:(0,r.jsx)(n.code,{children:"openai_server_starter.py"})}),"\n",(0,r.jsxs)(t,{children:[(0,r.jsx)("summary",{children:"Example: 1/openai_server_starter.py"}),(0,r.jsx)(l.A,{className:"language-text",children:g})]}),"\n",(0,r.jsx)(n.p,{children:"This utility handles starting, monitoring, and shutting down the backend SGLang server. It acts as your server controller, ensuring the backend is ready before the runner starts sending requests."}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["It wraps around subprocess management for launching ",(0,r.jsx)(n.code,{children:"sglang.launch_server"}),"."]}),"\n",(0,r.jsx)(n.li,{children:"It ensures the server runs properly, logs startup messages, and handles safe termination."}),"\n",(0,r.jsxs)(n.li,{children:["The class ",(0,r.jsx)(n.code,{children:"OpenAI_APIServer"})," can also be extended to support other backends like ",(0,r.jsx)(n.code,{children:"vLLM"}),", ",(0,r.jsx)(n.code,{children:"llama.cpp"}),", or ",(0,r.jsx)(n.code,{children:"TGI"}),", but here it\u2019s used for SGLang."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"dockerfile",children:(0,r.jsx)(n.code,{children:"Dockerfile"})}),"\n",(0,r.jsxs)(t,{children:[(0,r.jsx)("summary",{children:"Example: Dockerfile"}),(0,r.jsx)(l.A,{className:"language-text",children:x})]}),"\n",(0,r.jsx)(n.p,{children:"The Dockerfile defines the container environment used to run your model runner on Clarifai\u2019s infrastructure."}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["It builds on the official SGLang base image (",(0,r.jsx)(n.code,{children:"lmsysorg/sglang:v0.5.3-cu129"}),"), which includes CUDA and SGLang dependencies."]}),"\n",(0,r.jsxs)(n.li,{children:["It installs any Python packages listed in ",(0,r.jsx)(n.code,{children:"requirements.txt"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:["It copies your model files (",(0,r.jsx)(n.code,{children:"model.py"}),", ",(0,r.jsx)(n.code,{children:"config.yaml"}),", etc.) into the container."]}),"\n",(0,r.jsxs)(n.li,{children:["Optionally, it downloads checkpoints during build time if ",(0,r.jsx)(n.code,{children:'checkpoints.when = "build"'}),"."]}),"\n",(0,r.jsxs)(n.li,{children:["It starts the Clarifai runner loop using ",(0,r.jsx)(n.code,{children:"python -m clarifai.runners.server"}),"."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"configyaml",children:(0,r.jsx)(n.code,{children:"config.yaml"})}),"\n",(0,r.jsxs)(t,{children:[(0,r.jsx)("summary",{children:"Example: config.yaml"}),(0,r.jsx)(l.A,{className:"language-text",children:d})]}),"\n",(0,r.jsx)(n.p,{children:"This is the configuration file for your SGLang model runner."}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["It specifies model identifiers (",(0,r.jsx)(n.code,{children:"model.id"}),", ",(0,r.jsx)(n.code,{children:"user_id"}),", ",(0,r.jsx)(n.code,{children:"app_id"}),"), which together determine where your model will run on the Clarifai platform. Your Clarifai user ID is set by default from your ",(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/resources/api-overview/cli#clarifai-config",children:"active context"}),"."]}),"\n",(0,r.jsx)(n.li,{children:"It defines compute resources (CPU, GPU type, and memory)."}),"\n",(0,r.jsxs)(n.li,{children:["The ",(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/compute/upload/#hugging-face-model-checkpoints",children:(0,r.jsx)(n.code,{children:"checkpoints"})})," section tells the runner where and when to load model weights","\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Tip:"})," Use ",(0,r.jsx)(n.code,{children:"when: runtime"})," for large models to reduce image size and improve load times."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"requirementstxt",children:(0,r.jsx)(n.code,{children:"requirements.txt"})}),"\n",(0,r.jsxs)(t,{children:[(0,r.jsx)("summary",{children:"Example: requirements.txt"}),(0,r.jsx)(l.A,{className:"language-text",children:u})]}),"\n",(0,r.jsx)(n.p,{children:"This file lists all the Python dependencies required for the runner to work. If you haven\u2019t installed them yet, run the following command to install the dependencies:"}),"\n",(0,r.jsx)(o.A,{groupId:"code",children:(0,r.jsx)(a.A,{value:"bash",label:"Bash",children:(0,r.jsx)(l.A,{className:"language-bash",children:"pip install -r requirements.txt"})})}),"\n",(0,r.jsx)(n.h2,{id:"step-3-log-in-to-clarifai",children:"Step 3: Log In to Clarifai"}),"\n",(0,r.jsxs)(n.p,{children:["Log in and create a configuration ",(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/compute/local-runners/#step-2-create-a-context-optional",children:"context"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"clarifai login\n"})}),"\n",(0,r.jsx)(n.p,{children:"Enter the requested details:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"User ID"})," \u2013 Your Clarifai user ID"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"PAT"})," \u2013 Your personal access token (or type ",(0,r.jsx)(n.code,{children:"ENVVAR"})," to use the environment variable)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Context name"})," \u2013 Optional name for the config context (default: ",(0,r.jsx)(n.code,{children:'"default"'}),")"]}),"\n"]}),"\n",(0,r.jsxs)(t,{children:[(0,r.jsx)("summary",{children:"Example Output"}),(0,r.jsx)(l.A,{className:"language-text",children:h})]}),"\n",(0,r.jsx)(n.h2,{id:"step-4-start-your-local-runner",children:"Step 4: Start Your Local Runner"}),"\n",(0,r.jsx)(n.p,{children:"Next, start your Local Runner, which connects to the SGLang runtime to execute your model locally."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"clarifai model local-runner\n"})}),"\n",(0,r.jsx)(n.p,{children:"If any configuration contexts or defaults are missing, the CLI will automatically guide you through setting them up."}),"\n",(0,r.jsxs)(n.p,{children:["This process ensures that all required components \u2014 such as compute clusters, nodepools, and deployments \u2014 are correctly configured in your context, enabling seamless local execution of your SGLang model. For more details, see ",(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/compute/local-runners/#step-2-create-a-context-optional",children:"Local Runners documentation"}),"."]}),"\n",(0,r.jsxs)(t,{children:[(0,r.jsx)("summary",{children:"Example Output"}),(0,r.jsx)(l.A,{className:"language-text",children:m})]}),"\n",(0,r.jsx)(n.h2,{id:"step-5-test-your-runner",children:"Step 5: Test Your Runner"}),"\n",(0,r.jsxs)(n.p,{children:["After the Local Runner starts, you can use it to perform ",(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/compute/inference/open-ai",children:"inference"})," with your SGLang-based model."]}),"\n",(0,r.jsx)(n.p,{children:"You can run a test snippet in a separate terminal, within the same directory, to verify that your model is running and responding correctly."}),"\n",(0,r.jsx)(n.p,{children:"Here\u2019s an example snippet:"}),"\n",(0,r.jsx)(o.A,{groupId:"code",children:(0,r.jsx)(a.A,{value:"python",label:"Python SDK",children:(0,r.jsx)(l.A,{className:"language-python",children:f})})})]})}function I(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(j,{...e})}):j(e)}}}]);
"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[1842],{65537:(e,t,a)=>{a.d(t,{A:()=>y});var n=a(96540),i=a(18215),s=a(65627),o=a(56347),r=a(50372),l=a(30604),d=a(11861),c=a(78749);function u(e){return n.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,n.isValidElement)(e)&&function(e){const{props:t}=e;return!!t&&"object"==typeof t&&"value"in t}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function p(e){const{values:t,children:a}=e;return(0,n.useMemo)((()=>{const e=t??function(e){return u(e).map((e=>{let{props:{value:t,label:a,attributes:n,default:i}}=e;return{value:t,label:a,attributes:n,default:i}}))}(a);return function(e){const t=(0,d.XI)(e,((e,t)=>e.value===t.value));if(t.length>0)throw new Error(`Docusaurus error: Duplicate values "${t.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[t,a])}function h(e){let{value:t,tabValues:a}=e;return a.some((e=>e.value===t))}function m(e){let{queryString:t=!1,groupId:a}=e;const i=(0,o.W6)(),s=function(e){let{queryString:t=!1,groupId:a}=e;if("string"==typeof t)return t;if(!1===t)return null;if(!0===t&&!a)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return a??null}({queryString:t,groupId:a});return[(0,l.aZ)(s),(0,n.useCallback)((e=>{if(!s)return;const t=new URLSearchParams(i.location.search);t.set(s,e),i.replace({...i.location,search:t.toString()})}),[s,i])]}function g(e){const{defaultValue:t,queryString:a=!1,groupId:i}=e,s=p(e),[o,l]=(0,n.useState)((()=>function(e){let{defaultValue:t,tabValues:a}=e;if(0===a.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(t){if(!h({value:t,tabValues:a}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${t}" but none of its children has the corresponding value. Available values are: ${a.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return t}const n=a.find((e=>e.default))??a[0];if(!n)throw new Error("Unexpected error: 0 tabValues");return n.value}({defaultValue:t,tabValues:s}))),[d,u]=m({queryString:a,groupId:i}),[g,f]=function(e){let{groupId:t}=e;const a=function(e){return e?`docusaurus.tab.${e}`:null}(t),[i,s]=(0,c.Dv)(a);return[i,(0,n.useCallback)((e=>{a&&s.set(e)}),[a,s])]}({groupId:i}),_=(()=>{const e=d??g;return h({value:e,tabValues:s})?e:null})();(0,r.A)((()=>{_&&l(_)}),[_]);return{selectedValue:o,selectValue:(0,n.useCallback)((e=>{if(!h({value:e,tabValues:s}))throw new Error(`Can't select invalid tab value=${e}`);l(e),u(e),f(e)}),[u,f,s]),tabValues:s}}var f=a(9136);const _={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var v=a(74848);function x(e){let{className:t,block:a,selectedValue:n,selectValue:o,tabValues:r}=e;const l=[],{blockElementScrollPositionUntilNextRender:d}=(0,s.a_)(),c=e=>{const t=e.currentTarget,a=l.indexOf(t),i=r[a].value;i!==n&&(d(t),o(i))},u=e=>{let t=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const a=l.indexOf(e.currentTarget)+1;t=l[a]??l[0];break}case"ArrowLeft":{const a=l.indexOf(e.currentTarget)-1;t=l[a]??l[l.length-1];break}}t?.focus()};return(0,v.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,i.A)("tabs",{"tabs--block":a},t),children:r.map((e=>{let{value:t,label:a,attributes:s}=e;return(0,v.jsx)("li",{role:"tab",tabIndex:n===t?0:-1,"aria-selected":n===t,ref:e=>{l.push(e)},onKeyDown:u,onClick:c,...s,className:(0,i.A)("tabs__item",_.tabItem,s?.className,{"tabs__item--active":n===t}),children:a??t},t)}))})}function j(e){let{lazy:t,children:a,selectedValue:s}=e;const o=(Array.isArray(a)?a:[a]).filter(Boolean);if(t){const e=o.find((e=>e.props.value===s));return e?(0,n.cloneElement)(e,{className:(0,i.A)("margin-top--md",e.props.className)}):null}return(0,v.jsx)("div",{className:"margin-top--md",children:o.map(((e,t)=>(0,n.cloneElement)(e,{key:t,hidden:e.props.value!==s})))})}function b(e){const t=g(e);return(0,v.jsxs)("div",{className:(0,i.A)("tabs-container",_.tabList),children:[(0,v.jsx)(x,{...t,...e}),(0,v.jsx)(j,{...t,...e})]})}function y(e){const t=(0,f.A)();return(0,v.jsx)(b,{...e,children:u(e.children)},String(t))}},78730:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>T,contentTitle:()=>S,default:()=>E,frontMatter:()=>D,metadata:()=>n,toc:()=>N});const n=JSON.parse('{"id":"create/models/deep-fine-tuning/visual-detector","title":"Visual Detector","description":"Learn about our visual detector model type","source":"@site/docs/create/models/deep-fine-tuning/visual-detector.md","sourceDirName":"create/models/deep-fine-tuning","slug":"/create/models/deep-fine-tuning/visual-detector","permalink":"/create/models/deep-fine-tuning/visual-detector","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"description":"Learn about our visual detector model type","sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Visual Classifier","permalink":"/create/models/deep-fine-tuning/visual-classifier"},"next":{"title":"Visual Segmenter","permalink":"/create/models/deep-fine-tuning/visual-segmenter"}}');var i=a(74848),s=a(28453),o=a(65537),r=a(79329),l=a(58069);const d='from clarifai.client.user import User\n#replace your "user_id"\nclient = User(user_id="user_id")\napp = client.create_app(app_id="demo_train", base_workflow="Universal")',c="#importing load_module_dataloader for calling the dataloader object in dataset.py in the local data folder\nfrom clarifai.datasets.upload.utils import load_module_dataloader\n\n\n# Construct the path to the dataset folder\nmodule_path = os.path.join(os.getcwd().split('/models/model_train')[0],'datasets/upload/image_detection/voc')\n\n# Load the dataloader module using the provided function from your module\nvoc_dataloader = load_module_dataloader(module_path)\n\n# Create a Clarifai dataset with the specified dataset_id (\"image_dataset\")\ndataset = app.create_dataset(dataset_id=\"train_dataset\")\n\n# Upload the dataset using the provided dataloader and get the upload status\ndataset.upload_dataset(dataloader=voc_dataloader)\n",u="print(app.list_trainable_model_types())",p='MODEL_ID = "model_detector"\nMODEL_TYPE_ID = "visual-detector"\n# Create a model by passing the model name and model type as parameter\nmodel = app.create_model(model_id=MODEL_ID, model_type_id=MODEL_TYPE_ID)',h="print(model.list_training_templates())",m="# Get the params for the selected template\nmodel_params = model.get_params(template='MMDetection_SSD')\n# list the concepts to add in the params\nconcepts = [concept.id for concept in app.list_concepts()]\nmodel.update_params(dataset_id = 'train_dataset',concepts = concepts)\nprint(model.training_params)",g='import time\n#Starting the training\nmodel_version_id = model.train()\n\n#Checking the status of training\nwhile True:\n    status = model.training_status(version_id=model_version_id,training_logs=False)\n    if status.code == 21106: #MODEL_TRAINING_FAILED\n        print(status)\n        break\n    elif status.code == 21100: #MODEL_TRAINED\n        print(status)\n        break\n    else:\n        print("Current Status:",status)\n        print("Waiting---")\n        time.sleep(120)\n',f="import cv2\nimport matplotlib.pyplot as plt\nfrom urllib.request import urlopen\nimport numpy as np\n\n\nIMAGE_PATH = os.path.join(os.getcwd().split('/models')[0],'datasets/upload/image_detection/voc/images/2008_008526.jpg')\n\nprediction_response = model.predict_by_filepath(IMAGE_PATH, input_type=\"image\",inference_params={'detection_threshold': 0.5})\n\n# Get the output\nregions = prediction_response.outputs[0].data.regions\n\nimg = cv2.imread(IMAGE_PATH)\n\nfor region in regions:\n    # Accessing and rounding the bounding box values\n    top_row = round(region.region_info.bounding_box.top_row, 3) * img.shape[0]\n    left_col = round(region.region_info.bounding_box.left_col, 3)* img.shape[1]\n    bottom_row = round(region.region_info.bounding_box.bottom_row, 3)* img.shape[0]\n    right_col = round(region.region_info.bounding_box.right_col, 3)* img.shape[1]\n\n    cv2.rectangle(img, (int(left_col),int(top_row)), (int(right_col),int(bottom_row)), (36,255,12), 2)\n\n    # Get concept name\n    concept_name = region.data.concepts[0].name\n\n    # Display text\n    cv2.putText(img, concept_name, (int(left_col),int(top_row-10)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (36,255,12), 2)\n\nplt.axis('off')\nplt.imshow(img[...,::-1])\n",_="# Evaluate the model on a specific dataset with ID 'train_dataset'\nmodel.evaluate(dataset_id='train_dataset', eval_id='one')\n\n# Get the evaluation result by its ID 'one'\nresult = model.get_eval_by_id(eval_id=\"one\")\n\nprint(result.summary)",v="# Set the path to the module containing the data\nPATH=os.path.join(os.getcwd().split('/models/model_train')[0],'datasets/upload/data/voc_test')\n\n# Load the dataloader module from the specified path\nvoc_dataloader = load_module_dataloader(PATH)\n\n# Create a new dataset object with a unique ID 'test_dataset_1'\ntest_dataset = app.create_dataset(dataset_id=\"test_dataset_1\")\n\n# Upload the dataset using the previously loaded dataloader\ntest_dataset.upload_dataset(dataloader=voc_dataloader)\n\n# Evaluate the model using the uploaded dataset, with evaluation ID 'two'\nmodel.evaluate(dataset_id='test_dataset_1', eval_id='two')\n\n# Retrieve the evaluation result with ID 'two' for the model\nresult = model.get_eval_by_id(\"two\")\n\n# Print the summary of the evaluation result\nprint(result.summary)",x="# Importing the EvalResultCompare class from the clarifai.utils.evaluation module\nfrom clarifai.utils.evaluation import EvalResultCompare\n\n\n# Creating an EvalResultCompare object with specified models and datasets\neval_result = EvalResultCompare(models=[model], datasets=[dataset, test_dataset])\n\n# Printing a detailed summary of the evaluation result\nprint(eval_result.detailed_summary())\n",j="['visual-classifier',\n 'visual-detector',\n 'visual-segmenter',\n 'visual-embedder',\n 'clusterer',\n 'text-classifier',\n 'embedding-classifier',\n 'text-to-text']",b="['MMDetection_AdvancedConfig',\n 'MMDetection_FasterRCNN',\n 'MMDetection_SSD',\n 'MMDetection_YoloF',\n 'Clarifai_InceptionV2',\n 'Clarifai_InceptionV4',\n 'detection_msc10']",y="{'dataset_id': 'train_dataset',\n 'dataset_version_id': '',\n 'concepts': ['id-hamburger', 'id-ramen', 'id-prime_rib', 'id-beignets'],\n 'train_params': {'invalid_data_tolerance_percent': 5.0,\n  'template': 'Clarifai_ResNext',\n  'logreg': 1.0,\n  'image_size': 256.0,\n  'batch_size': 64.0,\n  'init_epochs': 25.0,\n  'step_epochs': 7.0,\n  'num_epochs': 65.0,\n  'per_item_lrate': 7.8125e-05,\n  'num_items_per_epoch': 0.0}}",w="mean_avg_precision_iou_50: 1.0\nmean_avg_precision_iou_range: 0.9453125",A="mean_avg_precision_iou_50: 1.0\nmean_avg_precision_iou_range: 0.9555555582046509",I="(     Concept  Average Precision  Total Labeled  True Positives  \\\n 0     id-cow                1.0              2               2   \n 0   id-horse                1.0              1               1   \n 0  id-bottle                1.0              2               2   \n 0    id-sofa                1.0              1               1   \n 0    id-bird                1.0              1               1   \n 0     id-cat                1.0              2               2   \n 0     id-dog                1.0              1               1   \n 0  id-person                1.0              8               8   \n 0     id-dog                1.0              1               1   \n 0  id-person                1.0              3               3   \n \n    False Positives  False Negatives  Recall  Precision        F1  \\\n 0                0                0     1.0      0.841  0.913634   \n 0                0                0     1.0      0.783  0.878295   \n 0                0                0     1.0      0.819  0.900495   \n 0                0                0     1.0      0.769  0.869418   \n 0                0                0     1.0      0.790  0.882682   \n 0                0                0     1.0      0.836  0.910675   \n 0                0                0     1.0      0.763  0.865570   \n 0                0                0     1.0      0.940  0.969072   \n 0                0                0     1.0      0.763  0.865570   \n 0                0                0     1.0      0.884  0.938429   \n \n           Dataset  \n 0  train_dataset2  \n 0  train_dataset2  \n 0  train_dataset2  \n 0  train_dataset2  \n 0  train_dataset2  \n 0  train_dataset2  \n 0  train_dataset2  \n 0  train_dataset2  \n 0  test_dataset_1  \n 0  test_dataset_1  ,\n             Total Concept  Average Precision  Total Labeled  True Positives  \\\n 0  Dataset:train_dataset2                1.0             18              18   \n 0  Dataset:test_dataset_1                1.0              4               4   \n \n    False Positives  False Negatives  Recall  Precision   F1  \n 0                0                0     1.0        1.0  1.0  \n 0                0                0     1.0        1.0  1.0  )",D={description:"Learn about our visual detector model type",sidebar_position:3},S="Visual Detector",T={},N=[{value:"Example Use Case",id:"example-use-case",level:2},{value:"Create and Train a Visual Detector",id:"create-and-train-a-visual-detector",level:2},{value:"Step 1: App Creation",id:"step-1-app-creation",level:3},{value:"Step 2: Dataset Upload",id:"step-2-dataset-upload",level:3},{value:"Step 3: Model Creation",id:"step-3-model-creation",level:3},{value:"Step 4: Template Selection",id:"step-4-template-selection",level:3},{value:"Step 5: Set Up Model Parameters",id:"step-5-set-up-model-parameters",level:3},{value:"Step 6: Initiate Model Training",id:"step-6-initiate-model-training",level:3},{value:"Step 7: Model Prediction",id:"step-7-model-prediction",level:3},{value:"Step 8: Model Evaluation",id:"step-8-model-evaluation",level:3}];function C(e){const t={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components},{Details:a}=t;return a||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.header,{children:(0,i.jsx)(t.h1,{id:"visual-detector",children:"Visual Detector"})}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.strong,{children:"Learn about our visual detector model type"})}),"\n",(0,i.jsx)("hr",{}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Input"}),": Images and videos"]}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Output"}),": Regions"]}),"\n",(0,i.jsx)(t.p,{children:"Visual detector, also known as object detection, is a type of deep fine-tuned model designed to identify and locate objects within images or video frames. It goes beyond simple image classification, where the goal is to assign a single label to an entire image."}),"\n",(0,i.jsx)(t.p,{children:'Instead, an object detection model can identify multiple objects of different classes within an image and provide their corresponding bounding box coordinates. They help answer the question "Where" are objects in your data.'}),"\n",(0,i.jsx)(t.p,{children:"The primary task of a visual detector model is twofold:"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Object localization"}),": The model identifies the location of objects within an image by predicting bounding box coordinates that tightly enclose each object."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Object classification"}),": The model classifies each detected object into one of several predefined classes or categories."]}),"\n"]}),"\n",(0,i.jsx)(t.admonition,{type:"info",children:(0,i.jsxs)(t.p,{children:["The visual detector model type also comes with various ",(0,i.jsx)(t.a,{href:"https://docs.clarifai.com/portal-guide/model/deep-training/visual-detection-templates",children:"templates"})," that give you the control to choose the specific architecture used by your neural network, as well as define a set of hyperparameters you can use to fine-tune the way your model learns."]})}),"\n",(0,i.jsx)(t.p,{children:"Visual detector models have a wide range of applications, including:"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Object detection"}),": This is the task of identifying and localizing objects in images. Visual detector models are used in a variety of applications, such as self-driving cars, security cameras, and robotics."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Image classification"}),': This is the task of classifying images into categories, such as "dog," "cat," or "tree." Visual detector models can be used to improve the accuracy of image classification models by providing them with information about the objects that are present in the image.']}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Visual tracking"}),": This is the task of tracking the movement of objects in images or videos. Visual detector models can be used to initialize visual trackers by identifying the objects that they need to track."]}),"\n"]}),"\n",(0,i.jsx)(t.p,{children:"You may choose the visual detector model type in cases where:"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:"You want to detect and localize objects within an image, and accuracy and the ability to carefully target solutions take priority over speed and ease of use."}),"\n",(0,i.jsxs)(t.li,{children:['You need a detection model to learn new features not recognized by the existing Clarifai models. In that case, you may need to "deep fine-tune" your custom model and integrate it directly within your ',(0,i.jsx)(t.a,{href:"https://docs.clarifai.com/portal-guide/workflows/",children:"workflows"}),"."]}),"\n",(0,i.jsx)(t.li,{children:"You have a custom-tailored dataset with bounding box annotations for objects, and the expertise and time to fine-tune models."}),"\n"]}),"\n",(0,i.jsx)(t.h2,{id:"example-use-case",children:"Example Use Case"}),"\n",(0,i.jsx)(t.p,{children:"A roofing company wants to provide insurance companies and customers with a consistent way of evaluating roof damage. This company captures images of roofs with a drone, and then feeds the images into a detection model. The detection model can then locate and classify specific areas of damage on the roofs."}),"\n",(0,i.jsx)(t.h2,{id:"create-and-train-a-visual-detector",children:"Create and Train a Visual Detector"}),"\n",(0,i.jsx)(t.p,{children:"Let's demonstrate how to create and train a visual detector model using our API."}),"\n",(0,i.jsx)(t.admonition,{type:"info",children:(0,i.jsxs)(t.p,{children:["Before using the ",(0,i.jsx)(t.a,{href:"https://docs.clarifai.com/additional-resources/api-overview/python-sdk",children:"Python SDK"}),", ",(0,i.jsx)(t.a,{href:"https://docs.clarifai.com/additional-resources/api-overview/nodejs-sdk",children:"Node.js SDK"}),", or any of our ",(0,i.jsx)(t.a,{href:"https://docs.clarifai.com/additional-resources/api-overview/grpc-clients",children:"gRPC clients"}),", ensure they are properly installed on your machine. Refer to their respective installation guides for instructions on how to install and initialize them."]})}),"\n","\n","\n","\n",(0,i.jsx)(t.h3,{id:"step-1-app-creation",children:"Step 1: App Creation"}),"\n",(0,i.jsxs)(t.p,{children:["Let's start by creating an ",(0,i.jsx)(t.a,{href:"https://docs.clarifai.com/create-manage/applications/create",children:"app"}),"."]}),"\n",(0,i.jsx)(o.A,{groupId:"code",children:(0,i.jsx)(r.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:d})})}),"\n",(0,i.jsx)(t.h3,{id:"step-2-dataset-upload",children:"Step 2: Dataset Upload"}),"\n",(0,i.jsxs)(t.p,{children:["Next, let\u2019s upload the ",(0,i.jsx)(t.a,{href:"https://docs.clarifai.com/create-manage/datasets/upload",children:"dataset"})," that will be used to train the model to the app."]}),"\n",(0,i.jsxs)(t.p,{children:["You can find the dataset we used ",(0,i.jsx)(t.a,{href:"https://github.com/Clarifai/examples/tree/main/datasets/upload/image_detection",children:"here"}),"."]}),"\n",(0,i.jsx)(o.A,{groupId:"code",children:(0,i.jsx)(r.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:c})})}),"\n",(0,i.jsx)(t.h3,{id:"step-3-model-creation",children:"Step 3: Model Creation"}),"\n",(0,i.jsx)(t.p,{children:"Let's list all the available trainable model types in the Clarifai platform."}),"\n",(0,i.jsx)(o.A,{groupId:"code",children:(0,i.jsx)(r.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:u})})}),"\n",(0,i.jsxs)(a,{children:[(0,i.jsx)("summary",{children:"Output"}),(0,i.jsx)(l.A,{className:"language-text",children:j})]}),"\n",(0,i.jsxs)(t.p,{children:["Next, let's select the ",(0,i.jsx)(t.code,{children:"visual-detector"})," model type and use it to create a model."]}),"\n",(0,i.jsx)(o.A,{groupId:"code",children:(0,i.jsx)(r.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:p})})}),"\n",(0,i.jsx)(t.h3,{id:"step-4-template-selection",children:"Step 4: Template Selection"}),"\n",(0,i.jsx)(t.p,{children:"Let's list all the available training templates in the Clarifai platform."}),"\n",(0,i.jsx)(o.A,{groupId:"code",children:(0,i.jsx)(r.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:h})})}),"\n",(0,i.jsxs)(a,{children:[(0,i.jsx)("summary",{children:"Output"}),(0,i.jsx)(l.A,{className:"language-text",children:b})]}),"\n",(0,i.jsxs)(t.p,{children:["Next, let's choose the ",(0,i.jsx)(t.code,{children:"'MMDetection_SSD'"})," template to use for training our model, as demonstrated below."]}),"\n",(0,i.jsx)(t.h3,{id:"step-5-set-up-model-parameters",children:"Step 5: Set Up Model Parameters"}),"\n",(0,i.jsx)(t.p,{children:"You can customize the model parameters as needed before starting the training process."}),"\n",(0,i.jsx)(o.A,{groupId:"code",children:(0,i.jsx)(r.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:m})})}),"\n",(0,i.jsxs)(a,{children:[(0,i.jsx)("summary",{children:"Output"}),(0,i.jsx)(l.A,{className:"language-text",children:y})]}),"\n",(0,i.jsx)(t.h3,{id:"step-6-initiate-model-training",children:"Step 6: Initiate Model Training"}),"\n",(0,i.jsxs)(t.p,{children:["To initiate the model training process, call the ",(0,i.jsx)(t.code,{children:"model.train()"})," method. The Clarifai API also provides features for monitoring training status and saving training logs to a local file."]}),"\n",(0,i.jsx)(t.admonition,{type:"note",children:(0,i.jsxs)(t.p,{children:["If the status code is ",(0,i.jsx)(t.code,{children:"MODEL-TRAINED"}),", it indicates that the model has been successfully trained and is ready for use."]})}),"\n",(0,i.jsx)(o.A,{groupId:"code",children:(0,i.jsx)(r.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:g})})}),"\n",(0,i.jsxs)(a,{children:[(0,i.jsx)("summary",{children:"Output"}),(0,i.jsx)("img",{src:"/img/python-sdk/vd_imt.png"})]}),"\n",(0,i.jsx)(t.h3,{id:"step-7-model-prediction",children:"Step 7: Model Prediction"}),"\n",(0,i.jsx)(t.p,{children:"After the model is trained and ready to use, you can run some predictions with it."}),"\n",(0,i.jsx)(o.A,{groupId:"code",children:(0,i.jsx)(r.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:f})})}),"\n",(0,i.jsxs)(a,{children:[(0,i.jsx)("summary",{children:"Image Output"}),(0,i.jsx)("img",{src:"/img/python-sdk/vd_mp.png"})]}),"\n",(0,i.jsx)(t.h3,{id:"step-8-model-evaluation",children:"Step 8: Model Evaluation"}),"\n",(0,i.jsx)(t.p,{children:"Let\u2019s evaluate the model using both the training and test datasets. We\u2019ll start by reviewing the evaluation metrics for the training dataset."}),"\n",(0,i.jsx)(o.A,{groupId:"code",children:(0,i.jsx)(r.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:_})})}),"\n",(0,i.jsxs)(a,{children:[(0,i.jsx)("summary",{children:"Output"}),(0,i.jsx)(l.A,{className:"language-text",children:w})]}),"\n",(0,i.jsx)(t.p,{children:"Before evaluating the model on the test dataset, ensure it is uploaded using the data loader. Once uploaded, proceed with the evaluation."}),"\n",(0,i.jsx)(o.A,{groupId:"code",children:(0,i.jsx)(r.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:v})})}),"\n",(0,i.jsxs)(a,{children:[(0,i.jsx)("summary",{children:"Output"}),(0,i.jsx)(l.A,{className:"language-text",children:A})]}),"\n",(0,i.jsxs)(t.p,{children:["Finally, to gain deeper insights into the model\u2019s performance, use the ",(0,i.jsx)(t.code,{children:"EvalResultCompare"})," method to compare results across multiple datasets."]}),"\n",(0,i.jsx)(o.A,{groupId:"code",children:(0,i.jsx)(r.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:x})})}),"\n",(0,i.jsxs)(a,{children:[(0,i.jsx)("summary",{children:"Output"}),(0,i.jsx)(l.A,{className:"language-text",children:I})]})]})}function E(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(C,{...e})}):C(e)}},79329:(e,t,a)=>{a.d(t,{A:()=>o});a(96540);var n=a(18215);const i={tabItem:"tabItem_Ymn6"};var s=a(74848);function o(e){let{children:t,hidden:a,className:o}=e;return(0,s.jsx)("div",{role:"tabpanel",className:(0,n.A)(i.tabItem,o),hidden:a,children:t})}}}]);
"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[3542],{65537:(e,n,r)=>{r.d(n,{A:()=>I});var t=r(96540),o=r(18215),i=r(65627),a=r(56347),s=r(50372),l=r(30604),c=r(11861),d=r(78749);function p(e){return t.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,t.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function u(e){const{values:n,children:r}=e;return(0,t.useMemo)((()=>{const e=n??function(e){return p(e).map((e=>{let{props:{value:n,label:r,attributes:t,default:o}}=e;return{value:n,label:r,attributes:t,default:o}}))}(r);return function(e){const n=(0,c.XI)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,r])}function m(e){let{value:n,tabValues:r}=e;return r.some((e=>e.value===n))}function h(e){let{queryString:n=!1,groupId:r}=e;const o=(0,a.W6)(),i=function(e){let{queryString:n=!1,groupId:r}=e;if("string"==typeof n)return n;if(!1===n)return null;if(!0===n&&!r)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return r??null}({queryString:n,groupId:r});return[(0,l.aZ)(i),(0,t.useCallback)((e=>{if(!i)return;const n=new URLSearchParams(o.location.search);n.set(i,e),o.replace({...o.location,search:n.toString()})}),[i,o])]}function f(e){const{defaultValue:n,queryString:r=!1,groupId:o}=e,i=u(e),[a,l]=(0,t.useState)((()=>function(e){let{defaultValue:n,tabValues:r}=e;if(0===r.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(n){if(!m({value:n,tabValues:r}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${n}" but none of its children has the corresponding value. Available values are: ${r.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return n}const t=r.find((e=>e.default))??r[0];if(!t)throw new Error("Unexpected error: 0 tabValues");return t.value}({defaultValue:n,tabValues:i}))),[c,p]=h({queryString:r,groupId:o}),[f,g]=function(e){let{groupId:n}=e;const r=function(e){return e?`docusaurus.tab.${e}`:null}(n),[o,i]=(0,d.Dv)(r);return[o,(0,t.useCallback)((e=>{r&&i.set(e)}),[r,i])]}({groupId:o}),y=(()=>{const e=c??f;return m({value:e,tabValues:i})?e:null})();(0,s.A)((()=>{y&&l(y)}),[y]);return{selectedValue:a,selectValue:(0,t.useCallback)((e=>{if(!m({value:e,tabValues:i}))throw new Error(`Can't select invalid tab value=${e}`);l(e),p(e),g(e)}),[p,g,i]),tabValues:i}}var g=r(9136);const y={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var v=r(74848);function _(e){let{className:n,block:r,selectedValue:t,selectValue:a,tabValues:s}=e;const l=[],{blockElementScrollPositionUntilNextRender:c}=(0,i.a_)(),d=e=>{const n=e.currentTarget,r=l.indexOf(n),o=s[r].value;o!==t&&(c(n),a(o))},p=e=>{let n=null;switch(e.key){case"Enter":d(e);break;case"ArrowRight":{const r=l.indexOf(e.currentTarget)+1;n=l[r]??l[0];break}case"ArrowLeft":{const r=l.indexOf(e.currentTarget)-1;n=l[r]??l[l.length-1];break}}n?.focus()};return(0,v.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,o.A)("tabs",{"tabs--block":r},n),children:s.map((e=>{let{value:n,label:r,attributes:i}=e;return(0,v.jsx)("li",{role:"tab",tabIndex:t===n?0:-1,"aria-selected":t===n,ref:e=>{l.push(e)},onKeyDown:p,onClick:d,...i,className:(0,o.A)("tabs__item",y.tabItem,i?.className,{"tabs__item--active":t===n}),children:r??n},n)}))})}function x(e){let{lazy:n,children:r,selectedValue:i}=e;const a=(Array.isArray(r)?r:[r]).filter(Boolean);if(n){const e=a.find((e=>e.props.value===i));return e?(0,t.cloneElement)(e,{className:(0,o.A)("margin-top--md",e.props.className)}):null}return(0,v.jsx)("div",{className:"margin-top--md",children:a.map(((e,n)=>(0,t.cloneElement)(e,{key:n,hidden:e.props.value!==i})))})}function b(e){const n=f(e);return(0,v.jsxs)("div",{className:(0,o.A)("tabs-container",y.tabList),children:[(0,v.jsx)(_,{...n,...e}),(0,v.jsx)(x,{...n,...e})]})}function I(e){const n=(0,g.A)();return(0,v.jsx)(b,{...e,children:p(e.children)},String(n))}},77413:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>v,contentTitle:()=>y,default:()=>b,frontMatter:()=>g,metadata:()=>t,toc:()=>_});const t=JSON.parse('{"id":"create/models/export","title":"Model Export","description":"Learn how to perform model export using Clarifai SDKs","source":"@site/docs/create/models/export.md","sourceDirName":"create/models","slug":"/create/models/export","permalink":"/create/models/export","draft":false,"unlisted":false,"editUrl":"https://github.com/Clarifai/docs/blob/main/docs/create/models/export.md","tags":[],"version":"current","sidebarPosition":7,"frontMatter":{"description":"Learn how to perform model export using Clarifai SDKs","sidebar_position":7},"sidebar":"tutorialSidebar","previous":{"title":"Evaluation Leaderboard","permalink":"/create/models/evaluate/leaderboard"},"next":{"title":"Workflows","permalink":"/create/workflows/"}}');var o=r(74848),i=r(28453),a=r(65537),s=r(79329),l=r(58069);const c='# Import necessary libraries\nimport os\nfrom clarifai.client.model import Model\n\n# Set the Clarifai API key as an environment variable (replace with your actual key)\nos.environ[\'CLARIFAI_PAT\'] = "YOUR_PAT" \n\n# Create a Model object using the model URL from the Clarifai portal\nmodel = Model("Model URL From Portal")\n\n# Set the model version ID (replace with the ID from the Clarifai portal)\nmodel.model_version.id = "Model Version ID From Portal" \n\n# Export the model to the specified path\nmodel.export("path to save .tar file")',d="import onnx  # Library for working with ONNX models\nimport onnxruntime as ort  # Library for running ONNX models\nimport numpy as np  # Library for numerical operations\nimport cv2  # Library for image processing\n\n# Commented-out code for model verification (uncomment if needed)\n# onnx_model = onnx.load(\"model/1/model.onnx\")\n# onnx.checker.check_model(onnx_model)\n\n# Load the input image using OpenCV\ninput_image = cv2.imread('ramen.png')\n\n# Expand the image dimension to match model input requirements \ninput_array = np.expand_dims(input_image, axis=0)\n\n# Swap axes and convert to float32 for potential model input requirements\ninput_array = np.swapaxes(input_array, 1, 3).astype(np.float32)\n\n# Create an inference session using the ONNX model\nort_sess = ort.InferenceSession('model/1/model.onnx') # replace with correct path to onnx model file\n\n# Run inference on the model with the preprocessed input\noutputs = ort_sess.run(None, {'input': input_array})\n\n# Extract the predicted class index from the output\npredicted = outputs[0][0].argmax(0)\n\n# Read class labels from a text file (replace filename if different)\nwith open(f'model/labels.txt') as f:\n    labels = f.readlines()\n\n# Print the predicted class label based on the index and labels list\nprint(labels[predicted])",p="# Import necessary libraries\nimport numpy as np\nimport cv2\nfrom tritonclient.grpc import (\n    InferenceServerClient,  # Client for interacting with Triton server\n    InferInput,  # Represents an input to the model\n    InferRequestedOutput,  # Represents an output requested for inference\n)\n\n# Define model name in our case the name is 'model'\nmodel_name = 'model'\n\n# Connect to Triton server running on localhost \ntriton_client = InferenceServerClient('127.0.0.1:9001')\n\n# Load the specified model onto the Triton server\ntriton_client.load_model(model_name)\n\n\n# Load the input image using OpenCV\ninput_image = cv2.imread('ramen.png') # replace with any image file\n\n# Expand the image dimension to match model input requirements\ninput_array = np.expand_dims(input_image, axis=0)\n\n# Define the model input object with its name, shape, and data type\nmodel_input = InferInput('input', input_array.shape, 'UINT8')\n\n# Set the data for the model input from the NumPy array\nmodel_input.set_data_from_numpy(input_array)\n\n# Run inference on the model with the provided input\nres = triton_client.infer(\n    model_name=model_name,  # Specify the model to use\n    inputs=[model_input],  # List of input objects\n    outputs=[InferRequestedOutput('probs')],  # List of requested outputs\n)\n\n# Read class labels from a text file \nwith open(f'{model_name}/labels.txt') as f:\n    labels = f.readlines()\n\n# Get the index of the class with the highest probability from the output\npredicted_class_index = np.argmax(res.as_numpy('probs'))\n\n# Print the predicted class label based on the index and labels list\nprint(labels[predicted_class_index])",u="Exporting model: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 912M/912M [00:22<00:00, 39.8MB/s]\n2024-04-18 11:44:30 INFO     clarifai.client.model:  Model ID model_classifier model.py:991\n                             with version 78a14f11871a4d5fa9dfa462fc81c1aa                 \n                             exported successfully to                                      \n                             /home/adithyansukumar/work/output/model.tar    ",m="id-ramen",h="root@c1ed01adc0c6:/run# ",f="I0418 10:22:19.527473 16089 server.cc:610] \n+---------+---------------------------------------+---------------------------------------+\n| Backend | Path                                  | Config                                |\n+---------+---------------------------------------+---------------------------------------+\n| python  | /opt/tritonserver/backends/python/lib | {\"cmdline\":{\"auto-complete-config\":\"f |\n|         | triton_python.so                      | alse\",\"min-compute-capability\":\"6.000 |\n|         |                                       | 000\",\"backend-directory\":\"/opt/triton |\n|         |                                       | server/backends\",\"default-max-batch-s |\n|         |                                       | ize\":\"4\"}}                            |\n|         |                                       |                                       |\n+---------+---------------------------------------+---------------------------------------+\n\nI0418 10:22:19.527498 16089 model_lifecycle.cc:264] ModelStates()\nI0418 10:22:19.527539 16089 server.cc:653] \n+-------+---------+--------+\n| Model | Version | Status |\n+-------+---------+--------+\n| model | 1       | READY  |\n+-------+---------+--------+\n\nI0418 10:22:19.652489 16089 metrics.cc:747] Collecting metrics for GPU 0: NVIDIA RTX 6000 Ada Generation\nI0418 10:22:19.652537 16089 metrics.cc:747] Collecting metrics for GPU 1: NVIDIA RTX 6000 Ada Generation\nI0418 10:22:19.652547 16089 metrics.cc:747] Collecting metrics for GPU 2: NVIDIA RTX 6000 Ada Generation\nI0418 10:22:19.652557 16089 metrics.cc:747] Collecting metrics for GPU 3: NVIDIA RTX 6000 Ada Generation\nI0418 10:22:19.652564 16089 metrics.cc:747] Collecting metrics for GPU 4: NVIDIA RTX 6000 Ada Generation\nI0418 10:22:19.652573 16089 metrics.cc:747] Collecting metrics for GPU 5: NVIDIA RTX 6000 Ada Generation\nI0418 10:22:19.652581 16089 metrics.cc:747] Collecting metrics for GPU 6: NVIDIA RTX 6000 Ada Generation\nI0418 10:22:19.652588 16089 metrics.cc:747] Collecting metrics for GPU 7: NVIDIA RTX 6000 Ada Generation\nI0418 10:22:19.653107 16089 metrics.cc:640] Collecting CPU metrics\nI0418 10:22:19.653341 16089 tritonserver.cc:2364] \n+----------------------------------+------------------------------------------------------+\n| Option                           | Value                                                |\n+----------------------------------+------------------------------------------------------+\n| server_id                        | triton                                               |\n| server_version                   | 2.32.0                                               |\n| server_extensions                | classification sequence model_repository model_repos |\n|                                  | itory(unload_dependents) schedule_policy model_confi |\n|                                  | guration system_shared_memory cuda_shared_memory bin |\n|                                  | ary_tensor_data parameters statistics trace logging  |\n| model_repository_path[0]         | /run                                                 |\n| model_control_mode               | MODE_EXPLICIT                                        |\n| startup_models_0                 | model                                                |\n| strict_model_config              | 1                                                    |\n| rate_limit                       | OFF                                                  |\n| pinned_memory_pool_byte_size     | 268435456                                            |\n| cuda_memory_pool_byte_size{0}    | 67108864                                             |\n| cuda_memory_pool_byte_size{1}    | 67108864                                             |\n| cuda_memory_pool_byte_size{2}    | 67108864                                             |\n| cuda_memory_pool_byte_size{3}    | 67108864                                             |\n| cuda_memory_pool_byte_size{4}    | 67108864                                             |\n| cuda_memory_pool_byte_size{5}    | 67108864                                             |\n| cuda_memory_pool_byte_size{6}    | 67108864                                             |\n| cuda_memory_pool_byte_size{7}    | 67108864                                             |\n| min_supported_compute_capability | 6.0                                                  |\n| strict_readiness                 | 1                                                    |\n| exit_timeout                     | 30                                                   |\n| cache_enabled                    | 0                                                    |\n+----------------------------------+------------------------------------------------------+\n\nI0418 10:22:19.654269 16089 grpc_server.cc:4888] === GRPC KeepAlive Options ===\nI0418 10:22:19.654283 16089 grpc_server.cc:4889] keepalive_time_ms: 7200000\nI0418 10:22:19.654288 16089 grpc_server.cc:4891] keepalive_timeout_ms: 20000\nI0418 10:22:19.654293 16089 grpc_server.cc:4893] keepalive_permit_without_calls: 0\nI0418 10:22:19.654299 16089 grpc_server.cc:4895] http2_max_pings_without_data: 2\nI0418 10:22:19.654305 16089 grpc_server.cc:4897] http2_min_recv_ping_interval_without_data_ms: 300000\nI0418 10:22:19.654312 16089 grpc_server.cc:4900] http2_max_ping_strikes: 2\nI0418 10:22:19.654320 16089 grpc_server.cc:4902] ==============================\nI0418 10:22:19.655139 16089 grpc_server.cc:227] Ready for RPC 'Check', 0\nI0418 10:22:19.655203 16089 grpc_server.cc:227] Ready for RPC 'ServerLive', 0\nI0418 10:22:19.655219 16089 grpc_server.cc:227] Ready for RPC 'ServerReady', 0\nI0418 10:22:19.655228 16089 grpc_server.cc:227] Ready for RPC 'ModelReady', 0\nI0418 10:22:19.655237 16089 grpc_server.cc:227] Ready for RPC 'ServerMetadata', 0\nI0418 10:22:19.655246 16089 grpc_server.cc:227] Ready for RPC 'ModelMetadata', 0\nI0418 10:22:19.655257 16089 grpc_server.cc:227] Ready for RPC 'ModelConfig', 0\nI0418 10:22:19.655270 16089 grpc_server.cc:227] Ready for RPC 'SystemSharedMemoryStatus', 0\nI0418 10:22:19.655277 16089 grpc_server.cc:227] Ready for RPC 'SystemSharedMemoryRegister', 0\nI0418 10:22:19.655286 16089 grpc_server.cc:227] Ready for RPC 'SystemSharedMemoryUnregister', 0\nI0418 10:22:19.655293 16089 grpc_server.cc:227] Ready for RPC 'CudaSharedMemoryStatus', 0\nI0418 10:22:19.655299 16089 grpc_server.cc:227] Ready for RPC 'CudaSharedMemoryRegister', 0\nI0418 10:22:19.655307 16089 grpc_server.cc:227] Ready for RPC 'CudaSharedMemoryUnregister', 0\nI0418 10:22:19.655316 16089 grpc_server.cc:227] Ready for RPC 'RepositoryIndex', 0\nI0418 10:22:19.655322 16089 grpc_server.cc:227] Ready for RPC 'RepositoryModelLoad', 0\nI0418 10:22:19.655330 16089 grpc_server.cc:227] Ready for RPC 'RepositoryModelUnload', 0\nI0418 10:22:19.655340 16089 grpc_server.cc:227] Ready for RPC 'ModelStatistics', 0\nI0418 10:22:19.655348 16089 grpc_server.cc:227] Ready for RPC 'Trace', 0\nI0418 10:22:19.655355 16089 grpc_server.cc:227] Ready for RPC 'Logging', 0\nI0418 10:22:19.655371 16089 grpc_server.cc:445] Thread started for CommonHandler\nI0418 10:22:19.655525 16089 grpc_server.cc:3952] New request handler for ModelInferHandler, 0\nI0418 10:22:19.655567 16089 grpc_server.cc:2844] Thread started for ModelInferHandler\nI0418 10:22:19.655706 16089 grpc_server.cc:3952] New request handler for ModelInferHandler, 0\nI0418 10:22:19.655748 16089 grpc_server.cc:2844] Thread started for ModelInferHandler\nI0418 10:22:19.655870 16089 grpc_server.cc:4348] New request handler for ModelStreamInferHandler, 0\nI0418 10:22:19.655901 16089 grpc_server.cc:2844] Thread started for ModelStreamInferHandler\nI0418 10:22:19.655909 16089 grpc_server.cc:4977] Started GRPCInferenceService at 0.0.0.0:9001\nI0418 10:22:19.656156 16089 http_server.cc:3518] Started HTTPService at 0.0.0.0:9000\nI0418 10:22:19.726777 16089 http_server.cc:186] Started Metrics Service at 0.0.0.0:9002\n",g={description:"Learn how to perform model export using Clarifai SDKs",sidebar_position:7},y="Model Export",v={},_=[{value:"Model Inference Using ONNX",id:"model-inference-using-onnx",level:2},{value:"Deployment Using Nvidia Triton",id:"deployment-using-nvidia-triton",level:2}];function x(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",header:"header",p:"p",pre:"pre",strong:"strong",...(0,i.R)(),...e.components},{Details:r}=n;return r||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"model-export",children:"Model Export"})}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Learn how to perform model export using Clarifai SDKs"})}),"\n",(0,o.jsx)("hr",{}),"\n",(0,o.jsxs)(n.p,{children:["Using the Clarifai SDKs, you can export models trained on the Clarifai Portal into a ",(0,o.jsx)(n.code,{children:".tar"})," file by specifying the model URL. This feature enables version control and facilitates seamless integration into various environments."]}),"\n",(0,o.jsxs)(n.p,{children:["The exported ",(0,o.jsx)(n.code,{children:".tar"})," file contains the model architecture, weights, and relevant training artifacts, providing a portable and deployment-ready package. Overall, model export via the Clarifai SDKs offers users greater flexibility and control over their machine learning workflows."]}),"\n",(0,o.jsx)(n.admonition,{type:"warning",children:(0,o.jsx)(n.p,{children:"Note that the model export functionality is only supported for specific model types on our platform."})}),"\n","\n","\n","\n",(0,o.jsx)(n.admonition,{type:"info",children:(0,o.jsxs)(n.p,{children:["Before using the ",(0,o.jsx)(n.a,{href:"https://docs.clarifai.com/additional-resources/api-overview/python-sdk",children:"Python SDK"}),", ",(0,o.jsx)(n.a,{href:"https://docs.clarifai.com/additional-resources/api-overview/nodejs-sdk",children:"Node.js SDK"}),", or any of our ",(0,o.jsx)(n.a,{href:"https://docs.clarifai.com/additional-resources/api-overview/grpc-clients",children:"gRPC clients"}),", ensure they are properly installed on your machine. Refer to their respective installation guides for instructions on how to install and initialize them."]})}),"\n",(0,o.jsx)(a.A,{children:(0,o.jsx)(s.A,{value:"python",label:"Python SDK",children:(0,o.jsx)(l.A,{className:"language-python",children:c})})}),"\n",(0,o.jsxs)(r,{children:[(0,o.jsx)("summary",{children:"Output"}),(0,o.jsx)(l.A,{className:"language-python",children:u})]}),"\n",(0,o.jsxs)(n.p,{children:["Before moving on to deployment, unpack the ",(0,o.jsx)(n.code,{children:"model.tar"})," file to get the required files.\nThe unpacked ",(0,o.jsx)(n.code,{children:"model.tar"})," folder structure will look like this,"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"\u251c\u2500\u2500 model\n   \u251c\u2500\u2500 1\n   \u2502   \u251c\u2500\u2500 lib\n   \u2502   \u251c\u2500\u2500 model.onnx\n   \u2502   \u251c\u2500\u2500 model.py\n   \u251c\u2500\u2500 config.pbtxt\n   \u251c\u2500\u2500 labels.txt\n   \u251c\u2500\u2500 requirements.txt\n   \u251c\u2500\u2500 triton_conda-cp3.8-72f240d2.tar.gz\n   \u2514\u2500\u2500 triton_server_info.proto\n"})}),"\n",(0,o.jsx)(n.h2,{id:"model-inference-using-onnx",children:"Model Inference Using ONNX"}),"\n",(0,o.jsx)(n.p,{children:"ONNX inference provides developers and data scientists with a standardized, efficient method for deploying machine learning models in production. By promoting interoperability across platforms and frameworks, ONNX simplifies deployment, enhances flexibility, and can improve performance."}),"\n",(0,o.jsx)(n.p,{children:"Acting as a universal bridge, ONNX enables seamless model execution without the need for repeated retraining or framework-specific conversions. This results in significant time and resource savings, making ONNX a powerful tool for scaling machine learning solutions across diverse environments."}),"\n",(0,o.jsxs)(n.p,{children:["Click ",(0,o.jsx)(n.a,{href:"https://onnxruntime.ai/docs/get-started/with-python.html",children:"here"})," to learn more about ONNX."]}),"\n",(0,o.jsx)(n.admonition,{type:"note",children:(0,o.jsxs)(n.p,{children:["Install the ",(0,o.jsx)(n.code,{children:"requirements.txt"})," file with ",(0,o.jsx)(n.code,{children:"pip install requirements.txt"}),"."]})}),"\n",(0,o.jsxs)(n.p,{children:["Below is an example of running predictions on a model using ONNX runtime. We are going to use ",(0,o.jsx)(n.code,{children:"model.onnx"})," file we received after unpacking the ",(0,o.jsx)(n.code,{children:"model.tar"})," file."]}),"\n",(0,o.jsx)(a.A,{children:(0,o.jsx)(s.A,{value:"python",label:"Python SDK",children:(0,o.jsx)(l.A,{className:"language-python",children:d})})}),"\n",(0,o.jsxs)(r,{children:[(0,o.jsx)("summary",{children:"Output"}),(0,o.jsx)(l.A,{className:"language-python",children:m})]}),"\n",(0,o.jsx)(n.h2,{id:"deployment-using-nvidia-triton",children:"Deployment Using Nvidia Triton"}),"\n",(0,o.jsx)(n.p,{children:"Once you've trained powerful machine learning models, deploying them efficiently for real-world applications becomes essential. NVIDIA Triton Inference Server serves as a robust bridge between your trained models and production environments."}),"\n",(0,o.jsx)(n.p,{children:"As an open-source platform, Triton is purpose-built to optimize and streamline the deployment and execution of machine learning models for inference, enabling high-performance, scalable, and flexible model serving across diverse use cases."}),"\n",(0,o.jsxs)(n.p,{children:["Click ",(0,o.jsx)(n.a,{href:"https://github.com/triton-inference-server/python_backend",children:"here"})," to learn more about Nvidia Triton."]}),"\n",(0,o.jsx)(n.p,{children:"Before we deploy our model we have to first set up the triton package on our local machine."}),"\n",(0,o.jsx)(n.admonition,{type:"info",children:(0,o.jsxs)(n.p,{children:["Make sure that you have Docker installed on your system. Follow the steps in this ",(0,o.jsx)(n.a,{href:"https://docs.docker.com/engine/install",children:"page"})," to install Docker."]})}),"\n",(0,o.jsx)(n.p,{children:"Execute the following command to run the triton inference container in your machine,"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"docker run --shm-size=1g --ulimit memlock=-1 -p 9000:9000 -p 9001:9001 -p 9002:9002 --ulimit stack=67108864 -ti nvcr.io/nvidia/tritonserver:23.03-py3 -v $(pwd):/run\n"})}),"\n",(0,o.jsxs)(r,{children:[(0,o.jsx)("summary",{children:"Output"}),(0,o.jsx)(l.A,{className:"language-python",children:h})]}),"\n",(0,o.jsx)(n.admonition,{type:"note",children:(0,o.jsxs)(n.p,{children:["Use a common directory to run the container and to extract the ",(0,o.jsx)(n.code,{children:"model.tar"})," file."]})}),"\n",(0,o.jsx)(n.p,{children:"Once you are inside the container then execute the following to start the triton server,"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"cd /run\n"})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"tritonserver --model-repository=/run --model-control-mode=explicit --disable-auto-complete-config --backend-config=python3,python-runtime=/usr/bin/python3 --backend-directory=/opt/tritonserver/backends --http-port=9000 --grpc-port=9001 --metrics-port=9002 --log-verbose=5 --load-model=model\n"})}),"\n",(0,o.jsx)(n.p,{children:"If you have followed the steps correctly, you should receive an output that looks similar to the one shown here,"}),"\n",(0,o.jsxs)(r,{children:[(0,o.jsx)("summary",{children:"Output"}),(0,o.jsx)(l.A,{className:"language-python",children:f})]}),"\n",(0,o.jsx)(n.p,{children:"Since the inference server is up and running successfully, let's create an inference script that will communicate with the server and return the prediction.\nBelow is an example inference script that does image classification using the exported model,"}),"\n",(0,o.jsx)(a.A,{children:(0,o.jsx)(s.A,{value:"python",label:"Python SDK",children:(0,o.jsx)(l.A,{className:"language-python",children:p})})}),"\n",(0,o.jsx)("br",{}),"\n",(0,o.jsxs)(r,{children:[(0,o.jsx)("summary",{children:"Output"}),(0,o.jsx)(l.A,{className:"language-python",children:m})]})]})}function b(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(x,{...e})}):x(e)}},79329:(e,n,r)=>{r.d(n,{A:()=>a});r(96540);var t=r(18215);const o={tabItem:"tabItem_Ymn6"};var i=r(74848);function a(e){let{children:n,hidden:r,className:a}=e;return(0,i.jsx)("div",{role:"tabpanel",className:(0,t.A)(o.tabItem,a),hidden:r,children:n})}}}]);
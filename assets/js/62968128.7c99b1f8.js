"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[1603],{58215:function(e,n,t){var o=t(67294);n.Z=function(e){var n=e.children,t=e.hidden,a=e.className;return o.createElement("div",{role:"tabpanel",hidden:t,className:a},n)}},26396:function(e,n,t){t.d(n,{Z:function(){return p}});var o=t(87462),a=t(67294),i=t(72389),r=t(79443);var s=function(){var e=(0,a.useContext)(r.Z);if(null==e)throw new Error('"useUserPreferencesContext" is used outside of "Layout" component.');return e},l=t(63616),d=t(86010),c="tabItem_vU9c";function u(e){var n,t,i,r=e.lazy,u=e.block,p=e.defaultValue,f=e.values,_=e.groupId,m=e.className,w=a.Children.map(e.children,(function(e){if((0,a.isValidElement)(e)&&void 0!==e.props.value)return e;throw new Error("Docusaurus error: Bad <Tabs> child <"+("string"==typeof e.type?e.type:e.type.name)+'>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.')})),h=null!=f?f:w.map((function(e){var n=e.props;return{value:n.value,label:n.label,attributes:n.attributes}})),O=(0,l.lx)(h,(function(e,n){return e.value===n.value}));if(O.length>0)throw new Error('Docusaurus error: Duplicate values "'+O.map((function(e){return e.value})).join(", ")+'" found in <Tabs>. Every value needs to be unique.');var D=null===p?p:null!=(n=null!=p?p:null==(t=w.find((function(e){return e.props.default})))?void 0:t.props.value)?n:null==(i=w[0])?void 0:i.props.value;if(null!==D&&!h.some((function(e){return e.value===D})))throw new Error('Docusaurus error: The <Tabs> has a defaultValue "'+D+'" but none of its children has the corresponding value. Available values are: '+h.map((function(e){return e.value})).join(", ")+". If you intend to show no default tab, use defaultValue={null} instead.");var g=s(),k=g.tabGroupChoices,I=g.setTabGroupChoices,b=(0,a.useState)(D),E=b[0],v=b[1],N=[],R=(0,l.o5)().blockElementScrollPositionUntilNextRender;if(null!=_){var W=k[_];null!=W&&W!==E&&h.some((function(e){return e.value===W}))&&v(W)}var T=function(e){var n=e.currentTarget,t=N.indexOf(n),o=h[t].value;o!==E&&(R(n),v(o),null!=_&&I(_,o))},L=function(e){var n,t=null;switch(e.key){case"ArrowRight":var o=N.indexOf(e.currentTarget)+1;t=N[o]||N[0];break;case"ArrowLeft":var a=N.indexOf(e.currentTarget)-1;t=N[a]||N[N.length-1]}null==(n=t)||n.focus()};return a.createElement("div",{className:"tabs-container"},a.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,d.Z)("tabs",{"tabs--block":u},m)},h.map((function(e){var n=e.value,t=e.label,i=e.attributes;return a.createElement("li",(0,o.Z)({role:"tab",tabIndex:E===n?0:-1,"aria-selected":E===n,key:n,ref:function(e){return N.push(e)},onKeyDown:L,onFocus:T,onClick:T},i,{className:(0,d.Z)("tabs__item",c,null==i?void 0:i.className,{"tabs__item--active":E===n})}),null!=t?t:n)}))),r?(0,a.cloneElement)(w.filter((function(e){return e.props.value===E}))[0],{className:"margin-vert--md"}):a.createElement("div",{className:"margin-vert--md"},w.map((function(e,n){return(0,a.cloneElement)(e,{key:n,hidden:e.props.value!==E})}))))}function p(e){var n=(0,i.Z)();return a.createElement(u,(0,o.Z)({key:String(n)},e))}},67574:function(e,n,t){t.r(n),t.d(n,{contentTitle:function(){return u},default:function(){return m},frontMatter:function(){return c},metadata:function(){return p},toc:function(){return f}});var o=t(87462),a=t(63366),i=(t(67294),t(3905)),r=t(26396),s=t(58215),l=t(19055),d=["components"],c={description:"Work with text in images, just like you work with encoded text.",sidebar_position:4},u="Visual Text Recognition",p={unversionedId:"api-guide/workflows/common-workflows/visual-text-recognition-walkthrough",id:"api-guide/workflows/common-workflows/visual-text-recognition-walkthrough",title:"Visual Text Recognition",description:"Work with text in images, just like you work with encoded text.",source:"@site/docs/api-guide/workflows/common-workflows/visual-text-recognition-walkthrough.md",sourceDirName:"api-guide/workflows/common-workflows",slug:"/api-guide/workflows/common-workflows/visual-text-recognition-walkthrough",permalink:"/api-guide/workflows/common-workflows/visual-text-recognition-walkthrough",tags:[],version:"current",sidebarPosition:4,frontMatter:{description:"Work with text in images, just like you work with encoded text.",sidebar_position:4},sidebar:"tutorialSidebar",previous:{title:"Custom KNN Face Classifier Workflow",permalink:"/api-guide/workflows/common-workflows/knn-face-classifier-workflow-walkthrough"},next:{title:"Search, Sort, Filter, and Save",permalink:"/api-guide/search/"}},f=[{value:"How VTR Works",id:"how-vtr-works",children:[],level:2},{value:"Building a VTR Workflow",id:"building-a-vtr-workflow",children:[],level:2}],_={toc:f};function m(e){var n=e.components,c=(0,a.Z)(e,d);return(0,i.kt)("wrapper",(0,o.Z)({},_,c,{components:n,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"visual-text-recognition"},"Visual Text Recognition"),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Work with text in images, just like you work with encoded text")),(0,i.kt)("hr",null),(0,i.kt)("p",null,"Visual text recognition (VTR) helps you convert printed text in images and videos into machine-encoded text. You can input a scanned document, a photo of a document, a scene-photo ","(","such as the text on signs and billboards",")",", or text superimposed on an image ","(","such as in a meme",")",", and output the words and individual characters present in the images."),(0,i.kt)("p",null,'VTR lets you "digitize" text so that it can be edited, searched, stored, displayed, and analyzed.'),(0,i.kt)("p",null,(0,i.kt)("img",{src:t(58025).Z})),(0,i.kt)("div",{className:"admonition admonition-tip alert alert--success"},(0,i.kt)("div",{parentName:"div",className:"admonition-heading"},(0,i.kt)("h5",{parentName:"div"},(0,i.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,i.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"12",height:"16",viewBox:"0 0 12 16"},(0,i.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"}))),"Note")),(0,i.kt)("div",{parentName:"div",className:"admonition-content"},(0,i.kt)("p",{parentName:"div"},"The current version of our VTR model is not designed for use with handwritten text or documents with tightly-packed text\u2014like you might see on the page of a novel, for example."))),(0,i.kt)("h2",{id:"how-vtr-works"},"How VTR Works"),(0,i.kt)("p",null,"VTR works by first detecting the location of text in your photos or video frames, then cropping the region where the text is present, and then finally running a specialized classification model that will extract text from the cropped image. To accomplish these different tasks, you will need to configure a workflow. "),(0,i.kt)("p",null,"You will then add these three models to your workflow:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Visual Text Detection")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"1.0 Cropper")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Visual Text Recognition"))),(0,i.kt)("div",{className:"admonition admonition-info alert alert--info"},(0,i.kt)("div",{parentName:"div",className:"admonition-heading"},(0,i.kt)("h5",{parentName:"div"},(0,i.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,i.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,i.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"}))),"info")),(0,i.kt)("div",{parentName:"div",className:"admonition-content"},(0,i.kt)("p",{parentName:"div"},"The initialization code used in the following example is outlined in detail on the ",(0,i.kt)("a",{parentName:"p",href:"https://docs.clarifai.com/api-guide/api-overview/api-clients/#client-installation-instructions"},"client installation page.")))),(0,i.kt)("h2",{id:"building-a-vtr-workflow"},"Building a VTR Workflow"),(0,i.kt)(r.Z,{mdxType:"Tabs"},(0,i.kt)(s.Z,{value:"grpc_python",label:"gRPC Python",mdxType:"TabItem"},(0,i.kt)(l.Z,{className:"language-python",mdxType:"CodeBlock"},"###################################################################################\n# In this section, we set the user authentication, app ID, and the details of the \n# VTR Workflow we want to build. Change these strings to run your own example.\n##################################################################################\n\nUSER_ID = 'YOUR_USER_ID_HERE'\n# Your PAT (Personal Access Token) can be found in the portal under Authentification\nPAT = 'YOUR_PAT_HERE'\nAPP_ID = 'YOUR_APP_ID_HERE'\n# Change these to build your own VTR Workflow\nWORKFLOW_ID = 'visual-text-recognition-id'\n\nWORKFLOWNODE_ID_1 = 'detect-concept'\nMODEL_ID_1 = '2419e2eae04d04f820e5cf3aba42d0c7'\nMODEL_VERSION_ID_1 = '75a5b92a0dec436a891b5ad224ac9170'\n\nWORKFLOWNODE_ID_2 = 'image-crop'\nMODEL_ID_2 = 'ce3f5832af7a4e56ae310d696cbbefd8'\nMODEL_VERSION_ID_2 = 'a78efb13f7774433aa2fd4864f41f0e6'\n\nWORKFLOWNODE_ID_3 = 'image-to-text'\nMODEL_ID_3 = '9fe78b4150a52794f86f237770141b33'\nMODEL_VERSION_ID_3 = 'd94413e582f341f68884cac72dbd2c7b'\n\n##########################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n##########################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (('authorization', 'Key ' + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID) # The userDataObject is required when using a PAT\n\npost_workflows_response = stub.PostWorkflows(\n    service_pb2.PostWorkflowsRequest(\n        user_app_id=userDataObject,  \n        workflows=[\n            resources_pb2.Workflow(\n                id=WORKFLOW_ID,\n                nodes=[\n                    resources_pb2.WorkflowNode(\n                        id=WORKFLOWNODE_ID_1,\n                        model=resources_pb2.Model(\n                            id=MODEL_ID_1,\n                            model_version=resources_pb2.ModelVersion(\n                                id=MODEL_VERSION_ID_1\n                            )\n                        )\n                    ),\n                    resources_pb2.WorkflowNode(\n                        id=WORKFLOWNODE_ID_2,\n                        model=resources_pb2.Model(\n                            id=MODEL_ID_2,\n                            model_version=resources_pb2.ModelVersion(\n                                id=MODEL_VERSION_ID_2\n                                )\n                            ),\n                            node_inputs=[\n                                resources_pb2.NodeInput(node_id=WORKFLOWNODE_ID_1)\n                            ]\n                        ),\n                    resources_pb2.WorkflowNode(\n                        id=WORKFLOWNODE_ID_3,\n                        model=resources_pb2.Model(\n                            id=MODEL_ID_3,\n                            model_version=resources_pb2.ModelVersion(\n                                id=MODEL_VERSION_ID_3\n                                )\n                            ),\n                            node_inputs=[\n                                resources_pb2.NodeInput(node_id=WORKFLOWNODE_ID_2)\n                            ]\n                        ),\n                ]\n            )\n        ]\n    ),\n    metadata=metadata\n)\n\nif post_workflows_response.status.code != status_code_pb2.SUCCESS:\n    raise Exception(\"Post workflows failed, status: \" + post_workflows_response.status.description)\n")),(0,i.kt)(s.Z,{value:"grpc_nodejs",label:"gRPC NodeJS",mdxType:"TabItem"},(0,i.kt)(l.Z,{className:"language-javascript",mdxType:"CodeBlock"},"//index.js file\n\n/////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, app ID, and the details of the \n// VTR Workflow we want to build. Change these strings to run your own example.\n/////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the portal under Authentification\nconst PAT = 'YOUR_PAT_HERE';\nconst APP_ID = 'YOUR_APP_ID_HERE';\n// Change these to build your own VTR Workflow\nconst WORKFLOW_ID = 'visual-text-recognition-id';\n\nconst WORKFLOWNODE_ID_1 = 'detect-concept';\nconst MODEL_ID_1 = '2419e2eae04d04f820e5cf3aba42d0c7';\nconst MODEL_VERSION_ID_1 = '75a5b92a0dec436a891b5ad224ac9170';\n\nconst WORKFLOWNODE_ID_2 = 'image-crop';\nconst MODEL_ID_2 = 'ce3f5832af7a4e56ae310d696cbbefd8';\nconst MODEL_VERSION_ID_2 = 'a78efb13f7774433aa2fd4864f41f0e6';\n\nconst WORKFLOWNODE_ID_3 = 'image-to-text';\nconst MODEL_ID_3 = '9fe78b4150a52794f86f237770141b33';\nconst MODEL_VERSION_ID_3 = 'd94413e582f341f68884cac72dbd2c7b';\n\n/////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n/////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require(\"clarifai-nodejs-grpc\");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set(\"authorization\", \"Key \" + PAT);\n\nstub.PostWorkflows(\n    {\n        user_app_id: {\n            app_id: APP_ID\n        },\n        workflows: [\n            {\n                id: WORKFLOW_ID,\n                nodes: [\n                    {\n                        id: WORKFLOWNODE_ID_1,\n                        model: {\n                            id: MODEL_ID_1,\n                            model_version: {\n                                id: MODEL_VERSION_ID_1\n                            }\n                        }\n                    },\n                    {\n                        id: WORKFLOWNODE_ID_2,\n                        model: {\n                            id: MODEL_ID_2,\n                            model_version: {\n                                id: MODEL_VERSION_ID_2\n                            }\n                        },\n                        node_inputs: [\n                            {\n                                node_id: WORKFLOWNODE_ID_1\n                            }\n                        ]\n                    },\n                    {\n                        id: WORKFLOWNODE_ID_3,\n                        model: {\n                            id: MODEL_ID_3,\n                            model_version: {\n                                id: MODEL_VERSION_ID_3\n                            }\n                        },\n                        node_inputs: [\n                            {\n                                node_id: WORKFLOWNODE_ID_2\n                            }\n                        ]\n                    },\n                ]\n            }\n        ]\n    },\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            console.log(response.status);\n            throw new Error(\"Post workflows failed, status: \" + response.status.description);\n        }\n    }\n);")),(0,i.kt)(s.Z,{value:"grpc_java",label:"gRPC Java",mdxType:"TabItem"},(0,i.kt)(l.Z,{className:"language-java",mdxType:"CodeBlock"},'package com.clarifai.example;\n\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.grpc.api.status.StatusCode;\n\npublic class ClarifaiExample {\n\n    /////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, app ID, and the details of the \n    // VTR Workflow we want to build. Change these strings to run your own example.\n    /////////////////////////////////////////////////////////////////////////////////////\n\n    static final String USER_ID = "YOUR_USER_ID_HERE";\n    //Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    static final String APP_ID = "YOUR_APP_ID_HERE";\n    // Change these to build your own VTR Workflow\n    static final String WORKFLOW_ID = "visual-text-recognition-id";\n\n    static final String WORKFLOWNODE_ID_1 = "detect-concept";\n    static final String MODEL_ID_1 = "2419e2eae04d04f820e5cf3aba42d0c7";\n    static final String MODEL_VERSION_ID_1 = "75a5b92a0dec436a891b5ad224ac9170";\n\n    static final String WORKFLOWNODE_ID_2 = "image-crop";\n    static final String MODEL_ID_2 = "ce3f5832af7a4e56ae310d696cbbefd8";\n    static final String MODEL_VERSION_ID_2 = "a78efb13f7774433aa2fd4864f41f0e6";\n\n    static final String WORKFLOWNODE_ID_3 = "image-to-text";\n    static final String MODEL_ID_3 = "9fe78b4150a52794f86f237770141b33";\n    static final String MODEL_VERSION_ID_3 = "d94413e582f341f68884cac72dbd2c7b";\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    public static void main(String[] args) {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n            .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        MultiWorkflowResponse postWorkflowsResponse = stub.postWorkflows(\n            PostWorkflowsRequest.newBuilder()\n            .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n            .addWorkflows(\n                Workflow.newBuilder()\n                .setId(WORKFLOW_ID)\n                .addNodes(\n                    WorkflowNode.newBuilder()\n                    .setId(WORKFLOWNODE_ID_1)\n                    .setModel(\n                        Model.newBuilder()\n                        .setId(MODEL_ID_1)\n                        .setModelVersion(\n                            ModelVersion.newBuilder()\n                            .setId(MODEL_VERSION_ID_1)\n                        )\n                    )\n                )\n                .addNodes(\n                    WorkflowNode.newBuilder()\n                    .setId(WORKFLOWNODE_ID_2)\n                    .setModel(\n                        Model.newBuilder()\n                        .setId(MODEL_ID_2)\n                        .setModelVersion(\n                            ModelVersion.newBuilder()\n                            .setId(MODEL_VERSION_ID_2)\n                        )\n                    )\n                    .addNodeInputs(NodeInput.newBuilder().setNodeId(WORKFLOWNODE_ID_1))\n                )\n                .addNodes(\n                    WorkflowNode.newBuilder()\n                    .setId(WORKFLOWNODE_ID_3)\n                    .setModel(\n                        Model.newBuilder()\n                        .setId(MODEL_ID_3)\n                        .setModelVersion(\n                            ModelVersion.newBuilder()\n                            .setId(MODEL_VERSION_ID_3)\n                        )\n                    )\n                    .addNodeInputs(NodeInput.newBuilder().setNodeId(WORKFLOWNODE_ID_2))\n                )\n            )\n            .build()\n        );\n\n        if (postWorkflowsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("Post workflows failed, status: " + postWorkflowsResponse.getStatus());\n        }\n\n    }\n\n}')),(0,i.kt)(s.Z,{value:"curl",label:"cURL",mdxType:"TabItem"},(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},'curl -X POST \'https://api.clarifai.com/v2/users/me/apps/{{app}}/workflows\' \\\n    -H \'Authorization: Key {{PAT}}\' \\\n    -H \'Content-Type: application/json\' \\\n    --data-raw \'{\n        "workflows": [\n            {\n                "id": "visual-text-recognition-id",\n                "nodes": [\n                    {\n                        "id": "detect-concept",\n                        "model": {\n                            "id": "2419e2eae04d04f820e5cf3aba42d0c7",\n                            "model_version": {\n                                "id": "75a5b92a0dec436a891b5ad224ac9170"\n                            }\n                        }\n                    },\n                    {\n                        "id": "image-crop",\n                        "model": {\n                            "id": "ce3f5832af7a4e56ae310d696cbbefd8",\n                            "model_version": {\n                                "id": "a78efb13f7774433aa2fd4864f41f0e6"\n                            }\n                        },\n                        "node_inputs": [\n                            {\n                                "node_id": "general-concept"\n                            }\n                        ]\n                    },\n                    {\n                        "id": "image-to-text",\n                        "model": {\n                            "id": "9fe78b4150a52794f86f237770141b33",\n                            "model_version": {\n                                "id": "d94413e582f341f68884cac72dbd2c7b"\n                            }\n                        },\n                        "node_inputs": [\n                            {\n                                "node_id": "image-crop"\n                            }\n                        ]\n                    },\n                ]\n            }\n        ]\n    }\'\n')))))}m.isMDXComponent=!0},58025:function(e,n,t){n.Z=t.p+"assets/images/vtr-7d53fc1ef292e02c58f04d0f9bed3df1.jpg"}}]);
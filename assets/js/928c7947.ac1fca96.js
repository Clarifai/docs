"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[2394],{11470:(e,n,t)=>{t.d(n,{A:()=>O});var a=t(96540),i=t(18215),s=t(17559),o=t(23104),r=t(56347),l=t(205),c=t(57485),p=t(31682),u=t(70679);function d(e){return a.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,a.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function h(e){const{values:n,children:t}=e;return(0,a.useMemo)(()=>{const e=n??function(e){return d(e).map(({props:{value:e,label:n,attributes:t,default:a}})=>({value:e,label:n,attributes:t,default:a}))}(t);return function(e){const n=(0,p.XI)(e,(e,n)=>e.value===n.value);if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[n,t])}function m({value:e,tabValues:n}){return n.some(n=>n.value===e)}function _({queryString:e=!1,groupId:n}){const t=(0,r.W6)(),i=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,c.aZ)(i),(0,a.useCallback)(e=>{if(!i)return;const n=new URLSearchParams(t.location.search);n.set(i,e),t.replace({...t.location,search:n.toString()})},[i,t])]}function f(e){const{defaultValue:n,queryString:t=!1,groupId:i}=e,s=h(e),[o,r]=(0,a.useState)(()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!m({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const t=n.find(e=>e.default)??n[0];if(!t)throw new Error("Unexpected error: 0 tabValues");return t.value}({defaultValue:n,tabValues:s})),[c,p]=_({queryString:t,groupId:i}),[d,f]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[t,i]=(0,u.Dv)(n);return[t,(0,a.useCallback)(e=>{n&&i.set(e)},[n,i])]}({groupId:i}),g=(()=>{const e=c??d;return m({value:e,tabValues:s})?e:null})();(0,l.A)(()=>{g&&r(g)},[g]);return{selectedValue:o,selectValue:(0,a.useCallback)(e=>{if(!m({value:e,tabValues:s}))throw new Error(`Can't select invalid tab value=${e}`);r(e),p(e),f(e)},[p,f,s]),tabValues:s}}var g=t(92303);const I={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var A=t(74848);function E({className:e,block:n,selectedValue:t,selectValue:a,tabValues:s}){const r=[],{blockElementScrollPositionUntilNextRender:l}=(0,o.a_)(),c=e=>{const n=e.currentTarget,i=r.indexOf(n),o=s[i].value;o!==t&&(l(n),a(o))},p=e=>{let n=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const t=r.indexOf(e.currentTarget)+1;n=r[t]??r[0];break}case"ArrowLeft":{const t=r.indexOf(e.currentTarget)-1;n=r[t]??r[r.length-1];break}}n?.focus()};return(0,A.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,i.A)("tabs",{"tabs--block":n},e),children:s.map(({value:e,label:n,attributes:a})=>(0,A.jsx)("li",{role:"tab",tabIndex:t===e?0:-1,"aria-selected":t===e,ref:e=>{r.push(e)},onKeyDown:p,onClick:c,...a,className:(0,i.A)("tabs__item",I.tabItem,a?.className,{"tabs__item--active":t===e}),children:n??e},e))})}function D({lazy:e,children:n,selectedValue:t}){const s=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=s.find(e=>e.props.value===t);return e?(0,a.cloneElement)(e,{className:(0,i.A)("margin-top--md",e.props.className)}):null}return(0,A.jsx)("div",{className:"margin-top--md",children:s.map((e,n)=>(0,a.cloneElement)(e,{key:n,hidden:e.props.value!==t}))})}function T(e){const n=f(e);return(0,A.jsxs)("div",{className:(0,i.A)(s.G.tabs.container,"tabs-container",I.tabList),children:[(0,A.jsx)(E,{...n,...e}),(0,A.jsx)(D,{...n,...e})]})}function O(e){const n=(0,g.A)();return(0,A.jsx)(T,{...e,children:d(e.children)},String(n))}},19365:(e,n,t)=>{t.d(n,{A:()=>o});t(96540);var a=t(18215);const i={tabItem:"tabItem_Ymn6"};var s=t(74848);function o({children:e,hidden:n,className:t}){return(0,s.jsx)("div",{role:"tabpanel",className:(0,a.A)(i.tabItem,t),hidden:n,children:e})}},66827:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>Rt,contentTitle:()=>St,default:()=>xt,frontMatter:()=>wt,metadata:()=>a,toc:()=>Pt});const a=JSON.parse('{"id":"compute/inference/clarifai/api-legacy","title":"Legacy Inference via API","description":"Generate predictions using our older method","source":"@site/docs/compute/inference/clarifai/api-legacy.md","sourceDirName":"compute/inference/clarifai","slug":"/compute/inference/clarifai/api-legacy","permalink":"/compute/inference/clarifai/api-legacy","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"description":"Generate predictions using our older method","sidebar_position":3,"toc_max_heading_level":4},"sidebar":"tutorialSidebar","previous":{"title":"Inference via API","permalink":"/compute/inference/clarifai/api"},"next":{"title":"Inference via UI","permalink":"/compute/inference/clarifai/ui"}}');var i=t(74848),s=t(28453),o=t(11470),r=t(19365),l=t(88149);const c='##################################################################################################\n# Change these strings to run your own example\n##################################################################################################\n\n# Your PAT (Personal Access Token) can be found in the Account\'s Security section\nPAT = "YOUR_PAT_HERE"\nUSER_ID = "YOUR_USER_ID_HERE"\nIMAGE_URL = "https://samples.clarifai.com/birds.jpg"\nMODEL_URL = "https://clarifai.com/qwen/qwen-VL/models/Qwen2_5-VL-7B-Instruct"\nDEPLOYMENT_ID = "YOUR_DEPLOYMENT_ID_HERE"\n\n##################################################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n##################################################################################################\n\nfrom clarifai.client.model import Model\n\n# Initialize the model\nmodel = Model(\n    url=MODEL_URL, # Or, use model_id="YOUR_MODEL_ID_HERE"\n    pat=PAT\n)\n\n# Make a unary-unary prediction using the image URL\nmodel_prediction = model.predict_by_url(\n    IMAGE_URL,\n    input_type="image",\n    user_id=USER_ID,\n    deployment_id=DEPLOYMENT_ID\n)\n\n# Output the model\'s response\nprint(model_prediction.outputs[0].data.text.raw)\n\n##################################################################################################\n# ADDITIONAL EXAMPLES\n##################################################################################################\n\n# Example prediction using a cluster and nodepool (no deployment ID needed):\n# model_prediction = Model(url=MODEL_URL, pat="YOUR_PAT_HERE").predict_by_url("INPUT_URL_HERE", input_type="image", user_id="YOUR_USER_ID_HERE", compute_cluster_id="YOUR_CLUSTER_ID_HERE", nodepool_id="YOUR_NODEPOOL_ID_HERE")\n\n# Example prediction via bytes:\n# model_prediction = Model(url=MODEL_URL, pat="YOUR_PAT_HERE").predict_by_bytes("INPUT_TEXT_HERE".encode(), input_type="text", user_id="YOUR_USER_ID_HERE", deployment_id="YOUR_DEPLOYMENT_ID_HERE")\n\n# Example prediction via filepath:\n# model_prediction = Model(url=MODEL_URL, pat="YOUR_PAT_HERE").predict_by_filepath("INPUT_FILEPATH_HERE", input_type="text", user_id="YOUR_USER_ID_HERE", deployment_id="YOUR_DEPLOYMENT_ID_HERE")\n',p='# Use the CLI to log in to the Clarifai platform first: https://docs.clarifai.com/additional-resources/api-overview/cli#login\n\nclarifai model predict --model_url https://clarifai.com/qwen/qwen-VL/models/Qwen2_5-VL-7B-Instruct --url https://samples.clarifai.com/birds.jpg --input_type image --deployment_id "YOUR_DEPLOYMENT_ID_HERE"\n',u='##################################################################################################\n# Change these strings to run your own example\n##################################################################################################\n\n# Your PAT (Personal Access Token) can be found in the Account\'s Security section\nPAT = "YOUR_PAT_HERE"\nUSER_ID = "YOUR_USER_ID_HERE"\nPROMPT = "What is the future of AI?"\nMODEL_URL = "https://clarifai.com/meta/Llama-3/models/Llama-3_2-3B-Instruct"\nDEPLOYMENT_ID = "YOUR_DEPLOYMENT_ID_HERE"\n\n##################################################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n##################################################################################################\n\nfrom clarifai.client.model import Model\n\n# Initialize the model\nmodel = Model(\n    url=MODEL_URL,  # Or, use model_id="YOUR_MODEL_ID_HERE"\n    pat=PAT\n)\n\n# Make a unary-stream prediction using the prompt as bytes\nresponse_stream = model.generate_by_bytes(\n    PROMPT.encode(),\n    input_type="text",\n    user_id=USER_ID,\n    deployment_id=DEPLOYMENT_ID\n)\n\n# Iterate through streamed responses and print them\nfor response in response_stream:\n    if response.outputs and response.outputs[0].data.text:\n        print(response.outputs[0].data.text.raw)\n\n# Print a newline at the end for better formatting\nprint()\n\n##################################################################################################\n# ADDITIONAL EXAMPLES\n##################################################################################################\n\n# Example stream prediction using a cluster and nodepool (no deployment ID needed):\n# for response in Model(url=MODEL_URL, pat="YOUR_PAT_HERE").generate_by_bytes("YOUR_PROMPT_HERE".encode(), input_type="text", user_id="YOUR_USER_ID_HERE", compute_cluster_id="YOUR_CLUSTER_ID", nodepool_id="YOUR_NODEPOOL_ID"):\n#     print(response.outputs[0].data.text.raw)\n\n# Example unary-stream prediction via URL:\n# for response in Model(url=MODEL_URL, pat="YOUR_PAT_HERE").generate_by_url("INPUT_URL_HERE", input_type="text", user_id="YOUR_USER_ID_HERE", deployment_id="YOUR_DEPLOYMENT_ID_HERE"):\n#     print(response.outputs[0].data.text.raw)\n\n# Example unary-stream prediction via filepath:\n# for response in Model(url=MODEL_URL, pat="YOUR_PAT_HERE").generate_by_filepath("INPUT_FILEPATH_HERE", input_type="text", user_id="YOUR_USER_ID_HERE", deployment_id="YOUR_DEPLOYMENT_ID_HERE"):\n#     print(response.outputs[0].data.text.raw)\n',d='##################################################################################################\n# Change these strings to run your own example\n##################################################################################################\n\n# Your PAT (Personal Access Token) can be found in the Account\'s Security section\nPAT = "YOUR_PAT_HERE"\nUSER_ID = "YOUR_USER_ID_HERE"\nPROMPTS = [\n    "What is the future of AI?",\n    "Explain quantum computing in simple terms.",\n    "How does climate change affect global economies?"\n]\nMODEL_URL = "https://clarifai.com/meta/Llama-3/models/Llama-3_2-3B-Instruct"\nDEPLOYMENT_ID = "YOUR_DEPLOYMENT_ID_HERE"\n\n##################################################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n##################################################################################################\n\nfrom clarifai.client.model import Model\n\n# Initialize the model\nmodel = Model(\n    url=MODEL_URL, # Or, use model_id="YOUR_MODEL_ID_HERE"\n    pat=PAT\n)\n\n# Prepare input iterator: each item is a bytes-encoded prompt\ninput_stream = (prompt.encode() for prompt in PROMPTS)\n\n# Stream-stream prediction using bytes\nresponse_stream = model.stream_by_bytes(\n    input_stream,\n    input_type="text",\n    user_id=USER_ID,\n    deployment_id=DEPLOYMENT_ID\n)\n\n# Iterate through streamed responses and print them\nfor response in response_stream:\n    if response.outputs and response.outputs[0].data.text:\n        print(response.outputs[0].data.text.raw)\n\n# Print a newline at the end for better formatting\nprint()\n\n##################################################################################################\n# ADDITIONAL EXAMPLES\n##################################################################################################\n\n# Example stream prediction using a cluster and nodepool (no deployment ID needed):\n# response_stream = Model(url=MODEL_URL, pat="YOUR_PAT_HERE").stream_by_bytes((prompt.encode() for prompt in PROMPTS), input_type="text", user_id="YOUR_USER_ID_HERE", compute_cluster_id="YOUR_CLUSTER_ID", nodepool_id="YOUR_NODEPOOL_ID")\n# for response in response_stream:\n#     print(response.outputs[0].data.text.raw)\n\n# Example stream prediction via URL:\n# response_stream = Model(url=MODEL_URL, pat="YOUR_PAT_HERE").stream_by_url(["INPUT_URL_1", "INPUT_URL_2"], input_type="text", user_id="YOUR_USER_ID_HERE", deployment_id="YOUR_DEPLOYMENT_ID_HERE")\n# for response in response_stream:\n#     print(response.outputs[0].data.text.raw)\n\n# Example stream prediction via filepath:\n# response_stream = Model(url=MODEL_URL, pat="YOUR_PAT_HERE").stream_by_filepath(["file1.txt", "file2.txt"], input_type="text", user_id="YOUR_USER_ID_HERE", deployment_id="YOUR_DEPLOYMENT_ID_HERE")\n# for response in response_stream:\n#     print(response.outputs[0].data.text.raw)\n',h="2025-04-04 13:27:15.990657 INFO     Qwen2_5-VL-7B-Instruct model is still deploying, please wait...         model.py:443\n2025-04-04 13:27:30.233847 INFO     Qwen2_5-VL-7B-Instruct model is still deploying, please wait...         model.py:443\n2025-04-04 13:27:45.624827 INFO     Qwen2_5-VL-7B-Instruct model is still deploying, please wait...         model.py:443\n2025-04-04 13:28:02.551081 INFO     Qwen2_5-VL-7B-Instruct model is still deploying, please wait...         model.py:443\nThis image captures three seagulls in flight over a body of water, likely a lake or river. The background is a natural setting with dry grass and trees, suggesting it might be late autumn or early spring. The seagulls appear to be gliding close to the water's surface, possibly searching for food. The lighting indicates it could be a sunny day. This scene is typical of coastal or lakeside environments where seagulls often congregate.\n",m="2025-04-04 15:10:09.952752 INFO     Llama-3_2-3B-Instruct model is still     model.py:726\n                                    deploying, please wait...\n2025-04-04 15:10:24.522422 INFO     Llama-3_2-3B-Instruct model is still     model.py:726\n                                    deploying, please wait...\nThe\n future\n of\n Artificial\n Intelligence\n (\nAI\n)\n is\n vast\n and\n rapidly\n evolving\n.\n Based\n on\n current\n trends\n and\n advancements\n,\n here\n are\n some\n potential\n developments\n that\n may\n shape\n the\n future\n of\n AI\n:\n\n\n**\nShort\n-term\n (\n202\n5\n-\n203\n0\n)**\n\n\n\n1\n.\n **\nIncreased\n Adoption\n**:\n AI\n will\n become\n more\n ubiquitous\n in\n various\n industries\n,\n including\n healthcare\n,\n finance\n,\n transportation\n,\n and\n education\n.\n\n2\n.\n **\nImproved\n Natural\n Language\n Processing\n (\nN\nLP\n)**\n:\n N\nLP\n will\n continue\n to\n advance\n,\n enabling\n more\n accurate\n and\n effective\n human\n-com\nputer\n interactions\n.\n\n3\n.\n **\nEnh\nanced\n Machine\n Learning\n (\nML\n)**\n:\n ML\n will\n become\n more\n sophisticated\n,\n allowing\n for\n more\n accurate\n predictions\n and\n decision\n-making\n.\n\n4\n.\n **\nR\nise\n of\n Explain\nable\n AI\n (\nX\nAI\n)**\n:\n X\nAI\n will\n become\n more\n prominent\n,\n enabling\n users\n to\n understand\n the\n reasoning\n behind\n AI\n decisions\n.\n\n\n**\nMid\n-term\n (\n203\n0\n-\n204\n0\n)**\n\n\n\n1\n.\n **\nArt\nificial\n General\n Intelligence\n (\nAG\nI\n)**\n:\n AG\nI\n,\n which\n refers\n to\n AI\n systems\n that\n can\n perform\n any\n intellectual\n task\n,\n may\n emerge\n.\n\n2\n.\n **\nQuant\num\n AI\n**:\n Quantum\n computing\n will\n be\n integrated\n with\n AI\n,\n leading\n to\n exponential\n advancements\n in\n processing\n power\n and\n AI\n capabilities\n.\n\n3\n.\n **\nEdge\n AI\n**:\n Edge\n AI\n will\n become\n more\n prevalent\n,\n enabling\n AI\n to\n be\n deployed\n at\n the\n edge\n of\n networks\n,\n reducing\n latency\n,\n and\n improving\n real\n-time\n decision\n-making\n.\n\n4\n.\n **\nHuman\n-A\nI\n Collaboration\n**:\n Humans\n and\n AI\n systems\n will\n collaborate\n more\n effectively\n,\n leading\n to\n increased\n productivity\n and\n innovation\n.\n\n\n**\nLong\n-term\n (\n204\n0\n-\n205\n0\n)**\n\n\n\n1\n.\n **\nM\nerging\n of\n Human\n and\n Machine\n Intelligence\n**:\n The\n line\n between\n human\n and\n machine\n intelligence\n will\n blur\n,\n leading\n to\n new\n forms\n of\n intelligence\n and\n cognition\n.\n\n2\n.\n **\nAut\nonomous\n Systems\n**:\n Autonomous\n systems\n,\n such\n as\n self\n-driving\n cars\n and\n drones\n,\n will\n become\n more\n common\n,\n revolution\nizing\n industries\n like\n transportation\n and\n logistics\n.\n\n3\n.\n **\nC\nognitive\n Architect\nures\n**:\n Cognitive\n architectures\n,\n which\n aim\n to\n create\n AI\n systems\n that\n can\n reason\n and\n learn\n like\n humans\n,\n will\n emerge\n.\n\n4\n.\n **\nAI\n Ethics\n and\n Governance\n**:\n As\n AI\n becomes\n more\n pervasive\n,\n there\n will\n be\n a\n growing\n need\n for\n AI\n ethics\n and\n governance\n frameworks\n to\n ensure\n responsible\n AI\n development\n and\n deployment\n.\n\n\n**\nPotential\n Ris\nks\n and\n Challenges\n**\n\n\n1\n.\n **\nJob\n Dis\nplacement\n**:\n AI\n may\n dis\nplace\n certain\n jobs\n,\n leading\n to\n significant\n social\n and\n economic\n impacts\n.\n\n2\n.\n **\nBias\n and\n Fair\nness\n**:\n AI\n systems\n may\n perpet\nuate\n existing\n biases\n and\n inequalities\n,\n highlighting\n the\n need\n for\n more\n diverse\n and\n inclusive\n AI\n development\n.\n\n3\n.\n **\nSecurity\n and\n Safety\n**:\n AI\n systems\n may\n pose\n new\n security\n and\n safety\n risks\n,\n such\n as\n autonomous\n systems\n malfunction\ning\n or\n being\n exploited\n.\n\n4\n.\n **\nValue\n Alignment\n**:\n AI\n systems\n may\n not\n align\n with\n human\n",_='from clarifai.client.model import Model\n\n# Your PAT (Personal Access Token) can be found in the Account\'s Security section\n# Specify the correct user_id/app_id pairings\n# Since you\'re making inferences outside your app\'s scope\n#USER_ID = "clarifai"\n#APP_ID = "main"\n\n# You can set the model using model URL or model ID.\n# Change these to whatever model you want to use\n# eg : MODEL_ID = "general-image-recognition"\n# You can also set a particular model version by specifying the  version ID\n# eg: MODEL_VERSION_ID = "aa7f35c01e0642fda5cf400f543e7c40"\n#  Model class objects can be inititalised by providing its URL or also by defining respective user_id, app_id and model_id\n\n# eg : model = Model(user_id="clarifai", app_id="main", model_id=MODEL_ID)\n\nmodel_url = "https://clarifai.com/clarifai/main/models/general-image-recognition"\nimage_url = "https://samples.clarifai.com/metro-north.jpg"\n\n# The predict API gives flexibility to generate predictions for data provided through URL,Filepath and bytes format.\n\n# Example for prediction through Bytes:\n# model_prediction = model.predict_by_bytes(input_bytes, input_type="image")\n\n# Example for prediction through Filepath:\n# model_prediction = Model(model_url).predict_by_filepath(filepath, input_type="image")\n\nmodel_prediction = Model(url=model_url, pat="YOUR_PAT").predict_by_url(\n    image_url, input_type="image"\n)\n\n# Get the output\nfor concept in model_prediction.outputs[0].data.concepts:\n    print(f"concept: {concept.name:<20} confidence: {round(concept.value, 3)}")\n',f='import { Model } from "clarifai-nodejs";\n\n/**\n    Your PAT (Personal Access Token) can be found in the Account\'s Security section\n    Specify the correct userId/appId pairings\n    Since you\'re making inferences outside your app\'s scope\n    USER_ID = "clarifai"\n    APP_ID = "main"\n\n    You can set the model using model URL or model ID.\n    Change these to whatever model you want to use\n    eg : MODEL_ID = "general-image-recognition"\n    You can also set a particular model version by specifying the  version ID\n    eg: MODEL_VERSION_ID = "aa7f35c01e0642fda5cf400f543e7c40"\n    Model class objects can be initialised by providing its URL or also by defining respective userId, appId and modelId\n\n    eg : \n    const model = new Model({\n        authConfig: {\n            userId: "clarifai",\n            appId: "main",\n            pat: process.env.CLARIFAI_PAT,\n        },\n        modelId: MODEL_ID,\n    });\n\n*/\n\n\nconst modelUrl =\n  "https://clarifai.com/clarifai/main/models/general-image-recognition";\nconst imageUrl = "https://samples.clarifai.com/metro-north.jpg";\n\nconst model = new Model({\n  url: modelUrl,\n  authConfig: {\n    pat: process.env.CLARIFAI_PAT,\n  },\n});\n/*\n        The predict API gives flexibility to generate predictions for data provided through URL, Filepath and bytes format.\n\n        Example for prediction through Bytes:\n        const modelPrediction = await model.predictByBytes({\n                                    inputBytes: Bytes,\n                                    inputType: "image"\n                                });\n\n        Example for prediction through Filepath:\n        const modelPrediction = await model.predictByFilepath({\n                                    filepath,\n                                    inputType: "image",\n                                });\n*/\n\nconst modelPrediction = await model.predictByUrl({\n  url: imageUrl,\n  inputType: "image",\n});\n\nconsole.log(modelPrediction?.[0].data);\n',g='from clarifai.client.model import Model\n\n# Your PAT (Personal Access Token) can be found in the Account\'s Security section\n# Specify the correct user_id/app_id pairings\n# Since you\'re making inferences outside your app\'s scope\n#USER_ID = "clarifai"\n#APP_ID = "main"\n\n# You can set the model using model URL or model ID.\n# Change these to whatever model you want to use\n# eg : MODEL_ID = \'general-image-detection\'\n# You can also set a particular model version by specifying the  version ID\n# eg: MODEL_VERSION_ID = \'1580bb1932594c93b7e2e04456af7c6f\'\n\n#  Model class objects can be inititalised by providing its URL or also by defining respective user_id, app_id and model_id\n# eg : model = Model(user_id="clarifai", app_id="main", model_id=MODEL_ID)\n\n\nDETECTION_IMAGE_URL = "https://s3.amazonaws.com/samples.clarifai.com/people_walking2.jpeg"\nmodel_url = "https://clarifai.com/clarifai/main/models/general-image-detection"\ndetector_model = Model(\n    url=model_url,\n    pat="YOUR_PAT",\n)\n\n\n# The predict API gives flexibility to generate predictions for data provided through URL,Filepath and bytes format.\n\n# Example for prediction through Bytes:\n# model_prediction = model.predict_by_bytes(input_bytes, input_type="image")\n\n# Example for prediction through Filepath:\n# model_prediction = Model(model_url).predict_by_filepath(filepath, input_type="image")\n\nprediction_response = detector_model.predict_by_url(\n    DETECTION_IMAGE_URL, input_type="image"\n)\n\n# Since we have one input, one output will exist here\nregions = prediction_response.outputs[0].data.regions\n\nfor region in regions:\n    # Accessing and rounding the bounding box values\n    top_row = round(region.region_info.bounding_box.top_row, 3)\n    left_col = round(region.region_info.bounding_box.left_col, 3)\n    bottom_row = round(region.region_info.bounding_box.bottom_row, 3)\n    right_col = round(region.region_info.bounding_box.right_col, 3)\n\n    for concept in region.data.concepts:\n        # Accessing and rounding the concept value\n        name = concept.name\n        value = round(concept.value, 4)\n\n        print(\n            (f"{name}: {value} BBox: {top_row}, {left_col}, {bottom_row}, {right_col}")\n        )\n',I='import { Model } from "clarifai-nodejs";\n\n/**\n    Your PAT (Personal Access Token) can be found in the Account\'s Security section\n    Specify the correct userId/appId pairings\n    Since you\'re making inferences outside your app\'s scope\n    USER_ID = "clarifai"\n    APP_ID = "main"\n\n    You can set the model using model URL or model ID.\n    Change these to whatever model you want to use\n    eg : MODEL_ID = \'general-image-detection\'\n    You can also set a particular model version by specifying the  version ID\n    eg: MODEL_VERSION_ID = \'1580bb1932594c93b7e2e04456af7c6f\'\n\n    Model class objects can be initialised by providing its URL or also by defining respective userId, appId and modelId\n    eg : \n    const model = new Model({\n        authConfig: {\n            userId: "clarifai",\n            appId: "main",\n            pat: process.env.CLARIFAI_PAT,\n        },\n        modelId: MODEL_ID,\n    });\n */\n\n\nconst DETECTION_IMAGE_URL =\n  "https://s3.amazonaws.com/samples.clarifai.com/people_walking2.jpeg";\nconst modelUrl =\n  "https://clarifai.com/clarifai/main/models/general-image-detection";\nconst detectorModel = new Model({\n  url: modelUrl,\n  authConfig: {\n    pat: process.env.CLARIFAI_PAT,\n  },\n});\n\n/*\n        The predict API gives flexibility to generate predictions for data provided through URL, Filepath and bytes format.\n\n        Example for prediction through Bytes:\n        const modelPrediction = await model.predictByBytes({\n                                    inputBytes: Bytes,\n                                    inputType: "image"\n                                });\n\n        Example for prediction through Filepath:\n        const modelPrediction = await model.predictByFilepath({\n                                    filepath,\n                                    inputType: "image",\n                                });\n*/\n\nconst detectorModelPrediction = await detectorModel.predictByUrl({\n  url: DETECTION_IMAGE_URL,\n  inputType: "image",\n});\n\n// Since we have one input, one output will exist here\nconst regions = detectorModelPrediction?.[0]?.data?.regionsList;\n\nif (regions) {\n  for (const region of regions) {\n    // Accessing and rounding the bounding box values\n    const top_row =\n      Math.round(region.regionInfo?.boundingBox?.topRow ?? 0 * 1000) / 1000;\n    const left_col =\n      Math.round(region.regionInfo?.boundingBox?.leftCol ?? 0 * 1000) / 1000;\n    const bottom_row =\n      Math.round(region.regionInfo?.boundingBox?.bottomRow ?? 0 * 1000) /\n      1000;\n    const right_col =\n      Math.round(region.regionInfo?.boundingBox?.rightCol ?? 0 * 1000) / 1000;\n\n    for (const concept of region.data?.conceptsList ?? []) {\n      // Accessing and rounding the concept value\n      const name = concept.name;\n      const value = Math.round(concept.value * 10000) / 10000;\n\n      console.log(\n        `${name}: ${value} BBox: ${top_row}, ${left_col}, ${bottom_row}, ${right_col}`,\n      );\n    }\n  }\n}\n',A='from clarifai.client.model import Model\n\n# Your PAT (Personal Access Token) can be found in the portal under # Authentification\n# Specify the correct user_id/app_id pairings\n# Since you\'re making inferences outside your app\'s scope\nUSER_ID = "clarifai"\nAPP_ID = "main"\n# Change these to whatever model and video URL you want to use\nMODEL_ID = "general-image-detection"\n# You can also set a particular model version by specifying the  version ID\n# eg: MODEL_VERSION_ID = \'1580bb1932594c93b7e2e04456af7c6f\'\n\nVIDEO_URL = "https://samples.clarifai.com/beer.mp4"\n# Change this to configure the FPS rate (If it\'s not configured, it defaults to 1 FPS)\n# The number must range betweeen 100 and 60000.\n# FPS = 1000/sample_ms\n\nSAMPLE_MS = 2000\n\n# Model class objects can be inititalised by providing its URL or also by defining respective user_id, app_id and model_id\n# eg: model = Model("https://clarifai.com/clarifai/main/models/general-image-detection")\n\n\nmodel = Model(user_id="clarifai", app_id="main", model_id=MODEL_ID, pat="YOUR_PAT")\noutput_config = {"sample_ms": SAMPLE_MS}  # Run inference every 2 seconds\nmodel_prediction = model.predict_by_url(\n    VIDEO_URL, input_type="video", output_config=output_config\n)\n\n# The predict API gives flexibility to generate predictions for data provided through filepath, URL and bytes format.\n\n# Example for prediction through Filepath:\n# model_prediction = model.predict_by_filepath(video_file_path, input_type="video", output_config=output_config)\n\n# Example for prediction through Bytes:\n# model_prediction = model.predict_by_bytes(input_video_bytes, input_type="video", output_config=output_config)\n\n\n# Print the frame info and the first concept name in each frame\nfor frame in model_prediction.outputs[0].data.frames:\n    print(f"Frame Info: {frame.frame_info} Concept: {frame.data.concepts[0].name}\\n")\n',E='import { Model } from "clarifai-nodejs";\nimport { OutputConfig } from "clarifai-nodejs-grpc/proto/clarifai/api/resources_pb";\n\n// Your PAT (Personal Access Token) can be found in the portal under Authentification\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\nconst USER_ID = "clarifai";\nconst APP_ID = "main";\n\n// Change these to whatever model and video URL you want to use\nconst MODEL_ID = "general-image-recognition";\n// You can also set a particular model version by specifying the version ID\n// eg: const MODEL_VERSION_ID = \'aa7f35c01e0642fda5cf400f543e7c40\';\n\nconst VIDEO_URL = "https://samples.clarifai.com/beer.mp4";\n// Change this to configure the FPS rate (If it\'s not configured, it defaults to 1 FPS)\n// The number must range between 100 and 60000.\n// FPS = 1000/sample_ms\n\nconst SAMPLE_MS = 2000;\n\n// Model class objects can be initialized by providing its URL or also by defining respective user_id, app_id, and model_id\n// eg: const model = new Model("https://clarifai.com/clarifai/main/models/general-image-recognition");\n\n/*\n        The predict API gives flexibility to generate predictions for data provided through URL, Filepath and bytes format.\n\n        Example for prediction through Bytes:\n        const modelPrediction = await model.predictByBytes({\n                                    inputBytes: Bytes,\n                                    inputType: "video"\n                                });\n\n        Example for prediction through Filepath:\n        const modelPrediction = await model.predictByFilepath({\n                                    filepath,\n                                    inputType: "video",\n                                });\n*/\n\nconst model = new Model({\n  authConfig: {\n    userId: USER_ID,\n    appId: APP_ID,\n    pat: process.env.CLARIFAI_PAT,\n  },\n  modelId: MODEL_ID,\n});\n\nconst outputConfig = new OutputConfig().setSampleMs(SAMPLE_MS); // Run inference every 2 seconds\n\n\nconst modelPrediction = await model.predictByUrl({\n  url: VIDEO_URL,\n  inputType: "video",\n  outputConfig,\n});\n\n// Print the frame info and the first concept name in each frame\nfor (const frame of modelPrediction?.[0].data?.framesList ?? []) {\n  console.log(\n    `Frame Info: ${JSON.stringify(frame.frameInfo, null, 2)} Concept: ${frame?.data?.conceptsList?.[0]?.name}\\n`,\n  );\n}\n',D='from clarifai.client.model import Model\n\n# Your PAT (Personal Access Token) can be found in the Account\'s Security section\n# Specify the correct user_id/app_id pairings\n# Since you\'re making inferences outside your app\'s scope\n#USER_ID = "clarifai"\n#APP_ID = "main"\n\n# You can set the model using model URL or model ID.\n# Change these to whatever model you want to use\n# eg : MODEL_ID = \'image-general-segmentation\'\n# You can also set a particular model version by specifying the  version ID\n# eg: MODEL_VERSION_ID = \'1581820110264581908ce024b12b4bfb\'\n#  Model class objects can be inititalised by providing its URL or also by defining respective user_id, app_id and model_id\n\n# eg : model = Model(user_id="clarifai", app_id="main", model_id=MODEL_ID)\n\nSEGMENT_IMAGE_URL = "https://s3.amazonaws.com/samples.clarifai.com/people_walking2.jpeg"\n\n# The predict API gives flexibility to generate predictions for data provided through URL,Filepath and bytes format.\n\n# Example for prediction through Bytes:\n# model_prediction = model.predict_by_bytes(input_bytes, input_type="image")\n\n# Example for prediction through Filepath:\n# model_prediction = Model(model_url).predict_by_filepath(filepath, input_type="image")\n\nmodel_url = "https://clarifai.com/clarifai/main/models/image-general-segmentation"\nsegmentor_model = Model(\n    url=model_url,\n    pat="YOUR_PAT",\n)\n\nprediction_response = segmentor_model.predict_by_url(\n    SEGMENT_IMAGE_URL, input_type="image"\n)\n\nregions = prediction_response.outputs[0].data.regions\n\nfor region in regions:\n    for concept in region.data.concepts:\n        # Accessing and rounding the concept\'s percentage of image covered\n        name = concept.name\n        value = round(concept.value, 4)\n        print((f"{name}: {value}"))\n',T='import { Model } from "clarifai-nodejs";\n\n/**\n    Your PAT (Personal Access Token) can be found in the Account\'s Security section\n    Specify the correct userId/appId pairings\n    Since you\'re making inferences outside your app\'s scope\n    USER_ID = "clarifai"\n    APP_ID = "main"\n\n    You can set the model using model URL or model ID.\n    Change these to whatever model you want to use\n    eg : MODEL_ID = \'image-general-segmentation\'\n    You can also set a particular model version by specifying the  version ID\n    eg: MODEL_VERSION_ID = "1581820110264581908ce024b12b4bfb"\n    Model class objects can be initialised by providing its URL or also by defining respective userId, appId and modelId\n\n    eg : \n    const model = new Model({\n        authConfig: {\n            userId: "clarifai",\n            appId: "main",\n            pat: process.env.CLARIFAI_PAT,\n        },\n        modelId: MODEL_ID,\n    });\n\n*/\n\nconst SEGMENT_IMAGE_URL =\n  "https://s3.amazonaws.com/samples.clarifai.com/people_walking2.jpeg";\n\n/*\n        The predict API gives flexibility to generate predictions for data provided through URL, Filepath and bytes format.\n\n        Example for prediction through Bytes:\n        const modelPrediction = await model.predictByBytes({\n                                    inputBytes: Bytes,\n                                    inputType: "image"\n                                });\n\n        Example for prediction through Filepath:\n        const modelPrediction = await model.predictByFilepath({\n                                    filepath,\n                                    inputType: "image",\n                                });\n    */\n\nconst modelUrl =\n  "https://clarifai.com/clarifai/main/models/image-general-segmentation";\nconst segmentorModel = new Model({\n  url: modelUrl,\n  authConfig: {\n    pat: process.env.CLARIFAI_PAT,\n  },\n});\n\n\nconst predictionResponse = await segmentorModel.predictByUrl({\n  url: SEGMENT_IMAGE_URL,\n  inputType: "image",\n});\n\nconst regions = predictionResponse?.[0]?.data?.regionsList ?? [];\n\nfor (const region of regions) {\n  for (const concept of region?.data?.conceptsList ?? []) {\n    // Accessing and rounding the concept\'s percentage of image covered\n    const name = concept.name;\n    const value = Math.round(concept.value * 10000) / 10000;\n    console.log(`${name}: ${value}`);\n  }\n}\n',O='from clarifai.client.model import Model\n\n# Your PAT (Personal Access Token) can be found in the Account\'s Security section\n# Specify the correct user_id/app_id pairings\n# Since you\'re making inferences outside your app\'s scope\n#USER_ID = "salesforce"\n#APP_ID = "blip"\n\n# You can set the model using model URL or model ID.\n# Change these to whatever model you want to use\n# eg : MODEL_ID = "general-english-image-caption-blip"\n# You can also set a particular model version by specifying the  version ID\n# eg: MODEL_VERSION_ID = "cdb690f13e62470ea6723642044f95e4"\n#  Model class objects can be inititalised by providing its URL or also by defining respective user_id, app_id and model_id\n\n# eg : model = Model(user_id="clarifai", app_id="main", model_id=MODEL_ID)\n\nmodel_url = (\n    "https://clarifai.com/salesforce/blip/models/general-english-image-caption-blip"\n)\nimage_url = "https://s3.amazonaws.com/samples.clarifai.com/featured-models/image-captioning-statue-of-liberty.jpeg"\n\n# The Predict API also accepts data through URL, Filepath & Bytes.\n\n# Example for predict by filepath:\n# model_prediction = Model(model_url).predict_by_filepath(filepath, input_type="image")\n\n# Example for predict by bytes:\n# model_prediction = Model(model_url).predict_by_bytes(image_bytes, input_type="image")\n\nmodel_prediction = Model(url=model_url, pat="YOUR_PAT").predict_by_url(\n    image_url, input_type="image"\n)\n\n# Get the output\nprint(model_prediction.outputs[0].data.text.raw)\n',b='import { Model } from "clarifai-nodejs";\n\n/**\n    Your PAT (Personal Access Token) can be found in the Account\'s Security section\n    Specify the correct userId/appId pairings\n    Since you\'re making inferences outside your app\'s scope\n    USER_ID = "salesforce"\n    APP_ID = "blip"\n\n    You can set the model using model URL or model ID.\n    Change these to whatever model you want to use\n    eg : MODEL_ID = "general-english-image-caption-blip"\n    You can also set a particular model version by specifying the  version ID\n    eg: MODEL_VERSION_ID = "cdb690f13e62470ea6723642044f95e4"\n    Model class objects can be initialised by providing its URL or also by defining respective userId, appId and modelId\n\n    eg : \n    const model = new Model({\n        authConfig: {\n            userId: "clarifai",\n            appId: "main",\n            pat: process.env.CLARIFAI_PAT,\n        },\n        modelId: MODEL_ID,\n    });\n\n*/\n\nconst modelUrl =\n  "https://clarifai.com/salesforce/blip/models/general-english-image-caption-blip";\nconst imageUrl =\n  "https://s3.amazonaws.com/samples.clarifai.com/featured-models/image-captioning-statue-of-liberty.jpeg";\n\n/*\n        The predict API gives flexibility to generate predictions for data provided through URL, Filepath and bytes format.\n\n        Example for prediction through Bytes:\n        const modelPrediction = await model.predictByBytes({\n                                    inputBytes: Bytes,\n                                    inputType: "image"\n                                });\n\n        Example for prediction through Filepath:\n        const modelPrediction = await model.predictByFilepath({\n                                    filepath,\n                                    inputType: "image",\n                                });\n\n    */\n\n\nconst model = new Model({\n  url: modelUrl,\n  authConfig: {\n    pat: process.env.CLARIFAI_PAT,\n  },\n});\nconst modelPrediction = await model.predictByUrl({\n  url: imageUrl,\n  inputType: "image",\n});\n\n// Get the output\nconsole.log(modelPrediction?.[0]?.data?.text?.raw);\n',y='from clarifai.client.model import Model\n\n# Your PAT (Personal Access Token) can be found in the Account\'s Security section\n# Specify the correct user_id/app_id pairings\n# Since you\'re making inferences outside your app\'s scope\n#USER_ID = "stability-ai"\n#APP_ID = "Upscale"\n\n# You can set the model using model URL or model ID.\n# Change these to whatever model you want to use\n# eg : MODEL_ID = \'stabilityai-upscale\'\n# You can also set a particular model version by specifying the  version ID\n# eg: MODEL_VERSION_ID = \'model_version\'\n#  Model class objects can be inititalised by providing its URL or also by defining respective user_id, app_id and model_id\n\n# eg : model = Model(user_id="clarifai", app_id="main", model_id=MODEL_ID)\n\n\ninference_params = dict(width=1024)\n\n# The predict API gives the flexibility to generate predictions for data provided through URL, Filepath and bytes format.\n\n# Example for prediction through Bytes:\n# model_prediction = model.predict_by_bytes(image_bytes, input_type="image")\n\n# Example for prediction through Filepath:\n# model_prediction = Model(model_url).predict_by_filepath(image_filepath, input_type="image")\n\nmodel_url = "https://clarifai.com/stability-ai/Upscale/models/stabilityai-upscale"\n\nimage_url = "https://s3.amazonaws.com/samples.clarifai.com/featured-models/image-captioning-statue-of-liberty.jpeg"\n\n\nmodel_prediction = Model(url=model_url, pat="YOUR_PAT").predict_by_url(\n    image_url, input_type="image", inference_params=inference_params\n)\n\n# Get the output\noutput_base64 = model_prediction.outputs[0].data.image.base64\n\nimage_info = model_prediction.outputs[0].data.image.image_info\n\nwith open("image.png", "wb") as f:\n    f.write(output_base64)\n',w='import { Model } from "clarifai-nodejs";\nimport fs from "fs";\n\n/**\n    Your PAT (Personal Access Token) can be found in the Account\'s Security section\n    Specify the correct userId/appId pairings\n    Since you\'re making inferences outside your app\'s scope\n    USER_ID = "stability-ai"\n    APP_ID = "Upscale"\n\n    You can set the model using model URL or model ID.\n    Change these to whatever model you want to use\n    eg : MODEL_ID = "stabilityai-upscale"\n    You can also set a particular model version by specifying the  version ID\n    eg: MODEL_VERSION_ID = "model_version"\n    Model class objects can be initialised by providing its URL or also by defining respective userId, appId and modelId\n\n    eg : \n    const model = new Model({\n        authConfig: {\n            userId: "clarifai",\n            appId: "main",\n            pat: process.env.CLARIFAI_PAT,\n        },\n        modelId: MODEL_ID,\n    });\n\n*/\n\nconst inferenceParams = { width: 1024 };\n\n/*\n        The predict API gives flexibility to generate predictions for data provided through URL, Filepath and bytes format.\n\n        Example for prediction through Bytes:\n        const modelPrediction = await model.predictByBytes({\n                                    inputBytes: Bytes,\n                                    inputType: "image"\n                                });\n\n        Example for prediction through Filepath:\n        const modelPrediction = await model.predictByFilepath({\n                                    filepath,\n                                    inputType: "image"\n                                });\n\n    */\n\nconst modelUrl =\n  "https://clarifai.com/stability-ai/Upscale/models/stabilityai-upscale";\n\nconst imageUrl =\n  "https://s3.amazonaws.com/samples.clarifai.com/featured-models/image-captioning-statue-of-liberty.jpeg";\n\n\nconst model = new Model({\n  url: modelUrl,\n  authConfig: {\n    pat: process.env.CLARIFAI_PAT,\n  },\n});\nconst prediction = await model.predictByUrl({\n  url: imageUrl,\n  inputType: "image",\n  inferenceParams,\n});\n\n// Get the output\nconst outputBase64 = prediction?.[0]?.data?.image?.base64 ?? "";\n\n// const imageInfo = prediction?.[0]?.data?.image?.imageInfo;\n\n// Write the output to a file\nfs.writeFileSync("image.png", outputBase64, "base64");\n',S='from clarifai.client.model import Model\n\n# Your PAT (Personal Access Token) can be found in the Account\'s Security section\n# Specify the correct user_id/app_id pairings\n# Since you\'re making inferences outside your app\'s scope\n#USER_ID = "clarifai"\n#APP_ID = "main"\n\n# You can set the model using model URL or model ID.\n# Change these to whatever model you want to use\n# eg : MODEL_ID = \'image-embedder-clip\'\n# You can also set a particular model version by specifying the  version ID\n# eg: MODEL_VERSION_ID = \'model_version\'\n#  Model class objects can be inititalised by providing its URL or also by defining respective user_id, app_id and model_id\n\n# eg : model = Model(user_id="clarifai", app_id="main", model_id=MODEL_ID)\n\nimage_url = "https://s3.amazonaws.com/samples.clarifai.com/featured-models/general-elephants.jpg"\n\n\n# The predict API gives the flexibility to generate predictions for data provided through URL, Filepath and bytes format.\n\n# Example for prediction through Bytes:\n# model_prediction = Model(model_url).predict_by_url(input_bytes ,input_type="image")\n\n# Example for prediction through Filepath:\n# model_prediction = Model(model_url).predict_by_filepath(image_filepath, input_type="image")\n\n\nmodel_url = "https://clarifai.com/clarifai/main/models/image-embedder-clip"\n\n# Model Predict\nmodel_prediction = Model(url=model_url, pat="YOUR_PAT").predict_by_url(\n    image_url, "image"\n)\n# print(model_prediction.outputs[0].data.text.raw)\n\nembeddings = model_prediction.outputs[0].data.embeddings[0].vector\n\nnum_dimensions = model_prediction.outputs[0].data.embeddings[0].num_dimensions\n\nprint(embeddings[:10])\n',R='import { Model } from "clarifai-nodejs";\n\n/**\n    Your PAT (Personal Access Token) can be found in the Account\'s Security section\n    Specify the correct userId/appId pairings\n    Since you\'re making inferences outside your app\'s scope\n    USER_ID = "clarifai"\n    APP_ID = "main"\n\n    You can set the model using model URL or model ID.\n    Change these to whatever model you want to use\n    eg : MODEL_ID = "image-embedder-clip"\n    You can also set a particular model version by specifying the  version ID\n    eg: MODEL_VERSION_ID = "model_version"\n    Model class objects can be initialised by providing its URL or also by defining respective userId, appId and modelId\n\n    eg : \n    const model = new Model({\n        authConfig: {\n            userId: "clarifai",\n            appId: "main",\n            pat: process.env.CLARIFAI_PAT,\n        },\n        modelId: MODEL_ID,\n    });\n*/\n\nconst imageUrl =\n  "https://s3.amazonaws.com/samples.clarifai.com/featured-models/general-elephants.jpg";\n\n/*\n        The predict API gives flexibility to generate predictions for data provided through URL, Filepath and bytes format.\n\n        Example for prediction through Bytes:\n        const modelPrediction = await model.predictByBytes({\n                                    inputBytes: Bytes,\n                                    inputType: "image"\n                                });\n\n        Example for prediction through Filepath:\n        const modelPrediction = await model.predictByFilepath({\n                                    filepath,\n                                    inputType: "image",\n                                });\n*/\n\nconst modelUrl =\n  "https://clarifai.com/clarifai/main/models/image-embedder-clip";\n\n\nconst model = new Model({\n  url: modelUrl,\n  authConfig: {\n    pat: process.env.CLARIFAI_PAT,\n  },\n});\n\n// Model Predict\nconst modelPrediction = await model.predictByUrl({\n  url: imageUrl,\n  inputType: "image",\n});\n\n// console.log(modelPrediction?.[0]?.data?.text?.raw);\n\nconst embeddings =\n  modelPrediction?.[0]?.data?.embeddingsList?.[0]?.vectorList ?? [];\n\n// const numDimensions =\n//   modelPrediction?.[0]?.data?.embeddingsList?.[0]?.numDimensions ?? 0;\n\nconsole.log(embeddings.slice(0, 10));\n',P="concept: statue               confidence: 0.996\nconcept: sculpture            confidence: 0.994\nconcept: architecture         confidence: 0.991\nconcept: travel               confidence: 0.988\nconcept: no person            confidence: 0.981\nconcept: art                  confidence: 0.98\nconcept: sky                  confidence: 0.973\nconcept: monument             confidence: 0.968\nconcept: city                 confidence: 0.962\nconcept: liberty              confidence: 0.955\nconcept: fame                 confidence: 0.949\nconcept: sightseeing          confidence: 0.948\nconcept: ancient              confidence: 0.945\nconcept: old                  confidence: 0.942\nconcept: public square        confidence: 0.938\nconcept: popularity           confidence: 0.928\nconcept: torch                confidence: 0.908\nconcept: bronze               confidence: 0.891\nconcept: outdoors             confidence: 0.884\nconcept: marble               confidence: 0.868",v="Footwear: 0.9618 BBox: 0.879, 0.305, 0.925, 0.327\n\nFootwear: 0.9593 BBox: 0.882, 0.284, 0.922, 0.305\n\nFootwear: 0.9571 BBox: 0.874, 0.401, 0.923, 0.418\n\nFootwear: 0.9546 BBox: 0.87, 0.712, 0.916, 0.732\n\nFootwear: 0.9518 BBox: 0.882, 0.605, 0.918, 0.623\n\nFootwear: 0.95 BBox: 0.847, 0.587, 0.907, 0.604\n\nFootwear: 0.9349 BBox: 0.878, 0.475, 0.917, 0.492\n\nTree: 0.9145 BBox: 0.009, 0.019, 0.451, 0.542\n\nFootwear: 0.9127 BBox: 0.858, 0.393, 0.909, 0.407\n\nFootwear: 0.8969 BBox: 0.812, 0.433, 0.844, 0.445\n\nFootwear: 0.8747 BBox: 0.852, 0.49, 0.912, 0.506\n\nJeans: 0.8699 BBox: 0.511, 0.255, 0.917, 0.336\n\nFootwear: 0.8203 BBox: 0.808, 0.453, 0.833, 0.465\n\nFootwear: 0.8186 BBox: 0.8, 0.378, 0.834, 0.391\n\nJeans: 0.7921 BBox: 0.715, 0.273, 0.895, 0.326\n\nTree: 0.7851 BBox: 0.0, 0.512, 0.635, 0.998\n\nWoman: 0.7693 BBox: 0.466, 0.36, 0.915, 0.449\n\nJeans: 0.7614 BBox: 0.567, 0.567, 0.901, 0.647\n\nFootwear: 0.7287 BBox: 0.847, 0.494, 0.884, 0.51\n\nTree: 0.7216 BBox: 0.002, 0.005, 0.474, 0.14\n\nJeans: 0.7098 BBox: 0.493, 0.447, 0.914, 0.528\n\nFootwear: 0.6929 BBox: 0.808, 0.424, 0.839, 0.437\n\nJeans: 0.6734 BBox: 0.728, 0.464, 0.887, 0.515\n\nWoman: 0.6141 BBox: 0.464, 0.674, 0.922, 0.782\n\nHuman leg: 0.6032 BBox: 0.681, 0.577, 0.897, 0.634\n\n...\n\nFootwear: 0.3527 BBox: 0.844, 0.5, 0.875, 0.515\n\nFootwear: 0.3395 BBox: 0.863, 0.396, 0.914, 0.413\n\nHuman hair: 0.3358 BBox: 0.443, 0.586, 0.505, 0.622\n\nTree: 0.3306 BBox: 0.6, 0.759, 0.805, 0.929",x="Frame Info: time: 1000\n\n Concept: beer\n\nFrame Info: index: 1\n\ntime: 3000\n\n Concept: beer\n\nFrame Info: index: 2\n\ntime: 5000\n\n Concept: beer\n\nFrame Info: index: 3\n\ntime: 7000\n\n Concept: beer\n\nFrame Info: index: 4\n\ntime: 9000\n\n Concept: beer",C="tree: 0.4965\n\nperson: 0.151\n\nhouse: 0.0872\n\npavement: 0.0694\n\nbush: 0.0588\n\nroad: 0.0519\n\nsky-other: 0.0401\n\ngrass: 0.0296\n\nbuilding-other: 0.0096\n\nunlabeled: 0.0035\n\nroof: 0.0017\n\nteddy bear: 0.0006",L="a photograph of a statue of liberty in front of a blue sky",U="[-0.016209319233894348,\n\n -0.03517452999949455,\n\n 0.0031261674594134092,\n\n 0.03941042721271515,\n\n 0.01166260801255703,\n\n -0.02489173412322998,\n\n 0.04667072370648384,\n\n 0.006998186931014061,\n\n 0.05729646235704422,\n\n 0.0077746850438416]",N="clarifai model predict --model_url https://clarifai.com/clarifai/main/models/general-image-recognition --url https://samples.clarifai.com/metro-north.jpg --input_type image",M="clarifai model predict --model_url https://clarifai.com/clarifai/main/models/general-image-detection --url https://s3.amazonaws.com/samples.clarifai.com/people_walking2.jpeg --input_type image",j='clarifai model predict --model_url https://clarifai.com/clarifai/main/models/general-image-detection --url https://samples.clarifai.com/beer.mp4 --input_type video --output_config "{\\"sample_ms\\":2000}"',H="clarifai model predict --model_url https://clarifai.com/clarifai/main/models/image-general-segmentation --url https://s3.amazonaws.com/samples.clarifai.com/people_walking2.jpeg --input_type image",B="clarifai model predict --model_url https://clarifai.com/salesforce/blip/models/general-english-image-caption-blip --url https://s3.amazonaws.com/samples.clarifai.com/featured-models/image-captioning-statue-of-liberty.jpeg --input_type image",Y='clarifai model predict --model_url https://clarifai.com/stability-ai/Upscale/models/stabilityai-upscale --url https://s3.amazonaws.com/samples.clarifai.com/featured-models/image-captioning-statue-of-liberty.jpeg --input_type image --inference_params "{\\"width\\":\\"1024\\"}"',k="clarifai model predict --model_url https://clarifai.com/clarifai/main/models/image-embedder-clip --url https://s3.amazonaws.com/samples.clarifai.com/featured-models/general-elephants.jpg --input_type image \n",$='from clarifai.client.model import Model\n\n# Your PAT (Personal Access Token) can be found in the Account\'s Security section\n# Specify the correct user_id/app_id pairings\n# Since you\'re making inferences outside your app\'s scope\n#USER_ID = "nlptownres"\n#APP_ID = "text-classification"\n\n# Text sentiment analysis with 3 classes positive, negative, neutral.\n# You can set the model using model URL or model ID.\n# Change these to whatever model you want to use\n# eg : MODEL_ID = \'sentiment-analysis-twitter-roberta-base\'\n# You can also set a particular model version by specifying the  version ID\n# eg: MODEL_VERSION_ID = \'aa7f35c01e0642fda5cf400f543e7c40\'\n#  Model class objects can be inititalised by providing its URL or also by defining respective user_id, app_id and model_id\n\n\n# eg : model = Model(user_id="clarifai", app_id="main", model_id=MODEL_ID)\n\nmodel_url = "https://clarifai.com/erfan/text-classification/models/sentiment-analysis-twitter-roberta-base"\n\n# The predict API gives flexibility to generate predictions for data provided through URL,Filepath and bytes format.\n\n# Example for prediction through Bytes:\n# model_prediction = model.predict_by_bytes(input_bytes, input_type="text")\n\n# Example for prediction through URL:\n# model_prediction = Model(model_url).predict_by_url(URL, input_type="text")\n\n\nfile_path = "datasets/upload/data/text_files/positive/0_9.txt"\nmodel_prediction = Model(url=model_url, pat="YOUR_PAT").predict_by_filepath(\n    file_path, input_type="text"\n)\n\n# Get the output\nfor concept in model_prediction.outputs[0].data.concepts:\n    print(f"concept: {concept.name:<20} confidence: {round(concept.value, 3)}")\n',F='import { Model } from "clarifai-nodejs";\nimport path from "path";\n\n/**\n    Your PAT (Personal Access Token) can be found in the Account\'s Security section\n    Specify the correct userId/appId pairings\n    Since you\'re making inferences outside your app\'s scope\n    USER_ID = "nlptownres"\n    APP_ID = "text-classification"\n\n    You can set the model using model URL or model ID.\n    Change these to whatever model you want to use\n    eg : MODEL_ID = "sentiment-analysis-twitter-roberta-base"\n    You can also set a particular model version by specifying the  version ID\n    eg: MODEL_VERSION_ID = "aa7f35c01e0642fda5cf400f543e7c40"\n    Model class objects can be initialised by providing its URL or also by defining respective userId, appId and modelId\n\n    eg : \n    const model = new Model({\n        authConfig: {\n            userId: "clarifai",\n            appId: "main",\n            pat: process.env.CLARIFAI_PAT,\n        },\n        modelId: MODEL_ID,\n    });\n\n*/\n\nconst modelUrl =\n  "https://clarifai.com/erfan/text-classification/models/sentiment-analysis-twitter-roberta-base";\n\n/*\n        The predict API gives flexibility to generate predictions for data provided through URL, Filepath and bytes format.\n\n        Example for prediction through Bytes:\n        const modelPrediction = await model.predictByBytes({\n                                    inputBytes: Bytes,\n                                    inputType: "Text"\n                                });\n\n        Example for prediction through URL:\n        const modelPrediction = await model.predictByURL({\n                                    url: URL,\n                                    inputType: "Text",\n                                });\n*/\n\nconst filepath = path.resolve(__dirname, "../../../assets/sample.txt");\n\nconst model = new Model({\n  url: modelUrl,\n  authConfig: {\n    pat: process.env.CLARIFAI_PAT,\n  },\n});\nconst modelPrediction = await model.predictByFilepath({\n  filepath,\n  inputType: "text",\n});\n\n// Get the output\nconsole.log(\n  modelPrediction?.[modelPrediction.length - 1]?.data?.conceptsList,\n);',G='from clarifai.client.model import Model\n\n# Your PAT (Personal Access Token) can be found in the Account\'s Security section\nprompt = "What\u2019s the future of AI?"\n# You can set the model using model URL or model ID.\nmodel_url="https://clarifai.com/openai/chat-completion/models/GPT-4"\n\n# The predict API gives the flexibility to generate predictions for data provided through URL, Filepath and Bytes format. \n\n# Example for prediction through URL:\n# model_prediction = Model(model_url).predict_by_url(url, input_type="text")\n\n# Example for prediction through Filepath\n# model_prediction = Model(model_url).predict_by_filepath(filepath, input_type="text")\n\n\n# Model Predict\nmodel_prediction = Model(url=model_url,pat="YOUR_PAT").predict_by_bytes(prompt.encode(), input_type="text")\n\nprint(model_prediction.outputs[0].data.text.raw)',V='from clarifai.client.model import Model\n\n# Your PAT (Personal Access Token) can be found in the Account\'s Security section\n\nprompt = "What\u2019s the future of AI?"\n\n# You can set inference parameters\nprompt_template = \'\'\'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\n{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\'\'\'\n\nsystem_prompt= "You\'re the helpful assistant"\n\ninference_params = dict(temperature=0.7, max_tokens=200, top_k = 50, top_p= 0.95, prompt_template= prompt_template, system_prompt=system_prompt)\n\n# You can set the model using model URL or model ID.\nmodel_url="https://clarifai.com/meta/Llama-3/models/llama-3_1-8b-instruct"\n\n# The predict API gives the flexibility to generate predictions for data provided through URL, Filepath and Bytes format.\n\n# Example for prediction through URL:\n# model_prediction = Model(model_url).predict_by_url(url, input_type="text", inference_params=inference_params)\n\n# Example for prediction through Filepath\n# model_prediction = Model(model_url).predict_by_filepath(filepath, input_type="text", inference_params=inference_params)\n\n# Model Predict\nmodel_prediction = Model(url=model_url,pat="YOUR_PAT").predict_by_bytes(prompt.encode(), input_type="text", inference_params=inference_params)\n\nprint(model_prediction.outputs[0].data.text.raw)\n',q='import { Model } from "clarifai-nodejs";\n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\nconst prompt = "What\u2019s the future of AI?";\n// You can set the model using model URL or model ID.\nconst modelUrl = "https://clarifai.com/openai/chat-completion/models/GPT-4";\n\n/*\n        The predict API gives flexibility to generate predictions for data provided through URL, Filepath and bytes format.\n\n        Example for prediction through Filepath:\n        const modelPrediction = await model.predictByFilepath({\n                                    filepath,\n                                    inputType: "Text",\n                                });\n\n        Example for prediction through URL:\n        const modelPrediction = await model.predictByURL({\n                                    url: URL,\n                                    inputType: "Text",\n                                });\n*/\n\n// Model Predict\nconst model = new Model({\n  url: modelUrl,\n  authConfig: {\n    pat: process.env.CLARIFAI_PAT,\n  },\n});\n\n\nconst modelPrediction = await model.predictByBytes({\n  inputBytes: Buffer.from(prompt),\n  inputType: "text",\n});\n\nconsole.log(modelPrediction?.[0]?.data?.text?.raw);\n',X='from clarifai.client.model import Model\n\nmodel_url = "https://clarifai.com/openai/chat-completion/models/GPT-4"\n\nprompt = """Classes: [`positive`, `negative`, `neutral`]\nText: Sunny weather makes me happy.\n\nClassify the text into one of the above classes."""\n\n# The predict API gives the flexibility to generate predictions for data provided through URL, Filepath and bytes format.\n\n# Example for prediction through URL:\n# model_prediction = Model(model_url).predict_by_url(url, input_type="text")\n\n# Example for prediction through Filepath\n# model_prediction = Model(model_url).predict_by_filepath(filepath, input_type="text")\n\n# Model Predict\nmodel_prediction = Model(model_url, pat="YOUR_PAT").predict_by_bytes(prompt.encode(), input_type="text")\n\n#Output\nprint(model_prediction.outputs[0].data.text.raw)',Q='import { Model } from "clarifai-nodejs";\n\nconst prompt = `Classes: [\'positive\', \'negative\', \'neutral\']\nText: Sunny weather makes me happy.\n\nClassify the text into one of the above classes.`;\n\n/*\n        The predict API gives flexibility to generate predictions for data provided through URL, Filepath and bytes format.\n\n        Example for prediction through Filepath:\n        const modelPrediction = await model.predictByFilepath({\n                                    filepath,\n                                    inputType: "Text",\n                                });\n\n        Example for prediction through URL:\n        const modelPrediction = await model.predictByURL({\n                                    url: URL,\n                                    inputType: "Text",\n                                });\n    */\n\n// Model Predict\nconst model = new Model({\n  url: "https://clarifai.com/openai/chat-completion/models/GPT-4",\n  authConfig: {\n    pat: process.env.CLARIFAI_PAT,\n  },\n});\n\n\nconst modelPrediction = await model.predictByBytes({\n  inputBytes: Buffer.from(prompt),\n  inputType: "text",\n});\n\nconsole.log(modelPrediction?.[0]?.data?.text?.raw);\n',K='from clarifai.client.model import Model\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\n# Your PAT (Personal Access Token) can be found in the Account\'s Security section\n# Specify the correct user_id/app_id pairings\n# Since you\'re making inferences outside your app\'s scope\n#USER_ID = "stability-ai"\n#APP_ID = "stable-diffusion-2"\n\n# You can set the model using model URL or model ID.\n# Change these to whatever model you want to use\n# eg : MODEL_ID = \'stable-diffusion-xl\'\n# You can also set a particular model version by specifying the  version ID\n# eg: MODEL_VERSION_ID = \'0c919cc1edfc455dbc96207753f178d7\'\n#  Model class objects can be inititalised by providing its URL or also by defining respective user_id, app_id and model_id\n\n# eg : model = Model(user_id="clarifai", app_id="main", model_id=MODEL_ID)\n\ninput_text = b"floor plan for 2 bedroom kitchen house"\n\n# The predict API gives flexibility to generate predictions for data provided through URL,Filepath and Bytes format.\n\n# Example for prediction through URL:\n# model_prediction = Model(model_url).predict_by_url(url, input_type="text")\n\n# Example for prediction through Filepath:\n# model_prediction = Model(model_url).predict_by_filepath(filepath, input_type="text")\n\n# Image Generation using Stable Diffusion XL\nmodel_url = "https://clarifai.com/stability-ai/stable-diffusion-2/models/stable-diffusion-xl"\n\nmodel_prediction = Model(url=model_url, pat="YOUR_PAT").predict_by_bytes(\n    input_text, input_type="text"\n)\n\n\n# Base64 image to numpy array\nim_b = model_prediction.outputs[0].data.image.base64\nimage_np = np.frombuffer(im_b, np.uint8)\nimg_np = cv2.imdecode(image_np, cv2.IMREAD_COLOR)\n# Display the image\nplt.axis("off")\nplt.imshow(img_np[..., ::-1])\n',W='import { Model } from "clarifai-nodejs";\nimport fs from "fs";\n\n/**\n    Your PAT (Personal Access Token) can be found in the Account\'s Security section\n    Specify the correct userId/appId pairings\n    Since you\'re making inferences outside your app\'s scope\n    USER_ID = "stability-ai"\n    APP_ID = "stable-diffusion-2"\n\n    You can set the model using model URL or model ID.\n    Change these to whatever model you want to use\n    eg : MODEL_ID = "stable-diffusion-xl"\n    You can also set a particular model version by specifying the  version ID\n    eg: MODEL_VERSION_ID = "0c919cc1edfc455dbc96207753f178d7"\n    Model class objects can be initialised by providing its URL or also by defining respective userId, appId and modelId\n\n    eg : \n    const model = new Model({\n        authConfig: {\n            userId: "clarifai",\n            appId: "main",\n            pat: process.env.CLARIFAI_PAT,\n        },\n        modelId: MODEL_ID,\n    });\n\n*/\n\nconst inputText: Buffer = Buffer.from("floor plan for 2 bedroom kitchen house");\n\n/*\n        The predict API gives flexibility to generate predictions for data provided through URL, Filepath and bytes format.\n\n        Example for prediction through Filepath:\n        const modelPrediction = await model.predictByFilepath({\n                                    filepath,\n                                    inputType: "Text",\n                                });\n\n        Example for prediction through URL:\n        const modelPrediction = await model.predictByURL({\n                                    url: URL,\n                                    inputType: "Text",\n                                });\n*/\n\n// Image Generation using Stable Diffusion XL\nconst modelUrl =\n  "https://clarifai.com/stability-ai/stable-diffusion-2/models/stable-diffusion-xl";\n\n\nconst model = new Model({\n  url: modelUrl,\n  authConfig: { pat: process.env.CLARIFAI_PAT },\n});\n\nconst modelPrediction = await model.predictByBytes({\n  inputBytes: inputText,\n  inputType: "text",\n});\n\n// Base64 image to numpy array\nconst outputBase64 = modelPrediction?.[0]?.data?.image?.base64 ?? "";\n\nfs.writeFileSync("image.png", outputBase64, "base64");\n',z='from clarifai.client.model import Model\n\n# Your PAT (Personal Access Token) can be found in the Account\'s Security section\n# Specify the correct user_id/app_id pairings\n# Since you\'re making inferences outside your app\'s scope\n#USER_ID = "eleven-labs"\n#APP_ID = "audio-generation"\n\n# You can set the model using model URL or model ID.\n# Change these to whatever model you want to use\n# eg : MODEL_ID = \'speech-synthesis\'\n# You can also set a particular model version by specifying the  version ID\n# eg: MODEL_VERSION_ID = \'f588d92c044d4487a38c8f3d7a3b0eb2\'\n#  Model class objects can be inititalised by providing its URL or also by defining respective user_id, app_id and model_id\n\n# eg : model = Model(user_id="clarifai", app_id="main", model_id=MODEL_ID)\n\n\ninput_text = "Hello, How are you doing today!"\n\n# The predict API gives flexibility to generate predictions for data provided through URL,Filepath and bytes format.\n\n# Example for prediction through URL:\n# model_prediction = Model(model_url).predict_by_url(url, input_type="text")\n\n# Example for prediction through Filepath:\n# model_prediction = Model(model_url).predict_by_filepath(filepath, input_type="text")\n\nmodel_url = "https://clarifai.com/eleven-labs/audio-generation/models/speech-synthesis"\n\nmodel_prediction = Model(url=model_url, pat="YOUR_PAT").predict_by_bytes(input_text, "text")\n\n# Save the audio file\nwith open("output_audio.wav", mode="bx") as f:\n    f.write(model_prediction.outputs[0].data.audio.base64)\n',J='import { Model } from "clarifai-nodejs";\nimport fs from "fs";\n\n/**\n    Your PAT (Personal Access Token) can be found in the Account\'s Security section\n    Specify the correct userId/appId pairings\n    Since you\'re making inferences outside your app\'s scope\n    USER_ID = "eleven-labs"\n    APP_ID = "audio-generation"\n\n    You can set the model using model URL or model ID.\n    Change these to whatever model you want to use\n    eg : MODEL_ID = \'speech-synthesis\'\n    You can also set a particular model version by specifying the  version ID\n    eg: MODEL_VERSION_ID = "f588d92c044d4487a38c8f3d7a3b0eb2"\n    Model class objects can be initialised by providing its URL or also by defining respective userId, appId and modelId\n\n    eg : \n    const model = new Model({\n        authConfig: {\n            userId: "clarifai",\n            appId: "main",\n            pat: process.env.CLARIFAI_PAT,\n        },\n        modelId: MODEL_ID,\n    });\n\n*/\n\nconst inputText = Buffer.from("Hello, How are you doing today!");\n\n/*\n        The predict API gives flexibility to generate predictions for data provided through URL, Filepath and bytes format.\n\n        Example for prediction through Filepath:\n        const modelPrediction = await model.predictByFilepath({\n                                    filepath,\n                                    inputType: "Text",\n                                });\n\n        Example for prediction through URL:\n        const modelPrediction = await model.predictByURL({\n                                    url: URL,\n                                    inputType: "Text",\n                                });\n*/\n\nconst modelUrl =\n  "https://clarifai.com/eleven-labs/audio-generation/models/speech-synthesis";\n\n\nconst model = new Model({\n  url: modelUrl,\n  authConfig: {\n    pat: process.env.CLARIFAI_PAT,\n  },\n});\nconst modelPrediction = await model.predictByBytes({\n  inputType: "text",\n  inputBytes: inputText,\n});\n\n// Save the audio file\n// Note: The following code assumes you have the necessary logic to write the audio data to a file in TypeScript.\n// You may need to modify this part based on your specific requirements.\nconst outputBase64 = modelPrediction?.[0]?.data?.audio?.base64 ?? "";\n\nfs.writeFileSync("audio.wav", outputBase64, "base64");\n',Z='from clarifai.client.model import Model\n\n# Your PAT (Personal Access Token) can be found in the Account\'s Security section\n# Specify the correct user_id/app_id pairings\n# Since you\'re making inferences outside your app\'s scope\n#USER_ID = "cohere"\n#APP_ID = "embed"\n\n# You can set the model using model URL or model ID.\n# Change these to whatever model you want to use\n# eg : MODEL_ID = \'cohere-embed-english-v3_0\'\n# You can also set a particular model version by specifying the  version ID\n# eg: MODEL_VERSION_ID = \'model_version\'\n#  Model class objects can be inititalised by providing its URL or also by defining respective user_id, app_id and model_id\n\n# eg : model = Model(user_id="clarifai", app_id="main", model_id=MODEL_ID)\n\ninput_text = """In India Green Revolution commenced in the early 1960s that led to an increase in food grain production, especially in Punjab, Haryana, and Uttar Pradesh. Major milestones in this undertaking were the development of high-yielding varieties of wheat. The Green revolution is revolutionary in character due to the introduction of new technology, new ideas, the new application of inputs like HYV seeds, fertilizers, irrigation water, pesticides, etc. As all these were brought suddenly and spread quickly to attain dramatic results thus it is termed as a revolution in green agriculture.\n"""\n# The predict API gives the flexibility to generate predictions for data provided through URL, Filepath and bytes format.\n\n# Example for prediction through URL:\n# model_prediction = Model(model_url).predict_by_url(URL ,input_type="text")\n\n# Example for prediction through Filepath:\n# model_prediction = Model(model_url).predict_by_filepath(image_filepath, input_type="text")\n\n\nmodel_url = "https://clarifai.com/cohere/embed/models/cohere-embed-english-v3_0"\n\nmodel_prediction = Model(url=model_url, pat="YOUR_PAT").predict_by_bytes(\n    input_text, "text"\n)\n\nembeddings = model_prediction.outputs[0].data.embeddings[0].vector\n\nnum_dimensions = model_prediction.outputs[0].data.embeddings[0].num_dimensions\n\nprint(embeddings[:10])\n',ee='import { Model } from "clarifai-nodejs";\n\n/**\n    Your PAT (Personal Access Token) can be found in the Account\'s Security section\n    Specify the correct userId/appId pairings\n    Since you\'re making inferences outside your app\'s scope\n    USER_ID = "cohere"\n    APP_ID = "embed"\n\n    You can set the model using model URL or model ID.\n    Change these to whatever model you want to use\n    eg : MODEL_ID = \'cohere-embed-english-v3_0\'\n    You can also set a particular model version by specifying the  version ID\n    eg: MODEL_VERSION_ID = "model_version"\n    Model class objects can be initialised by providing its URL or also by defining respective userId, appId and modelId\n\n    eg : \n    const model = new Model({\n        authConfig: {\n            userId: "clarifai",\n            appId: "main",\n            pat: process.env.CLARIFAI_PAT,\n        },\n        modelId: MODEL_ID,\n    });\n\n*/\n\nconst inputText = Buffer.from(\n  `In India Green Revolution commenced in the early 1960s that led to an increase in food grain production, especially in Punjab, Haryana, and Uttar Pradesh. Major milestones in this undertaking were the development of high-yielding varieties of wheat. The Green revolution is revolutionary in character due to the introduction of new technology, new ideas, the new application of inputs like HYV seeds, fertilizers, irrigation water, pesticides, etc. As all these were brought suddenly and spread quickly to attain dramatic results thus it is termed as a revolution in green agriculture.`,\n);\n\n/*\n        The predict API gives flexibility to generate predictions for data provided through URL, Filepath and bytes format.\n\n        Example for prediction through Filepath:\n        const modelPrediction = await model.predictByFilepath({\n                                    filepath,\n                                    inputType: "Text",\n                                });\n\n        Example for prediction through URL:\n        const modelPrediction = await model.predictByURL({\n                                    url: URL,\n                                    inputType: "Text",\n                                });\n*/\n\nconst modelUrl =\n  "https://clarifai.com/cohere/embed/models/cohere-embed-english-v3_0";\n\n\nconst model = new Model({\n  url: modelUrl,\n  authConfig: {\n    pat: process.env.CLARIFAI_PAT,\n  },\n});\n\nconst modelPrediction = await model.predictByBytes({\n  inputBytes: inputText,\n  inputType: "text",\n});\n\nconst embeddings =\n  modelPrediction?.[0]?.data?.embeddingsList?.[0]?.vectorList ?? [];\n\n// const numDimensions =\n//   modelPrediction?.[0]?.data?.embeddingsList?.[0]?.numDimensions;\n\nconsole.log(embeddings.slice(0, 10));\n',ne="clarifai model predict --model_url https://clarifai.com/erfan/text-classification/models/sentiment-analysis-twitter-roberta-base --url https://samples.clarifai.com/finantial-sentiment-1.txt --input_type text",te='clarifai model predict --model_url https://clarifai.com/meta/Llama-3/models/llama-3_1-8b-instruct --bytes "What is the future of AI?" --input_type text --inference_params "{\\"temperature\\":0.7,\\"max_tokens\\":200,\\"top_k\\":50,\\"top_p\\":0.95}"\n',ae='clarifai model predict --model_url https://clarifai.com/openai/chat-completion/models/GPT-4 --bytes "What is the future of AI?" --input_type text',ie='clarifai model predict --model_url "https://clarifai.com/openai/chat-completion/models/GPT-4" --bytes "Classes: [`positive`, `negative`, `neutral`] Text: Sunny weather makes me happy. Classify the text into one of the above classes." --input_type "text" \n',se='clarifai model predict --model_url "https://clarifai.com/stability-ai/stable-diffusion-2/models/stable-diffusion-xl" --bytes "floor plan for 2 bedroom kitchen house" --input_type "text" \n',oe='clarifai model predict --model_url "https://clarifai.com/eleven-labs/audio-generation/models/speech-synthesis" --bytes "Hello, How are you doing today!" --input_type "text" ',re='clarifai model predict --model_url "https://clarifai.com/cohere/embed/models/cohere-embed-english-v3_0" --bytes "In India Green Revolution commenced in the early 1960s that led to an increase in food grain production, especially in Punjab, Haryana, and Uttar Pradesh. Major milestones in this undertaking were the development of high-yielding varieties of wheat. The Green revolution is revolutionary in character due to the introduction of new technology, new ideas, the new application of inputs like HYV seeds, fertilizers, irrigation water, pesticides, etc. As all these were brought suddenly and spread quickly to attain dramatic results thus it is termed as a revolution in green agriculture." --input_type "text" \n',le="concept: LABEL_0              confidence: 0.605\nconcept: LABEL_1              confidence: 0.306\nconcept: LABEL_2              confidence: 0.089",ce="The future of AI is vast and holds immense potential. Here are a few possibilities:\n\n1. Enhanced Personalization: AI will be able to understand and predict user preferences with increasing accuracy. This will allow for highly personalized experiences, from product recommendations to personalized healthcare.\n\n2. Automation: AI will continue to automate routine tasks, freeing up time for individuals to focus on more complex problems. This could be in any field, from manufacturing to customer service.\n\n3. Advanced Data Analysis: AI will be able to analyze and interpret large amounts of data more efficiently. This could lead to significant breakthroughs in fields like climate science, medicine, and economics.\n\n4. AI in Healthcare: AI is expected to revolutionize healthcare, from predicting diseases before symptoms appear, to assisting in surgeries, to personalized treatment plans.\n\n5. Improved AI Ethics: As AI becomes more integral to our lives, there will be an increased focus on ensuring it is used ethically and responsibly. This could lead to advancements in AI that are more transparent, fair, and accountable.\n\n6. General AI: Perhaps the most exciting (and daunting) prospect is the development of Artificial General Intelligence (AGI) - AI systems that possess the ability to understand, learn, adapt, and implement knowledge across a wide array of tasks, much like a human brain.\n\nRemember, while AI holds great promise, it's also important to consider the challenges and implications it brings, such as job displacement due to automation, privacy concerns, and ethical considerations.",pe="`positive`",ue="[-0.02596100978553295,\n\n 0.023946398869156837,\n\n -0.07173235714435577,\n\n 0.032294824719429016,\n\n 0.020313993096351624,\n\n -0.026998838409781456,\n\n 0.008684193715453148,\n\n -0.016651064157485962,\n\n -0.012316598556935787,\n\n 0.00042328768176957965]",de='from clarifai.client.model import Model\n\n# Your PAT (Personal Access Token) can be found in the Account\'s Security section\n# Specify the correct user_id/app_id pairings\n# Since you\'re making inferences outside your app\'s scope\n#USER_ID = "facebook"\n#APP_ID = "asr"\n\n# You can set the model using model URL or model ID.\n# Change these to whatever model you want to use\n# eg : MODEL_ID = \'asr-wav2vec2-base-960h-english\'\n# You can also set a particular model version by specifying the  version ID\n# eg: MODEL_VERSION_ID = \'model_version\'\n# Model class objects can be inititalised by providing its URL or also by defining respective user_id, app_id and model_id\n\n# eg : model = Model(user_id="clarifai", app_id="main", model_id=MODEL_ID)\n\naudio_url = "https://s3.amazonaws.com/samples.clarifai.com/GoodMorning.wav"\n\n# The predict API gives the flexibility to generate predictions for data provided through URL, Filepath and bytes format.\n\n# Example for prediction through Bytes:\n# model_prediction = model.predict_by_bytes(audio_bytes, input_type="audio")\n\n# Example for prediction through Filepath:\n# model_prediction = Model(model_url).predict_by_filepath(audio_filepath, input_type="audio")\n\nmodel_url = "https://clarifai.com/facebook/asr/models/asr-wav2vec2-large-robust-ft-swbd-300h-english"\n\nmodel_prediction = Model(url=model_url, pat="YOUR_PAT").predict_by_url(\n    audio_url, "audio"\n)\n\n# Print the output\nprint(model_prediction.outputs[0].data.text.raw)\n\n',he='import { Model } from "clarifai-nodejs";\n\n/**\n    Your PAT (Personal Access Token) can be found in the Account\'s Security section\n    Specify the correct userId/appId pairings\n    Since you\'re making inferences outside your app\'s scope\n    USER_ID = "facebook"\n    APP_ID = "asr"\n\n    You can set the model using model URL or model ID.\n    Change these to whatever model you want to use\n    eg : MODEL_ID = \'asr-wav2vec2-base-960h-english\'\n    You can also set a particular model version by specifying the  version ID\n    eg: MODEL_VERSION_ID = "model_version"\n    Model class objects can be initialised by providing its URL or also by defining respective userId, appId and modelId\n\n    eg : \n    const model = new Model({\n        authConfig: {\n            userId: "clarifai",\n            appId: "main",\n            pat: process.env.CLARIFAI_PAT,\n        },\n        modelId: MODEL_ID,\n    });\n\n*/\n\nconst audioUrl =\n  "https://s3.amazonaws.com/samples.clarifai.com/GoodMorning.wav";\n\n/*\n        The predict API gives flexibility to generate predictions for data provided through URL, Filepath and bytes format.\n\n        Example for prediction through Bytes:\n        const modelPrediction = await model.predictByBytes({\n                                    inputBytes: Bytes,\n                                    inputType: "audio"\n                                });\n\n        Example for prediction through Filepath:\n        const modelPrediction = await model.predictByFilepath({\n                                    filepath,\n                                    inputType: "audio",\n                                });\n\n    */\n\nconst modelUrl =\n  "https://clarifai.com/facebook/asr/models/asr-wav2vec2-large-robust-ft-swbd-300h-english";\n\n\nconst model = new Model({\n  url: modelUrl,\n  authConfig: {\n    pat: process.env.CLARIFAI_PAT,\n  },\n});\nconst modelPrediction = await model.predictByUrl({\n  url: audioUrl,\n  inputType: "audio",\n});\n\n// Print the output\nconsole.log(modelPrediction?.[0]?.data?.text?.raw);\n',me="clarifai model predict --model_url https://clarifai.com/facebook/asr/models/asr-wav2vec2-large-robust-ft-swbd-300h-english --url https://s3.amazonaws.com/samples.clarifai.com/GoodMorning.wav --input_type audio",_e="GOOD MORNING I THINK THIS IS GOING TO BE A GREAT PRESENTATION",fe='from clarifai.client.model import Model\nfrom clarifai.client.input import Inputs\n\n# Your PAT (Personal Access Token) can be found in the Account\'s Security section\n# Specify the correct user_id/app_id pairings\n# Since you\'re making inferences outside your app\'s scope\n#USER_ID = "openai"\n#APP_ID = "chat-completion"\n\n# You can set the model using model URL or model ID.\n# Change these to whatever model you want to use\n# eg : MODEL_ID = \'openai-gpt-4-vision\'\n# You can also set a particular model version by specifying the  version ID\n# eg: MODEL_VERSION_ID = \'model_version\'\n#  Model class objects can be inititalised by providing its URL or also by defining respective user_id, app_id and model_id\n\n# eg : model = Model(user_id="clarifai", app_id="main", model_id=MODEL_ID)\n\nprompt = "What time of day is it?"\nimage_url = "https://samples.clarifai.com/metro-north.jpg"\n\nmodel_url = "https://clarifai.com/openai/chat-completion/models/openai-gpt-4-vision"\ninference_params = dict(temperature=0.2, max_tokens=100)\nmulti_inputs = Inputs.get_multimodal_input(input_id="", image_url=image_url, raw_text=prompt)\n\n# Predicts the model based on the given inputs.\n\nmodel_prediction = Model(url=model_url, pat="YOUR_PAT").predict(\n    inputs=[\n        multi_inputs\n    ],\n    inference_params=inference_params,\n)\n\nprint(model_prediction.outputs[0].data.text.raw)',ge='import { Model, Input } from "clarifai-nodejs";\n\n/**\n    Your PAT (Personal Access Token) can be found in the Account\'s Security section\n    Specify the correct userId/appId pairings\n    Since you\'re making inferences outside your app\'s scope\n    USER_ID = "openai"\n    APP_ID = "chat-completion"\n\n    You can set the model using model URL or model ID.\n    Change these to whatever model you want to use\n    eg : MODEL_ID = \'openai-gpt-4-vision\'\n    You can also set a particular model version by specifying the  version ID\n    eg: MODEL_VERSION_ID = "model_version"\n    Model class objects can be initialised by providing its URL or also by defining respective userId, appId and modelId\n\n    eg : \n    const model = new Model({\n        authConfig: {\n            userId: "clarifai",\n            appId: "main",\n            pat: process.env.CLARIFAI_PAT,\n        },\n        modelId: MODEL_ID,\n    });\n\n*/\n\nconst prompt = "What time of day is it?";\nconst imageUrl = "https://samples.clarifai.com/metro-north.jpg";\nconst modelUrl =\n  "https://clarifai.com/openai/chat-completion/models/openai-gpt-4-vision";\nconst inferenceParams = { temperature: 0.2, maxTokens: 100 };\nconst multiInputs = Input.getMultimodalInput({\n  inputId: "",\n  imageUrl,\n  rawText: prompt,\n});\n\n/*\n        The predict API gives flexibility to generate predictions for data provided through URL, Filepath and bytes format.\n\n        Example for prediction through Bytes:\n        const modelPrediction = await model.predictByBytes({\n                                    inputBytes: Bytes,\n                                    inputType: "image"\n                                });\n\n        Example for prediction through Filepath:\n        const modelPrediction = await model.predictByFilepath({\n                                    filepath,\n                                    inputType: "image",\n                                });\n\n    */\n\n\nconst model = new Model({\n  url: modelUrl,\n  authConfig: { pat: process.env.CLARIFAI_PAT },\n});\n\nconst modelPrediction = await model.predict({\n  inputs: [multiInputs],\n  inferenceParams,\n});\n\nconsole.log(modelPrediction?.[0]?.data?.text?.raw);\n',Ie='from clarifai.client.model import Model\nfrom clarifai.client.input import Inputs\n\nIMAGE_FILE_LOCATION = \'LOCAL IMAGE PATH\'\nwith open(IMAGE_FILE_LOCATION, "rb") as f:\n    file_bytes = f.read()\n\n\nprompt = "What time of day is it?"\ninference_params = dict(temperature=0.2, max_tokens=100)\n\nmodel_prediction = Model("https://clarifai.com/openai/chat-completion/models/openai-gpt-4-vision").predict(inputs = [Inputs.get_multimodal_input(input_id="", image_bytes = file_bytes, raw_text=prompt)], inference_params=inference_params)\nprint(model_prediction.outputs[0].data.text.raw)\n',Ae="import { Model } from 'clarifai';\nimport { Inputs } from 'clarifai';\n\nconst IMAGE_FILE_LOCATION = 'LOCAL IMAGE PATH';\nconst fs = require('fs');\n\nconst file_bytes = fs.readFileSync(IMAGE_FILE_LOCATION);\n\nconst prompt = \"What time of day is it?\";\nconst inference_params = { temperature: 0.2, max_tokens: 100 };\n\nconst model = new Model(\"https://clarifai.com/openai/chat-completion/models/openai-gpt-4-vision\");\nmodel.predict({\n    inputs: [\n        Inputs.getMultimodalInput({\n            input_id: \"\",\n            image_bytes: file_bytes,\n            raw_text: prompt\n        })\n    ],\n    inference_params: inference_params\n}).then((model_prediction) => {\n    console.log(model_prediction.outputs[0].data.text.raw);\n});\n\n",Ee="The time of day in the image appears to be either dawn or dusk, given the light in the sky. It's not possible to determine the exact time without additional context, but the sky has a mix of light and dark hues, which typically occurs during sunrise or sunset. The presence of snow and the lighting at the train station suggest that it might be winter, and depending on the location, this could influence whether it's morning or evening.",De="##################################################################################################\n# In this section, we set the user authentication, user and app ID, model details, and the URL\n# of the image we want as an input. Change these strings to run your own example.\n#################################################################################################\n\n# Your PAT (Personal Access Token) can be found in the Account's Security section\nPAT = 'YOUR_PAT_HERE'\n# Specify the correct user_id/app_id pairings\n# Since you're making inferences outside your app's scope\nUSER_ID = 'clarifai'\nAPP_ID = 'main'\n# Change these to whatever model and image URL you want to use\nMODEL_ID = 'general-image-recognition'\nMODEL_VERSION_ID = 'aa7f35c01e0642fda5cf400f543e7c40'\nIMAGE_URL = 'https://samples.clarifai.com/metro-north.jpg'\n\n############################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n############################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (('authorization', 'Key ' + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\npost_model_outputs_response = stub.PostModelOutputs(\n    service_pb2.PostModelOutputsRequest(\n        user_app_id=userDataObject,  # The userDataObject is created in the overview and is required when using a PAT\n        model_id=MODEL_ID,\n        version_id=MODEL_VERSION_ID,  # This is optional. Defaults to the latest model version\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    image=resources_pb2.Image(\n                        url=IMAGE_URL\n                    )\n                )\n            )\n        ]\n    ),\n    metadata=metadata\n)\nif post_model_outputs_response.status.code != status_code_pb2.SUCCESS:\n    print(post_model_outputs_response.status)\n    raise Exception(\"Post model outputs failed, status: \" + post_model_outputs_response.status.description)\n\n# Since we have one input, one output will exist here\noutput = post_model_outputs_response.outputs[0]\n\nprint(\"Predicted concepts:\")\nfor concept in output.data.concepts:\n    print(\"%s %.2f\" % (concept.name, concept.value))\n\n# Uncomment this line to print the raw output\n#print(output)",Te="post_model_outputs_response = stub.PostModelOutputs(\n    service_pb2.PostModelOutputsRequest(\n        ...\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    image=resources_pb2.Image(\n                        url=IMAGE_URL\n                    )\n                )\n            ),\n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    image=resources_pb2.Image(\n                        url=IMAGE_URL\n                    )\n                )\n            ),\n            # and so on...\n        ]\n    ),\n    ...\n)",Oe="######################################################################################################\n# In this section, we set the user authentication, user and app ID, model details, and the location\n# of the image we want as an input. Change these strings to run your own example.\n#####################################################################################################\n\n# Your PAT (Personal Access Token) can be found in the Account's Security section\nPAT = 'YOUR_PAT_HERE'\n# Specify the correct user_id/app_id pairings\n# Since you're making inferences outside your app's scope\nUSER_ID = 'clarifai'\nAPP_ID = 'main'\n# Change these to whatever model and image input you want to use\nMODEL_ID = 'general-image-recognition'\nMODEL_VERSION_ID = 'aa7f35c01e0642fda5cf400f543e7c40'\nIMAGE_FILE_LOCATION = 'YOUR_IMAGE_FILE_LOCATION_HERE'\n\n############################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n############################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (('authorization', 'Key ' + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\nwith open(IMAGE_FILE_LOCATION, \"rb\") as f:\n    file_bytes = f.read()\n\npost_model_outputs_response = stub.PostModelOutputs(\n    service_pb2.PostModelOutputsRequest(\n        user_app_id=userDataObject,  # The userDataObject is created in the overview and is required when using a PAT\n        model_id=MODEL_ID,\n        version_id=MODEL_VERSION_ID,  # This is optional. Defaults to the latest model version\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    image=resources_pb2.Image(\n                        base64=file_bytes\n                    )\n                )\n            )\n        ]\n    ),\n    metadata=metadata\n)\nif post_model_outputs_response.status.code != status_code_pb2.SUCCESS:\n    print(post_model_outputs_response.status)\n    raise Exception(\"Post model outputs failed, status: \" + post_model_outputs_response.status.description)\n\n# Since we have one input, one output will exist here\noutput = post_model_outputs_response.outputs[0]\n\nprint(\"Predicted concepts:\")\nfor concept in output.data.concepts:\n    print(\"%s %.2f\" % (concept.name, concept.value))\n\n# Uncomment this line to print the raw output\n#print(output)",be="##################################################################################################\n# In this section, we set the user authentication, user and app ID, model details, and the URL\n# of the image we want as an input. Change these strings to run your own example.\n#################################################################################################\n\n# Your PAT (Personal Access Token) can be found in the Account's Security section\nPAT = 'YOUR_PAT_HERE'\n# Specify the correct user_id/app_id pairings\n# Since you're making inferences outside your app's scope\nUSER_ID = 'clarifai'\nAPP_ID = 'main'\n# Change these to whatever model and image URL you want to use\nMODEL_ID = 'general-image-detection'\nMODEL_VERSION_ID = '1580bb1932594c93b7e2e04456af7c6f'\nIMAGE_URL = 'https://samples.clarifai.com/metro-north.jpg'\n# To use a local file, assign the location variable\n# IMAGE_FILE_LOCATION = 'YOUR_IMAGE_FILE_LOCATION_HERE'\n\n############################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n############################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (('authorization', 'Key ' + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\n# To use a local file, uncomment the following lines\n# with open(IMAGE_FILE_LOCATION, \"rb\") as f:\n#     file_bytes = f.read()\n\npost_model_outputs_response = stub.PostModelOutputs(\n    service_pb2.PostModelOutputsRequest(\n        user_app_id=userDataObject,  # The userDataObject is created in the overview and is required when using a PAT\n        model_id=MODEL_ID,\n        version_id=MODEL_VERSION_ID,  # This is optional. Defaults to the latest model version\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    image=resources_pb2.Image(\n                        url=IMAGE_URL\n                       # base64=file_bytes\n                    )\n                )\n            )\n        ]\n    ),\n    metadata=metadata\n)\nif post_model_outputs_response.status.code != status_code_pb2.SUCCESS:\n    print(post_model_outputs_response.status)\n    raise Exception(\"Post model outputs failed, status: \" + post_model_outputs_response.status.description)\n    \nregions = post_model_outputs_response.outputs[0].data.regions\n\nfor region in regions:\n    # Accessing and rounding the bounding box values\n    top_row = round(region.region_info.bounding_box.top_row, 3)\n    left_col = round(region.region_info.bounding_box.left_col, 3)\n    bottom_row = round(region.region_info.bounding_box.bottom_row, 3)\n    right_col = round(region.region_info.bounding_box.right_col, 3)\n    \n    for concept in region.data.concepts:\n        # Accessing and rounding the concept value\n        name = concept.name\n        value = round(concept.value, 4)\n\n        print((f\"{name}: {value} BBox: {top_row}, {left_col}, {bottom_row}, {right_col}\"))\n",ye="##################################################################################################\n# In this section, we set the user authentication, user and app ID, model details, and the URL\n# of the image we want as an input. Change these strings to run your own example.\n#################################################################################################\n\n# Your PAT (Personal Access Token) can be found in the Account's Security section\nPAT = 'YOUR_PAT_HERE'\n# Specify the correct user_id/app_id pairings\n# Since you're making inferences outside your app's scope\nUSER_ID = 'clarifai'\nAPP_ID = 'main'\n# Change these to whatever model and image URL you want to use\nMODEL_ID = 'image-general-segmentation'\nMODEL_VERSION_ID = '1581820110264581908ce024b12b4bfb'\nIMAGE_URL = 'https://samples.clarifai.com/metro-north.jpg'\n# To use a local file, assign the location variable\n# IMAGE_FILE_LOCATION = 'YOUR_IMAGE_FILE_LOCATION_HERE'\n\n############################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n############################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (('authorization', 'Key ' + PAT),)\n\n# To use a local file, uncomment the following lines\n# with open(IMAGE_FILE_LOCATION, \"rb\") as f:\n#     file_bytes = f.read()\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\npost_model_outputs_response = stub.PostModelOutputs(\n    service_pb2.PostModelOutputsRequest(\n        user_app_id=userDataObject,  # The userDataObject is created in the overview and is required when using a PAT\n        model_id=MODEL_ID,\n        version_id=MODEL_VERSION_ID,  # This is optional. Defaults to the latest model version\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    image=resources_pb2.Image(\n                        url=IMAGE_URL\n                        # base64=file_bytes\n                    )\n                )\n            )\n        ]\n    ),\n    metadata=metadata\n)\nif post_model_outputs_response.status.code != status_code_pb2.SUCCESS:\n    print(post_model_outputs_response.status)\n    raise Exception(\"Post model outputs failed, status: \" + post_model_outputs_response.status.description)\n\nregions = post_model_outputs_response.outputs[0].data.regions\n\nfor region in regions:\n    for concept in region.data.concepts:\n        # Accessing and rounding the concept's percentage of image covered\n        name = concept.name\n        value = round(concept.value, 4)\n        print((f\"{name}: {value}\"))\n",we='##################################################################################################\n# In this section, we set the user authentication, user and app ID, model details, and the URL\n# of the image we want as an input. Change these strings to run your own example.\n#################################################################################################\n\n# Your PAT (Personal Access Token) can be found in the Account\'s Security section\nPAT = "YOUR_PAT_HERE"\n# Specify the correct user_id/app_id pairings\n# Since you\'re making inferences outside your app\'s scope\nUSER_ID = "salesforce"\nAPP_ID = "blip"\n# Change these to whatever model and image URL you want to use\nMODEL_ID = "general-english-image-caption-blip"\nMODEL_VERSION_ID = "cdb690f13e62470ea6723642044f95e4"\nIMAGE_URL = "https://samples.clarifai.com/metro-north.jpg"\n# To use a local file, assign the location variable\n# IMAGE_FILE_LOCATION = "YOUR_IMAGE_FILE_LOCATION_HERE"\n\n############################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n############################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (("authorization", "Key " + PAT),)\n\n# To use a local file, uncomment the following lines\n# with open(IMAGE_FILE_LOCATION, "rb") as f:\n#     file_bytes = f.read()\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\npost_model_outputs_response = stub.PostModelOutputs(\n    service_pb2.PostModelOutputsRequest(\n        user_app_id=userDataObject,  # The userDataObject is created in the overview and is required when using a PAT\n        model_id=MODEL_ID,\n        version_id=MODEL_VERSION_ID,  # This is optional. Defaults to the latest model version\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    image=resources_pb2.Image(\n                        url=IMAGE_URL\n                        # base64=file_bytes\n                        )\n                    )\n            )\n        ],\n    ),\n    metadata=metadata,\n)\nif post_model_outputs_response.status.code != status_code_pb2.SUCCESS:\n    print(post_model_outputs_response.status)\n    raise Exception(\n        "Post model outputs failed, status: "\n        + post_model_outputs_response.status.description\n    )\n\n# Since we have one input, one output will exist here\noutput = post_model_outputs_response.outputs[0]\n\nprint("Image caption:")\nprint(output.data.text.raw)\n\n# Uncomment this line to print the raw output\n# print(output)\n',Se="\x3c!--index.html file--\x3e\n\n<script>\n    ///////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, and the URL\n    // of the image we want as an input. Change these strings to run your own example.\n    //////////////////////////////////////////////////////////////////////////////////////////////////\n\n    // Your PAT (Personal Access Token) can be found in the Account's Security section\n    const PAT = 'YOUR_PAT_HERE';\n    // Specify the correct user_id/app_id pairings\n    // Since you're making inferences outside your app's scope\n    const USER_ID = 'clarifai';       \n    const APP_ID = 'main';\n    // Change these to whatever model and image URL you want to use\n    const MODEL_ID = 'general-image-recognition';\n    const MODEL_VERSION_ID = 'aa7f35c01e0642fda5cf400f543e7c40';    \n    const IMAGE_URL = 'https://samples.clarifai.com/metro-north.jpg';\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    const raw = JSON.stringify({\n        \"user_app_id\": {\n            \"user_id\": USER_ID,\n            \"app_id\": APP_ID\n        },\n        \"inputs\": [\n            {\n                \"data\": {\n                    \"image\": {\n                        \"url\": IMAGE_URL\n                    }\n                }\n            }\n        ]\n    });\n\n    const requestOptions = {\n        method: 'POST',\n        headers: {\n            'Accept': 'application/json',\n            'Authorization': 'Key ' + PAT\n        },\n        body: raw\n    };\n\n    // NOTE: MODEL_VERSION_ID is optional, you can also call prediction with the MODEL_ID only\n    // https://api.clarifai.com/v2/models/{YOUR_MODEL_ID}/outputs\n    // this will default to the latest version_id\n\n    fetch(\"https://api.clarifai.com/v2/models/\" + MODEL_ID + \"/versions/\" + MODEL_VERSION_ID + \"/outputs\", requestOptions)\n        .then(response => response.text())\n        .then(result => console.log(result))\n        .catch(error => console.log('error', error));\n<\/script>",Re="\x3c!--index.html file--\x3e\n\n<script>\n    /////////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, and the location\n    // of the image we want as an input. Change these strings to run your own example.\n    ///////////////////////////////////////////////////////////////////////////////////////////////////////\n\n    // Your PAT (Personal Access Token) can be found in the Account's Security section\n    const PAT = 'YOUR_PAT_HERE';\n    // Specify the correct user_id/app_id pairings\n    // Since you're making inferences outside your app's scope\n    const USER_ID = 'clarifai';    \n    const APP_ID = 'main';\n    // Change these to whatever model and image input you want to use\n    const MODEL_ID = 'general-image-recognition';\n    const MODEL_VERSION_ID = 'aa7f35c01e0642fda5cf400f543e7c40';    \n    const IMAGE_BYTES_STRING = '/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAUDBAQEAwUEBAQFBQUGBwwIBwcHBw8LCwkMEQ8SEhEPERETFhwXExQaFRERGCEYGh0dHx8fExciJCIeJBweHx7/2wBDAQUFBQcGBw4ICA4eFBEUHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh7/wAARCAAoACgDASIAAhEBAxEB/8QAGQAAAgMBAAAAAAAAAAAAAAAAAAYDBQcE/8QAMBAAAQMDAwMDAgQHAAAAAAAAAQIDBAAFEQYSIQcTMTJBURRhCBYikSNScXKhsdH/xAAZAQACAwEAAAAAAAAAAAAAAAAFBgIDBAf/xAAtEQABAwMBBgQHAQAAAAAAAAABAgMRAAQhMQUSE0FRYQaBocEUFiJCcrHR8P/aAAwDAQACEQMRAD8A3+RYY1unSYzCS0ttZUkAgktn0q5yT7jPyDUC4wdGwycH5U2Kt9ZQ7VI1qw5PkvQy3CSVPpf7aQjuKyFH25xzn3pHn3TVNy01Hl2hyy6YdkSpKsS9sl/6RlI3rRu3dxWd6spwnAGPIJTfl925fcLaoSDHXvyo6i9SlCQrU9wKln3OyWiaDN1RAbW3kKbSd7gPtwMkH/tTWy9afuy1iPfnXMAblITwkE4yf08cn3pSbYt1uts24XH6fUbiLAuY1MWyGkLEmUW0rcCRvUpQ5CtwKQCPgi4S1ZbDe4sd9NntDEe79m3uOBLTr0IR9jzodSMqUpTu9JJ8owD7UTT4ZCfv9PbP7860m+s+HBSrejWRuz2kAxoesGYxTW/Zlpkwo1vkuSly3UgKWQUhHJUvIHsAaKTemF8XE6sWmxyZkiaZrMh1jv8ArQNpUVqB8FW0njHqx4zRVVhsph1KlKk5xQ+7uHmikaSJrQerMByet2IwvtuTLa4xv2k7Rk84H9x/esHv92d01boenLXGcuiWrFIhLlpbcaQ2/JdK3VJCkAq2pAR7Zz7YxWudY9fxNIdQbNGkR5TyX4aisNNpUMFZAzkj4NK0jq9ZpbLr0PSlzkhrlZDaQlP3P8Q4/ap3F87bPucJEkx/hHv60b2TYXLrKN5sramYECSQRk9M6c6zmJ+eb5Hi22M7cnWGIQgFLbX0zSo4PDa1YBcTgDyMjJ/qbGPabH08SJt1Uzc9QqRliGg5QySPKvgc+TyfYDmmTUWpNYz7ctxoQdPQshCktupckDJUPUcJT6DwMq8YyaQ9VL0pCS8zapcq4SVOBZmPDO8/cnknlWcDBwn4NYnPjLkQ+qE9OtOVlYpeVHDCEkkkJyT+SuQzy5Y0ru6Ez511/Efa5s1fdkOtyVurIxgdlQAA9gOKKPwolU7remU5hCGYEgo38KUv9I/0TRTDYJCWQBSF4rIN/CRgAR0iTpVD1j1g/qDqJcJqlKcjB9bcda142MpOEJAzgeMnjyTSyze5KEuNRpDoDvC0oe4X9iAeaKKFK+oya6fbOqYbDTeEiAPKpHdS3gBLYc7RQkp3ApQog+cq8nwPJrljzxnPZbUfnugn/NFFRgEVch9xKsH0H8pg6e3x3T3UC1ajaZITGkJLoS4MKbOUrzz/ACKVRRRVzVwtoQmhG1NkWu0HuI+JI8u/Kv/Z';\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    const raw = JSON.stringify({\n        \"user_app_id\": {\n            \"user_id\": USER_ID,\n            \"app_id\": APP_ID\n        },\n        \"inputs\": [\n            {\n                \"data\": {\n                    \"image\": {\n                        \"base64\": IMAGE_BYTES_STRING\n                    }\n                }\n            }\n        ]\n    });\n\n    const requestOptions = {\n        method: 'POST',\n        headers: {\n            'Accept': 'application/json',\n            'Authorization': 'Key ' + PAT\n        },\n        body: raw\n    };\n\n    // NOTE: MODEL_VERSION_ID is optional, you can also call prediction with the MODEL_ID only\n    // https://api.clarifai.com/v2/models/{YOUR_MODEL_ID}/outputs\n    // this will default to the latest version_id\n\n    fetch(\"https://api.clarifai.com/v2/models/\" + MODEL_ID + \"/versions/\" + MODEL_VERSION_ID + \"/outputs\", requestOptions)\n        .then(response => response.text())\n        .then(result => console.log(result))\n        .catch(error => console.log('error', error));\n<\/script>",Pe="\x3c!--index.html file--\x3e\n\n<script>\n  ///////////////////////////////////////////////////////////////////////////////////////////////////\n  // In this section, we set the user authentication, user and app ID, model details, and the URL\n  // of the image we want as an input. Change these strings to run your own example.\n  //////////////////////////////////////////////////////////////////////////////////////////////////\n\n  // Your PAT (Personal Access Token) can be found in the Account's Security section\n  const PAT = 'YOUR_PAT_HERE';\n  // Specify the correct user_id/app_id pairings\n  // Since you're making inferences outside your app's scope\n  const USER_ID = 'clarifai';\n  const APP_ID = 'main';\n  // Change these to whatever model and image URL you want to use\n  const MODEL_ID = 'general-image-detection';\n  const MODEL_VERSION_ID = '1580bb1932594c93b7e2e04456af7c6f';\n  const IMAGE_URL = 'https://samples.clarifai.com/metro-north.jpg';\n  // To use image bytes, assign its variable   \n  // const IMAGE_BYTES_STRING = '/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAUDBAQEAwUEBAQFBQUGBwwIBwcHBw8LCwkMEQ8SEhEPERETFhwXExQaFRERGCEYGh0dHx8fExciJCIeJBweHx7/2wBDAQUFBQcGBw4ICA4eFBEUHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh7/wAARCAAoACgDASIAAhEBAxEB/8QAGQAAAgMBAAAAAAAAAAAAAAAAAAYDBQcE/8QAMBAAAQMDAwMDAgQHAAAAAAAAAQIDBAAFEQYSIQcTMTJBURRhCBYikSNScXKhsdH/xAAZAQACAwEAAAAAAAAAAAAAAAAFBgIDBAf/xAAtEQABAwMBBgQHAQAAAAAAAAABAgMRAAQhMQUSE0FRYQaBocEUFiJCcrHR8P/aAAwDAQACEQMRAD8A3+RYY1unSYzCS0ttZUkAgktn0q5yT7jPyDUC4wdGwycH5U2Kt9ZQ7VI1qw5PkvQy3CSVPpf7aQjuKyFH25xzn3pHn3TVNy01Hl2hyy6YdkSpKsS9sl/6RlI3rRu3dxWd6spwnAGPIJTfl925fcLaoSDHXvyo6i9SlCQrU9wKln3OyWiaDN1RAbW3kKbSd7gPtwMkH/tTWy9afuy1iPfnXMAblITwkE4yf08cn3pSbYt1uts24XH6fUbiLAuY1MWyGkLEmUW0rcCRvUpQ5CtwKQCPgi4S1ZbDe4sd9NntDEe79m3uOBLTr0IR9jzodSMqUpTu9JJ8owD7UTT4ZCfv9PbP7860m+s+HBSrejWRuz2kAxoesGYxTW/Zlpkwo1vkuSly3UgKWQUhHJUvIHsAaKTemF8XE6sWmxyZkiaZrMh1jv8ArQNpUVqB8FW0njHqx4zRVVhsph1KlKk5xQ+7uHmikaSJrQerMByet2IwvtuTLa4xv2k7Rk84H9x/esHv92d01boenLXGcuiWrFIhLlpbcaQ2/JdK3VJCkAq2pAR7Zz7YxWudY9fxNIdQbNGkR5TyX4aisNNpUMFZAzkj4NK0jq9ZpbLr0PSlzkhrlZDaQlP3P8Q4/ap3F87bPucJEkx/hHv60b2TYXLrKN5sramYECSQRk9M6c6zmJ+eb5Hi22M7cnWGIQgFLbX0zSo4PDa1YBcTgDyMjJ/qbGPabH08SJt1Uzc9QqRliGg5QySPKvgc+TyfYDmmTUWpNYz7ctxoQdPQshCktupckDJUPUcJT6DwMq8YyaQ9VL0pCS8zapcq4SVOBZmPDO8/cnknlWcDBwn4NYnPjLkQ+qE9OtOVlYpeVHDCEkkkJyT+SuQzy5Y0ru6Ez511/Efa5s1fdkOtyVurIxgdlQAA9gOKKPwolU7remU5hCGYEgo38KUv9I/0TRTDYJCWQBSF4rIN/CRgAR0iTpVD1j1g/qDqJcJqlKcjB9bcda142MpOEJAzgeMnjyTSyze5KEuNRpDoDvC0oe4X9iAeaKKFK+oya6fbOqYbDTeEiAPKpHdS3gBLYc7RQkp3ApQog+cq8nwPJrljzxnPZbUfnugn/NFFRgEVch9xKsH0H8pg6e3x3T3UC1ajaZITGkJLoS4MKbOUrzz/ACKVRRRVzVwtoQmhG1NkWu0HuI+JI8u/Kv/Z';\n\n  ///////////////////////////////////////////////////////////////////////////////////\n  // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n  ///////////////////////////////////////////////////////////////////////////////////\n\n  const raw = JSON.stringify({\n      \"user_app_id\": {\n          \"user_id\": USER_ID,\n          \"app_id\": APP_ID\n      },\n      \"inputs\": [\n          {\n              \"data\": {\n                  \"image\": {\n                      \"url\": IMAGE_URL\n                      // \"base64\": IMAGE_BYTES_STRING\n                  }\n              }\n          }\n      ]\n  });\n\n  const requestOptions = {\n      method: 'POST',\n      headers: {\n          'Accept': 'application/json',\n          'Authorization': 'Key ' + PAT\n      },\n      body: raw\n  };\n\n  // NOTE: MODEL_VERSION_ID is optional, you can also call prediction with the MODEL_ID only\n  // https://api.clarifai.com/v2/models/{YOUR_MODEL_ID}/outputs\n  // this will default to the latest version_id\n\n  fetch(\"https://api.clarifai.com/v2/models/\" + MODEL_ID + \"/versions/\" + MODEL_VERSION_ID + \"/outputs\", requestOptions)\n      .then(response => response.json())\n      .then(result => {\n\n          const regions = result.outputs[0].data.regions;\n\n          regions.forEach(region => {\n              // Accessing and rounding the bounding box values\n              const boundingBox = region.region_info.bounding_box;\n              const topRow = boundingBox.top_row.toFixed(3);\n              const leftCol = boundingBox.left_col.toFixed(3);\n              const bottomRow = boundingBox.bottom_row.toFixed(3);\n              const rightCol = boundingBox.right_col.toFixed(3);\n\n              region.data.concepts.forEach(concept => {\n                  // Accessing and rounding the concept value\n                  const name = concept.name;\n                  const value = concept.value.toFixed(4);\n\n                  console.log(`${name}: ${value} BBox: ${topRow}, ${leftCol}, ${bottomRow}, ${rightCol}`);\n                  \n              });\n          });\n\n      })\n      .catch(error => console.log('error', error));\n<\/script>",ve="\x3c!--index.html file--\x3e\n\n<script>\n    ///////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, and the URL\n    // of the image we want as an input. Change these strings to run your own example.\n    //////////////////////////////////////////////////////////////////////////////////////////////////\n\n    // Your PAT (Personal Access Token) can be found in the Account's Security section\n    const PAT = 'YOUR_PAT_HERE';\n    // Specify the correct user_id/app_id pairings\n    // Since you're making inferences outside your app's scope\n    const USER_ID = 'clarifai';\n    const APP_ID = 'main';\n    // Change these to whatever model and image URL you want to use\n    const MODEL_ID = 'image-general-segmentation';\n    const MODEL_VERSION_ID = '1581820110264581908ce024b12b4bfb';\n    const IMAGE_URL = 'https://samples.clarifai.com/metro-north.jpg';\n    // To use image bytes, assign its variable   \n    // const IMAGE_BYTES_STRING = '/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAUDBAQEAwUEBAQFBQUGBwwIBwcHBw8LCwkMEQ8SEhEPERETFhwXExQaFRERGCEYGh0dHx8fExciJCIeJBweHx7/2wBDAQUFBQcGBw4ICA4eFBEUHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh7/wAARCAAoACgDASIAAhEBAxEB/8QAGQAAAgMBAAAAAAAAAAAAAAAAAAYDBQcE/8QAMBAAAQMDAwMDAgQHAAAAAAAAAQIDBAAFEQYSIQcTMTJBURRhCBYikSNScXKhsdH/xAAZAQACAwEAAAAAAAAAAAAAAAAFBgIDBAf/xAAtEQABAwMBBgQHAQAAAAAAAAABAgMRAAQhMQUSE0FRYQaBocEUFiJCcrHR8P/aAAwDAQACEQMRAD8A3+RYY1unSYzCS0ttZUkAgktn0q5yT7jPyDUC4wdGwycH5U2Kt9ZQ7VI1qw5PkvQy3CSVPpf7aQjuKyFH25xzn3pHn3TVNy01Hl2hyy6YdkSpKsS9sl/6RlI3rRu3dxWd6spwnAGPIJTfl925fcLaoSDHXvyo6i9SlCQrU9wKln3OyWiaDN1RAbW3kKbSd7gPtwMkH/tTWy9afuy1iPfnXMAblITwkE4yf08cn3pSbYt1uts24XH6fUbiLAuY1MWyGkLEmUW0rcCRvUpQ5CtwKQCPgi4S1ZbDe4sd9NntDEe79m3uOBLTr0IR9jzodSMqUpTu9JJ8owD7UTT4ZCfv9PbP7860m+s+HBSrejWRuz2kAxoesGYxTW/Zlpkwo1vkuSly3UgKWQUhHJUvIHsAaKTemF8XE6sWmxyZkiaZrMh1jv8ArQNpUVqB8FW0njHqx4zRVVhsph1KlKk5xQ+7uHmikaSJrQerMByet2IwvtuTLa4xv2k7Rk84H9x/esHv92d01boenLXGcuiWrFIhLlpbcaQ2/JdK3VJCkAq2pAR7Zz7YxWudY9fxNIdQbNGkR5TyX4aisNNpUMFZAzkj4NK0jq9ZpbLr0PSlzkhrlZDaQlP3P8Q4/ap3F87bPucJEkx/hHv60b2TYXLrKN5sramYECSQRk9M6c6zmJ+eb5Hi22M7cnWGIQgFLbX0zSo4PDa1YBcTgDyMjJ/qbGPabH08SJt1Uzc9QqRliGg5QySPKvgc+TyfYDmmTUWpNYz7ctxoQdPQshCktupckDJUPUcJT6DwMq8YyaQ9VL0pCS8zapcq4SVOBZmPDO8/cnknlWcDBwn4NYnPjLkQ+qE9OtOVlYpeVHDCEkkkJyT+SuQzy5Y0ru6Ez511/Efa5s1fdkOtyVurIxgdlQAA9gOKKPwolU7remU5hCGYEgo38KUv9I/0TRTDYJCWQBSF4rIN/CRgAR0iTpVD1j1g/qDqJcJqlKcjB9bcda142MpOEJAzgeMnjyTSyze5KEuNRpDoDvC0oe4X9iAeaKKFK+oya6fbOqYbDTeEiAPKpHdS3gBLYc7RQkp3ApQog+cq8nwPJrljzxnPZbUfnugn/NFFRgEVch9xKsH0H8pg6e3x3T3UC1ajaZITGkJLoS4MKbOUrzz/ACKVRRRVzVwtoQmhG1NkWu0HuI+JI8u/Kv/Z';\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    const raw = JSON.stringify({\n        \"user_app_id\": {\n            \"user_id\": USER_ID,\n            \"app_id\": APP_ID\n        },\n        \"inputs\": [\n            {\n                \"data\": {\n                    \"image\": {\n                        \"url\": IMAGE_URL\n                        // \"base64\": IMAGE_BYTES_STRING\n                    }\n                }\n            }\n        ]\n    });\n\n    const requestOptions = {\n        method: 'POST',\n        headers: {\n            'Accept': 'application/json',\n            'Authorization': 'Key ' + PAT\n        },\n        body: raw\n    };\n\n    // NOTE: MODEL_VERSION_ID is optional, you can also call prediction with the MODEL_ID only\n    // https://api.clarifai.com/v2/models/{YOUR_MODEL_ID}/outputs\n    // this will default to the latest version_id\n\n    fetch(\"https://api.clarifai.com/v2/models/\" + MODEL_ID + \"/versions/\" + MODEL_VERSION_ID + \"/outputs\", requestOptions)\n        .then(response => response.json())\n        .then(result => {\n\n            const regions = result.outputs[0].data.regions;\n\n            for (const region of regions) {\n                for (const concept of region.data.concepts) {\n                    // Accessing and rounding the concept's percentage of image covered\n                    const name = concept.name;\n                    const value = concept.value.toFixed(4);\n                    console.log(`${name}: ${value}`);\n                }\n            }\n        })\n        .catch(error => console.log('error', error));\n<\/script>",xe='\x3c!--index.html file--\x3e\n\n<script>\n    ///////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, and the URL\n    // of the image we want as an input. Change these strings to run your own example.\n    //////////////////////////////////////////////////////////////////////////////////////////////////\n  \n    // Your PAT (Personal Access Token) can be found in the Account\'s Security section\n    const PAT = "YOUR_PAT_HERE";\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    const USER_ID = "salesforce";\n    const APP_ID = "blip";\n    // Change these to whatever model and image URL you want to use\n    const MODEL_ID = "general-english-image-caption-blip";\n    const MODEL_VERSION_ID = "cdb690f13e62470ea6723642044f95e4";\n    const IMAGE_URL = "https://samples.clarifai.com/metro-north.jpg";\n    // To use image bytes, assign its variable   \n    // const IMAGE_BYTES_STRING = \'/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAUDBAQEAwUEBAQFBQUGBwwIBwcHBw8LCwkMEQ8SEhEPERETFhwXExQaFRERGCEYGh0dHx8fExciJCIeJBweHx7/2wBDAQUFBQcGBw4ICA4eFBEUHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh7/wAARCAAoACgDASIAAhEBAxEB/8QAGQAAAgMBAAAAAAAAAAAAAAAAAAYDBQcE/8QAMBAAAQMDAwMDAgQHAAAAAAAAAQIDBAAFEQYSIQcTMTJBURRhCBYikSNScXKhsdH/xAAZAQACAwEAAAAAAAAAAAAAAAAFBgIDBAf/xAAtEQABAwMBBgQHAQAAAAAAAAABAgMRAAQhMQUSE0FRYQaBocEUFiJCcrHR8P/aAAwDAQACEQMRAD8A3+RYY1unSYzCS0ttZUkAgktn0q5yT7jPyDUC4wdGwycH5U2Kt9ZQ7VI1qw5PkvQy3CSVPpf7aQjuKyFH25xzn3pHn3TVNy01Hl2hyy6YdkSpKsS9sl/6RlI3rRu3dxWd6spwnAGPIJTfl925fcLaoSDHXvyo6i9SlCQrU9wKln3OyWiaDN1RAbW3kKbSd7gPtwMkH/tTWy9afuy1iPfnXMAblITwkE4yf08cn3pSbYt1uts24XH6fUbiLAuY1MWyGkLEmUW0rcCRvUpQ5CtwKQCPgi4S1ZbDe4sd9NntDEe79m3uOBLTr0IR9jzodSMqUpTu9JJ8owD7UTT4ZCfv9PbP7860m+s+HBSrejWRuz2kAxoesGYxTW/Zlpkwo1vkuSly3UgKWQUhHJUvIHsAaKTemF8XE6sWmxyZkiaZrMh1jv8ArQNpUVqB8FW0njHqx4zRVVhsph1KlKk5xQ+7uHmikaSJrQerMByet2IwvtuTLa4xv2k7Rk84H9x/esHv92d01boenLXGcuiWrFIhLlpbcaQ2/JdK3VJCkAq2pAR7Zz7YxWudY9fxNIdQbNGkR5TyX4aisNNpUMFZAzkj4NK0jq9ZpbLr0PSlzkhrlZDaQlP3P8Q4/ap3F87bPucJEkx/hHv60b2TYXLrKN5sramYECSQRk9M6c6zmJ+eb5Hi22M7cnWGIQgFLbX0zSo4PDa1YBcTgDyMjJ/qbGPabH08SJt1Uzc9QqRliGg5QySPKvgc+TyfYDmmTUWpNYz7ctxoQdPQshCktupckDJUPUcJT6DwMq8YyaQ9VL0pCS8zapcq4SVOBZmPDO8/cnknlWcDBwn4NYnPjLkQ+qE9OtOVlYpeVHDCEkkkJyT+SuQzy5Y0ru6Ez511/Efa5s1fdkOtyVurIxgdlQAA9gOKKPwolU7remU5hCGYEgo38KUv9I/0TRTDYJCWQBSF4rIN/CRgAR0iTpVD1j1g/qDqJcJqlKcjB9bcda142MpOEJAzgeMnjyTSyze5KEuNRpDoDvC0oe4X9iAeaKKFK+oya6fbOqYbDTeEiAPKpHdS3gBLYc7RQkp3ApQog+cq8nwPJrljzxnPZbUfnugn/NFFRgEVch9xKsH0H8pg6e3x3T3UC1ajaZITGkJLoS4MKbOUrzz/ACKVRRRVzVwtoQmhG1NkWu0HuI+JI8u/Kv/Z\';\n  \n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n  \n    const raw = JSON.stringify({\n      "user_app_id": {\n        "user_id": USER_ID,\n        "app_id": APP_ID\n      },\n      "inputs": [\n        {\n          "data": {\n            "image": {\n              "url": IMAGE_URL\n              // "base64": IMAGE_BYTES_STRING\n            }\n          }\n        }\n      ]\n    });\n  \n    const requestOptions = {\n      method: "POST",\n      headers: {\n        "Accept": "application/json",\n        "Authorization": "Key " + PAT\n      },\n      body: raw\n    };\n  \n    // NOTE: MODEL_VERSION_ID is optional, you can also call prediction with the MODEL_ID only\n    // https://api.clarifai.com/v2/models/{YOUR_MODEL_ID}/outputs\n    // this will default to the latest version_id\n  \n    fetch("https://api.clarifai.com/v2/models/" + MODEL_ID + "/versions/" + MODEL_VERSION_ID + "/outputs", requestOptions)\n      .then(response => response.text())\n      .then(result => console.log(result))\n      .catch(error => console.log("error", error));\n  <\/script>',Ce='//index.js file\n\n///////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, and the URL\n// of the image we want as an input. Change these strings to run your own example.\n///////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\nconst PAT = \'YOUR_PAT_HERE\';\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\nconst USER_ID = \'clarifai\';\nconst APP_ID = \'main\';\n// Change these to whatever model and image URL you want to use\nconst MODEL_ID = \'general-image-recognition\';\nconst MODEL_VERSION_ID = \'aa7f35c01e0642fda5cf400f543e7c40\';\nconst IMAGE_URL = \'https://samples.clarifai.com/metro-north.jpg\';\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\nstub.PostModelOutputs(\n    {\n        user_app_id: {\n            "user_id": USER_ID,\n            "app_id": APP_ID\n        },\n        model_id: MODEL_ID,\n        version_id: MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n        inputs: [\n            { data: { image: { url: IMAGE_URL, allow_duplicate_url: true } } }\n        ]\n    },\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            throw new Error("Post model outputs failed, status: " + response.status.description);\n        }\n\n        // Since we have one input, one output will exist here\n        const output = response.outputs[0];\n\n        console.log("Predicted concepts:");\n        for (const concept of output.data.concepts) {\n            console.log(concept.name + " " + concept.value);\n        }\n    }\n\n);',Le='//index.js file\n\n////////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, and the location\n// of the image we want as an input. Change these strings to run your own example.\n///////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\nconst PAT = \'YOUR_PAT_HERE\';\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\nconst USER_ID = \'clarifai\';\nconst APP_ID = \'main\';\n// Change these to whatever model and image input you want to use\nconst MODEL_ID = \'general-image-recognition\';\nconst MODEL_VERSION_ID = \'aa7f35c01e0642fda5cf400f543e7c40\';\nconst IMAGE_FILE_LOCATION = \'YOUR_IMAGE_FILE_LOCATION_HERE\';\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\nconst fs = require("fs");\nconst imageBytes = fs.readFileSync(IMAGE_FILE_LOCATION);\n\nstub.PostModelOutputs(\n    {\n        user_app_id: {\n            "user_id": USER_ID,\n            "app_id": APP_ID\n        },\n        model_id: MODEL_ID,\n        version_id: MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n        inputs: [\n            { data: { image: { base64: imageBytes } } }\n        ]\n    },\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            throw new Error("Post model outputs failed, status: " + response.status.description);\n        }\n\n        // Since we have one input, one output will exist here\n        const output = response.outputs[0];\n\n        console.log("Predicted concepts:");\n        for (const concept of output.data.concepts) {\n            console.log(concept.name + " " + concept.value);\n        }\n    }\n);',Ue="//index.js file\n\n///////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, and the URL\n// of the image we want as an input. Change these strings to run your own example.\n///////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account's Security section\nconst PAT = 'YOUR_PAT_HERE';\n// Specify the correct user_id/app_id pairings\n// Since you're making inferences outside your app's scope\nconst USER_ID = 'clarifai';\nconst APP_ID = 'main';\n// Change these to whatever model and image URL you want to use\nconst MODEL_ID = 'general-image-detection';\nconst MODEL_VERSION_ID = '1580bb1932594c93b7e2e04456af7c6f';\nconst IMAGE_URL = 'https://samples.clarifai.com/metro-north.jpg';\n// To use a local file, assign the location variable\n// const IMAGE_FILE_LOCATION = 'YOUR_IMAGE_FILE_LOCATION_HERE'\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require(\"clarifai-nodejs-grpc\");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set(\"authorization\", \"Key \" + PAT);\n\n// To use a local text file, uncomment the following lines\n// const fs = require(\"fs\");\n// const imageBytes = fs.readFileSync(IMAGE_FILE_LOCATION);\n\nstub.PostModelOutputs(\n    {\n        user_app_id: {\n            \"user_id\": USER_ID,\n            \"app_id\": APP_ID\n        },\n        model_id: MODEL_ID,\n        version_id: MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n        inputs: [\n            {\n                data: {\n                    image: {\n                        url: IMAGE_URL,\n                        // base64: imageBytes,\n                        allow_duplicate_url: true\n                    }\n                }\n            }\n        ]\n    },\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            throw new Error(\"Post model outputs failed, status: \" + response.status.description);\n        }\n\n        const regions = response.outputs[0].data.regions;\n\n        regions.forEach(region => {\n            // Accessing and rounding the bounding box values\n            const boundingBox = region.region_info.bounding_box;\n            const topRow = boundingBox.top_row.toFixed(3);\n            const leftCol = boundingBox.left_col.toFixed(3);\n            const bottomRow = boundingBox.bottom_row.toFixed(3);\n            const rightCol = boundingBox.right_col.toFixed(3);\n\n            region.data.concepts.forEach(concept => {\n                // Accessing and rounding the concept value\n                const name = concept.name;\n                const value = concept.value.toFixed(4);\n\n                console.log(`${name}: ${value} BBox: ${topRow}, ${leftCol}, ${bottomRow}, ${rightCol}`);\n\n            });\n        });\n    }\n\n);",Ne="//index.js file\n\n///////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, and the URL\n// of the image we want as an input. Change these strings to run your own example.\n///////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account's Security section\nconst PAT = 'YOUR_PAT_HERE';\n// Specify the correct user_id/app_id pairings\n// Since you're making inferences outside your app's scope\nconst USER_ID = 'clarifai';\nconst APP_ID = 'main';\n// Change these to whatever model and image URL you want to use\nconst MODEL_ID = 'image-general-segmentation';\nconst MODEL_VERSION_ID = '1581820110264581908ce024b12b4bfb';\nconst IMAGE_URL = 'https://samples.clarifai.com/metro-north.jpg';\n// To use a local file, assign the location variable\n// const IMAGE_FILE_LOCATION = 'YOUR_IMAGE_FILE_LOCATION_HERE'\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require(\"clarifai-nodejs-grpc\");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set(\"authorization\", \"Key \" + PAT);\n\n// To use a local text file, uncomment the following lines\n// const fs = require(\"fs\");\n// const imageBytes = fs.readFileSync(IMAGE_FILE_LOCATION);\n\nstub.PostModelOutputs(\n    {\n        user_app_id: {\n            \"user_id\": USER_ID,\n            \"app_id\": APP_ID\n        },\n        model_id: MODEL_ID,\n        version_id: MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n        inputs: [\n            {\n                data: {\n                    image: {\n                        url: IMAGE_URL,\n                        // base64: imageBytes,\n                        allow_duplicate_url: true\n                    }\n                }\n            }\n        ]\n    },\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            throw new Error(\"Post model outputs failed, status: \" + response.status.description);\n        }\n\n        const regions = response.outputs[0].data.regions;\n\n        for (const region of regions) {\n            for (const concept of region.data.concepts) {\n                // Accessing and rounding the concept's percentage of image covered\n                const name = concept.name;\n                const value = concept.value.toFixed(4);\n                console.log(`${name}: ${value}`);\n            }\n        }\n    }\n);",Me='//index.js file\n\n///////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, and the URL\n// of the image we want as an input. Change these strings to run your own example.\n///////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\nconst PAT = "YOUR_PAT_HERE";\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\nconst USER_ID = "salesforce";\nconst APP_ID = "blip";\n// Change these to whatever model and image URL you want to use\nconst MODEL_ID = "general-english-image-caption-blip";\nconst MODEL_VERSION_ID = "cdb690f13e62470ea6723642044f95e4";\nconst IMAGE_URL = "https://samples.clarifai.com/metro-north.jpg";\n// To use a local file, assign the location variable\n// const IMAGE_FILE_LOCATION = "YOUR_IMAGE_FILE_LOCATION_HERE"\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\n// To use a local text file, uncomment the following lines\n// const fs = require("fs");\n// const imageBytes = fs.readFileSync(IMAGE_FILE_LOCATION);\n\nstub.PostModelOutputs({\n    user_app_id: {\n        "user_id": USER_ID,\n        "app_id": APP_ID\n    },\n    model_id: MODEL_ID,\n    version_id: MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n    inputs: [{\n        data: {\n            image: {\n                url: IMAGE_URL,\n                // base64: imageBytes,\n                allow_duplicate_url: true\n            }\n        }\n    }]\n},\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            throw new Error("Post model outputs failed, status: " + response.status.description);\n        }\n\n        // Since we have one input, one output will exist here\n        const output = response.outputs[0];\n\n        console.log("Image caption::");\n        console.log(output.data.text.raw);\n\n    }\n\n);\n',je='package com.clarifai.example;\n\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.status.StatusCode;\n\npublic class ClarifaiExample {\n\t\n\t////////////////////////////////////////////////////////////////////////////////////////////////////\n\t// In this section, we set the user authentication, user and app ID, model details, and the URL\n\t// of the image we want as an input. Change these strings to run your own example.\n\t///////////////////////////////////////////////////////////////////////////////////////////////////\n\t\n\t// Your PAT (Personal Access Token) can be found in the portal under Authentication\n\tstatic final String PAT = "YOUR_PAT_HERE";\n\t// Specify the correct user_id/app_id pairings\n\t// Since you\'re making inferences outside your app\'s scope\n\tstatic final String USER_ID = "clarifai";\t\n\tstatic final String APP_ID = "main";\n\t// Change these to whatever model and image URL you want to use\n\tstatic final String MODEL_ID = "general-image-recognition";\t\n\tstatic final String MODEL_VERSION_ID = "aa7f35c01e0642fda5cf400f543e7c40";\t\n\tstatic final String IMAGE_URL = "https://samples.clarifai.com/metro-north.jpg";\t\t\t\n\t\n\t///////////////////////////////////////////////////////////////////////////////////\n\t// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n\t///////////////////////////////////////////////////////////////////////////////////\t\n\n\tpublic static void main(String[] args) {\n\t\t\n\t\tV2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n\t\t\t    .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\t\t\n\t\tMultiOutputResponse postModelOutputsResponse = stub.postModelOutputs(\n\t\t    PostModelOutputsRequest.newBuilder()\n\t\t    \t.setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\t\t \t\t     \n\t\t        .setModelId(MODEL_ID)\n\t\t        .setVersionId(MODEL_VERSION_ID)  // This is optional. Defaults to the latest model version.\n\t\t        .addInputs(\n\t\t            Input.newBuilder().setData(\n\t\t                Data.newBuilder().setImage(\n\t\t                    Image.newBuilder().setUrl(IMAGE_URL)\n\t\t                )\n\t\t            )\n\t\t        )\n\t\t        .build()\n\t\t);\n\n\t\tif (postModelOutputsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n\t\t  throw new RuntimeException("Post model outputs failed, status: " + postModelOutputsResponse.getStatus());\n\t\t}\n\n\t\t// Since we have one input, one output will exist here.\n\t\tOutput output = postModelOutputsResponse.getOutputs(0);\n\n\t\tSystem.out.println("Predicted concepts:");\n\t\tfor (Concept concept : output.getData().getConceptsList()) {\n\t\t    System.out.printf("%s %.2f%n", concept.getName(), concept.getValue());\n\t\t}\n\n\t}\n\n}\n',He='package com.clarifai.example;\n\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.status.StatusCode;\nimport com.google.protobuf.ByteString;\nimport java.io.File;\nimport java.io.IOException;\nimport java.nio.file.Files;\n\npublic class ClarifaiExample {\n\n    //////////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, and the location\n    // of the image we want as an input. Change these strings to run your own example.\n    /////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n    //Your PAT (Personal Access Token) can be found in the portal under Authentication\t\n    static final String PAT = "YOUR_PAT_HERE";\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    static final String USER_ID = "clarifai";    \n    static final String APP_ID = "main";\n    // Change these to whatever model and image input you want to use\n    static final String MODEL_ID = "general-image-recognition"; \n    static final String MODEL_VERSION_ID = "aa7f35c01e0642fda5cf400f543e7c40";   \n    static final String IMAGE_FILE_LOCATION = "YOUR_IMAGE_FILE_LOCATION_HERE";\n    \n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\t\n\n    public static void main(String[] args) throws IOException {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n            .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        MultiOutputResponse postModelOutputsResponse = stub.postModelOutputs(\n            PostModelOutputsRequest.newBuilder()\n            .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n            .setModelId(MODEL_ID)\n            .setVersionId(MODEL_VERSION_ID) // This is optional. Defaults to the latest model version\n            .addInputs(\n                Input.newBuilder().setData(\n                    Data.newBuilder().setImage(\n                        Image.newBuilder()\n                        .setBase64(ByteString.copyFrom(Files.readAllBytes(\n                            new File(IMAGE_FILE_LOCATION).toPath()\n                        )))\n                    )\n                )\n            )\n            .build()\n        );\n\n        if (postModelOutputsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("Post model outputs failed, status: " + postModelOutputsResponse.getStatus());\n        }\n\n        // Since we have one input, one output will exist here.\n        Output output = postModelOutputsResponse.getOutputs(0);\n\n        System.out.println("Predicted concepts:");\n        for (Concept concept: output.getData().getConceptsList()) {\n            System.out.printf("%s %.2f%n", concept.getName(), concept.getValue());\n        }\n\n    }\n\n}',Be='package com.clarifai.example;\n\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.status.StatusCode; \nimport java.util.*;\n\nimport com.google.protobuf.ByteString;\nimport java.io.File;\nimport java.io.IOException;\nimport java.nio.file.Files;\n\npublic class ClarifaiExample {\n\n    ////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, and the URL\n    // of the image we want as an input. Change these strings to run your own example.\n    ///////////////////////////////////////////////////////////////////////////////////////////////////\n\n    // Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    static final String USER_ID = "clarifai";\n    static final String APP_ID = "main";\n    // Change these to whatever model and image URL you want to use\n    static final String MODEL_ID = "general-image-detection";\n    static final String MODEL_VERSION_ID = "1580bb1932594c93b7e2e04456af7c6f";\n    static final String IMAGE_URL = "https://samples.clarifai.com/metro-north.jpg";\n    // Or, to use a local image file, assign the location variable\n    // static final String IMAGE_FILE_LOCATION = "YOUR_IMAGE_FILE_LOCATION_HERE";\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\t\n\n    public static void main(String[] args) throws IOException {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n                .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        MultiOutputResponse postModelOutputsResponse = stub.postModelOutputs(\n                PostModelOutputsRequest.newBuilder()\n                        .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n                        .setModelId(MODEL_ID)\n                        .setVersionId(MODEL_VERSION_ID) // This is optional. Defaults to the latest model version.\n                        .addInputs(\n                                Input.newBuilder().setData(\n                                        Data.newBuilder().setImage(\n                                                Image.newBuilder().setUrl(IMAGE_URL)\n                                         // Image.newBuilder().setBase64(ByteString.copyFrom(Files.readAllBytes(\n                                         // new File(IMAGE_FILE_LOCATION).toPath()\n                                        // )))\n                                        )\n                                )\n                        )\n                        .build()\n        );\n\n        if (postModelOutputsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("Post model outputs failed, status: " + postModelOutputsResponse.getStatus());\n        }\n               \n        List<Region> regions = postModelOutputsResponse.getOutputs(0).getData().getRegionsList();\n\n        for (Region region : regions) {\n            // Accessing and rounding the bounding box values\n            double topRow = region.getRegionInfo().getBoundingBox().getTopRow();\n            double leftCol = region.getRegionInfo().getBoundingBox().getLeftCol();\n            double bottomRow = region.getRegionInfo().getBoundingBox().getBottomRow();\n            double rightCol = region.getRegionInfo().getBoundingBox().getRightCol();\n\n            for (Concept concept : region.getData().getConceptsList()) {\n                // Accessing and rounding the concept value\n                String name = concept.getName();\n                double value = concept.getValue();\n\n                System.out.println(name + ": " + value + " BBox: " + topRow + ", " + leftCol + ", " + bottomRow + ", " + rightCol);\n            }\n        }\n    }\n\n}\n ',Ye='package com.clarifai.example;\n\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.status.StatusCode;\nimport java.util.*;\n\nimport com.google.protobuf.ByteString;\nimport java.io.File;\nimport java.io.IOException;\nimport java.nio.file.Files;\n\npublic class ClarifaiExample {\n\n    ////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, and the URL\n    // of the image we want as an input. Change these strings to run your own example.\n    ///////////////////////////////////////////////////////////////////////////////////////////////////\n\n    // Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    static final String USER_ID = "clarifai";\n    static final String APP_ID = "main";\n    // Change these to whatever model and image URL you want to use\n    static final String MODEL_ID = "image-general-segmentation";\n    static final String MODEL_VERSION_ID = "1581820110264581908ce024b12b4bfb";\n    static final String IMAGE_URL = "https://samples.clarifai.com/metro-north.jpg";\n    // Or, to use a local image file, assign the location variable\n    // static final String IMAGE_FILE_LOCATION = "YOUR_IMAGE_FILE_LOCATION_HERE";\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\t\n\n    public static void main(String[] args) throws IOException {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n                .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        MultiOutputResponse postModelOutputsResponse = stub.postModelOutputs(\n                PostModelOutputsRequest.newBuilder()\n                        .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n                        .setModelId(MODEL_ID)\n                        .setVersionId(MODEL_VERSION_ID) // This is optional. Defaults to the latest model version.\n                        .addInputs(\n                                Input.newBuilder().setData(\n                                        Data.newBuilder().setImage(\n                                                Image.newBuilder().setUrl(IMAGE_URL)\n                                        // Image.newBuilder().setBase64(ByteString.copyFrom(Files.readAllBytes(\n                                        // new File(IMAGE_FILE_LOCATION).toPath()\n                                        // )))\n                                        )\n                                )\n                        )\n                        .build()\n        );\n\n        if (postModelOutputsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("Post model outputs failed, status: " + postModelOutputsResponse.getStatus());\n        }\n\n        List<Region> regions = postModelOutputsResponse.getOutputs(0).getData().getRegionsList();\n\n        for (Region region : regions) {\n            for (Concept concept : region.getData().getConceptsList()) {\n                // Accessing and rounding the concept\'s percentage of image covered\n                String name = concept.getName();\n                double value = concept.getValue();\n\n                System.out.println(name + ": " + value);\n            }\n        }\n\n    }\n\n}\n',ke='package com.clarifai.example;\n\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.status.StatusCode;\n\nimport com.google.protobuf.ByteString;\nimport java.io.File;\nimport java.io.IOException;\nimport java.nio.file.Files;\n\npublic class ClarifaiExample {\n\n    ////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, and the URL\n    // of the image we want as an input. Change these strings to run your own example.\n    ///////////////////////////////////////////////////////////////////////////////////////////////////\n\n    // Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    static final String USER_ID = "salesforce";\n    static final String APP_ID = "blip";\n    // Change these to whatever model and image URL you want to use\n    static final String MODEL_ID = "general-english-image-caption-blip";\n    static final String MODEL_VERSION_ID = "cdb690f13e62470ea6723642044f95e4";\n    static final String IMAGE_URL = "https://samples.clarifai.com/metro-north.jpg";\n    // Or, to use a local image file, assign the location variable\n    //static final String IMAGE_FILE_LOCATION = "YOUR_IMAGE_FILE_LOCATION_HERE";\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\t\n\n    public static void main(String[] args) throws IOException {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n                .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        MultiOutputResponse postModelOutputsResponse = stub.postModelOutputs(\n                PostModelOutputsRequest.newBuilder()\n                        .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n                        .setModelId(MODEL_ID)\n                        .setVersionId(MODEL_VERSION_ID) // This is optional. Defaults to the latest model version.\n                        .addInputs(\n                                Input.newBuilder().setData(\n                                        Data.newBuilder().setImage(\n                                                Image.newBuilder().setUrl(IMAGE_URL)\n                                        // Image.newBuilder().setBase64(ByteString.copyFrom(Files.readAllBytes(\n                                        // new File(IMAGE_FILE_LOCATION).toPath()\n                                        // )))\n                                        )\n                                )\n                        )\n                        .build()\n        );\n\n        if (postModelOutputsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("Post model outputs failed, status: " + postModelOutputsResponse.getStatus());\n        }\n\n        // Since we have one input, one output will exist here.\n        Output output = postModelOutputsResponse.getOutputs(0);\n\n        System.out.println("Image caption:");\n        System.out.println(output.getData().getText().getRaw());\n\n        // Uncomment this line to print the raw output\n        // System.out.println(output);\n    }\n\n}\n',$e="<?php\n\nrequire __DIR__ . '/vendor/autoload.php';\n\n//////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, and the URL\n// of the image we want as an input. Change these strings to run your own example.\n//////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account's Security section\n$PAT = 'YOUR_PAT_HERE';\n// Specify the correct user_id/app_id pairings\n// Since you're making inferences outside your app's scope\n$USER_ID = 'clarifai';\n$APP_ID = 'main';\n// Change these to whatever model and image URL you want to use\n$MODEL_ID = 'general-image-recognition';\n$MODEL_VERSION_ID = 'aa7f35c01e0642fda5cf400f543e7c40';\n$IMAGE_URL = 'https://samples.clarifai.com/metro-north.jpg';\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Image;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\PostModelOutputsRequest;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ['Authorization' => ['Key ' . $PAT ]];\n\n$userDataObject = new UserAppIDSet([\n    'user_id' => $USER_ID, \n    'app_id' => $APP_ID \n]);\n\n// Let's make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client->PostModelOutputs(\n    // The request object carries the request along with the request status and other metadata related to the request itself\n    new PostModelOutputsRequest([\n        'user_app_id' => $userDataObject,\n        'model_id' => $MODEL_ID,  \n        'version_id' => $MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n        'inputs' => [\n            new Input([ // The Input object wraps the Data object in order to meet the API specification                \n                'data' => new Data([ // The Data object is constructed around the Image object. It offers a container that has additional image independent\n                                    // metadata. In this particular use case, no other metadata is needed to be specified\n                    'image' => new Image([ // In the Clarifai platform, an image is defined by a special Image object\n                        'url' => $IMAGE_URL\n                    ])\n                ])\n            ])\n        ]\n    ]),\n    $metadata\n)->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception(\"Error: {$status->details}\");\n}\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure \n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    throw new Exception(\"Failure response: \" . $response->getStatus()->getDescription() . \" \" .\n        $response->getStatus()->getDetails());\n}\n\n// The output of a successful call can be used in many ways. In this example, we loop through all of the predicted concepts \n// and print them out along with their numerical prediction value (confidence)\necho \"Predicted concepts: </br>\";\nforeach ($response->getOutputs()[0]->getData()->getConcepts() as $concept) {\n    echo $concept->getName() . \": \" . number_format($concept->getValue(), 2) . \"</br>\";\n}\n\n?>",Fe="<?php\n\nrequire __DIR__ . '/vendor/autoload.php';\n\n////////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, and the location\n// of the image we want as an input. Change these strings to run your own example.\n///////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account's Security section\n$PAT = 'YOUR_PAT_HERE';\n// Specify the correct user_id/app_id pairings\n// Since you're making inferences outside your app's scope\n$USER_ID = 'clarifai';\n$APP_ID = 'main';\n// Change these to whatever model and image input you want to use\n$MODEL_ID = 'general-image-recognition';\n$MODEL_VERSION_ID = 'aa7f35c01e0642fda5cf400f543e7c40';\n$IMAGE_FILE_LOCATION = 'YOUR_IMAGE_FILE_LOCATION_HERE';\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Image;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\PostModelOutputsRequest;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ['Authorization' => ['Key ' . $PAT ]];\n\n$userDataObject = new UserAppIDSet([\n    'user_id' => $USER_ID, \n    'app_id' => $APP_ID \n]);\n\n$imageData = file_get_contents($IMAGE_FILE_LOCATION); // Get the image bytes data from the location\n\n// Let's make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client->PostModelOutputs(\n    // The request object carries the request along with the request status and other metadata related to the request itself\n    new PostModelOutputsRequest([\n        'user_app_id' => $userDataObject,\n        'model_id' => $MODEL_ID,  \n        'version_id' => $MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n        'inputs' => [\n            new Input([ // The Input object wraps the Data object in order to meet the API specification                \n                'data' => new Data([ // The Data object is constructed around the Image object. It offers a container that has additional image independent\n                                    // metadata. In this particular use case, no other metadata is needed to be specified\n                    'image' => new Image([ // In the Clarifai platform, an image is defined by a special Image object\n                        'base64' => $imageData\n                    ])\n                ])\n            ])\n        ]\n    ]),\n    $metadata\n)->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception(\"Error: {$status->details}\");\n}\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure \n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    throw new Exception(\"Failure response: \" . $response->getStatus()->getDescription() . \" \" .\n        $response->getStatus()->getDetails());\n}\n\n// The output of a successful call can be used in many ways. In this example, we loop through all of the predicted concepts \n// and print them out along with their numerical prediction value (confidence)\necho \"Predicted concepts: </br>\";\nforeach ($response->getOutputs()[0]->getData()->getConcepts() as $concept) {\n    echo $concept->getName() . \": \" . number_format($concept->getValue(), 2) . \"</br>\";\n}\n\n?>",Ge='<?php\n\nrequire __DIR__ . "/vendor/autoload.php";\n\n//////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, and the URL\n// of the image we want as an input. Change these strings to run your own example.\n//////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\n$PAT = "YOUR_PAT_HERE";\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\n$USER_ID = "clarifai";\n$APP_ID = "main";\n// Change these to whatever model and image URL you want to use\n$MODEL_ID = "general-image-detection";\n$MODEL_VERSION_ID = "1580bb1932594c93b7e2e04456af7c6f";\n$IMAGE_URL = "https://samples.clarifai.com/metro-north.jpg";\n// To use a local file, assign the location variable\n//$IMAGE_FILE_LOCATION = "YOUR_IMAGE_FILE_LOCATION_HERE";\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Image;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\PostModelOutputsRequest;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ["Authorization" => ["Key " . $PAT]];\n\n$userDataObject = new UserAppIDSet([\n    "user_id" => $USER_ID,\n    "app_id" => $APP_ID,\n]);\n\n// To use a local text file, uncomment the following lines\n// $imageData = file_get_contents($IMAGE_FILE_LOCATION);\n\n// Let\'s make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client\n    ->PostModelOutputs(\n        // The request object carries the request along with the request status and other metadata related to the request itself\n        new PostModelOutputsRequest([\n            "user_app_id" => $userDataObject,\n            "model_id" => $MODEL_ID,\n            "version_id" => $MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n            "inputs" => [\n                new Input([\n                    // The Input object wraps the Data object in order to meet the API specification\n                    "data" => new Data([\n                        // The Data object is constructed around the Image object. It offers a container that has additional image independent\n                        // metadata. In this particular use case, no other metadata is needed to be specified\n                        "image" => new Image([\n                            // In the Clarifai platform, an image is defined by a special Image object\n                            "url" => $IMAGE_URL,\n                            //"base64" => $imageData,\n                        ]),\n                    ]),\n                ]),\n            ],\n        ]),\n        $metadata\n    )\n    ->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception("Error: {$status->details}");\n}\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure\n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    throw new Exception(\n        "Failure response: " .\n            $response->getStatus()->getDescription() .\n            " " .\n            $response->getStatus()->getDetails()\n    );\n}\n\n$regions = $response->getOutputs()[0]->getData()->getRegions();\n\nforeach ($regions as $region) {\n    // Accessing and rounding the bounding box values\n    $topRow = $region->getRegionInfo()->getBoundingBox()->getTopRow();\n    $leftCol = $region->getRegionInfo()->getBoundingBox()->getLeftCol();\n    $bottomRow = $region->getRegionInfo()->getBoundingBox()->getBottomRow();\n    $rightCol = $region->getRegionInfo()->getBoundingBox()->getRightCol();\n\n    foreach ($region->getData()->getConcepts() as $concept) {\n        // Accessing and rounding the concept value\n        $name = $concept->getName();\n        $value = $concept->getValue();\n\n        echo $name . ": " . $value . " BBox: " . $topRow . ", " . $leftCol . ", " . $bottomRow . ", " . $rightCol . "\\n";\n    }\n}\n\n?>\n',Ve='<?php\n\nrequire __DIR__ . "/vendor/autoload.php";\n\n//////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, and the URL\n// of the image we want as an input. Change these strings to run your own example.\n//////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\n$PAT = "YOUR_PAT_HERE";\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\n$USER_ID = "clarifai";\n$APP_ID = "main";\n// Change these to whatever model and image URL you want to use\n$MODEL_ID = "image-general-segmentation";\n$MODEL_VERSION_ID = "1581820110264581908ce024b12b4bfb";\n$IMAGE_URL = "https://samples.clarifai.com/metro-north.jpg";\n// To use a local file, assign the location variable\n//$IMAGE_FILE_LOCATION = "YOUR_IMAGE_FILE_LOCATION_HERE";\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Image;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\PostModelOutputsRequest;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ["Authorization" => ["Key " . $PAT]];\n\n$userDataObject = new UserAppIDSet([\n    "user_id" => $USER_ID,\n    "app_id" => $APP_ID,\n]);\n\n// To use a local text file, uncomment the following lines\n// $imageData = file_get_contents($IMAGE_FILE_LOCATION);\n\n// Let\'s make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client\n    ->PostModelOutputs(\n        // The request object carries the request along with the request status and other metadata related to the request itself\n        new PostModelOutputsRequest([\n            "user_app_id" => $userDataObject,\n            "model_id" => $MODEL_ID,\n            "version_id" => $MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n            "inputs" => [\n                new Input([\n                    // The Input object wraps the Data object in order to meet the API specification\n                    "data" => new Data([\n                        // The Data object is constructed around the Image object. It offers a container that has additional image independent\n                        // metadata. In this particular use case, no other metadata is needed to be specified\n                        "image" => new Image([\n                            // In the Clarifai platform, an image is defined by a special Image object\n                            "url" => $IMAGE_URL,\n                            //"base64" => $imageData,\n                        ]),\n                    ]),\n                ]),\n            ],\n        ]),\n        $metadata\n    )\n    ->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception("Error: {$status->details}");\n}\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure\n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    throw new Exception(\n        "Failure response: " .\n            $response->getStatus()->getDescription() .\n            " " .\n            $response->getStatus()->getDetails()\n    );\n}\n\n$regions = $response->getOutputs()[0]->getData()->getRegions();\n\nforeach ($regions as $region) {\n    foreach ($region->getData()->getConcepts() as $concept) {\n        // Accessing and rounding the concept\'s percentage of image covered\n        $name = $concept->getName();\n        $value = $concept->getValue();\n\n        echo $name . ": " . $value . "\\n";\n    }\n}\n\n?>\n',qe='<?php\n\nrequire __DIR__ . "/vendor/autoload.php";\n\n//////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, and the URL\n// of the image we want as an input. Change these strings to run your own example.\n//////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\n$PAT = "YOUR_PAT_HERE";\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\n$USER_ID = "salesforce";\n$APP_ID = "blip";\n// Change these to whatever model and image URL you want to use\n$MODEL_ID = "general-english-image-caption-blip";\n$MODEL_VERSION_ID = "cdb690f13e62470ea6723642044f95e4";\n$IMAGE_URL = "https://samples.clarifai.com/metro-north.jpg";\n// To use a local file, assign the location variable\n// $IMAGE_FILE_LOCATION = "YOUR_IMAGE_FILE_LOCATION_HERE";\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Image;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\PostModelOutputsRequest;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ["Authorization" => ["Key " . $PAT]];\n\n$userDataObject = new UserAppIDSet([\n    "user_id" => $USER_ID,\n    "app_id" => $APP_ID,\n]);\n\n// To use a local text file, uncomment the following lines\n// $imageData = file_get_contents($IMAGE_FILE_LOCATION);\n\n// Let\'s make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client\n    ->PostModelOutputs(\n        // The request object carries the request along with the request status and other metadata related to the request itself\n        new PostModelOutputsRequest([\n            "user_app_id" => $userDataObject,\n            "model_id" => $MODEL_ID,\n            "version_id" => $MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n            "inputs" => [\n                new Input([\n                    // The Input object wraps the Data object in order to meet the API specification\n                    "data" => new Data([\n                        // The Data object is constructed around the Image object. It offers a container that has additional image independent\n                        // metadata. In this particular use case, no other metadata is needed to be specified\n                        "image" => new Image([\n                            // In the Clarifai platform, an image is defined by a special Image object\n                            "url" => $IMAGE_URL,\n                            // "base64" => $imageData,\n                        ]),\n                    ]),\n                ]),\n            ],\n        ]),\n        $metadata\n    )\n    ->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception("Error: {$status->details}");\n}\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure\n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    throw new Exception(\n        "Failure response: " .\n            $response->getStatus()->getDescription() .\n            " " .\n            $response->getStatus()->getDetails()\n    );\n}\n\n// Since we have one input, one output will exist here\n$output = $response->getOutputs()[0];\n\necho "Image caption:" . "\\n";\necho $output->getData()->getText()->getRaw() . "\\n";\n\n// Uncomment this line to print the raw output \n// echo json_encode($output->serializeToJsonString()) . "\\n";\n\n?>\n',Xe='curl -X POST "https://api.clarifai.com/v2/users/clarifai/apps/main/models/general-image-recognition/versions/aa7f35c01e0642fda5cf400f543e7c40/outputs" \\\n    -H "Authorization: Key YOUR_PAT_HERE" \\\n    -H "Content-Type: application/json" \\\n    -d \'{\n      "inputs": [\n        {\n          "data": {\n            "image": {\n              "url": "https://samples.clarifai.com/metro-north.jpg"\n            }\n          }\n        }\n      ]\n    }\'\n   ',Qe='# Smaller files (195 KB or less)\n\ncurl -X POST "https://api.clarifai.com/v2/users/clarifai/apps/main/models/general-image-recognition/versions/aa7f35c01e0642fda5cf400f543e7c40/outputs" \\\n  -H "Authorization: Key YOUR_PAT_HERE" \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "inputs": [\n      {\n        "data": {\n          "image": {\n            "base64": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAUDBAQEAwUEBAQFBQUGBwwIBwcHBw8LCwkMEQ8SEhEPERETFhwXExQaFRERGCEYGh0dHx8fExciJCIeJBweHx7/2wBDAQUFBQcGBw4ICA4eFBEUHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh7/wAARCAAoACgDASIAAhEBAxEB/8QAGQAAAgMBAAAAAAAAAAAAAAAAAAYDBQcE/8QAMBAAAQMDAwMDAgQHAAAAAAAAAQIDBAAFEQYSIQcTMTJBURRhCBYikSNScXKhsdH/xAAZAQACAwEAAAAAAAAAAAAAAAAFBgIDBAf/xAAtEQABAwMBBgQHAQAAAAAAAAABAgMRAAQhMQUSE0FRYQaBocEUFiJCcrHR8P/aAAwDAQACEQMRAD8A3+RYY1unSYzCS0ttZUkAgktn0q5yT7jPyDUC4wdGwycH5U2Kt9ZQ7VI1qw5PkvQy3CSVPpf7aQjuKyFH25xzn3pHn3TVNy01Hl2hyy6YdkSpKsS9sl/6RlI3rRu3dxWd6spwnAGPIJTfl925fcLaoSDHXvyo6i9SlCQrU9wKln3OyWiaDN1RAbW3kKbSd7gPtwMkH/tTWy9afuy1iPfnXMAblITwkE4yf08cn3pSbYt1uts24XH6fUbiLAuY1MWyGkLEmUW0rcCRvUpQ5CtwKQCPgi4S1ZbDe4sd9NntDEe79m3uOBLTr0IR9jzodSMqUpTu9JJ8owD7UTT4ZCfv9PbP7860m+s+HBSrejWRuz2kAxoesGYxTW/Zlpkwo1vkuSly3UgKWQUhHJUvIHsAaKTemF8XE6sWmxyZkiaZrMh1jv8ArQNpUVqB8FW0njHqx4zRVVhsph1KlKk5xQ+7uHmikaSJrQerMByet2IwvtuTLa4xv2k7Rk84H9x/esHv92d01boenLXGcuiWrFIhLlpbcaQ2/JdK3VJCkAq2pAR7Zz7YxWudY9fxNIdQbNGkR5TyX4aisNNpUMFZAzkj4NK0jq9ZpbLr0PSlzkhrlZDaQlP3P8Q4/ap3F87bPucJEkx/hHv60b2TYXLrKN5sramYECSQRk9M6c6zmJ+eb5Hi22M7cnWGIQgFLbX0zSo4PDa1YBcTgDyMjJ/qbGPabH08SJt1Uzc9QqRliGg5QySPKvgc+TyfYDmmTUWpNYz7ctxoQdPQshCktupckDJUPUcJT6DwMq8YyaQ9VL0pCS8zapcq4SVOBZmPDO8/cnknlWcDBwn4NYnPjLkQ+qE9OtOVlYpeVHDCEkkkJyT+SuQzy5Y0ru6Ez511/Efa5s1fdkOtyVurIxgdlQAA9gOKKPwolU7remU5hCGYEgo38KUv9I/0TRTDYJCWQBSF4rIN/CRgAR0iTpVD1j1g/qDqJcJqlKcjB9bcda142MpOEJAzgeMnjyTSyze5KEuNRpDoDvC0oe4X9iAeaKKFK+oya6fbOqYbDTeEiAPKpHdS3gBLYc7RQkp3ApQog+cq8nwPJrljzxnPZbUfnugn/NFFRgEVch9xKsH0H8pg6e3x3T3UC1ajaZITGkJLoS4MKbOUrzz/ACKVRRRVzVwtoQmhG1NkWu0HuI+JI8u/Kv/Z"\n          }\n        }\n      }\n    ]\n  }\'\n\n# Larger Files (Greater than 195 KB)\n\ncurl -X POST "https://api.clarifai.com/v2/users/clarifai/apps/main/models/general-image-recognition/versions/aa7f35c01e0642fda5cf400f543e7c40/outputs" \\\n  -H "Authorization: Key YOUR_PAT_HERE" \\\n  -H "Content-Type: application/json" \\\n  -d @-  << FILEIN\n  {\n    "inputs": [\n      {\n        "data": {\n          "image": {\n            "base64": "$(base64 /home/user/image.png)"\n          }\n        }\n      }\n    ]\n  }\nFILEIN',Ke='curl -X POST "https://api.clarifai.com/v2/users/clarifai/apps/main/models/general-image-detection/versions/1580bb1932594c93b7e2e04456af7c6f/outputs" \\\n    -H "Authorization: Key YOUR_PAT_HERE" \\\n    -H "Content-Type: application/json" \\\n    -d \'{\n      "inputs": [\n        {\n          "data": {\n            "image": {\n              "url": "https://samples.clarifai.com/metro-north.jpg"\n            }\n          }\n        }\n      ]\n    }\'\n   ',We='curl -X POST "https://api.clarifai.com/v2/users/clarifai/apps/main/models/image-general-segmentation/versions/1581820110264581908ce024b12b4bfb/outputs" \\\n    -H "Authorization: Key YOUR_PAT_HERE" \\\n    -H "Content-Type: application/json" \\\n    -d \'{\n      "inputs": [\n        {\n          "data": {\n            "image": {\n              "url": "https://samples.clarifai.com/metro-north.jpg"\n            }\n          }\n        }\n      ]\n    }\'\n   ',ze='curl -X POST "https://api.clarifai.com/v2/users/clarifai/apps/main/models/general-image-recognition/versions/aa7f35c01e0642fda5cf400f543e7c40/outputs" \\\n    -H "Authorization: Key YOUR_PAT_HERE" \\\n    -H "Content-Type: application/json" \\\n    -d \'{\n      "inputs": [\n        {\n          "data": {\n            "image": {\n              "url": "https://samples.clarifai.com/metro-north.jpg"\n            }\n          }\n        },\n        {\n          "data": {\n            "image": {\n              "url": "...any other valid image url..."\n            }\n          }\n        },\n        # ... and so on\n      ]\n    }\'\n   ',Je='curl -X POST "https://api.clarifai.com/v2/users/salesforce/apps/blip/models/general-english-image-caption-blip/versions/cdb690f13e62470ea6723642044f95e4/outputs" \\\n    -H "Authorization: Key YOUR_PAT_HERE" \\\n    -H "Content-Type: application/json" \\\n    -d \'{\n      "inputs": [\n        {\n          "data": {\n            "image": {\n              "url": "https://samples.clarifai.com/metro-north.jpg"\n            }\n          }\n        }\n      ]\n    }\'\n   ',Ze="#######################################################################################################\n# In this section, we set the user authentication, user and app ID, model details, URL of the video\n# we want as an input, and sample_ms. Change these strings to run your own example.\n#######################################################################################################\n\n# Your PAT (Personal Access Token) can be found in the Account's Security section\nPAT = 'YOUR_PAT_HERE'\n# Specify the correct user_id/app_id pairings\n# Since you're making inferences outside your app's scope\nUSER_ID = 'clarifai'\nAPP_ID = 'main'\n# Change these to whatever model and video URL you want to use\nMODEL_ID = 'general-image-recognition'\nMODEL_VERSION_ID = 'aa7f35c01e0642fda5cf400f543e7c40'\nVIDEO_URL = 'https://samples.clarifai.com/beer.mp4'\n# Change this to configure the FPS rate (If it's not configured, it defaults to 1 FPS) \nSAMPLE_MS = 500\n\n############################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n############################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (('authorization', 'Key ' + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\npost_model_outputs_response = stub.PostModelOutputs(\n    service_pb2.PostModelOutputsRequest(\n        user_app_id=userDataObject,  # The userDataObject is created in the overview and is required when using a PAT\n        model_id=MODEL_ID,\n        version_id=MODEL_VERSION_ID,  # This is optional. Defaults to the latest model version\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    video=resources_pb2.Video(\n                        url=VIDEO_URL\n                    )\n                )\n            )\n        ],\n        model=resources_pb2.Model(\n            output_info=resources_pb2.OutputInfo(\n                output_config=resources_pb2.OutputConfig(sample_ms=SAMPLE_MS)\n            )\n        ),\n    ),\n    metadata=metadata\n)\nif post_model_outputs_response.status.code != status_code_pb2.SUCCESS:\n    print(post_model_outputs_response.status)\n    raise Exception(\"Post model outputs failed, status: \" + post_model_outputs_response.status.description)\n\n# Since we have one input, one output will exist here\noutput = post_model_outputs_response.outputs[0]\n\n# A separate prediction is available for each \"frame\"\nfor frame in output.data.frames:\n    print(\"Predicted concepts on frame \" + str(frame.frame_info.time) + \":\")\n    for concept in frame.data.concepts:\n        print(\"\\t%s %.2f\" % (concept.name, concept.value))\n\n# Uncomment this line to print the raw output\n#print(output)",en="############################################################################################################\n# In this section, we set the user authentication, user and app ID, model details, location of the video\n# we want as an input, and sample_ms. Change these strings to run your own example.\n###########################################################################################################\n\n# Your PAT (Personal Access Token) can be found in the Account's Security section\nPAT = 'YOUR_PAT_HERE'\n# Specify the correct user_id/app_id pairings\n# Since you're making inferences outside your app's scope\nUSER_ID = 'clarifai'\nAPP_ID = 'main'\n# Change these to whatever model and video input you want to use\nMODEL_ID = 'general-image-recognition'\nMODEL_VERSION_ID = 'aa7f35c01e0642fda5cf400f543e7c40'\nVIDEO_FILE_LOCATION = 'YOUR_VIDEO_FILE_LOCATION_HERE'\n# Change this to configure the FPS rate (If it's not configured, it defaults to 1 FPS) \nSAMPLE_MS = 500\n\n############################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n############################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (('authorization', 'Key ' + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\nwith open(VIDEO_FILE_LOCATION, \"rb\") as f:\n    file_bytes = f.read()\n\npost_model_outputs_response = stub.PostModelOutputs(\n    service_pb2.PostModelOutputsRequest(\n        user_app_id=userDataObject,  # The userDataObject is created in the overview and is required when using a PAT\n        model_id=MODEL_ID,\n        version_id=MODEL_VERSION_ID,  # This is optional. Defaults to the latest model version\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    video=resources_pb2.Video(\n                        base64=file_bytes\n                    )\n                )\n            )\n        ],\n        model=resources_pb2.Model(\n            output_info=resources_pb2.OutputInfo(\n                output_config=resources_pb2.OutputConfig(sample_ms=SAMPLE_MS)\n            )\n        ),\n    ),\n    metadata=metadata\n)\nif post_model_outputs_response.status.code != status_code_pb2.SUCCESS:\n    print(post_model_outputs_response.status)\n    raise Exception(\"Post model outputs failed, status: \" + post_model_outputs_response.status.description)\n\n# Since we have one input, one output will exist here\noutput = post_model_outputs_response.outputs[0]\n\n# A separate prediction is available for each \"frame\"\nfor frame in output.data.frames:\n    print(\"Predicted concepts on frame \" + str(frame.frame_info.time) + \":\")\n    for concept in frame.data.concepts:\n        print(\"\\t%s %.2f\" % (concept.name, concept.value))\n\n# Uncomment this line to print the raw output\n#print(output)",nn="\x3c!--index.html file--\x3e\n\n<script>\n    ////////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, URL of the video\n    // we want as an input, and sample_ms. Change these strings to run your own example.\n    ///////////////////////////////////////////////////////////////////////////////////////////////////////\n\n    // Your PAT (Personal Access Token) can be found in the Account's Security section\n    const PAT = 'YOUR_PAT_HERE';\n    // Specify the correct user_id/app_id pairings\n    // Since you're making inferences outside your app's scope\n    const USER_ID = 'clarifai';    \n    const APP_ID = 'main';\n    // Change these to whatever model and video URL you want to use\n    const MODEL_ID = 'general-image-recognition';\n    const MODEL_VERSION_ID = 'aa7f35c01e0642fda5cf400f543e7c40';    \n    const VIDEO_URL = 'https://samples.clarifai.com/beer.mp4';\n    // Change this to configure the FPS rate (If it's not configured, it defaults to 1 FPS) \n    const SAMPLE_MS = 500;\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    const raw = JSON.stringify({\n        \"user_app_id\": {\n            \"user_id\": USER_ID,\n            \"app_id\": APP_ID\n        },\n        \"inputs\": [\n            {\n                \"data\": {\n                    \"video\": {\n                        \"url\": VIDEO_URL\n                    }\n                }\n            }\n        ],\n        \"model\": {\n            \"output_info\": {\n                \"output_config\": {\n                    \"sample_ms\": SAMPLE_MS\n                }\n\n            }\n        }\n    });\n\n    const requestOptions = {\n        method: 'POST',\n        headers: {\n            'Accept': 'application/json',\n            'Authorization': 'Key ' + PAT\n        },\n        body: raw\n    };\n\n    // NOTE: MODEL_VERSION_ID is optional, you can also call prediction with the MODEL_ID only\n    // https://api.clarifai.com/v2/models/{YOUR_MODEL_ID}/outputs\n    // this will default to the latest version_id\n\n    fetch(\"https://api.clarifai.com/v2/models/\" + MODEL_ID + \"/versions/\" + MODEL_VERSION_ID + \"/outputs\", requestOptions)\n        .then(response => response.text())\n        .then(result => console.log(result))\n        .catch(error => console.log('error', error));\n<\/script>",tn="\x3c!--index.html file--\x3e\n\n<script>\n    ///////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, bytes of the \n    // video we want as an input, and sample_ms. Change these strings to run your own example.\n    //////////////////////////////////////////////////////////////////////////////////////////////////\n\n    // Your PAT (Personal Access Token) can be found in the Account's Security section\n    const PAT = 'YOUR_PAT_HERE';\n    // Specify the correct user_id/app_id pairings\n    // Since you're making inferences outside your app's scope\n    const USER_ID = 'clarifai';   \n    const APP_ID = 'main';\n    // Change these to whatever model and video input you want to use\n    const MODEL_ID = 'general-image-recognition';\n    const MODEL_VERSION_ID = 'aa7f35c01e0642fda5cf400f543e7c40';\n    const VIDEO_BYTES_STRING = 'YOUR_BYTES_STRING_HERE';\n    // Change this to configure the FPS rate (If it's not configured, it defaults to 1 FPS) \n    const SAMPLE_MS = 500;\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    const raw = JSON.stringify({\n        \"user_app_id\": {\n            \"user_id\": USER_ID,\n            \"app_id\": APP_ID\n        },\n        \"inputs\": [\n            {\n                \"data\": {\n                    \"video\": {\n                        \"base64\": VIDEO_BYTES_STRING\n                    }\n                }\n            }\n        ],\n        \"model\": {\n            \"output_info\": {\n                \"output_config\": {\n                    \"sample_ms\": SAMPLE_MS\n                }\n\n            }\n        }\n    });\n\n    const requestOptions = {\n        method: 'POST',\n        headers: {\n            'Accept': 'application/json',\n            'Authorization': 'Key ' + PAT\n        },\n        body: raw\n    };\n\n    // NOTE: MODEL_VERSION_ID is optional, you can also call prediction with the MODEL_ID only\n    // https://api.clarifai.com/v2/models/{YOUR_MODEL_ID}/outputs\n    // this will default to the latest version_id\n\n    fetch(\"https://api.clarifai.com/v2/models/\" + MODEL_ID + \"/versions/\" + MODEL_VERSION_ID + \"/outputs\", requestOptions)\n        .then(response => response.text())\n        .then(result => console.log(result))\n        .catch(error => console.log('error', error));\n<\/script>",an='//index.js file\n\n/////////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, URL of the video\n// we want as an input, and sample_ms. Change these strings to run your own example.\n////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\nconst PAT = "YOUR_PAT_HERE";\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\nconst USER_ID = "clarifai";\nconst APP_ID = "main";\n// Change these to whatever model and video URL you want to use\nconst MODEL_ID = "general-image-recognition";\nconst MODEL_VERSION_ID = "aa7f35c01e0642fda5cf400f543e7c40";\nconst VIDEO_URL = "https://samples.clarifai.com/beer.mp4";\n// Change this to configure the FPS rate (If it\'s not configured, it defaults to 1 FPS)\nconst SAMPLE_MS = 500;\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\nstub.PostModelOutputs(\n  {\n    user_app_id: {\n      user_id: USER_ID,\n      app_id: APP_ID,\n    },\n    model_id: MODEL_ID,\n    version_id: MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n    inputs: [\n      {\n        data: {\n          video: {\n            url: VIDEO_URL,\n            allow_duplicate_url: true,\n          },\n        },\n      },\n    ],\n    model: {\n      output_info: {\n        output_config: {\n          sample_ms: SAMPLE_MS,\n        },\n      },\n    },\n  },\n  metadata,\n  (err, response) => {\n    if (err) {\n      throw new Error(err);\n    }\n\n    if (response.status.code !== 10000) {\n      throw new Error(\n        "Post model outputs failed, status: " + response.status.description\n      );\n    }\n\n    // Since we have one input, one output will exist here\n    const output = response.outputs[0];\n\n    // A separate prediction is available for each "frame"\n    for (const frame of output.data.frames) {\n      console.log("Predicted concepts on frame " + frame.frame_info.time + ":");\n      for (const concept of frame.data.concepts) {\n        console.log("\\t" + concept.name + " " + concept.value);\n      }\n    }\n  }\n);\n',sn='//index.js file\n\n////////////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, location of the video\n// we want as an input, and sample_ms. Change these strings to run your own example.\n////////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\nconst PAT = "YOUR_PAT_HERE";\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\nconst USER_ID = "clarifai";\nconst APP_ID = "main";\n// Change these to whatever model and video input you want to use\nconst MODEL_ID = "general-image-recognition";\nconst MODEL_VERSION_ID = "aa7f35c01e0642fda5cf400f543e7c40";\nconst VIDEO_FILE_LOCATION = "YOUR_VIDEO_FILE_LOCATION_HERE";\n// Change this to configure the FPS rate (If it\'s not configured, it defaults to 1 FPS)\nconst SAMPLE_MS = 500;\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\nconst fs = require("fs");\nconst videoBytes = fs.readFileSync(VIDEO_FILE_LOCATION);\n\nstub.PostModelOutputs(\n  {\n    user_app_id: {\n      user_id: USER_ID,\n      app_id: APP_ID,\n    },\n    model_id: MODEL_ID,\n    version_id: MODEL_VERSION_ID, // This is optional. Defaults to the latest model version.\n    inputs: [\n      {\n        data: {\n          video: {\n            base64: videoBytes,\n          },\n        },\n      },\n    ],\n    model: {\n      output_info: {\n        output_config: {\n          sample_ms: SAMPLE_MS,\n        },\n      },\n    },\n  },\n  metadata,\n  (err, response) => {\n    if (err) {\n      throw new Error(err);\n    }\n\n    if (response.status.code !== 10000) {\n      throw new Error(\n        "Post model outputs failed, status: " + response.status.description\n      );\n    }\n\n    // Since we have one input, one output will exist here.\n    const output = response.outputs[0];\n\n    // A separate prediction is available for each "frame".\n    for (const frame of output.data.frames) {\n      console.log("Predicted concepts on frame " + frame.frame_info.time + ":");\n      for (const concept of frame.data.concepts) {\n        console.log("\\t" + concept.name + " " + concept.value);\n      }\n    }\n  }\n);\n',on='package com.clarifai.example;\n\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.status.StatusCode;\n\npublic class ClarifaiExample {\n\n    ////////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, URL of the video\n    // we want as an input, and sample_ms. Change these strings to run your own example.\n    ///////////////////////////////////////////////////////////////////////////////////////////////////////\n\n    //Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    static final String USER_ID = "clarifai";    \n    static final String APP_ID = "main";\n    // Change these to whatever model and video URL you want to use\n    static final String MODEL_ID = "general-image-recognition";\n    static final String MODEL_VERSION_ID = "aa7f35c01e0642fda5cf400f543e7c40";\n    static final String VIDEO_URL = "https://samples.clarifai.com/beer.mp4";\n    // Change this to configure the FPS rate (If it\'s not configured, it defaults to 1 FPS)\n    static final int SAMPLE_MS = 500;\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\t\n\n    public static void main(String[] args) {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n            .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        MultiOutputResponse postModelOutputsResponse = stub.postModelOutputs(\n            PostModelOutputsRequest.newBuilder()\n            .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n            .setModelId(MODEL_ID)\n            .setVersionId(MODEL_VERSION_ID) // This is optional. Defaults to the latest model version\n            .addInputs(\n                Input.newBuilder().setData(\n                    Data.newBuilder().setVideo(\n                        Video.newBuilder().setUrl(VIDEO_URL)\n                    )\n                )\n            )\n            .setModel(\n                Model.newBuilder().setOutputInfo(\n                    OutputInfo.newBuilder().setOutputConfig(\n                        OutputConfig.newBuilder().setSampleMs(SAMPLE_MS)\n                    )\n                )\n            )\n            .build()\n        );\n\n        if (postModelOutputsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("Post model outputs failed, status: " + postModelOutputsResponse.getStatus());\n        }\n\n        // Since we have one input, one output will exist here\n        Output output = postModelOutputsResponse.getOutputs(0);\n\n        // A separate prediction is available for each "frame"\n        for (Frame frame: output.getData().getFramesList()) {\n            System.out.println("Predicted concepts on frame " + frame.getFrameInfo().getTime() + ":");\n            for (Concept concept: frame.getData().getConceptsList()) {\n                System.out.printf("\\t%s %.2f%n", concept.getName(), concept.getValue());\n            }\n        }\n\n    }\n\n}',rn='package com.clarifai.example;\n\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.status.StatusCode;\nimport java.io.File;\nimport java.io.IOException;\nimport java.nio.file.Files;\nimport com.google.protobuf.ByteString;\n\npublic class ClarifaiExample {\n\n    ////////////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, location of the video\n    // we want as an input, and sample_ms. Change these strings to run your own example.\n    ////////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n    //Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    static final String USER_ID = "clarifai";    \n    static final String APP_ID = "main";\n    // Change these to whatever model and video input you want to use\n    static final String MODEL_ID = "general-image-recognition";\n    static final String MODEL_VERSION_ID = "aa7f35c01e0642fda5cf400f543e7c40";\n    static final String VIDEO_FILE_LOCATION = "YOUR_VIDEO_FILE_LOCATION_HERE";\n    // Change this to configure the FPS rate (If it\'s not configured, it defaults to 1 FPS)\n    static final int SAMPLE_MS = 500;\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\t\n\n    public static void main(String[] args) throws IOException {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n            .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        MultiOutputResponse postModelOutputsResponse = stub.postModelOutputs(\n            PostModelOutputsRequest.newBuilder()\n            .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n            .setModelId(MODEL_ID)\n            .setVersionId(MODEL_VERSION_ID) // This is optional. Defaults to the latest model version\n            .addInputs(\n                Input.newBuilder().setData(\n                    Data.newBuilder().setVideo(\n                        Video.newBuilder()\n                        .setBase64(ByteString.copyFrom(Files.readAllBytes(\n                            new File(VIDEO_FILE_LOCATION).toPath()\n                        )))\n                    )\n                )\n            )\n            .setModel(\n                Model.newBuilder().setOutputInfo(\n                    OutputInfo.newBuilder().setOutputConfig(\n                        OutputConfig.newBuilder().setSampleMs(SAMPLE_MS)\n                    )\n                )\n            )\n            .build()\n        );\n\n        if (postModelOutputsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("Post model outputs failed, status: " + postModelOutputsResponse.getStatus());\n        }\n\n        // Since we have one input, one output will exist here\n        Output output = postModelOutputsResponse.getOutputs(0);\n\n        // A separate prediction is available for each "frame"\n        for (Frame frame: output.getData().getFramesList()) {\n            System.out.println("Predicted concepts on frame " + frame.getFrameInfo().getTime() + ":");\n            for (Concept concept: frame.getData().getConceptsList()) {\n                System.out.printf("\\t%s %.2f%n", concept.getName(), concept.getValue());\n            }\n        }\n\n    }\n\n}',ln="<?php\n\nrequire __DIR__ . '/vendor/autoload.php';\n\n////////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, URL of the video\n// we want as an input, and sample_ms. Change these strings to run your own example.\n///////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account's Security section\n$PAT = 'YOUR_PAT_HERE';\n// Specify the correct user_id/app_id pairings\n// Since you're making inferences outside your app's scope\n$USER_ID = 'clarifai';\n$APP_ID = 'main';\n// Change these to whatever model and video URL you want to use\n$MODEL_ID = 'general-image-recognition';\n$MODEL_VERSION_ID = 'aa7f35c01e0642fda5cf400f543e7c40';\n$VIDEO_URL = 'https://samples.clarifai.com/beer.mp4';\n// Change this to configure the FPS rate (If it's not configured, it defaults to 1 FPS)\n$SAMPLE_MS = 500;\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Video;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\PostModelOutputsRequest;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\nuse Clarifai\\Api\\Model;\nuse Clarifai\\Api\\OutputInfo;\nuse Clarifai\\Api\\OutputConfig;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ['Authorization' => ['Key ' . $PAT ]];\n\n$userDataObject = new UserAppIDSet([\n    'user_id' => $USER_ID, \n    'app_id' => $APP_ID \n]);\n\n// Let's make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client->PostModelOutputs(\n    // The request object carries the request along with the request status and other metadata related to the request itself\n    new PostModelOutputsRequest([\n        'user_app_id' => $userDataObject,\n        'model_id' => $MODEL_ID,  \n        'version_id' => $MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n        'inputs' => [\n            new Input([ // The Input object wraps the Data object in order to meet the API specification                \n                'data' => new Data([ // The Data object is constructed around the Video object. It offers a container that has additional video independent\n                                    // metadata. In this particular use case, no other metadata is needed to be specified\n                    'video' => new Video([ // In the Clarifai platform, a video is defined by a special Video object\n                        'url' => $VIDEO_URL \n                    ])\n                ])\n            ])\n        ],\n        'model' => new Model([\n            'output_info' => new OutputInfo([\n                'output_config'=> new OutputConfig([\n                    'sample_ms'=> $SAMPLE_MS\n            ])\n            ])\n        ])\n    ]),\n    $metadata\n)->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception(\"Error: {$status->details}\");\n}\n\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure \n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    throw new Exception(\"Failure response: \" . $response->getStatus()->getDescription() . \" \" .\n        $response->getStatus()->getDetails());\n}\n\n// The output of a successful call can be used in many ways. In this example, we loop through all of the frames of the video and print out the predicted \n// concepts for each along with their numerical prediction value (confidence). \necho \"Predicted concepts: </br>\";\nforeach ($response->getOutputs()[0]->getData()->getFrames() as $frame) {\n    echo \"Predicted concepts on frame \" . $frame->getFrameInfo()->getTime() . \":\";\n    foreach ($frame->getData()->getConcepts() as $concept) {\n        echo \"   \" . $concept->getName() . \": \" . number_format($concept->getValue(), 2) . \"</br>\";\n    }\n}\n\n?>",cn="<?php\n\nrequire __DIR__ . '/vendor/autoload.php';\n\n//////////////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, location of the video\n// we want as an input, and sample_ms. Change these strings to run your own example.\n/////////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account's Security section\n$PAT = 'YOUR_PAT_HERE';\n// Specify the correct user_id/app_id pairings\n// Since you're making inferences outside your app's scope\n$USER_ID = 'clarifai';\n$APP_ID = 'main';\n// Change these to whatever model and video input you want to use\n$MODEL_ID = 'general-image-recognition';\n$MODEL_VERSION_ID = 'aa7f35c01e0642fda5cf400f543e7c40';\n$VIDEO_FILE_LOCATION = 'YOUR_VIDEO_FILE_LOCATION_HERE';\n// Change this to configure the FPS rate (If it's not configured, it defaults to 1 FPS)\n$SAMPLE_MS = 500;\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Video;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\PostModelOutputsRequest;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\nuse Clarifai\\Api\\Model;\nuse Clarifai\\Api\\OutputInfo;\nuse Clarifai\\Api\\OutputConfig;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ['Authorization' => ['Key ' . $PAT ]];\n\n$userDataObject = new UserAppIDSet([\n    'user_id' => $USER_ID, \n    'app_id' => $APP_ID \n]);\n\n$videoData = file_get_contents($VIDEO_FILE_LOCATION); // Get the video bytes data from the location\n\n// Let's make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client->PostModelOutputs(\n    // The request object carries the request along with the request status and other metadata related to the request itself\n    new PostModelOutputsRequest([\n        'user_app_id' => $userDataObject,\n        'model_id' => $MODEL_ID,  \n        'version_id' => $MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n        'inputs' => [\n            new Input([ // The Input object wraps the Data object in order to meet the API specification                \n                'data' => new Data([ // The Data object is constructed around the Video object. It offers a container that has additional video independent\n                                    // metadata. In this particular use case, no other metadata is needed to be specified\n                    'video' => new Video([ // In the Clarifai platform, a video is defined by a special Video object\n                        'base64' => $videoData \n                    ])\n                ])\n            ])\n        ],\n        'model' => new Model([\n            'output_info' => new OutputInfo([\n                'output_config'=> new OutputConfig([\n                    'sample_ms'=> $SAMPLE_MS\n            ])\n            ])\n        ])\n    ]),\n    $metadata\n)->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception(\"Error: {$status->details}\");\n}\n\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure \n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    throw new Exception(\"Failure response: \" . $response->getStatus()->getDescription() . \" \" .\n        $response->getStatus()->getDetails());\n}\n\n// The output of a successful call can be used in many ways. In this example, we loop through all of the frames of the video and print out the predicted \n// concepts for each along with their numerical prediction value (confidence). \necho \"Predicted concepts: </br>\";\nforeach ($response->getOutputs()[0]->getData()->getFrames() as $frame) {\n    echo \"Predicted concepts on frame \" . $frame->getFrameInfo()->getTime() . \":\";\n    foreach ($frame->getData()->getConcepts() as $concept) {\n        echo \"   \" . $concept->getName() . \": \" . number_format($concept->getValue(), 2) . \"</br>\";\n    }\n}\n\n?>",pn='# Model version ID is optional. It defaults to the latest model version, if omitted\n\ncurl -X POST "https://api.clarifai.com/v2/users/clarifai/apps/main/models/general-image-recognition/versions/aa7f35c01e0642fda5cf400f543e7c40/outputs" \\\n    -H "Authorization: Key YOUR_PAT_HERE" \\\n    -H "Content-Type: application/json" \\\n    -d \'{\n      "inputs": [\n        {\n          "data": {\n            "video": {\n              "url": "https://samples.clarifai.com/beer.mp4"\n            }\n          }\n        }\n      ]\n    }\'\n   ',un='# Model version ID is optional. It defaults to the latest model version, if omitted\n\ncurl -X POST "https://api.clarifai.com/v2/users/clarifai/apps/main/models/general-image-recognition/versions/aa7f35c01e0642fda5cf400f543e7c40/outputs" \\\n    -H "Authorization: Key YOUR_PAT_HERE" \\\n    -H "Content-Type: application/json" \\\n    -d \'{\n      "inputs": [\n        {\n          "data": {\n            "video": {\n              "base64": "YOUR_BYTES_STRING_HERE"\n            }\n          }\n        }\n      ]\n    }\'\n   ',dn="######################################################################################################\n# In this section, we set the user authentication, user and app ID, model details, and the URL of \n# the text we want as an input. Change these strings to run your own example.\n######################################################################################################\n\n# Your PAT (Personal Access Token) can be found in the Account's Security section\nPAT = 'YOUR_PAT_HERE'\n# Specify the correct user_id/app_id pairings\n# Since you're making inferences outside your app's scope\nUSER_ID = 'nlptownres'\nAPP_ID = 'text-classification'\n# Change these to whatever model and text URL you want to use\nMODEL_ID = 'multilingual-uncased-sentiment'\nMODEL_VERSION_ID = '29d5fef0229a4936a607380d7ef775dd'\nTEXT_FILE_URL = 'https://samples.clarifai.com/negative_sentence_12.txt'\n\n############################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n############################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (('authorization', 'Key ' + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\npost_model_outputs_response = stub.PostModelOutputs(\n    service_pb2.PostModelOutputsRequest(\n        user_app_id=userDataObject,  # The userDataObject is created in the overview and is required when using a PAT\n        model_id=MODEL_ID,\n        version_id=MODEL_VERSION_ID,  # This is optional. Defaults to the latest model version\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    text=resources_pb2.Text(\n                        url=TEXT_FILE_URL\n                    )\n                )\n            )\n        ]\n    ),\n    metadata=metadata\n)\nif post_model_outputs_response.status.code != status_code_pb2.SUCCESS:\n    print(post_model_outputs_response.status)\n    raise Exception(\"Post model outputs failed, status: \" + post_model_outputs_response.status.description)\n\n# Since we have one input, one output will exist here\noutput = post_model_outputs_response.outputs[0]\n\nprint(\"Predicted concepts:\")\nfor concept in output.data.concepts:\n    print(\"%s %.2f\" % (concept.name, concept.value))\n\n# Uncomment this line to print the raw output\n#print(output)",hn="#######################################################################################################\n# In this section, we set the user authentication, user and app ID, model details, and the location\n# of the text we want as an input. Change these strings to run your own example.    \n#######################################################################################################\n\n# Your PAT (Personal Access Token) can be found in the Account's Security section\nPAT = 'YOUR_PAT_HERE'\n# Specify the correct user_id/app_id pairings\n# Since you're making inferences outside your app's scope\nUSER_ID = 'nlptownres'\nAPP_ID = 'text-classification'\n# Change these to whatever model and text input you want to use\nMODEL_ID = 'multilingual-uncased-sentiment'\nMODEL_VERSION_ID = '29d5fef0229a4936a607380d7ef775dd'\nTEXT_FILE_LOCATION = 'YOUR_TEXT_FILE_LOCATION_HERE'\n\n############################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n############################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (('authorization', 'Key ' + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\nwith open(TEXT_FILE_LOCATION, \"rb\") as f:\n    file_bytes = f.read()\n\npost_model_outputs_response = stub.PostModelOutputs(\n    service_pb2.PostModelOutputsRequest(\n        user_app_id=userDataObject,  # The userDataObject is created in the overview and is required when using a PAT\n        model_id=MODEL_ID,\n        version_id=MODEL_VERSION_ID,  # This is optional. Defaults to the latest model version\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    text=resources_pb2.Text(\n                        raw=file_bytes\n                    )\n                )\n            )\n        ]\n    ),\n    metadata=metadata\n)\nif post_model_outputs_response.status.code != status_code_pb2.SUCCESS:\n    print(post_model_outputs_response.status)\n    raise Exception(\"Post model outputs failed, status: \" + post_model_outputs_response.status.description)\n\n# Since we have one input, one output will exist here.\noutput = post_model_outputs_response.outputs[0]\n\nprint(\"Predicted concepts:\")\nfor concept in output.data.concepts:\n    print(\"%s %.2f\" % (concept.name, concept.value))\n\n# Uncomment this line to print the raw output\n#print(output)",mn="#########################################################################################################\n# In this section, we set the user authentication, user and app ID, model details, and the raw text\n# we want as an input. Change these strings to run your own example.\n########################################################################################################\n\n# Your PAT (Personal Access Token) can be found in the Account's Security section\nPAT = 'YOUR_PAT_HERE'\n# Specify the correct user_id/app_id pairings\n# Since you're making inferences outside your app's scope\nUSER_ID = 'nlptownres'\nAPP_ID = 'text-classification'\n# Change these to whatever model and raw text you want to use\nMODEL_ID = 'multilingual-uncased-sentiment'\nMODEL_VERSION_ID = '29d5fef0229a4936a607380d7ef775dd'\nRAW_TEXT = 'I love your product very much'\n\n############################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n############################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (('authorization', 'Key ' + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\npost_model_outputs_response = stub.PostModelOutputs(\n    service_pb2.PostModelOutputsRequest(\n        user_app_id=userDataObject,  # The userDataObject is created in the overview and is required when using a PAT\n        model_id=MODEL_ID,\n        version_id=MODEL_VERSION_ID,  # This is optional. Defaults to the latest model version\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    text=resources_pb2.Text(\n                        raw=RAW_TEXT\n                    )\n                )\n            )\n        ]\n    ),\n    metadata=metadata\n)\nif post_model_outputs_response.status.code != status_code_pb2.SUCCESS:\n    print(post_model_outputs_response.status)\n    raise Exception(\"Post model outputs failed, status: \" + post_model_outputs_response.status.description)\n\n# Since we have one input, one output will exist here\noutput = post_model_outputs_response.outputs[0]\n\nprint(\"Predicted concepts:\")\nfor concept in output.data.concepts:\n    print(\"%s %.2f\" % (concept.name, concept.value))\n\n# Uncomment this line to print the raw output\n#print(output)",_n="#################################################################################################################\n# In this section, we set the user authentication, user and app ID, model details, and the prompt text we want\n# to provide as an input. Change these strings to run your own example.\n#################################################################################################################\n\n# Your PAT (Personal Access Token) can be found in the Account's Security section\nPAT = 'YOUR_PAT_HERE'\n# Specify the correct user_id/app_id pairings\n# Since you're making inferences outside your app's scope\nUSER_ID = 'stability-ai'\nAPP_ID = 'stable-diffusion-2'\n# Change these to whatever model and text URL you want to use\nMODEL_ID = 'stable-diffusion-xl'\nMODEL_VERSION_ID = '0c919cc1edfc455dbc96207753f178d7'\nRAW_TEXT = 'A penguin watching the sunset.'\n# To use a hosted text file, assign the URL variable\n# TEXT_FILE_URL = 'https://samples.clarifai.com/negative_sentence_12.txt'\n# Or, to use a local text file, assign the location variable\n# TEXT_FILE_LOCATION = 'YOUR_TEXT_FILE_LOCATION_HERE'\n\n############################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n############################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (('authorization', 'Key ' + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\n# To use a local text file, uncomment the following lines\n# with open(TEXT_FILE_LOCATION, \"rb\") as f:\n#    file_bytes = f.read()\n\npost_model_outputs_response = stub.PostModelOutputs(\n    service_pb2.PostModelOutputsRequest(\n        user_app_id=userDataObject,  # The userDataObject is created in the overview and is required when using a PAT\n        model_id=MODEL_ID,\n        version_id=MODEL_VERSION_ID,  # This is optional. Defaults to the latest model version\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    text=resources_pb2.Text(\n                        raw=RAW_TEXT\n                        # url=TEXT_FILE_URL\n                        # raw=file_bytes\n                    )\n                )\n            )\n        ]\n    ),\n    metadata=metadata\n)\nif post_model_outputs_response.status.code != status_code_pb2.SUCCESS:\n    print(post_model_outputs_response.status)\n    raise Exception(\"Post model outputs failed, status: \" + post_model_outputs_response.status.description)\n\n# Since we have one input, one output will exist here\noutput = post_model_outputs_response.outputs[0].data.image.base64\n\nimage_filename = f\"gen-image.jpg\"\nwith open(image_filename, 'wb') as f:\n      f.write(output)\n",fn='#################################################################################################################\n# In this section, we set the user authentication, user and app ID, model details, and the text we want\n# to provide as an input. Change these strings to run your own example.\n#################################################################################################################\n\n# Your PAT (Personal Access Token) can be found in the Account\'s Security section\nPAT = "YOUR_PAT_HERE"\n# Specify the correct user_id/app_id pairings\n# Since you\'re making inferences outside your app\'s scope\nUSER_ID = "eleven-labs"\nAPP_ID = "audio-generation"\n# Change these to whatever model and text URL you want to use\nMODEL_ID = "speech-synthesis"\nMODEL_VERSION_ID = "f588d92c044d4487a38c8f3d7a3b0eb2"\nRAW_TEXT = "I love your product very much!"\n# To use a hosted text file, assign the URL variable\n# TEXT_FILE_URL = "https://samples.clarifai.com/negative_sentence_12.txt"\n# Or, to use a local text file, assign the location variable\n# TEXT_FILE_LOCATION = "YOUR_TEXT_FILE_LOCATION_HERE"\n\n############################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n############################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\nfrom google.protobuf.struct_pb2 import Struct\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nparams = Struct()\nparams.update(\n    {\n        "model_id": "eleven_multilingual_v1",\n        "voice_id": "pNInz6obpgDQGcFmaJgB",\n        "stability": 0.4,\n        "similarity_boost": 0.7,\n    }\n)\n\nmetadata = (("authorization", "Key " + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\n# To use a local text file, uncomment the following lines\n# with open(TEXT_FILE_LOCATION, "rb") as f:\n#    file_bytes = f.read()\n\npost_model_outputs_response = stub.PostModelOutputs(\n    service_pb2.PostModelOutputsRequest(\n        user_app_id=userDataObject,  # The userDataObject is created in the overview and is required when using a PAT\n        model_id=MODEL_ID,\n        version_id=MODEL_VERSION_ID,  # This is optional. Defaults to the latest model version\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    text=resources_pb2.Text(\n                        raw=RAW_TEXT\n                        # url=TEXT_FILE_URL\n                        # raw=file_bytes\n                    )\n                )\n            )\n        ],\n        model=resources_pb2.Model(\n            model_version=resources_pb2.ModelVersion(\n                output_info=resources_pb2.OutputInfo(params=params)\n            )\n        ),\n    ),\n    metadata=metadata,\n)\nif post_model_outputs_response.status.code != status_code_pb2.SUCCESS:\n    print(post_model_outputs_response.status)\n    raise Exception("Post model outputs failed, status: " + post_model_outputs_response.status.description )\n\n# Uncomment this line to print the full Response JSON\n# print(post_model_outputs_response)\n\n# Since we have one input, one output will exist here\noutput = post_model_outputs_response.outputs[0].data.audio.base64\n\naudio_filename = f"audio_file.wav"\n\nwith open(audio_filename, "wb") as f:\n    f.write(output)\n',gn="\x3c!--index.html file--\x3e\n\n<script>\n    ////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, and the URL\n    // of the text we want as an input. Change these strings to run your own example.\n    ///////////////////////////////////////////////////////////////////////////////////////////////////\n\n    // Your PAT (Personal Access Token) can be found in the Account's Security section\n    const PAT = 'YOUR_PAT_HERE';\n    // Specify the correct user_id/app_id pairings\n    // Since you're making inferences outside your app's scope\n    const USER_ID = 'nlptownres';    \n    const APP_ID = 'text-classification';\n    // Change these to whatever model and text URL you want to use\n    const MODEL_ID = 'multilingual-uncased-sentiment';\n    const MODEL_VERSION_ID = '29d5fef0229a4936a607380d7ef775dd';    \n    const TEXT_FILE_URL = 'https://samples.clarifai.com/negative_sentence_12.txt';\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    const raw = JSON.stringify({\n        \"user_app_id\": {\n            \"user_id\": USER_ID,\n            \"app_id\": APP_ID\n        },\n        \"inputs\": [\n            {\n                \"data\": {\n                    \"text\": {\n                        \"url\": TEXT_FILE_URL\n                    }\n                }\n            }\n        ]\n    });\n\n    const requestOptions = {\n        method: 'POST',\n        headers: {\n            'Accept': 'application/json',\n            'Authorization': 'Key ' + PAT\n        },\n        body: raw\n    };\n\n    // NOTE: MODEL_VERSION_ID is optional, you can also call prediction with the MODEL_ID only\n    // https://api.clarifai.com/v2/models/{YOUR_MODEL_ID}/outputs\n    // this will default to the latest version_id\n\n    fetch(\"https://api.clarifai.com/v2/models/\" + MODEL_ID + \"/versions/\" + MODEL_VERSION_ID + \"/outputs\", requestOptions)\n        .then(response => response.text())\n        .then(result => console.log(result))\n        .catch(error => console.log('error', error));\n<\/script>",In="\x3c!--index.html file--\x3e\n\n<script>\n    /////////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, and the location\n    // of the text we want as an input. Change these strings to run your own example.\n    /////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n    // Your PAT (Personal Access Token) can be found in the Account's Security section\n    const PAT = 'YOUR_PAT_HERE';\n    // Specify the correct user_id/app_id pairings\n    // Since you're making inferences outside your app's scope\n    const USER_ID = 'nlptownres';    \n    const APP_ID = 'text-classification';\n    // Change these to whatever model and text input you want to use\n    const MODEL_ID = 'multilingual-uncased-sentiment';\n    const MODEL_VERSION_ID = '29d5fef0229a4936a607380d7ef775dd';    \n    const TEXT_FILE_BYTES = 'YOUR_TEXT_FILE_BYTES_HERE';\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    const raw = JSON.stringify({\n        \"user_app_id\": {\n            \"user_id\": USER_ID,\n            \"app_id\": APP_ID\n        },\n        \"inputs\": [\n            {\n                \"data\": {\n                    \"text\": {\n                        \"raw\": TEXT_FILE_BYTES\n                    }\n                }\n            }\n        ]\n    });\n\n    const requestOptions = {\n        method: 'POST',\n        headers: {\n            'Accept': 'application/json',\n            'Authorization': 'Key ' + PAT\n        },\n        body: raw\n    };\n\n    // NOTE: MODEL_VERSION_ID is optional, you can also call prediction with the MODEL_ID only\n    // https://api.clarifai.com/v2/models/{YOUR_MODEL_ID}/outputs\n    // this will default to the latest version_id\n\n    fetch(\"https://api.clarifai.com/v2/models/\" + MODEL_ID + \"/versions/\" + MODEL_VERSION_ID + \"/outputs\", requestOptions)\n        .then(response => response.text())\n        .then(result => console.log(result))\n        .catch(error => console.log('error', error));\n<\/script>",An="\x3c!--index.html file--\x3e\n\n<script>\n    //////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, and the raw\n    // text we want as an input. Change these strings to run your own example.\n    /////////////////////////////////////////////////////////////////////////////////////////////////////\n\n    // Your PAT (Personal Access Token) can be found in the Account's Security section\n    const PAT = 'YOUR_PAT_HERE';\n    // Specify the correct user_id/app_id pairings\n    // Since you're making inferences outside your app's scope\n    const USER_ID = 'nlptownres';   \n    const APP_ID = 'text-classification';\n    // Change these to whatever model and raw text you want to use\n    const MODEL_ID = 'multilingual-uncased-sentiment';\n    const MODEL_VERSION_ID = '29d5fef0229a4936a607380d7ef775dd';    \n    const RAW_TEXT = 'I love your product very much';\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    const raw = JSON.stringify({\n        \"user_app_id\": {\n            \"user_id\": USER_ID,\n            \"app_id\": APP_ID\n        },\n        \"inputs\": [\n            {\n                \"data\": {\n                    \"text\": {\n                        \"raw\": RAW_TEXT\n                    }\n                }\n            }\n        ]\n    });\n\n    const requestOptions = {\n        method: 'POST',\n        headers: {\n            'Accept': 'application/json',\n            'Authorization': 'Key ' + PAT\n        },\n        body: raw\n    };\n\n    // NOTE: MODEL_VERSION_ID is optional, you can also call prediction with the MODEL_ID only\n    // https://api.clarifai.com/v2/models/{YOUR_MODEL_ID}/outputs\n    // this will default to the latest version_id\n\n    fetch(\"https://api.clarifai.com/v2/models/\" + MODEL_ID + \"/versions/\" + MODEL_VERSION_ID + \"/outputs\", requestOptions)\n        .then(response => response.text())\n        .then(result => console.log(result))\n        .catch(error => console.log('error', error));\n<\/script>",En="\x3c!--index.html file--\x3e\n\n<script>\n    /////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, and the prompt text we want\n    // to provide as an input. Change these strings to run your own example.\n    /////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n    // Your PAT (Personal Access Token) can be found in the Account's Security section\n    const PAT = 'YOUR_PAT_HERE';    \n    // Specify the correct user_id/app_id pairings\n    // Since you're making inferences outside your app's scope\n    const USER_ID = 'stability-ai';\n    const APP_ID = 'stable-diffusion-2';\n    // Change these to whatever model and text you want to use\n    const MODEL_ID = 'stable-diffusion-xl';\n    const MODEL_VERSION_ID = '0c919cc1edfc455dbc96207753f178d7';\n    const RAW_TEXT = 'A penguin watching the sunset.';\n    // To use a hosted text file, assign the URL variable\n    // const TEXT_FILE_URL = 'https://samples.clarifai.com/negative_sentence_12.txt'\n    \n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    const raw = JSON.stringify({  \n        \"inputs\": [\n        {\n            \"data\": {\n              \"text\": {\n                   \"raw\": RAW_TEXT\n                  // \"url\": TEXT_FILE_URL\n               }\n            }\n        }\n    ],\n         \n    });\n\n    const requestOptions = {\n        method: 'POST',\n        headers: {\n            'Accept': 'application/json',\n            'Authorization': 'Key ' + PAT\n        },\n        body: raw\n    };\n\n    fetch(`https://api.clarifai.com/v2/users/${USER_ID}/apps/${APP_ID}/models/${MODEL_ID}/versions/${MODEL_VERSION_ID}/outputs`, requestOptions)\n        .then(response => response.json())\n        .then(result => {\n                const imageBase64 = result.outputs[0].data.image.base64;\n                // Create an anchor element for downloading the image\n                const downloadLink = document.createElement('a');\n                downloadLink.href = `data:image/jpeg;base64,${imageBase64}`;\n                downloadLink.download = 'gen-image.jpg';\n                // Trigger a click event on the link to prompt the download\n                downloadLink.click();\n        })      \n        .catch(error => console.log('error', error));\n\n<\/script>",Dn="\x3c!--index.html file--\x3e\n\n<script>\n    //////////////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, and the text we want\n    // to provide as an input. Change these strings to run your own example.\n    //////////////////////////////////////////////////////////////////////////////////////////////////////////////\n  \n    // Your PAT (Personal Access Token) can be found in the Account's Security section\n    const PAT = 'YOUR_PAT_HERE';    \n    // Specify the correct user_id/app_id pairings\n    // Since you're making inferences outside your app's scope\n    const USER_ID = 'eleven-labs';\n    const APP_ID = 'audio-generation';\n    // Change these to whatever model and text you want to use\n    const MODEL_ID = 'speech-synthesis';\n    const MODEL_VERSION_ID = 'f588d92c044d4487a38c8f3d7a3b0eb2';\n    const RAW_TEXT = 'I love your product very much!';\n    // To use a hosted text file, assign the URL variable\n    // const TEXT_FILE_URL = 'https://samples.clarifai.com/negative_sentence_12.txt'\n    \n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n  \n    const raw = JSON.stringify({  \n        \"inputs\": [\n        {\n            \"data\": {\n              \"text\": {\n                   \"raw\": RAW_TEXT\n                  // \"url\": TEXT_FILE_URL\n               }\n            }\n        }\n    ],  \n      \"model\": {\n          \"model_version\": {\n              \"output_info\": {\n                  \"params\": {\n                      \"model_id\": \"eleven_multilingual_v1\",\n                      \"voice_id\": \"EXAVITQu4vr4xnSDxMaL\",\n                      \"similarity_boost\": 0,\n                      \"stability\": 0.5,\n                  }\n              }\n          }\n      }\n    });\n  \n    const requestOptions = {\n        method: 'POST',\n        headers: {\n            'Accept': 'application/json',\n            'Authorization': 'Key ' + PAT\n        },\n        body: raw\n    };\n  \n    fetch(`https://api.clarifai.com/v2/users/${USER_ID}/apps/${APP_ID}/models/${MODEL_ID}/versions/${MODEL_VERSION_ID}/outputs`, requestOptions)\n        .then(response => response.json())\n        .then(result => {\n                const audioBase64 = result.outputs[0].data.audio.base64;\n                // Create an anchor element for downloading the audio\n                const downloadLink = document.createElement('a');\n                downloadLink.href = `data:audio/wav;base64,${audioBase64}`;\n                downloadLink.download = 'audio_file.wav';\n                // Trigger a click event on the link to prompt the download\n                downloadLink.click();\n        })      \n        .catch(error => console.log('error', error));\n  \n  <\/script>",Tn='//index.js file\n\n////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, and the URL\n// of the text we want as an input. Change these strings to run your own example.\n/////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\nconst PAT = \'YOUR_PAT_HERE\';\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\nconst USER_ID = \'nlptownres\';\nconst APP_ID = \'text-classification\';\n// Change these to whatever model and text URL you want to use\nconst MODEL_ID = \'multilingual-uncased-sentiment\';\nconst MODEL_VERSION_ID = \'29d5fef0229a4936a607380d7ef775dd\';\nconst TEXT_URL = \'https://samples.clarifai.com/negative_sentence_12.txt\';\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\nstub.PostModelOutputs(\n    {\n        user_app_id: {\n            "user_id": USER_ID,\n            "app_id": APP_ID\n        },\n        model_id: MODEL_ID,\n        version_id: MODEL_VERSION_ID, // This is optional. Defaults to the latest model version.\n        inputs: [\n            { data: { text: { url: TEXT_URL, allow_duplicate_url: true } } }\n        ]\n    },\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            throw new Error("Post model outputs failed, status: " + response.status.description);\n        }\n\n        // Since we have one input, one output will exist here.\n        const output = response.outputs[0];\n\n        console.log("Predicted concepts:");\n        for (const concept of output.data.concepts) {\n            console.log(concept.name + " " + concept.value);\n        }\n    }\n\n);',On='//index.js file\n\n/////////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, and the location\n// of the text we want as an input. Change these strings to run your own example.\n/////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\nconst PAT = \'YOUR_PAT_HERE\';\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\nconst USER_ID = \'nlptownres\';\nconst APP_ID = \'text-classification\';\n// Change these to whatever model and text input you want to use\nconst MODEL_ID = \'multilingual-uncased-sentiment\';\nconst MODEL_VERSION_ID = \'29d5fef0229a4936a607380d7ef775dd\';\nconst TEXT_FILE_LOCATION = \'YOUR_TEXT_FILE_LOCATION_HERE\';\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\nconst fs = require("fs");\nconst fileBytes = fs.readFileSync(TEXT_FILE_LOCATION);\n\nstub.PostModelOutputs(\n    {\n        user_app_id: {\n            "user_id": USER_ID,\n            "app_id": APP_ID\n        },\n        model_id: MODEL_ID,\n        version_id: MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n        inputs: [\n            { data: { text: { raw: fileBytes } } }\n        ]\n    },\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            throw new Error("Post model outputs failed, status: " + response.status.description);\n        }\n\n        // Since we have one input, one output will exist here\n        const output = response.outputs[0];\n\n        console.log("Predicted concepts:");\n        for (const concept of output.data.concepts) {\n            console.log(concept.name + " " + concept.value);\n        }\n    }\n\n);',bn='//index.js file\n\n////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, and the raw\n// text we want as an input. Change these strings to run your own example.\n////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\nconst PAT = \'YOUR_PAT_HERE\';\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\nconst USER_ID = \'nlptownres\';\nconst APP_ID = \'text-classification\';\n// Change these to whatever model and raw text you want to use\nconst MODEL_ID = \'multilingual-uncased-sentiment\';\nconst MODEL_VERSION_ID = \'29d5fef0229a4936a607380d7ef775dd\';\nconst RAW_TEXT = \'I love your product very much\';\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\nstub.PostModelOutputs(\n    {\n        user_app_id: {\n            "user_id": USER_ID,\n            "app_id": APP_ID\n        },\n        model_id: MODEL_ID,\n        version_id: MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n        inputs: [\n            { data: { text: { raw: RAW_TEXT } } }\n        ]\n    },\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            throw new Error("Post model outputs failed, status: " + response.status.description);\n        }\n\n        // Since we have one input, one output will exist here\n        const output = response.outputs[0];\n\n        console.log("Predicted concepts:");\n        for (const concept of output.data.concepts) {\n            console.log(concept.name + " " + concept.value);\n        }\n    }\n\n);',yn="//index.js file\n\n//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, and the prompt text we want\n// to provide as an input. Change these strings to run your own example.\n//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account's Security section\nconst PAT = 'YOUR_PAT_HERE';    \n// Specify the correct user_id/app_id pairings\n// Since you're making inferences outside your app's scope\nconst USER_ID = 'stability-ai';\nconst APP_ID = 'stable-diffusion-2';\n// Change these to whatever model and text you want to use\nconst MODEL_ID = 'stable-diffusion-xl';\nconst MODEL_VERSION_ID = '0c919cc1edfc455dbc96207753f178d7';\nconst RAW_TEXT = 'A penguin watching the sunset.';\n// To use a hosted text file, assign the URL variable\n// const TEXT_FILE_URL = 'https://samples.clarifai.com/negative_sentence_12.txt'\n// Or, to use a local text file, assign the location variable\n// TEXT_FILE_LOCATION = 'YOUR_TEXT_FILE_LOCATION_HERE'\n\n/////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n/////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require(\"clarifai-nodejs-grpc\");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set(\"authorization\", \"Key \" + PAT);\n\n// To use a local text file, uncomment the following lines\n// const fs = require(\"fs\");\n// const fileBytes = fs.readFileSync(TEXT_FILE_LOCATION);\n\nstub.PostModelOutputs(\n    {\n        user_app_id: {\n            \"user_id\": USER_ID,\n            \"app_id\": APP_ID\n        },\n        model_id: MODEL_ID,\n        version_id: MODEL_VERSION_ID,  // This is optional. Defaults to the latest model version\n        inputs: [\n            {\n                \"data\": {\n                    \"text\": {\n                        \"raw\": RAW_TEXT\n                        // url: TEXT_FILE_URL\n                        // raw: fileBytes\n                    }\n                }\n            }\n        ],\n\n    },\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            throw new Error(\"Post models failed, status: \" + response.status.description);\n        }\n        // Since we have one input, one output will exist here.\n        const output = response.outputs[0].data.image.base64;\n\n        const fs = require('fs');\n        const imageFilename = 'gen-image.jpg';\n        fs.writeFileSync(imageFilename, Buffer.from(output, 'base64'));\n        console.log(`Image saved as ${imageFilename}`);\n\n    }\n);",wn='//index.js file\n\n////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, and the text we want\n// to provide as an input. Change these strings to run your own example.\n///////////////////////////////////////////////////////////////////////////////////////////////////////////////// \n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\nconst PAT = "YOUR_PAT_HERE";\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\nconst USER_ID = "eleven-labs";\nconst APP_ID = "audio-generation";\n// Change these to whatever model and text you want to use\nconst MODEL_ID = "speech-synthesis";\nconst MODEL_VERSION_ID = "f588d92c044d4487a38c8f3d7a3b0eb2";\nconst RAW_TEXT = "I love your product very much!";\n// To use a hosted text file, assign the URL variable\n// const TEXT_FILE_URL = "https://samples.clarifai.com/negative_sentence_12.txt"\n// Or, to use a local text file, assign the location variable\n// TEXT_FILE_LOCATION = "YOUR_TEXT_FILE_LOCATION_HERE"\n\n/////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n/////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\n// To use a local text file, uncomment the following lines\n// const fs = require("fs");\n// const fileBytes = fs.readFileSync(TEXT_FILE_LOCATION);\n\nstub.PostModelOutputs(\n    {\n        user_app_id: {\n            "user_id": USER_ID,\n            "app_id": APP_ID\n        },\n        model_id: MODEL_ID,\n        version_id: MODEL_VERSION_ID,  // This is optional. Defaults to the latest model version\n        inputs: [\n            {\n                "data": {\n                    "text": {\n                        "raw": RAW_TEXT\n                        // url: TEXT_FILE_URL\n                        // raw: fileBytes\n                    }\n                }\n            }\n        ],\n        "model": {\n            "model_version": {\n                "output_info": {\n                    "params": {\n                        "model_id": "eleven_multilingual_v1",\n                        "voice_id": "EXAVITQu4vr4xnSDxMaL",\n                        "similarity_boost": 0,\n                        "stability": 0.5,\n                    }\n                }\n            }\n        }\n    },\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            throw new Error("Post models failed, status: " + response.status.description);\n        }\n        // Since we have one input, one output will exist here.\n        const output = response.outputs[0].data.audio.base64;\n\n        const fs = require("fs");\n        const audioFilename = "audio_file.wav";\n        fs.writeFileSync(audioFilename, Buffer.from(output, \'base64\'));\n        console.log(`Image saved as ${audioFilename}`);\n\n    }\n);',Sn='package com.clarifai.example;\n\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.status.StatusCode;\n\npublic class ClarifaiExample {\n\n    ///////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, and the URL\n    // of the text we want as an input. Change these strings to run your own example.\n    //////////////////////////////////////////////////////////////////////////////////////////////////\n\n    //Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    static final String USER_ID = "nlptownres";    \n    static final String APP_ID = "text-classification";\n    // Change these to whatever model and text URL you want to use\n    static final String MODEL_ID = "multilingual-uncased-sentiment"; \n    static final String MODEL_VERSION_ID = "29d5fef0229a4936a607380d7ef775dd";   \n    static final String TEXT_URL = "https://samples.clarifai.com/negative_sentence_12.txt";     \n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\t\n\n    public static void main(String[] args) {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n            .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        MultiOutputResponse postModelOutputsResponse = stub.postModelOutputs(\n            PostModelOutputsRequest.newBuilder()\n            .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n            .setModelId(MODEL_ID)\n            .setVersionId(MODEL_VERSION_ID) // This is optional. Defaults to the latest model version\n            .addInputs(\n                Input.newBuilder().setData(\n                    Data.newBuilder().setText(\n                        Text.newBuilder().setUrl(TEXT_URL)\n                    )\n                )\n            )\n            .build()\n        );\n\n        if (postModelOutputsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("Post model outputs failed, status: " + postModelOutputsResponse.getStatus());\n        }\n\n        // Since we have one input, one output will exist here\n        Output output = postModelOutputsResponse.getOutputs(0);\n\n        System.out.println("Predicted concepts:");\n        for (Concept concept: output.getData().getConceptsList()) {\n            System.out.printf("%s %.2f%n", concept.getName(), concept.getValue());\n        }\n\n    }\n\n}',Rn='package com.clarifai.example;\n\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.status.StatusCode;\nimport com.google.protobuf.ByteString;\nimport java.io.File;\nimport java.io.IOException;\nimport java.nio.file.Files;\n\npublic class ClarifaiExample {\n\n    /////////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, and the location\n    // of the text we want as an input. Change these strings to run your own example.\n    /////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n    //Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    static final String USER_ID = "nlptownres";    \n    static final String APP_ID = "text-classification";\n    // Change these to whatever model and text input you want to use\n    static final String MODEL_ID = "multilingual-uncased-sentiment"; \n     static final String MODEL_VERSION_ID = "29d5fef0229a4936a607380d7ef775dd";   \n    static final String TEXT_FILE_LOCATION = "YOUR_TEXT_FILE_LOCATION_HERE";\n   \n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\t\n\n    public static void main(String[] args) throws IOException {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n            .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        MultiOutputResponse postModelOutputsResponse = stub.postModelOutputs(\n            PostModelOutputsRequest.newBuilder()\n            .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n            .setModelId(MODEL_ID)\n            .setVersionId(MODEL_VERSION_ID) // This is optional. Defaults to the latest model version\n            .addInputs(\n                Input.newBuilder().setData(\n                    Data.newBuilder().setText(\n                        Text.newBuilder()\n                        .setRawBytes(ByteString.copyFrom(Files.readAllBytes(\n                            new File(TEXT_FILE_LOCATION).toPath()\n                        )))\n                    )\n                )\n            )\n            .build()\n        );\n\n        if (postModelOutputsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("Post model outputs failed, status: " + postModelOutputsResponse.getStatus());\n        }\n\n        // Since we have one input, one output will exist here\n        Output output = postModelOutputsResponse.getOutputs(0);\n\n        System.out.println("Predicted concepts:");\n        for (Concept concept: output.getData().getConceptsList()) {\n            System.out.printf("%s %.2f%n", concept.getName(), concept.getValue());\n        }\n\n    }\n\n}',Pn='package com.clarifai.example;\n\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.status.StatusCode;\n\npublic class ClarifaiExample {\n\n    ////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, and the raw\n    // text we want as an input. Change these strings to run your own example.\n    ///////////////////////////////////////////////////////////////////////////////////////////////////\n\n    //Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    static final String USER_ID = "nlptownres";   \n    static final String APP_ID = "text-classification";\n    // Change these to whatever model and raw text you want to use\n    static final String MODEL_ID = "multilingual-uncased-sentiment";\n    static final String MODEL_VERSION_ID = "29d5fef0229a4936a607380d7ef775dd";    \n    static final String RAW_TEXT = "I love your product very much";    \n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\t\n\n    public static void main(String[] args) {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n            .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        MultiOutputResponse postModelOutputsResponse = stub.postModelOutputs(\n            PostModelOutputsRequest.newBuilder()\n            .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n            .setModelId(MODEL_ID)\n            .setVersionId(MODEL_VERSION_ID) // This is optional. Defaults to the latest model version\n            .addInputs(\n                Input.newBuilder().setData(\n                    Data.newBuilder().setText(\n                        Text.newBuilder().setRaw(RAW_TEXT)\n                    )\n                )\n            )\n            .build()\n        );\n\n        if (postModelOutputsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("Post model outputs failed, status: " + postModelOutputsResponse.getStatus());\n        }\n\n        // Since we have one input, one output will exist here\n        Output output = postModelOutputsResponse.getOutputs(0);\n\n        System.out.println("Predicted concepts:");\n        for (Concept concept: output.getData().getConceptsList()) {\n            System.out.printf("%s %.2f%n", concept.getName(), concept.getValue());\n        }\n\n    }\n\n}',vn='package com.clarifai.example;\n\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.status.StatusCode;\nimport com.google.protobuf.ByteString;\nimport java.io.FileOutputStream;\nimport java.io.IOException;\n// import java.io.File; //  Uncomment to use a local text file\n// import java.nio.file.Files; //  Uncomment to use a local text file\n\npublic class ClarifaiExample {\n\n    ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, and the prompt text we want\n    // to provide as an input. Change these strings to run your own example.\n    //////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n    // Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    static final String USER_ID = "stability-ai";\n    static final String APP_ID = "stable-diffusion-2";\n    // Change these to whatever model you want to use\n    static final String MODEL_ID = "stable-diffusion-xl";\n    static final String MODEL_VERSION_ID = "0c919cc1edfc455dbc96207753f178d7";\n    static final String RAW_TEXT = "A penguin watching the sunset.";\n    // To use a hosted text file, assign the URL variable\n    // static final String TEXT_FILE_URL = "https://samples.clarifai.com/negative_sentence_12.txt";\n    // Or, to use a local text file, assign the location variable\n    // static final String TEXT_FILE_LOCATION = "YOUR_TEXT_FILE_LOCATION_HERE";\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\t\n    public static void main(String[] args) {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n                .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        MultiOutputResponse postModelOutputsResponse = stub.postModelOutputs(\n                PostModelOutputsRequest.newBuilder()\n                        .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n                        .setModelId(MODEL_ID)\n                        .setVersionId(MODEL_VERSION_ID) // This is optional. Defaults to the latest model version.\n                        .addInputs(\n                                Input.newBuilder().setData(\n                                        Data.newBuilder().setText(\n                                                Text.newBuilder().setRaw(RAW_TEXT)\n                                        // Text.newBuilder().setUrl(TEXT_FILE_URL)\n                                        // Text.newBuilder().setRawBytes(ByteString.copyFrom(Files.readAllBytes(\n                                        // new File(TEXT_FILE_LOCATION).toPath()\n                                        // )))\n                                        )\n                                )\n                        )\n                        .build()\n        );\n\n        if (postModelOutputsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("Post model outputs failed, status: " + postModelOutputsResponse.getStatus());\n        }\n\n        // Extract the base64-encoded image data\n        ByteString imageData = postModelOutputsResponse.getOutputs(0).getData().getImage().getBase64();\n\n        // Save the image to a file\n        try {\n            FileOutputStream outputStream = new FileOutputStream("gen-image.jpg");\n            outputStream.write(imageData.toByteArray());\n            outputStream.close();\n        } catch (IOException e) {\n            System.err.println("Error writing image to file: " + e.getMessage());\n            System.exit(1);\n        }\n\n    }\n}\n',xn='package com.clarifai.example;\n\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.status.StatusCode;\nimport java.io.FileOutputStream;\nimport com.google.protobuf.Struct;\nimport com.google.protobuf.Value;\n\nimport com.google.protobuf.ByteString;\nimport java.io.File;\nimport java.io.IOException;\nimport java.nio.file.Files;\n\npublic class ClarifaiExample {\n\n    ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, and the text we want\n    // to provide as an input. Change these strings to run your own example.\n    //////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n    // Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    static final String USER_ID = "eleven-labs";\n    static final String APP_ID = "audio-generation";\n    // Change these to whatever model you want to use\n    static final String MODEL_ID = "speech-synthesis";\n    static final String MODEL_VERSION_ID = "f588d92c044d4487a38c8f3d7a3b0eb2";\n    static final String RAW_TEXT = "I love your product very much!";\n    // To use a hosted text file, assign the URL variable\n    // static final String TEXT_FILE_URL = "https://samples.clarifai.com/negative_sentence_12.txt";\n    // Or, to use a local text file, assign the location variable\n    // static final String TEXT_FILE_LOCATION = "YOUR_TEXT_FILE_LOCATION_HERE";\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\t\n    \n    public static void main(String[] args) throws IOException {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n                .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        Struct.Builder params = Struct.newBuilder()\n                .putFields("model_id", Value.newBuilder().setStringValue("eleven_multilingual_v1").build())\n                .putFields("voice_id", Value.newBuilder().setStringValue("pNInz6obpgDQGcFmaJgB").build())\n                .putFields("stability", Value.newBuilder().setNumberValue(0.4).build())\n                .putFields("similarity_boost", Value.newBuilder().setNumberValue(.7).build());\n\n        MultiOutputResponse postModelOutputsResponse = stub.postModelOutputs(\n                PostModelOutputsRequest.newBuilder()\n                        .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n                        .setModelId(MODEL_ID)\n                        .setVersionId(MODEL_VERSION_ID) // This is optional. Defaults to the latest model version.\n                        .addInputs(\n                                Input.newBuilder().setData(\n                                        Data.newBuilder().setText(\n                                                Text.newBuilder().setRaw(RAW_TEXT)\n                                        // Text.newBuilder().setUrl(TEXT_FILE_URL)\n                                        // Text.newBuilder().setRawBytes(ByteString.copyFrom(Files.readAllBytes(\n                                        // new File(TEXT_FILE_LOCATION).toPath()\n                                        // )))\n                                        )\n                                )\n                        )\n                        .setModel(Model.newBuilder()\n                                .setModelVersion(ModelVersion.newBuilder()\n                                        .setOutputInfo(OutputInfo.newBuilder().setParams(params))\n                                )\n                        )\n                        .build()\n        );\n\n        if (postModelOutputsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("Post model outputs failed, status: " + postModelOutputsResponse.getStatus());\n        }\n\n        // Extract the base64-encoded audio data\n        ByteString audioData = postModelOutputsResponse.getOutputs(0).getData().getAudio().getBase64();\n\n        // Save the audio to a file\n        try {\n            FileOutputStream outputStream = new FileOutputStream("audio_file.wav");\n            outputStream.write(audioData.toByteArray());\n            outputStream.close();\n        } catch (IOException e) {\n            System.err.println("Error writing image to file: " + e.getMessage());\n            System.exit(1);\n        }\n\n    }\n}\n',Cn="<?php\n\nrequire __DIR__ . '/vendor/autoload.php';\n\n////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, and the URL\n// of the text we want as an input. Change these strings to run your own example.\n////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account's Security section\n$PAT = 'YOUR_PAT_HERE';\n// Specify the correct user_id/app_id pairings\n// Since you're making inferences outside your app's scope\n$USER_ID = 'nlptownres';\n$APP_ID = 'text-classification';\n// Change these to whatever model and text URL you want to use\n$MODEL_ID = 'multilingual-uncased-sentiment';\n$MODEL_VERSION_ID = '29d5fef0229a4936a607380d7ef775dd';\n$TEXT_URL = 'https://samples.clarifai.com/negative_sentence_12.txt';\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Text;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\PostModelOutputsRequest;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ['Authorization' => ['Key ' . $PAT ]];\n\n$userDataObject = new UserAppIDSet([\n    'user_id' => $USER_ID, \n    'app_id' => $APP_ID \n]);\n\n// Let's make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client->PostModelOutputs(\n    // The request object carries the request along with the request status and other metadata related to the request itself\n    new PostModelOutputsRequest([\n        'user_app_id' => $userDataObject,\n        'model_id' => $MODEL_ID,  \n        'version_id' => $MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n        'inputs' => [\n            new Input([ // The Input object wraps the Data object in order to meet the API specification                \n                'data' => new Data([ // The Data object is constructed around the Text object. It offers a container that has additional text independent\n                                    // metadata. In this particular use case, no other metadata is needed to be specified\n                    'text' => new Text([ // In the Clarifai platform, a text is defined by a special Text object\n                        'url' => $TEXT_URL \n                    ])\n                ])\n            ])\n        ]        \n    ]),\n    $metadata\n)->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception(\"Error: {$status->details}\");\n}\n\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure \n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    throw new Exception(\"Failure response: \" . $response->getStatus()->getDescription() . \" \" .\n        $response->getStatus()->getDetails());\n}\n\n// The output of a successful call can be used in many ways. In this example, we loop through all of the predicted concepts and print them out along with\n// their numerical prediction value (confidence)\necho \"Predicted concepts: </br>\";\nforeach ($response->getOutputs()[0]->getData()->getConcepts() as $concept) {\n    echo $concept->getName() . \": \" . number_format($concept->getValue(), 2) . \"</br>\";\n}\n\n?>",Ln="<?php\n\nrequire __DIR__ . '/vendor/autoload.php';\n\n/////////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, and the location\n// of the text we want as an input. Change these strings to run your own example.\n////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account's Security section\n$PAT = 'YOUR_PAT_HERE';\n// Specify the correct user_id/app_id pairings\n// Since you're making inferences outside your app's scope\n$USER_ID = 'nlptownres';\n$APP_ID = 'text-classification';\n// Change these to whatever model and text URL you want to use\n$MODEL_ID = 'multilingual-uncased-sentiment';\n$MODEL_VERSION_ID = '29d5fef0229a4936a607380d7ef775dd';\n$TEXT_FILE_LOCATION = 'YOUR_TEXT_FILE_LOCATION_HERE';\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Text;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\PostModelOutputsRequest;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ['Authorization' => ['Key ' . $PAT ]];\n\n$userDataObject = new UserAppIDSet([\n    'user_id' => $USER_ID, \n    'app_id' => $APP_ID \n]);\n\n$textData = file_get_contents($TEXT_FILE_LOCATION); // Get the text bytes data from the location\n\n// Let's make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client->PostModelOutputs(\n    // The request object carries the request along with the request status and other metadata related to the request itself\n    new PostModelOutputsRequest([\n        'user_app_id' => $userDataObject,\n        'model_id' => $MODEL_ID,  \n        'version_id' => $MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n        'inputs' => [\n            new Input([ // The Input object wraps the Data object in order to meet the API specification                \n                'data' => new Data([ // The Data object is constructed around the Text object. It offers a container that has additional text independent\n                                    // metadata. In this particular use case, no other metadata is needed to be specified\n                    'text' => new Text([ // In the Clarifai platform, a text is defined by a special Text object\n                        'raw' => $textData \n                    ])\n                ])\n            ])\n        ]        \n    ]),\n    $metadata\n)->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception(\"Error: {$status->details}\");\n}\n\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure \n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    throw new Exception(\"Failure response: \" . $response->getStatus()->getDescription() . \" \" .\n        $response->getStatus()->getDetails());\n}\n\n// The output of a successful call can be used in many ways. In this example, we loop through all of the predicted concepts and print them out along with\n// their numerical prediction value (confidence)\necho \"Predicted concepts: </br>\";\nforeach ($response->getOutputs()[0]->getData()->getConcepts() as $concept) {\n    echo $concept->getName() . \": \" . number_format($concept->getValue(), 2) . \"</br>\";\n}\n\n?>",Un="<?php\n\nrequire __DIR__ . '/vendor/autoload.php';\n\n/////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, and the raw\n// text we want as an input. Change these strings to run your own example.\n////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account's Security section\n$PAT = 'YOUR_PAT_HERE';\n// Specify the correct user_id/app_id pairings\n// Since you're making inferences outside your app's scope\n$USER_ID = 'nlptownres';\n$APP_ID = 'text-classification';\n// Change these to whatever model and raw text you want to use\n$MODEL_ID = 'multilingual-uncased-sentiment';\n$MODEL_VERSION_ID = '29d5fef0229a4936a607380d7ef775dd';\n$RAW_TEXT = 'I love your product very much';\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Text;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\PostModelOutputsRequest;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ['Authorization' => ['Key ' . $PAT ]];\n\n$userDataObject = new UserAppIDSet([\n    'user_id' => $USER_ID, \n    'app_id' => $APP_ID \n]);\n\n// Let's make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client->PostModelOutputs(\n    // The request object carries the request along with the request status and other metadata related to the request itself\n    new PostModelOutputsRequest([\n        'user_app_id' => $userDataObject,\n        'model_id' => $MODEL_ID,  \n        'version_id' => $MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n        'inputs' => [\n            new Input([ // The Input object wraps the Data object in order to meet the API specification                \n                'data' => new Data([ // The Data object is constructed around the Text object. It offers a container that has additional text independent\n                                    // metadata. In this particular use case, no other metadata is needed to be specified\n                    'text' => new Text([ // In the Clarifai platform, a text is defined by a special Text object\n                        'raw' => $RAW_TEXT \n                    ])\n                ])\n            ])\n        ]        \n    ]),\n    $metadata\n)->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception(\"Error: {$status->details}\");\n}\n\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure \n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    throw new Exception(\"Failure response: \" . $response->getStatus()->getDescription() . \" \" .\n        $response->getStatus()->getDetails());\n}\n\n// The output of a successful call can be used in many ways. In this example, we loop through all of the predicted concepts and print them out along with\n// their numerical prediction value (confidence)\necho \"Predicted concepts: </br>\";\nforeach ($response->getOutputs()[0]->getData()->getConcepts() as $concept) {\n    echo $concept->getName() . \": \" . number_format($concept->getValue(), 2) . \"</br>\";\n}\n\n?>",Nn="<?php\n\nrequire __DIR__ . '/vendor/autoload.php';\n\n////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, and the prompt text we want\n// to provide as an input. Change these strings to run your own example.\n//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account's Security section\n$PAT = 'YOUR_PAT_HERE';\n// Specify the correct user_id/app_id pairings\n// Since you're making inferences outside your app's scope\n$USER_ID = 'stability-ai';\n$APP_ID = 'stable-diffusion-2';\n// Change these to whatever model and image URL you want to use\n$MODEL_ID = 'stable-diffusion-xl';\n$MODEL_VERSION_ID = '0c919cc1edfc455dbc96207753f178d7';\n$RAW_TEXT = 'A penguin watching the sunset.';\n# To use a hosted text file, assign the URL variable\n# $TEXT_FILE_URL = 'https://samples.clarifai.com/negative_sentence_12.txt';\n# Or, to use a local text file, assign the location variable\n# $TEXT_FILE_LOCATION = 'YOUR_TEXT_FILE_LOCATION_HERE';\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Text;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\PostModelOutputsRequest;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ['Authorization' => ['Key ' . $PAT]];\n\n$userDataObject = new UserAppIDSet([\n    'user_id' => $USER_ID,\n    'app_id' => $APP_ID\n]);\n\n//$textData = file_get_contents($TEXT_FILE_LOCATION); // Get the text bytes data from the location\n\n// Let's make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client->PostModelOutputs(\n    // The request object carries the request along with the request status and other metadata related to the request itself\n    new PostModelOutputsRequest([\n        'user_app_id' => $userDataObject,\n        'model_id' => $MODEL_ID,\n        'version_id' => $MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n        'inputs' => [\n            new Input([ // The Input object wraps the Data object in order to meet the API specification                \n                'data' => new Data([ // The Data object is constructed around the Text object. It offers a container that has additional text independent\n                    // metadata. In this particular use case, no other metadata is needed to be specified\n                    'text' => new Text([ // In the Clarifai platform, a text is defined by a special Text object\n                        'raw' => $RAW_TEXT\n                        // 'url' => $TEXT_FILE_URL \n                        // 'raw' => $textData \n                    ])\n                ])\n            ])\n        ]\n    ]),\n    $metadata\n)->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception(\"Error: {$status->details}\");\n}\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure \n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    throw new Exception(\"Failure response: \" . $response->getStatus()->getDescription() . \" \" .\n        $response->getStatus()->getDetails());\n}\n\n// Save the output image\n$output = $response->getOutputs()[0]->getData()->getImage()->getBase64();\n\n$imageFilename = \"gen-image.jpg\";\nfile_put_contents($imageFilename, base64_decode($output));\n\n?>",Mn='<?php\n\nrequire __DIR__ . "/vendor/autoload.php";\n\n//////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, and the text we want\n// to provide as an input. Change these strings to run your own example.\n//////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\n$PAT = "YOUR_PAT_HERE";\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\n$USER_ID = "eleven-labs";\n$APP_ID = "audio-generation";\n// Change these to whatever model and image URL you want to use\n$MODEL_ID = "speech-synthesis";\n$MODEL_VERSION_ID = "f588d92c044d4487a38c8f3d7a3b0eb2";\n$RAW_TEXT = "I love your product very much!";\n# To use a hosted text file, assign the URL variable\n# $TEXT_FILE_URL = "https://samples.clarifai.com/negative_sentence_12.txt";\n# Or, to use a local text file, assign the location variable\n# $TEXT_FILE_LOCATION = "YOUR_TEXT_FILE_LOCATION_HERE";\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Text;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\Model;\nuse Clarifai\\Api\\ModelVersion;\nuse Clarifai\\Api\\OutputInfo;\nuse Clarifai\\Api\\PostModelOutputsRequest;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\nuse Google\\Protobuf\\Struct;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ["Authorization" => ["Key " . $PAT]];\n\n$userDataObject = new UserAppIDSet([\n    "user_id" => $USER_ID,\n    "app_id" => $APP_ID,\n]);\n\n// create Struct instance\n$params = new Struct();\n$params->model_id = "eleven_multilingual_v1";\n$params->voice_id = "pNInz6obpgDQGcFmaJgB";\n$params->stability = 0.4;\n$params->similarity_boost = 0.7;\n\n//$textData = file_get_contents($TEXT_FILE_LOCATION); // Get the text bytes data from the location\n\n// Let\'s make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client->PostModelOutputs(\n        // The request object carries the request along with the request status and other metadata related to the request itself\n        new PostModelOutputsRequest([\n            "user_app_id" => $userDataObject,\n            "model_id" => $MODEL_ID,\n            "version_id" => $MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n            "inputs" => [\n                new Input([\n                    // The Input object wraps the Data object in order to meet the API specification\n                    "data" => new Data([\n                        // The Data object is constructed around the Text object. It offers a container that has additional text independent\n                        // metadata. In this particular use case, no other metadata is needed to be specified\n                        "text" => new Text([\n                            // In the Clarifai platform, a text is defined by a special Text object\n                            "raw" => $RAW_TEXT,\n                            // "url" => $TEXT_FILE_URL\n                            // "raw" => $textData\n                        ]),\n                    ]),\n                ]),\n            ],\n            "model" => new Model([\n                "model_version" => new ModelVersion([\n                    "output_info" => new OutputInfo(["params" => $params]),\n                ]),\n            ]),\n        ]),\n        $metadata\n    )\n    ->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception("Error: {$status->details}");\n}\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure\n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    throw new Exception(\n        "Failure response: " . $response->getStatus()->getDescription() . " " .\n            $response->getStatus()->getDetails()\n    );\n}\n\n// Save the output file\n$output = $response->getOutputs()[0]->getData()->getAudio()->getBase64();\n\n$audioFilename = "audio_file.wav";\nfile_put_contents($audioFilename, base64_decode($output));\n\n?>\n',jn='curl -X POST "https://api.clarifai.com/v2/users/nlptownres/apps/text-classification/models/multilingual-uncased-sentiment/versions/29d5fef0229a4936a607380d7ef775dd/outputs" \\\n    -H "Authorization: Key YOUR_PAT_HERE" \\\n    -H "Content-Type: application/json" \\\n    -d \'{\n      "inputs": [\n        {\n          "data": {\n            "text": {\n              "url": "https://samples.clarifai.com/negative_sentence_12.txt"\n            }\n          }\n        }\n      ]\n    }\'\n   ',Hn='# Smaller files (195 KB or less)\n\ncurl -X POST "https://api.clarifai.com/v2/users/nlptownres/apps/text-classification/models/multilingual-uncased-sentiment/versions/29d5fef0229a4936a607380d7ef775dd/outputs" \\\n    -H "Authorization: Key YOUR_PAT_HERE" \\\n    -H "Content-Type: application/json" \\\n    -d \'{\n      "inputs": [\n        {\n          "data": {\n            "text": {\n              "raw": "YOUR_TEXT_FILE_BYTES_HERE"\n            }\n          }\n        }\n      ]\n    }\'\n   ',Bn='curl -X POST "https://api.clarifai.com/v2/users/stability-ai/apps/stable-diffusion-2/models/stable-diffusion-xl/versions/0c919cc1edfc455dbc96207753f178d7/outputs" \\\n    -H "Authorization: Key YOUR_PAT_HERE" \\\n    -H "Content-Type: application/json" \\\n    -d \'{\n      "inputs": [\n        {\n          "data": {\n            "text": {\n              "raw": "A penguin watching the sunset."\n            }\n          }\n        }\n      ]\n    }\'\n   ',Yn='curl -X POST "https://api.clarifai.com/v2/users/nlptownres/apps/text-classification/models/multilingual-uncased-sentiment/versions/29d5fef0229a4936a607380d7ef775dd/outputs" \\\n    -H "Authorization: Key YOUR_PAT_HERE" \\\n    -H "Content-Type: application/json" \\\n    -d \'{\n      "inputs": [\n        {\n          "data": {\n            "text": {\n              "raw": "I love your product very much"\n            }\n          }\n        }\n      ]\n    }\'\n   ',kn='curl -X POST "https://api.clarifai.com/v2/users/eleven-labs/apps/audio-generation/models/speech-synthesis/versions/f588d92c044d4487a38c8f3d7a3b0eb2/outputs" \\\n    -H "Authorization: Key YOUR_PAT_HERE" \\\n    -H "Content-Type: application/json" \\\n    -d \'{\n    "inputs": [\n        {\n            "data": {\n                "text": {\n                    "raw": "I love your product very much!"\n                }\n            }\n        }\n    ],\n    "model": {\n        "model_version": {\n            "output_info": {\n                "params": {\n                    "model_id": "eleven_multilingual_v1",\n                    "voice_id": "EXAVITQu4vr4xnSDxMaL",\n                    "similarity_boost": 0,\n                    "stability": 0.5\n                }\n            }\n        }\n    }\n}\'\n\n',$n='curl -X POST "https://api.clarifai.com/v2/users/openai/apps/dall-e/models/dall-e-3/versions/dc9dcb6ee67543cebc0b9a025861b868/outputs" \\\n    -H "Authorization: Key YOUR_PAT_HERE" \\\n    -H "Content-Type: application/json" \\\n    -d \'{\n    "inputs": [\n        {\n            "data": {\n                "text": {\n                    "raw": "An expressive oil painting of a basketball player dunking, depicted as an explosion of a nebula"\n                }\n            }\n        }\n    ],\n    "model": {\n        "model_version": {\n            "output_info": {\n                "params": {\n                    "size":"1024x1024",\n                    "quality":"hd",\n                    "api_key":"ADD_THIRD_PARTY_KEY_HERE"\n                }\n            }\n        }\n    }\n}\'',Fn="######################################################################################################\n# In this section, we set the user authentication, user and app ID, model details, and the raw\n# text we want as a prompt. Change these strings to run your own example.\n######################################################################################################\n\n# Your PAT (Personal Access Token) can be found in the Account's Security section\nPAT = 'YOUR_PAT_HERE'\n# Specify the correct user_id/app_id pairings\n# Since you're making inferences outside your app's scope\nUSER_ID = 'meta'\nAPP_ID = 'Llama-2'\n# Change these to whatever model and text you want to use\nMODEL_ID = 'llama2-7b-chat'\nMODEL_VERSION_ID = 'e52af5d6bc22445aa7a6761f327f7129'\nRAW_TEXT = 'I love your product very much'\n\n############################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n############################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (('authorization', 'Key ' + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\npost_model_outputs_response = stub.PostModelOutputs(\n    service_pb2.PostModelOutputsRequest(\n        user_app_id=userDataObject,  # The userDataObject is created in the overview and is required when using a PAT\n        model_id=MODEL_ID,\n        version_id=MODEL_VERSION_ID,  # This is optional. Defaults to the latest model version\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    text=resources_pb2.Text(\n                        raw=RAW_TEXT\n                    )\n                )\n            )\n        ]\n    ),\n    metadata=metadata\n)\nif post_model_outputs_response.status.code != status_code_pb2.SUCCESS:\n    print(post_model_outputs_response.status)\n    raise Exception(f\"Post model outputs failed, status: {post_model_outputs_response.status.description}\")\n\n# Since we have one input, one output will exist here\noutput = post_model_outputs_response.outputs[0]\n\nprint(\"Completion:\\n\")\nprint(output.data.text.raw)",Gn="\x3c!--index.html file--\x3e\n\n<script>        \n    ////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, and the raw\n    // text we want as a prompt. Change these strings to run your own example.\n    ///////////////////////////////////////////////////////////////////////////////////////////////////\n\n    // Your PAT (Personal Access Token) can be found in the Account's Security section\n    const PAT = 'YOUR_PAT_HERE';\n    // Specify the correct user_id/app_id pairings\n    // Since you're making inferences outside your app's scope\n    const USER_ID = 'meta';    \n    const APP_ID = 'Llama-2';\n    // Change these to whatever model and text you want to use  \n    const MODEL_ID = 'llama2-7b-chat';\n    const MODEL_VERSION_ID = 'e52af5d6bc22445aa7a6761f327f7129';    \n    const RAW_TEXT = 'I love your product very much';\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    const raw = JSON.stringify({\n        \"user_app_id\": {\n            \"user_id\": USER_ID,\n            \"app_id\": APP_ID\n        },\n        \"inputs\": [\n            {\n                \"data\": {\n                    \"text\": {\n                        \"raw\": RAW_TEXT\n                    }\n                }\n            }\n        ]\n    });\n\n    const requestOptions = {\n        method: 'POST',\n        headers: {\n            'Accept': 'application/json',\n            'Authorization': 'Key ' + PAT\n        },\n        body: raw\n    };\n\n    // NOTE: MODEL_VERSION_ID is optional, you can also call prediction with the MODEL_ID only\n    // https://api.clarifai.com/v2/models/{YOUR_MODEL_ID}/outputs\n    // this will default to the latest version_id\n\n    fetch(\"https://api.clarifai.com/v2/models/\" + MODEL_ID + \"/versions/\" + MODEL_VERSION_ID + \"/outputs\", requestOptions)\n        .then((response) => {\n            return response.json();\n        })\n        .then((data) => {\n            if(data.status.code != 10000) console.log(data.status);\n            else console.log(data['outputs'][0]['data']['text']['raw']);\n        }).catch(error => console.log('error', error));\n\n<\/script>",Vn="//index.js file\n\n////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, and the raw\n// text we want as a prompt. Change these strings to run your own example.\n/////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account's Security section\nconst PAT = 'YOUR_PAT_HERE';\n// Specify the correct user_id/app_id pairings\n// Since you're making inferences outside your app's scope\nconst USER_ID = 'meta';    \nconst APP_ID = 'Llama-2';\n// Change these to whatever model and text you want to use\nconst MODEL_ID = 'llama2-7b-chat';\nconst MODEL_VERSION_ID = 'e52af5d6bc22445aa7a6761f327f7129';   \nconst RAW_TEXT = 'I love your product very much';\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require(\"clarifai-nodejs-grpc\");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set(\"authorization\", \"Key \" + PAT);\n\nstub.PostModelOutputs(\n    {\n        user_app_id: {\n            \"user_id\": USER_ID,\n            \"app_id\": APP_ID\n        },\n        model_id: MODEL_ID,\n        version_id: MODEL_VERSION_ID, // This is optional. Defaults to the latest model version.\n        inputs: [\n            { data: { text: { raw: RAW_TEXT } } }\n        ]\n    },\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            console.log(response)\n            throw new Error(\"Post model outputs failed, status: \" + response.status.description);\n        }\n\n        // Since we have one input, one output will exist here.\n        const output = response.outputs[0];\n\n        console.log(\"Completion:\\n\");\n        console.log(output.data.text.raw);\n    }\n\n);\n",qn='package com.clarifai.example;\n\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.status.StatusCode;\n\npublic class ClarifaiExample {\n\n    ///////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, and the raw\n    // text we want as a prompt. Change these strings to run your own example.\n    //////////////////////////////////////////////////////////////////////////////////////////////////\n\n    //Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    static final String USER_ID = "meta";    \n    static final String APP_ID = "Llama-2";\n    // Change these to whatever model and text you want to use\n    static final String MODEL_ID = "llama2-7b-chat"; \n    static final String MODEL_VERSION_ID = "e52af5d6bc22445aa7a6761f327f7129";   \n    static final String RAW_TEXT = "I love your product very much";\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\t\n\n    public static void main(String[] args) {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n            .withCallCredentials(new ClarifaiCallCredentials(PAT));\n        \n        MultiOutputResponse postModelOutputsResponse = stub.postModelOutputs(\n            PostModelOutputsRequest.newBuilder()\n            .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n            .setModelId(MODEL_ID)\n            .setVersionId(MODEL_VERSION_ID) // This is optional. Defaults to the latest model version\n            .addInputs(\n                Input.newBuilder().setData(\n                    Data.newBuilder().setText(\n                        Text.newBuilder().setRaw(RAW_TEXT)\n                    )\n                )\n            )\n            .build()\n        );\n\n        if (postModelOutputsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("Post model outputs failed, status: " + postModelOutputsResponse.getStatus());\n        }\n\n        // Since we have one input, one output will exist here\n        Output output = postModelOutputsResponse.getOutputs(0);\n\n        System.out.println("Completion:");\n        System.out.println(output.getData().getText().getRaw());\n        \n    }\n\n}\n',Xn="<?php\n\nrequire __DIR__ . '/vendor/autoload.php';\n\n////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, and the raw\n// text we want as a prompt. Change these strings to run your own example.\n////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account's Security section\n$PAT = 'YOUR_PAT_HERE';\n// Specify the correct user_id/app_id pairings\n// Since you're making inferences outside your app's scope\n$USER_ID = 'meta';\n$APP_ID = 'Llama-2';\n// Change these to whatever model and text you want to use\n$MODEL_ID = 'llama2-7b-chat';\n$MODEL_VERSION_ID = 'e52af5d6bc22445aa7a6761f327f7129';\n$RAW_TEXT = 'I love your product very much';\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Text;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\PostModelOutputsRequest;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ['Authorization' => ['Key ' . $PAT ]];\n\n$userDataObject = new UserAppIDSet([\n    'user_id' => $USER_ID, \n    'app_id' => $APP_ID \n]);\n\n// Let's make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client->PostModelOutputs(\n    // The request object carries the request along with the request status and other metadata related to the request itself\n    new PostModelOutputsRequest([\n        'user_app_id' => $userDataObject,\n        'model_id' => $MODEL_ID,  \n        'version_id' => $MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n        'inputs' => [\n            new Input([ // The Input object wraps the Data object in order to meet the API specification                \n                'data' => new Data([ // The Data object is constructed around the Text object. It offers a container that has additional text independent\n                                    // metadata. In this particular use case, no other metadata is needed to be specified\n                    'text' => new Text([ // In the Clarifai platform, a text is defined by a special Text object\n                        'raw' => $RAW_TEXT\n                    ])\n                ])\n            ])\n        ]        \n    ]),\n    $metadata\n)->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception(\"Error: {$status->details}\");\n}\n\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure \n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    throw new Exception(\"Failure response: \" . $response->getStatus()->getDescription() . \" \" .\n        $response->getStatus()->getDetails());\n}\n\n// The output of a successful call can be used in many ways. In this example, we loop through all of the predicted concepts and print them out along with\n// their numerical prediction value (confidence)\necho \"Completion: </br>\";\necho $response->getOutputs()[0]->getData()->getText()->getRaw()\n\n?>\n",Qn='curl -X POST "https://api.clarifai.com/v2/users/meta/apps/Llama-2/models/llama2-7b-chat/versions/e52af5d6bc22445aa7a6761f327f7129/outputs" \\\n    -H "Authorization: Key YOUR_PAT_HERE" \\\n    -H "Content-Type: application/json" \\\n    -d \'{\n      "inputs": [\n        {\n          "data": {\n            "text": {\n              "raw": "I love your product very much"\n            }\n          }\n        }\n      ]\n    }\'\n   ',Kn="#####################################################################################################################\n# In this section, we set the user authentication, user and app ID, model details, raw text we want as a prompt,\n# and the parameters. Change these values to run your own example.\n#####################################################################################################################\n\n# Your PAT (Personal Access Token) can be found in the Account's Security section\nPAT = 'YOUR_PAT_HERE'\n# Specify the correct user_id/app_id pairings\n# Since you're making inferences outside your app's scope\nUSER_ID = 'meta'\nAPP_ID = 'Llama-2'\n# Change these to whatever model and text you want to use\nMODEL_ID = 'llama2-7b-chat'\nMODEL_VERSION_ID = 'e52af5d6bc22445aa7a6761f327f7129'\nRAW_TEXT = 'I love your product very much'\n\n############################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n############################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\nfrom google.protobuf.struct_pb2 import Struct\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (('authorization', 'Key ' + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\nparams = Struct()\nparams.update({\n    \"temperature\": 0.5,\n    \"max_tokens\": 2048,\n    \"top_k\": 0.95\n})\n\npost_model_outputs_response = stub.PostModelOutputs(\n    service_pb2.PostModelOutputsRequest(\n        user_app_id=userDataObject,  # The userDataObject is created in the overview and is required when using a PAT\n        model_id=MODEL_ID,\n        version_id=MODEL_VERSION_ID,  # This is optional. Defaults to the latest model version\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    text=resources_pb2.Text(\n                        raw=RAW_TEXT\n                    )\n                )\n            )\n        ],\n        model=resources_pb2.Model(\n            model_version=resources_pb2.ModelVersion(\n                output_info=resources_pb2.OutputInfo(\n                    params=params\n                )\n            )\n        )\n\n    ),\n    metadata=metadata\n)\nif post_model_outputs_response.status.code != status_code_pb2.SUCCESS:\n    print(post_model_outputs_response.status)\n    raise Exception(f\"Post model outputs failed, status: {post_model_outputs_response.status.description}\")\n\n# Since we have one input, one output will exist here\noutput = post_model_outputs_response.outputs[0]\n\nprint(\"Completion:\\n\")\nprint(output.data.text.raw)",Wn="\x3c!--index.html file--\x3e\n\n<script>\n    /////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, raw text we want as a prompt,\n    // and the parameters. Change these values to run your own example.\n    /////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n    // Your PAT (Personal Access Token) can be found in the Account's Security section\n    const PAT = 'YOUR_PAT_HERE';    \n    // Specify the correct user_id/app_id pairings\n    // Since you're making inferences outside your app's scope\n    const USER_ID = 'meta';\n    const APP_ID = 'Llama-2';\n    // Change these to whatever model and text you want to use\n    const MODEL_ID = 'llama2-7b-chat';\n    const MODEL_VERSION_ID = 'e52af5d6bc22445aa7a6761f327f7129';\n    const RAW_TEXT = 'I love your product very much'\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    const raw = JSON.stringify({  \n        \"inputs\": [\n        {\n            \"data\": {\n                \"text\": {\n                    \"raw\": RAW_TEXT\n                }\n            }\n        }\n    ],\n    \"model\": {\n        \"model_version\": {\n            \"output_info\": {\n                \"params\": {\n                    \"temperature\": 0.5,\n                    \"max_tokens\": 2048,\n                    \"top_k\": 0.95\n                }\n            }\n        }\n    }      \n    });\n\n    const requestOptions = {\n        method: 'POST',\n        headers: {\n            'Accept': 'application/json',\n            'Authorization': 'Key ' + PAT\n        },\n        body: raw\n    };\n\n    fetch(`https://api.clarifai.com/v2/users/${USER_ID}/apps/${APP_ID}/models/${MODEL_ID}/versions/${MODEL_VERSION_ID}/outputs`, requestOptions)\n        .then(response => response.text())\n        .then(result => console.log(result))\n        .catch(error => console.log('error', error));\n\n<\/script>",zn='//index.js file\n\n//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, raw text we want as a prompt,\n// and the parameters. Change these values to run your own example.\n//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\nconst PAT = \'YOUR_PAT_HERE\';    \n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\nconst USER_ID = \'meta\';\nconst APP_ID = \'Llama-2\';\n// Change these to whatever model and text you want to use\nconst MODEL_ID = \'llama2-7b-chat\';\nconst MODEL_VERSION_ID = \'e52af5d6bc22445aa7a6761f327f7129\';\nconst RAW_TEXT = \'I love your product very much\'\n\n/////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n/////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\nstub.PostModelOutputs(\n    {\n        user_app_id: {\n            "user_id": USER_ID,\n            "app_id": APP_ID\n        },\n        model_id: MODEL_ID,\n        version_id: MODEL_VERSION_ID,  // This is optional. Defaults to the latest model version\n        inputs: [\n            {\n                "data": {\n                    "text": {\n                        "raw": RAW_TEXT\n                    }\n                }\n            }\n        ],\n        model: {\n            "model_version": {\n                "output_info": {\n                    "params": {\n                        "temperature": 0.5,\n                        "max_tokens": 2048,\n                        "top_k": 0.95\n                    }\n                }\n            }\n        } \n    },\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            throw new Error("Post models failed, status: " + response.status.description);\n        }\n        // Since we have one input, one output will exist here.\n        const output = response.outputs[0];\n\n        console.log("Completion:\\n");\n        console.log(output.data.text.raw);\n    }\n);',Jn='package com.clarifai.example;\n\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.status.StatusCode;\nimport java.io.FileOutputStream;\nimport com.google.protobuf.Struct;\nimport com.google.protobuf.Value;\n\nimport com.google.protobuf.ByteString;\nimport java.io.File;\nimport java.io.IOException;\nimport java.nio.file.Files;\n\npublic class ClarifaiExample {\n\n    /////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, and raw text we want as a prompt.\n    // Change these values to run your own example.\n    /////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n    // Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    static final String USER_ID = "meta";\n    static final String APP_ID = "Llama-2";\n    // Change these to whatever model you want to use\n    static final String MODEL_ID = "llama2-7b-chat";\n    static final String MODEL_VERSION_ID = "e52af5d6bc22445aa7a6761f327f7129";\n    static final String RAW_TEXT = "I love your product very much!";\n    // To use a hosted text file, assign the URL variable\n    // static final String TEXT_FILE_URL = "https://samples.clarifai.com/negative_sentence_12.txt";\n    // Or, to use a local text file, assign the location variable\n    // static final String TEXT_FILE_LOCATION = "YOUR_TEXT_FILE_LOCATION_HERE";\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\t\n    \n    public static void main(String[] args) throws IOException {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n                .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        Struct.Builder params = Struct.newBuilder()\n                .putFields("temperature", Value.newBuilder().setNumberValue(0.5).build())\n                .putFields("max_tokens", Value.newBuilder().setNumberValue(2048).build())\n                .putFields("top_k", Value.newBuilder().setNumberValue(0.95).build());\n\n        MultiOutputResponse postModelOutputsResponse = stub.postModelOutputs(\n                PostModelOutputsRequest.newBuilder()\n                        .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n                        .setModelId(MODEL_ID)\n                        .setVersionId(MODEL_VERSION_ID) // This is optional. Defaults to the latest model version.\n                        .addInputs(\n                                Input.newBuilder().setData(\n                                        Data.newBuilder().setText(\n                                                Text.newBuilder().setRaw(RAW_TEXT)\n                                        // Text.newBuilder().setUrl(TEXT_FILE_URL)\n                                        // Text.newBuilder().setRawBytes(ByteString.copyFrom(Files.readAllBytes(\n                                        // new File(TEXT_FILE_LOCATION).toPath()\n                                        // )))\n                                        )\n                                )\n                        )\n                        .setModel(Model.newBuilder()\n                                .setModelVersion(ModelVersion.newBuilder()\n                                        .setOutputInfo(OutputInfo.newBuilder().setParams(params))\n                                )\n                        )\n                        .build()\n        );\n\n        if (postModelOutputsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("Post model outputs failed, status: " + postModelOutputsResponse.getStatus());\n        }\n\n        // Since we have one input, one output will exist here\n        Output output = postModelOutputsResponse.getOutputs(0);\n\n        System.out.println("Completion:");\n        System.out.println(output.getData().getText().getRaw());\n\n    }\n}\n',Zn='<?php\n\nrequire __DIR__ . "/vendor/autoload.php";\n\n///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, and raw text we want as a prompt.\n// Change these values to run your own example.\n///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\n$PAT = "YOUR_PAT_HERE";\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\n$USER_ID = "meta";\n$APP_ID = "Llama-2";\n// Change these to whatever model and image URL you want to use\n$MODEL_ID = "llama2-7b-chat";\n$MODEL_VERSION_ID = "e52af5d6bc22445aa7a6761f327f7129";\n$RAW_TEXT = "I love your product very much!";\n// To use a hosted text file, assign the URL variable\n// $TEXT_FILE_URL = "https://samples.clarifai.com/negative_sentence_12.txt";\n// Or, to use a local text file, assign the location variable\n// $TEXT_FILE_LOCATION = "YOUR_TEXT_FILE_LOCATION_HERE";\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Text;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\Model;\nuse Clarifai\\Api\\ModelVersion;\nuse Clarifai\\Api\\OutputInfo;\nuse Clarifai\\Api\\PostModelOutputsRequest;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\nuse Google\\Protobuf\\Struct;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ["Authorization" => ["Key " . $PAT]];\n\n$userDataObject = new UserAppIDSet([\n    "user_id" => $USER_ID,\n    "app_id" => $APP_ID,\n]);\n\n// Create Struct instance\n$params = new Struct();\n$params->temperature = 0.5;\n$params->max_tokens = 2048;\n$params->top_k = 0.95;\n\n// $textData = file_get_contents($TEXT_FILE_LOCATION); // Get the text bytes data from the location\n\n// Let\'s make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client->PostModelOutputs(\n    // The request object carries the request along with the request status and other metadata related to the request itself\n    new PostModelOutputsRequest([\n        "user_app_id" => $userDataObject,\n        "model_id" => $MODEL_ID,\n        "version_id" => $MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n        "inputs" => [\n            new Input([\n                // The Input object wraps the Data object in order to meet the API specification\n                "data" => new Data([\n                    // The Data object is constructed around the Text object. It offers a container that has additional text independent\n                    // metadata. In this particular use case, no other metadata is needed to be specified\n                    "text" => new Text([\n                        // In the Clarifai platform, a text is defined by a special Text object\n                        "raw" => $RAW_TEXT,\n                        // "url" => $TEXT_FILE_URL\n                        // "raw" => $textData\n                    ]),\n                ]),\n            ]),\n        ],\n        "model" => new Model([\n            "model_version" => new ModelVersion([\n                "output_info" => new OutputInfo(["params" => $params]),\n            ]),\n        ]),\n    ]),\n    $metadata\n)\n    ->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception("Error: {$status->details}");\n}\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure\n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    throw new Exception(\n        "Failure response: " . $response->getStatus()->getDescription() . " " .\n            $response->getStatus()->getDetails()\n    );\n}\n\necho "Completion: </br>";\n# Since we have one input, one output will exist here\necho $response->getOutputs()[0]->getData()->getText()->getRaw()\n\n?>',et='curl -X POST "https://api.clarifai.com/v2/users/meta/apps/Llama-2/models/llama2-7b-chat/versions/e52af5d6bc22445aa7a6761f327f7129/outputs" \\\n    -H "Authorization: Key YOUR_PAT_HERE" \\\n    -H "Content-Type: application/json" \\\n    -d \'{\n    "inputs": [\n        {\n            "data": {\n                "text": {\n                    "raw": "I love your product very much"\n                }\n            }\n        }\n    ],\n    "model": {\n        "model_version": {\n            "output_info": {\n                "params": {\n                    "temperature": 0.5,\n                    "max_tokens": 2048,\n                    "top_k": 0.95\n                }\n            }\n        }\n    }\n}\'',nt='#########################################################################################\n# In this section, we set the user authentication, user and app ID, model ID, and\n# audio URL. Change these strings to run your own example.\n########################################################################################\n\n# Your PAT (Personal Access Token) can be found in the Account\'s Security section\nPAT = "YOUR_PAT_HERE"\n# Specify the correct user_id/app_id pairings\n# Since you\'re making inferences outside your app\'s scope\nUSER_ID = "facebook"\nAPP_ID = "asr"\n# Change these to make your own predictions\nMODEL_ID = "asr-wav2vec2-base-960h-english"\nAUDIO_URL = "https://samples.clarifai.com/negative_sentence_1.wav"\n\n##########################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n##########################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (("authorization", "Key " + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(\n    user_id=USER_ID, app_id=APP_ID\n)  # The userDataObject is required when using a PAT\n\npost_model_outputs_response = stub.PostModelOutputs(\n    service_pb2.PostModelOutputsRequest(\n        user_app_id=userDataObject,\n        model_id=MODEL_ID,\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(audio=resources_pb2.Audio(url=AUDIO_URL))\n            )\n        ],\n    ),\n    metadata=metadata,\n)\nif post_model_outputs_response.status.code != status_code_pb2.SUCCESS:\n    print(post_model_outputs_response.status)\n    raise Exception(\n        "Post workflow results failed, status: "\n        + post_model_outputs_response.status.description\n    )\n\n# Since we have one input, one output will exist here\noutput = post_model_outputs_response.outputs[0]\n\n# Print the output\nprint(output.data.text.raw)\n',tt='#########################################################################################\n# In this section, we set the user authentication, user and app ID, model ID, and\n# audio file location. Change these strings to run your own example.\n########################################################################################\n\n# Your PAT (Personal Access Token) can be found in the Account\'s Security section\nPAT = "YOUR_PAT_HERE"\n# Specify the correct user_id/app_id pairings\n# Since you\'re making inferences outside your app\'s scope\nUSER_ID = "facebook"\nAPP_ID = "asr"\n# Change these to make your own predictions\nMODEL_ID = "asr-wav2vec2-base-960h-english"\nAUDIO_FILE_LOCATION = "YOUR_AUDIO_FILE_LOCATION_HERE"\n\n##########################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n##########################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (("authorization", "Key " + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(\n    user_id=USER_ID, app_id=APP_ID\n)  # The userDataObject is required when using a PAT\n\nwith open(AUDIO_FILE_LOCATION, "rb") as f:\n    file_bytes = f.read()\n\npost_model_outputs_response = stub.PostModelOutputs(\n    service_pb2.PostModelOutputsRequest(\n        user_app_id=userDataObject,\n        model_id=MODEL_ID,\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(audio=resources_pb2.Audio(base64=file_bytes))\n            )\n        ],\n    ),\n    metadata=metadata,\n)\nif post_model_outputs_response.status.code != status_code_pb2.SUCCESS:\n    print(post_model_outputs_response.status)\n    raise Exception(\n        "Post workflow results failed, status: "\n        + post_model_outputs_response.status.description\n    )\n\n# Since we have one input, one output will exist here\noutput = post_model_outputs_response.outputs[0]\n\n# Print the output\nprint(output.data.text.raw)\n',at='\x3c!--index.html file--\x3e\n\n<script>\n  ////////////////////////////////////////////////////////////////////////////////////////\n  // In this section, we set the user authentication, user and app ID, model ID, and\n  // audio URL. Change these strings to run your own example.\n  ///////////////////////////////////////////////////////////////////////////////////////\n\n  // Your PAT (Personal Access Token) can be found in the Account\'s Security section\n  const PAT = "YOUR_PAT_HERE";\n  // Specify the correct user_id/app_id pairings\n  // Since you\'re making inferences outside your app\'s scope\n  const USER_ID = "facebook";\n  const APP_ID = "asr";\n  // Change these to make your own predictions\n  const MODEL_ID = "asr-wav2vec2-base-960h-english";\n  const AUDIO_URL = "https://samples.clarifai.com/negative_sentence_1.wav";\n\n  ///////////////////////////////////////////////////////////////////////////////////\n  // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n  /////////////////////////////////////////////////////////////////////////////////// \n\n  const raw = JSON.stringify({\n      "user_app_id": {\n          "user_id": USER_ID,\n          "app_id": APP_ID\n      },\n      "inputs": [\n          {\n              "data": {\n                  "audio": {\n                      "url": AUDIO_URL\n                  }\n              }\n          }\n      ]\n  });\n\n  const requestOptions = {\n      method: \'POST\',\n      headers: {\n          \'Accept\': \'application/json\',\n          \'Authorization\': \'Key \' + PAT\n      },\n      body: raw\n  };\n\n  fetch(`https://api.clarifai.com/v2/models/${MODEL_ID}/outputs`, requestOptions)\n      .then(response => response.text())\n      .then(result => console.log(result))\n      .catch(error => console.log(\'error\', error));\n<\/script>',it='\x3c!--index.html file--\x3e\n\n<script>\n  //////////////////////////////////////////////////////////////////////////////////////////////\n  // In this section, we set the user authentication, user and app ID, model ID, and bytes\n  // of the audio we want as an input. Change these strings to run your own example.\n  /////////////////////////////////////////////////////////////////////////////////////////////\n\n  // Your PAT (Personal Access Token) can be found in the Account\'s Security section\n  const PAT = "YOUR_PAT_HERE";\n  // Specify the correct user_id/app_id pairings\n  // Since you\'re making inferences outside your app\'s scope\n  const USER_ID = "facebook";\n  const APP_ID = "asr";\n  // Change these to make your own predictions\n  const MODEL_ID = "asr-wav2vec2-base-960h-english";\n  const AUDIO_BYTES_STRING = "YOUR_BYTES_STRING_HERE";\n\n  ///////////////////////////////////////////////////////////////////////////////////\n  // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n  /////////////////////////////////////////////////////////////////////////////////// \n\n  const raw = JSON.stringify({\n    "user_app_id": {\n      "user_id": USER_ID,\n      "app_id": APP_ID\n    },\n    "inputs": [\n      {\n        "data": {\n          "audio": {\n            "base64": AUDIO_BYTES_STRING\n          }\n        }\n      }\n    ]\n  });\n\n  const requestOptions = {\n    method: \'POST\',\n    headers: {\n      \'Accept\': \'application/json\',\n      \'Authorization\': \'Key \' + PAT\n    },\n    body: raw\n  };\n\n  fetch(`https://api.clarifai.com/v2/models/${MODEL_ID}/outputs`, requestOptions)\n    .then(response => response.text())\n    .then(result => console.log(result))\n    .catch(error => console.log(\'error\', error));\n<\/script>',st='//index.js file\n\n////////////////////////////////////////////////////////////////////////////////////////\n//  In this section, we set the user authentication, user and app ID, model ID, and\n// audio URL. Change these strings to run your own example.\n///////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\nconst PAT = "YOUR_PAT_HERE"\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\nconst USER_ID = "facebook"\nconst APP_ID = "asr"\n// Change these to make your own predictions\nconst MODEL_ID = "asr-wav2vec2-base-960h-english"\nconst AUDIO_URL = "https://samples.clarifai.com/negative_sentence_1.wav"\n\n/////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n/////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\nstub.PostModelOutputs(\n  {\n    user_app_id: {\n      "user_id": USER_ID,\n      "app_id": APP_ID,\n    },\n    model_id: MODEL_ID,\n    inputs: [{ data: { audio: { url: AUDIO_URL } } }],\n  },\n  metadata,\n  (err, response) => {\n    if (err) {\n      throw new Error(err);\n    }\n\n    if (response.status.code !== 10000) {\n      throw new Error(\n        "Post workflow results failed, status: " + response.status.description\n      );\n    }\n\n    // Since we have one input, one output will exist here\n    const output = response.outputs[0];\n\n    // Print the output\n    console.log(output.data.text.raw)\n\n  }\n);\n',ot='//index.js file\n\n////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model ID, and\n// audio file location. Change these strings to run your own example.\n///////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\nconst PAT = "YOUR_PAT_HERE"\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\nconst USER_ID = "facebook"\nconst APP_ID = "asr"\n// Change these to make your own predictions\nconst MODEL_ID = "asr-wav2vec2-base-960h-english"\nconst AUDIO_FILE_LOCATION = "YOUR_AUDIO_FILE_LOCATION_HERE"\n\n/////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n/////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\nconst fs = require("fs");\nconst audioBytes = fs.readFileSync(AUDIO_FILE_LOCATION);\n\nstub.PostModelOutputs(\n  {\n    user_app_id: {\n      "user_id": USER_ID,\n      "app_id": APP_ID,\n    },\n    model_id: MODEL_ID,\n    inputs: [{ data: { audio: { base64: audioBytes } } }],\n  },\n  metadata,\n  (err, response) => {\n    if (err) {\n      throw new Error(err);\n    }\n\n    if (response.status.code !== 10000) {\n      throw new Error(\n        "Post workflow results failed, status: " + response.status.description\n      );\n    }\n\n    // Since we have one input, one output will exist here\n    const output = response.outputs[0];\n\n    // Print the output\n    console.log(output.data.text.raw)\n\n  }\n);\n',rt='package com.clarifai.example;\n\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.grpc.api.status.StatusCode;\n\npublic class ClarifaiExample {\n\n    //////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model ID, and\n    // audio URL. Change these strings to run your own example.\n    //////////////////////////////////////////////////////////////////////////////////////\n    \n    //Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    static final String USER_ID = "facebook";\n    static final String APP_ID = "asr";\n    // Change these to make your own predictions\n    static final String MODEL_ID = "asr-wav2vec2-base-960h-english";\n    static final String AUDIO_URL = "https://samples.clarifai.com/negative_sentence_1.wav";\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n    public static void main(String[] args) {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n                .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        MultiOutputResponse postModelOutputsResponse = stub.postModelOutputs(\n                PostModelOutputsRequest.newBuilder()\n                        .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n                        .setModelId(MODEL_ID)\n                        .addInputs(\n                                Input.newBuilder().setData(\n                                        Data.newBuilder().setAudio(\n                                                Audio.newBuilder().setUrl(AUDIO_URL)\n                                        )\n                                )\n                        )\n                        .build()\n        );\n\n        if (postModelOutputsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            System.out.println(postModelOutputsResponse.getStatus());\n            throw new RuntimeException("Post workflow results failed, status: " + postModelOutputsResponse.getStatus().getDescription());\n        }\n\n        Output output = postModelOutputsResponse.getOutputs(0);\n\n        // Print the output\n        System.out.println(output.getData().getText().getRaw());\n\n    }\n\n}\n',lt='package com.clarifai.example;\n\nimport java.io.File;\nimport java.io.IOException;\nimport java.nio.file.Files;\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.grpc.api.status.StatusCode;\nimport com.google.protobuf.ByteString;\n\npublic class ClarifaiExample {\n\n    ///////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model ID, and\n    // audio file location. Change these strings to run your own example.\n    ///////////////////////////////////////////////////////////////////////////////////////\n\n    //Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    static final String USER_ID = "facebook";\n    static final String APP_ID = "asr";\n    // Change these to make your own predictions\n    static final String MODEL_ID = "asr-wav2vec2-base-960h-english";\n    static final String AUDIO_FILE_LOCATION = "YOUR_AUDIO_FILE_LOCATION_HERE";\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n    public static void main(String[] args) throws IOException {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n                .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        MultiOutputResponse postModelOutputsResponse = stub.postModelOutputs(\n                PostModelOutputsRequest.newBuilder()\n                        .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n                        .setModelId(MODEL_ID)\n                        .addInputs(\n                                Input.newBuilder().setData(\n                                        Data.newBuilder().setAudio(\n                                                Audio.newBuilder().setBase64(ByteString.copyFrom(Files.readAllBytes(\n                                                        new File(AUDIO_FILE_LOCATION).toPath()\n                                                )))\n                                        )\n                                )\n                        )\n                        .build()\n        );\n\n        if (postModelOutputsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            System.out.println(postModelOutputsResponse.getStatus());\n            throw new RuntimeException("Post workflow results failed, status: " + postModelOutputsResponse.getStatus().getDescription());\n        }\n\n        Output output = postModelOutputsResponse.getOutputs(0);\n\n        // Print the output\n        System.out.println(output.getData().getText().getRaw());\n\n    }\n\n}\n',ct='<?php\n\nrequire __DIR__ . "/vendor/autoload.php";\n\n///////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model ID, and\n// audio URL. Change these strings to run your own example.\n///////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\n$PAT = "YOUR_PAT_HERE";\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\n$USER_ID = "facebook";\n$APP_ID = "asr";\n// Change these to make your own predictions\n$MODEL_ID = "asr-wav2vec2-base-960h-english";\n$AUDIO_URL = "https://samples.clarifai.com/negative_sentence_1.wav";\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\Api\\Audio;\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\PostModelOutputsRequest;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ["Authorization" => ["Key " . $PAT]];\n\n$userDataObject = new UserAppIDSet([\n    "user_id" => $USER_ID,\n    "app_id" => $APP_ID\n]);\n\n// Let\'s make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client->PostModelOutputs(\n    // The request object carries the request along with the request status and other metadata related to the request itself\n    new PostModelOutputsRequest([\n        "user_app_id" => $userDataObject,\n        "model_id" => $MODEL_ID,\n        "inputs" => [\n            new Input([\n                "data" => new Data([\n                    "audio" => new Audio([\n                        "url" => $AUDIO_URL\n                    ])\n                ])\n            ])\n        ]\n    ]),\n    $metadata\n)->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception("Error: {$status->details}");\n}\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure \n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    print $response->getStatus()->getDetails();\n    throw new Exception("Failure response: " . $response->getStatus()->getDescription());\n}\n\n// We\'ll get one output for each input we used above. Because of one input, we have here one output\n$output = $response->getOutputs()[0];\n\n// Print the output\necho $output->getData()->getText()->getRaw();\n\n?>',pt='<?php\n\nrequire __DIR__ . "/vendor/autoload.php";\n\n///////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model ID, and\n// audio file location. Change these strings to run your own example.\n//////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\n$PAT = "YOUR_PAT_HERE";\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\n$USER_ID = "facebook";\n$APP_ID = "asr";\n// Change these to make your own predictions\n$MODEL_ID = "asr-wav2vec2-base-960h-english";\n$AUDIO_FILE_LOCATION = "YOUR_AUDIO_FILE_LOCATION_HERE";\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\Api\\Audio;\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\PostModelOutputsRequest;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ["Authorization" => ["Key " . $PAT]];\n\n$userDataObject = new UserAppIDSet([\n    "user_id" => $USER_ID,\n    "app_id" => $APP_ID\n]);\n\n$audioData = file_get_contents($AUDIO_FILE_LOCATION); // Get the audio bytes data from the location\n\n// Let\'s make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client->PostModelOutputs(\n    // The request object carries the request along with the request status and other metadata related to the request itself\n    new PostModelOutputsRequest([\n        "user_app_id" => $userDataObject,\n        "model_id" => $MODEL_ID,\n        "inputs" => [\n            new Input([\n                "data" => new Data([\n                    "audio" => new Audio([\n                        "base64" => $audioData\n                    ])\n                ])\n            ])\n        ]\n    ]),\n    $metadata\n)->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception("Error: {$status->details}");\n}\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure \n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    print $response->getStatus()->getDetails();\n    throw new Exception("Failure response: " . $response->getStatus()->getDescription());\n}\n\n// We\'ll get one output for each input we used above. Because of one input, we have here one output\n$output = $response->getOutputs()[0];\n\n// Print the output\necho $output->getData()->getText()->getRaw();\n\n?>',ut='curl -X POST "https://api.clarifai.com/v2/users/facebook/apps/asr/models/asr-wav2vec2-base-960h-english/outputs" \\\n  -H "authorization: Key YOUR_PAT_HERE" \\\n  -H "content-type: application/json" \\\n  -d \'{\n    "inputs": [\n        {\n          "data": {\n            "audio": {\n              "url": "https://samples.clarifai.com/negative_sentence_1.wav"\n          }\n        }\n      }\n    ]\n}\'',dt='curl -X POST "https://api.clarifai.com/v2/users/facebook/apps/asr/models/asr-wav2vec2-base-960h-english/outputs" \\\n  -H "authorization: Key YOUR_PAT_HERE" \\\n  -H "content-type: application/json" \\\n  -d \'{\n    "inputs": [\n        {\n          "data": {\n            "audio": {\n              "base64": "YOUR_BYTES_STRING_HERE"\n          }\n        }\n      }\n    ]\n}\'',ht='##############################################################################################################\n# In this section, we set the user authentication, user and app ID, model details, and input details.\n# Change these values to run your own example.\n##############################################################################################################\n\n# Your PAT (Personal Access Token) can be found in the Account\'s Security section\nPAT = "YOUR_PAT_HERE"\n# Specify the correct user_id/app_id pairings\n# Since you\'re making inferences outside your app\'s scope\nUSER_ID = "openai"\nAPP_ID = "chat-completion"\n# Change these to whatever model and inputs you want to use\nMODEL_ID = "openai-gpt-4-vision"\nMODEL_VERSION_ID = "266df29bc09843e0aee9b7bf723c03c2"\nRAW_TEXT = "Write a caption for the image"\n# To use a hosted text file, assign the URL variable\n# TEXT_FILE_URL = "https://samples.clarifai.com/negative_sentence_12.txt"\n# Or, to use a local text file, assign the location variable\n# TEXT_FILE_LOCATION = "YOUR_TEXT_FILE_LOCATION_HERE"\nIMAGE_URL = "https://samples.clarifai.com/metro-north.jpg"\n# To use a local file, assign the location variable\n# IMAGE_FILE_LOCATION = "YOUR_IMAGE_FILE_LOCATION_HERE"\n\n############################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n############################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\nfrom google.protobuf.struct_pb2 import Struct\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (("authorization", "Key " + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\n# To use a local text file, uncomment the following lines\n# with open(TEXT_FILE_LOCATION, "rb") as f:\n#    file_bytes = f.read()\n\n# To use a local image file, uncomment the following lines\n# with open(IMAGE_FILE_LOCATION, "rb") as f:\n#     file_bytes = f.read()\n\nparams = Struct()\nparams.update(\n    {\n        "temperature": 0.5,\n        "max_tokens": 2048,\n        "top_p": 0.95,\n        # "api_key": "ADD_THIRD_PARTY_KEY_HERE"\n    }\n)\n\npost_model_outputs_response = stub.PostModelOutputs(\n    service_pb2.PostModelOutputsRequest(\n        user_app_id=userDataObject,  # The userDataObject is created in the overview and is required when using a PAT\n        model_id=MODEL_ID,\n        version_id=MODEL_VERSION_ID,  # This is optional. Defaults to the latest model version\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    text=resources_pb2.Text(\n                        raw=RAW_TEXT\n                        # url=TEXT_FILE_URL\n                        # raw=file_bytes\n                    ),\n                    image=resources_pb2.Image(\n                        url=IMAGE_URL\n                        # base64=file_bytes\n                    ),\n                )\n            )\n        ],\n        model=resources_pb2.Model(\n            model_version=resources_pb2.ModelVersion(\n                output_info=resources_pb2.OutputInfo(params=params)\n            )\n        ),\n    ),\n    metadata=metadata,\n)\nif post_model_outputs_response.status.code != status_code_pb2.SUCCESS:\n    print(post_model_outputs_response.status)\n    raise Exception(f"Post model outputs failed, status: {post_model_outputs_response.status.description}")\n\n# Since we have one input, one output will exist here\noutput = post_model_outputs_response.outputs[0]\n\nprint(output.data.text.raw)\n',mt='\x3c!--index.html file--\x3e\n\n<script>\n    /////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, and input details.\n    // Change these values to run your own example.\n    /////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n    // Your PAT (Personal Access Token) can be found in the Account\'s Security section\n    const PAT = "YOUR_PAT_HERE";\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    const USER_ID = "openai";\n    const APP_ID = "chat-completion";\n    // Change these to whatever model and inputs you want to use\n    const MODEL_ID = "openai-gpt-4-vision";\n    const MODEL_VERSION_ID = "266df29bc09843e0aee9b7bf723c03c2";\n    const RAW_TEXT = "Write a caption for the image";\n    // To use a hosted text file, assign the URL variable\n    // const TEXT_FILE_URL = "https://samples.clarifai.com/negative_sentence_12.txt";\n    const IMAGE_URL = "https://samples.clarifai.com/metro-north.jpg";\n    // To use image bytes, assign its variable   \n    // const IMAGE_BYTES_STRING = "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAUDBAQEAwUEBAQFBQUGBwwIBwcHBw8LCwkMEQ8SEhEPERETFhwXExQaFRERGCEYGh0dHx8fExciJCIeJBweHx7/2wBDAQUFBQcGBw4ICA4eFBEUHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh7/wAARCAAoACgDASIAAhEBAxEB/8QAGQAAAgMBAAAAAAAAAAAAAAAAAAYDBQcE/8QAMBAAAQMDAwMDAgQHAAAAAAAAAQIDBAAFEQYSIQcTMTJBURRhCBYikSNScXKhsdH/xAAZAQACAwEAAAAAAAAAAAAAAAAFBgIDBAf/xAAtEQABAwMBBgQHAQAAAAAAAAABAgMRAAQhMQUSE0FRYQaBocEUFiJCcrHR8P/aAAwDAQACEQMRAD8A3+RYY1unSYzCS0ttZUkAgktn0q5yT7jPyDUC4wdGwycH5U2Kt9ZQ7VI1qw5PkvQy3CSVPpf7aQjuKyFH25xzn3pHn3TVNy01Hl2hyy6YdkSpKsS9sl/6RlI3rRu3dxWd6spwnAGPIJTfl925fcLaoSDHXvyo6i9SlCQrU9wKln3OyWiaDN1RAbW3kKbSd7gPtwMkH/tTWy9afuy1iPfnXMAblITwkE4yf08cn3pSbYt1uts24XH6fUbiLAuY1MWyGkLEmUW0rcCRvUpQ5CtwKQCPgi4S1ZbDe4sd9NntDEe79m3uOBLTr0IR9jzodSMqUpTu9JJ8owD7UTT4ZCfv9PbP7860m+s+HBSrejWRuz2kAxoesGYxTW/Zlpkwo1vkuSly3UgKWQUhHJUvIHsAaKTemF8XE6sWmxyZkiaZrMh1jv8ArQNpUVqB8FW0njHqx4zRVVhsph1KlKk5xQ+7uHmikaSJrQerMByet2IwvtuTLa4xv2k7Rk84H9x/esHv92d01boenLXGcuiWrFIhLlpbcaQ2/JdK3VJCkAq2pAR7Zz7YxWudY9fxNIdQbNGkR5TyX4aisNNpUMFZAzkj4NK0jq9ZpbLr0PSlzkhrlZDaQlP3P8Q4/ap3F87bPucJEkx/hHv60b2TYXLrKN5sramYECSQRk9M6c6zmJ+eb5Hi22M7cnWGIQgFLbX0zSo4PDa1YBcTgDyMjJ/qbGPabH08SJt1Uzc9QqRliGg5QySPKvgc+TyfYDmmTUWpNYz7ctxoQdPQshCktupckDJUPUcJT6DwMq8YyaQ9VL0pCS8zapcq4SVOBZmPDO8/cnknlWcDBwn4NYnPjLkQ+qE9OtOVlYpeVHDCEkkkJyT+SuQzy5Y0ru6Ez511/Efa5s1fdkOtyVurIxgdlQAA9gOKKPwolU7remU5hCGYEgo38KUv9I/0TRTDYJCWQBSF4rIN/CRgAR0iTpVD1j1g/qDqJcJqlKcjB9bcda142MpOEJAzgeMnjyTSyze5KEuNRpDoDvC0oe4X9iAeaKKFK+oya6fbOqYbDTeEiAPKpHdS3gBLYc7RQkp3ApQog+cq8nwPJrljzxnPZbUfnugn/NFFRgEVch9xKsH0H8pg6e3x3T3UC1ajaZITGkJLoS4MKbOUrzz/ACKVRRRVzVwtoQmhG1NkWu0HuI+JI8u/Kv/Z";\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    const raw = JSON.stringify({\n        "inputs": [\n            {\n                "data": {\n                    "text": {\n                        "raw": RAW_TEXT\n                        // "url": TEXT_FILE_URL\n                    },\n                    "image": {\n                        "url": IMAGE_URL\n                        // "base64": IMAGE_BYTES_STRING\n                    }\n                }\n            }\n        ],\n        "model": {\n            "model_version": {\n                "output_info": {\n                    "params": {\n                        "temperature": 0.5,\n                        "max_tokens": 2048,\n                        "top_p": 0.95,\n                        // "api_key": "ADD_THIRD_PARTY_KEY_HERE"\n                    }\n                }\n            }\n        }\n    });\n\n    const requestOptions = {\n        method: "POST",\n        headers: {\n            "Accept": "application/json",\n            "Authorization": "Key " + PAT\n        },\n        body: raw\n    };\n\n    fetch(`https://api.clarifai.com/v2/users/${USER_ID}/apps/${APP_ID}/models/${MODEL_ID}/versions/${MODEL_VERSION_ID}/outputs`, requestOptions)\n        .then(response => response.text())\n        .then(result => console.log(result))\n        .catch(error => console.log(\'error\', error));\n\n<\/script>',_t='//index.js file\n\n////////////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, and input details.\n// Change these values to run your own example.\n////////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\nconst PAT = "YOUR_PAT_HERE";\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\nconst USER_ID = "openai";\nconst APP_ID = "chat-completion";\n// Change these to whatever model and inputs you want to use\nconst MODEL_ID = "openai-gpt-4-vision";\nconst MODEL_VERSION_ID = "266df29bc09843e0aee9b7bf723c03c2";\nconst RAW_TEXT = "Write a caption for the image";\n// To use a hosted text file, assign the URL variable\n// const TEXT_FILE_URL = "https://samples.clarifai.com/negative_sentence_12.txt";\n// Or, to use a local text file, assign the location variable\n// TEXT_FILE_LOCATION = "YOUR_TEXT_FILE_LOCATION_HERE";\nconst IMAGE_URL = "https://samples.clarifai.com/metro-north.jpg";\n// To use a local file, assign the location variable;\n// const IMAGE_FILE_LOCATION = "YOUR_IMAGE_FILE_LOCATION_HERE";\n\n/////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n/////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\n// To use a local text file, uncomment the following lines\n// const fs = require("fs");\n// const fileBytes = fs.readFileSync(TEXT_FILE_LOCATION);\n\n// To use a local image file, uncomment the following lines\n// const fs = require("fs");\n// const imageBytes = fs.readFileSync(IMAGE_FILE_LOCATION);\n\nstub.PostModelOutputs(\n    {\n        user_app_id: {\n            "user_id": USER_ID,\n            "app_id": APP_ID\n        },\n        model_id: MODEL_ID,\n        version_id: MODEL_VERSION_ID,  // This is optional. Defaults to the latest model version\n        inputs: [\n            {\n                "data": {\n                    "text": {\n                        "raw": RAW_TEXT,\n                        // url: TEXT_FILE_URL,\n                        // raw: fileBytes\n                    },\n                    "image": {\n                        "url": IMAGE_URL,\n                        // base64: imageBytes                      \n                    }\n                }\n            }\n        ],\n        model: {\n            "model_version": {\n                "output_info": {\n                    "params": {\n                        "temperature": 0.5,\n                        "max_tokens": 2048,\n                        "top_p": 0.95\n                        // "api_key": "ADD_THIRD_PARTY_KEY_HERE"\n                    }\n                }\n            }\n        }\n    },\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            throw new Error("Post models failed, status: " + response.status.description);\n        }\n        // Since we have one input, one output will exist here.\n        const output = response.outputs[0];\n\n        console.log(output.data.text.raw);\n    }\n);\n',ft='package com.clarifai.example;\n\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.status.StatusCode;\nimport com.google.protobuf.Struct;\nimport com.google.protobuf.Value;\n\nimport com.google.protobuf.ByteString;\nimport java.io.File;\nimport java.io.IOException;\nimport java.nio.file.Files;\n\npublic class ClarifaiExample {\n\n    /////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, and input details.\n    // Change these values to run your own example.\n    /////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n    // Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    static final String USER_ID = "openai";\n    static final String APP_ID = "chat-completion";\n    // Change these to whatever model and inputs you want to use\n    static final String MODEL_ID = "openai-gpt-4-vision";\n    static final String MODEL_VERSION_ID = "266df29bc09843e0aee9b7bf723c03c2";\n    static final String RAW_TEXT = "Write a caption for the image";\n    // To use a hosted text file, assign the URL variable\n    // static final String TEXT_FILE_URL = "https://samples.clarifai.com/negative_sentence_12.txt";\n    // Or, to use a local text file, assign the location variable\n    // static final String TEXT_FILE_LOCATION = "YOUR_TEXT_FILE_LOCATION_HERE";\n    static final String IMAGE_URL = "https://samples.clarifai.com/metro-north.jpg";\n    // Or, to use a local image file, assign the location variable\n    //static final String IMAGE_FILE_LOCATION = "YOUR_IMAGE_FILE_LOCATION_HERE";\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\t\n\n    public static void main(String[] args) throws IOException {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n                .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        Struct.Builder params = Struct.newBuilder()\n                .putFields("temperature", Value.newBuilder().setNumberValue(0.5).build())\n                .putFields("max_tokens", Value.newBuilder().setNumberValue(2048).build())\n                .putFields("top_p", Value.newBuilder().setNumberValue(0.95).build());\n               // .putFields("api_key", Value.newBuilder().setNumberValue("ADD_THIRD_PARTY_KEY_HERE").build());\n\n        MultiOutputResponse postModelOutputsResponse = stub.postModelOutputs(\n                PostModelOutputsRequest.newBuilder()\n                        .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n                        .setModelId(MODEL_ID)\n                        .setVersionId(MODEL_VERSION_ID) // This is optional. Defaults to the latest model version\n                        .addInputs(\n                                Input.newBuilder().setData(\n                                        Data.newBuilder().setText(\n                                                Text.newBuilder().setRaw(RAW_TEXT)\n                                        // Text.newBuilder().setUrl(TEXT_FILE_URL)\n                                        // Text.newBuilder().setRawBytes(ByteString.copyFrom(Files.readAllBytes(\n                                        // new File(TEXT_FILE_LOCATION).toPath()\n                                        // )))\n                                        )\n                                                .setImage(\n                                                        Image.newBuilder().setUrl(IMAGE_URL)\n                                                // Image.newBuilder().setBase64(ByteString.copyFrom(Files.readAllBytes(\n                                                // new File(IMAGE_FILE_LOCATION).toPath()\n                                                // )))\n                                                )\n                                )\n                        )\n                        .setModel(Model.newBuilder()\n                                .setModelVersion(ModelVersion.newBuilder()\n                                        .setOutputInfo(OutputInfo.newBuilder().setParams(params))\n                                )\n                        )\n                        .build()\n        );\n\n        if (postModelOutputsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("Post model outputs failed, status: " + postModelOutputsResponse.getStatus());\n        }\n\n        // Since we have one input, one output will exist here\n        Output output = postModelOutputsResponse.getOutputs(0);\n\n        System.out.println(output.getData().getText().getRaw());\n\n    }\n}\n',gt='<?php\n\nrequire __DIR__ . "/vendor/autoload.php";\n\n///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, and input details.\n// Change these values to run your own example.\n///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\n$PAT = "YOUR_PAT_HERE";\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\n$USER_ID = "openai";\n$APP_ID = "chat-completion";\n// Change these to whatever model and inputs you want to use\n$MODEL_ID = "openai-gpt-4-vision";\n$MODEL_VERSION_ID = "266df29bc09843e0aee9b7bf723c03c2";\n$RAW_TEXT = "Write a caption for the image";\n// To use a hosted text file, assign the URL variable\n// $TEXT_FILE_URL = "https://samples.clarifai.com/negative_sentence_12.txt";\n// Or, to use a local text file, assign the location variable\n// $TEXT_FILE_LOCATION = "YOUR_TEXT_FILE_LOCATION_HERE";\n$IMAGE_URL = "https://samples.clarifai.com/metro-north.jpg";\n// To use a local file, assign the location variable\n// $IMAGE_FILE_LOCATION = "YOUR_IMAGE_FILE_LOCATION_HERE";\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Text;\nuse Clarifai\\Api\\Image;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\Model;\nuse Clarifai\\Api\\ModelVersion;\nuse Clarifai\\Api\\OutputInfo;\nuse Clarifai\\Api\\PostModelOutputsRequest;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\nuse Google\\Protobuf\\Struct;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ["Authorization" => ["Key " . $PAT]];\n\n$userDataObject = new UserAppIDSet([\n    "user_id" => $USER_ID,\n    "app_id" => $APP_ID,\n]);\n\n// Create Struct instance\n$params = new Struct();\n$params->temperature = 0.5;\n$params->max_tokens = 2048;\n$params->top_p = 0.95;\n// $params->api_key = "ADD_THIRD_PARTY_KEY_HERE";\n\n// To use a local text file, uncomment the following lines\n// $textData = file_get_contents($TEXT_FILE_LOCATION); // Get the text bytes data from the location\n\n// To use a local image file, uncomment the following lines\n// $imageData = file_get_contents($IMAGE_FILE_LOCATION);\n\n// Let\'s make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client->PostModelOutputs(\n    // The request object carries the request along with the request status and other metadata related to the request itself\n    new PostModelOutputsRequest([\n        "user_app_id" => $userDataObject,\n        "model_id" => $MODEL_ID,\n        "version_id" => $MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n        "inputs" => [\n            new Input([\n                // The Input object wraps the Data object in order to meet the API specification\n                "data" => new Data([\n                    // The Data object is constructed around the Text object. It offers a container that has additional text independent\n                    // metadata. In this particular use case, no other metadata is needed to be specified\n                    "text" => new Text([\n                        // In the Clarifai platform, a text is defined by a special Text object\n                        "raw" => $RAW_TEXT,\n                        // "url" => $TEXT_FILE_URL\n                        // "raw" => $textData\n                    ]),\n                    "image" => new Image([\n                        // In the Clarifai platform, an image is defined by a special Image object\n                        "url" => $IMAGE_URL,\n                        // "base64" => $imageData,\n                    ]),\n                ]),\n            ]),\n        ],\n        "model" => new Model([\n            "model_version" => new ModelVersion([\n                "output_info" => new OutputInfo(["params" => $params]),\n            ]),\n        ]),\n    ]),\n    $metadata\n)->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception("Error: {$status->details}");\n}\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure\n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    throw new Exception("Failure response: " . $response->getStatus()->getDescription() . " " . $response->getStatus()->getDetails() );\n}\n\n# Since we have one input, one output will exist here\necho $response->getOutputs()[0]->getData()->getText()->getRaw();\n\n?>',It='curl -X POST "https://api.clarifai.com/v2/users/openai/apps/chat-completion/models/openai-gpt-4-vision/versions/266df29bc09843e0aee9b7bf723c03c2/outputs" \\\n    -H "Authorization: Key YOUR_PAT_HERE" \\\n    -H "Content-Type: application/json" \\\n    -d \'{\n    "inputs": [\n        {\n            "data": {\n                "text": {\n                    "raw": "Write a caption for the image"\n                },\n                "image": {\n                    "url": "https://samples.clarifai.com/metro-north.jpg"\n                }\n            }\n        }\n    ],\n    "model": {\n        "model_version": {\n            "output_info": {\n                "params": {\n                    "temperature": 0.5,\n                    "max_tokens": 2048,\n                    "top_p": 0.95,\n                    "api_key": "ADD_THIRD_PARTY_KEY_HERE"\n                }\n            }\n        }\n    }\n}\'\n',At='#################################################################################################################\n# In this section, we set the user authentication, user and app ID, model details, and the text we want\n# to provide as an input. Change these strings to run your own example.\n#################################################################################################################\n\n# Your PAT (Personal Access Token) can be found in the Account\'s Security section\nPAT = "YOUR_PAT_HERE"\n# Specify the correct user_id/app_id pairings\n# Since you\'re making inferences outside your app\'s scope\nUSER_ID = "cohere"\nAPP_ID = "embed"\n# Change these to whatever model and text URL you want to use\nMODEL_ID = "cohere-embed-english-v3_0"\nMODEL_VERSION_ID = "e2dd848faf454fbda85c26cf89c4926e"\nRAW_TEXT = "Give me an exotic yet tasty recipe for some noodle dish"\n# To use a hosted text file, assign the URL variable\n# TEXT_FILE_URL = "https://samples.clarifai.com/negative_sentence_12.txt"\n# Or, to use a local text file, assign the location variable\n# TEXT_FILE_LOCATION = "YOUR_TEXT_FILE_LOCATION_HERE"\n\n############################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n############################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\nfrom google.protobuf.struct_pb2 import Struct\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nparams = Struct()\nparams.update(\n    {\n        "input_type": "search_query" \n    }\n)\n\nmetadata = (("authorization", "Key " + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\n# To use a local text file, uncomment the following lines\n# with open(TEXT_FILE_LOCATION, "rb") as f:\n#    file_bytes = f.read()\n\npost_model_outputs_response = stub.PostModelOutputs(\n    service_pb2.PostModelOutputsRequest(\n        user_app_id=userDataObject,  # The userDataObject is created in the overview and is required when using a PAT\n        model_id=MODEL_ID,\n        version_id=MODEL_VERSION_ID,  # This is optional. Defaults to the latest model version\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    text=resources_pb2.Text(\n                        raw=RAW_TEXT\n                        # url=TEXT_FILE_URL\n                        # raw=file_bytes\n                    )\n                )\n            )\n        ],\n        model=resources_pb2.Model(\n            model_version=resources_pb2.ModelVersion(\n                output_info=resources_pb2.OutputInfo(params=params)\n            )\n        ),\n    ),\n    metadata=metadata,\n)\nif post_model_outputs_response.status.code != status_code_pb2.SUCCESS:\n    print(post_model_outputs_response.status)\n    raise Exception("Post model outputs failed, status: " + post_model_outputs_response.status.description )\n\n# Uncomment this line to print the raw output\n# print(post_model_outputs_response)\n\n# Since we have one input, one output will exist here\noutput = post_model_outputs_response.outputs[0].data.embeddings\nprint(output)\n',Et='\x3c!--index.html file--\x3e\n\n<script>\n    //////////////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, and the text we want\n    // to provide as an input. Change these strings to run your own example.\n    //////////////////////////////////////////////////////////////////////////////////////////////////////////////\n  \n    // Your PAT (Personal Access Token) can be found in the Account\'s Security section\n    const PAT = "YOUR_PAT_HERE";    \n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    const USER_ID = "cohere";\n    const APP_ID = "embed";\n    // Change these to whatever model and text you want to use\n    const MODEL_ID = "cohere-embed-english-v3_0";\n    const MODEL_VERSION_ID = "e2dd848faf454fbda85c26cf89c4926e";\n    const RAW_TEXT = "Give me an exotic yet tasty recipe for some noodle dish";\n    // To use a hosted text file, assign the URL variable\n    // const TEXT_FILE_URL = \'https://samples.clarifai.com/negative_sentence_12.txt\'\n    \n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n  \n    const raw = JSON.stringify({  \n        "inputs": [\n        {\n            "data": {\n              "text": {\n                   "raw": RAW_TEXT\n                  // "url": TEXT_FILE_URL\n               }\n            }\n        }\n    ],  \n      "model": {\n          "model_version": {\n              "output_info": {\n                  "params": {\n                      "input_type": "search_query"   \n                  }\n              }\n          }\n      }\n    });\n  \n    const requestOptions = {\n        method: "POST",\n        headers: {\n            "Accept": "application/json",\n            "Authorization": "Key " + PAT\n        },\n        body: raw\n    };\n  \n    fetch(`https://api.clarifai.com/v2/users/${USER_ID}/apps/${APP_ID}/models/${MODEL_ID}/versions/${MODEL_VERSION_ID}/outputs`, requestOptions)\n        .then(response => response.text())\n        .then(result => console.log(result))    \n        .catch(error => console.log("error", error));\n  \n  <\/script>',Dt='//index.js file\n\n////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, and the text we want\n// to provide as an input. Change these strings to run your own example.\n///////////////////////////////////////////////////////////////////////////////////////////////////////////////// \n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\nconst PAT = "YOUR_PAT_HERE";\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\nconst USER_ID = "cohere";\nconst APP_ID = "embed";\n// Change these to whatever model and text you want to use\nconst MODEL_ID = "cohere-embed-english-v3_0";\nconst MODEL_VERSION_ID = "e2dd848faf454fbda85c26cf89c4926e";\nconst RAW_TEXT = "Give me an exotic yet tasty recipe for some noodle dish";\n// To use a hosted text file, assign the URL variable\n// const TEXT_FILE_URL = "https://samples.clarifai.com/negative_sentence_12.txt"\n// Or, to use a local text file, assign the location variable\n// TEXT_FILE_LOCATION = "YOUR_TEXT_FILE_LOCATION_HERE"\n\n/////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n/////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\n// To use a local text file, uncomment the following lines\n// const fs = require("fs");\n// const fileBytes = fs.readFileSync(TEXT_FILE_LOCATION);\n\nstub.PostModelOutputs(\n    {\n        user_app_id: {\n            "user_id": USER_ID,\n            "app_id": APP_ID\n        },\n        model_id: MODEL_ID,\n        version_id: MODEL_VERSION_ID,  // This is optional. Defaults to the latest model version\n        inputs: [\n            {\n                "data": {\n                    "text": {\n                        "raw": RAW_TEXT\n                        // url: TEXT_FILE_URL\n                        // raw: fileBytes\n                    }\n                }\n            }\n        ],\n        "model": {\n            "model_version": {\n                "output_info": {\n                    "params": {\n                        "input_type": "search_query"   \n                    }\n                }\n            }\n        }\n    },\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            throw new Error("Post models failed, status: " + response.status.description);\n        }\n        // Since we have one input, one output will exist here.\n        const output = response.outputs[0].data.embeddings;\n\n        console.log(output);\n\n    }\n);',Tt='package com.clarifai.example;\n\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.status.StatusCode;\nimport com.google.protobuf.Struct;\nimport com.google.protobuf.Value;\n\nimport com.google.protobuf.ByteString;\nimport java.io.File;\nimport java.io.IOException;\nimport java.nio.file.Files;\n\npublic class ClarifaiExample {\n\n    ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, and the text we want\n    // to provide as an input. Change these strings to run your own example.\n    //////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n    // Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    static final String USER_ID = "cohere";\n    static final String APP_ID = "embed";\n    // Change these to whatever model you want to use\n    static final String MODEL_ID = "cohere-embed-english-v3_0";\n    static final String MODEL_VERSION_ID = "e2dd848faf454fbda85c26cf89c4926e";\n    static final String RAW_TEXT = "Give me an exotic yet tasty recipe for some noodle dish";\n    // To use a hosted text file, assign the URL variable\n    // static final String TEXT_FILE_URL = "https://samples.clarifai.com/negative_sentence_12.txt";\n    // Or, to use a local text file, assign the location variable\n    // static final String TEXT_FILE_LOCATION = "YOUR_TEXT_FILE_LOCATION_HERE";\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\t\n\n    public static void main(String[] args) throws IOException {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n                .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        Struct.Builder params = Struct.newBuilder()\n                .putFields("input_type", Value.newBuilder().setStringValue("search_query").build());\n\n        MultiOutputResponse postModelOutputsResponse = stub.postModelOutputs(\n                PostModelOutputsRequest.newBuilder()\n                        .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n                        .setModelId(MODEL_ID)\n                        .setVersionId(MODEL_VERSION_ID) // This is optional. Defaults to the latest model version.\n                        .addInputs(\n                                Input.newBuilder().setData(\n                                        Data.newBuilder().setText(\n                                                Text.newBuilder().setRaw(RAW_TEXT)\n                                        // Text.newBuilder().setUrl(TEXT_FILE_URL)\n                                        // Text.newBuilder().setRawBytes(ByteString.copyFrom(Files.readAllBytes(\n                                        // new File(TEXT_FILE_LOCATION).toPath()\n                                        // )))\n                                        )\n                                )\n                        )\n                        .setModel(Model.newBuilder()\n                                .setModelVersion(ModelVersion.newBuilder()\n                                        .setOutputInfo(OutputInfo.newBuilder().setParams(params))\n                                )\n                        )\n                        .build()\n        );\n\n        if (postModelOutputsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("Post model outputs failed, status: " + postModelOutputsResponse.getStatus());\n        }\n\n        // Since we have one input, one output will exist here\n        Output output = postModelOutputsResponse.getOutputs(0);\n\n        System.out.println(output.getData().getEmbeddingsList());\n\n    }\n}\n',Ot='<?php\n\nrequire __DIR__ . "/vendor/autoload.php";\n\n//////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, and the text we want\n// to provide as an input. Change these strings to run your own example.\n//////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\n$PAT = "YOUR_PAT_HERE";\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\n$USER_ID = "cohere";\n$APP_ID = "embed";\n// Change these to whatever model and image URL you want to use\n$MODEL_ID = "cohere-embed-english-v3_0";\n$MODEL_VERSION_ID = "e2dd848faf454fbda85c26cf89c4926e";\n$RAW_TEXT = "Give me an exotic yet tasty recipe for some noodle dish!";\n# To use a hosted text file, assign the URL variable\n# $TEXT_FILE_URL = "https://samples.clarifai.com/negative_sentence_12.txt";\n# Or, to use a local text file, assign the location variable\n# $TEXT_FILE_LOCATION = "YOUR_TEXT_FILE_LOCATION_HERE";\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Text;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\Model;\nuse Clarifai\\Api\\ModelVersion;\nuse Clarifai\\Api\\OutputInfo;\nuse Clarifai\\Api\\PostModelOutputsRequest;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\nuse Google\\Protobuf\\Struct;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ["Authorization" => ["Key " . $PAT]];\n\n$userDataObject = new UserAppIDSet([\n    "user_id" => $USER_ID,\n    "app_id" => $APP_ID,\n]);\n\n// create Struct instance\n$params = new Struct();\n$params->input_type = "search_query";\n\n//$textData = file_get_contents($TEXT_FILE_LOCATION); // Get the text bytes data from the location\n\n// Let\'s make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client->PostModelOutputs(\n        // The request object carries the request along with the request status and other metadata related to the request itself\n        new PostModelOutputsRequest([\n            "user_app_id" => $userDataObject,\n            "model_id" => $MODEL_ID,\n            "version_id" => $MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n            "inputs" => [\n                new Input([\n                    // The Input object wraps the Data object in order to meet the API specification\n                    "data" => new Data([\n                        // The Data object is constructed around the Text object. It offers a container that has additional text independent\n                        // metadata. In this particular use case, no other metadata is needed to be specified\n                        "text" => new Text([\n                            // In the Clarifai platform, a text is defined by a special Text object\n                            "raw" => $RAW_TEXT,\n                            // "url" => $TEXT_FILE_URL\n                            // "raw" => $textData\n                        ]),\n                    ]),\n                ]),\n            ],\n            "model" => new Model([\n                "model_version" => new ModelVersion([\n                    "output_info" => new OutputInfo(["params" => $params]),\n                ]),\n            ]),\n        ]),\n        $metadata\n    )\n    ->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception("Error: {$status->details}");\n}\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure\n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    throw new Exception("Failure response: " . $response->getStatus()->getDescription() . " " . $response->getStatus()->getDetails());\n}\n\n// Since we have one input, one output will exist here\n$output = $response->getOutputs()[0]->getData()->getEmbeddings();\n\nprint_r($output);\n\n?>\n',bt='curl -X POST "https://api.clarifai.com/v2/users/cohere/apps/embed/models/cohere-embed-english-v3_0/versions/e2dd848faf454fbda85c26cf89c4926e/outputs" \\\n    -H "Authorization: Key YOUR_PAT_HERE" \\\n    -H "Content-Type: application/json" \\\n    -d \'{\n    "inputs": [\n        {\n            "data": {\n                "text": {\n                    "raw": "Give me an exotic yet tasty recipe for some noodle dish"\n                }\n            }\n        }\n    ],\n    "model": {\n        "model_version": {\n            "output_info": {\n                "params": {\n                    "input_type":"search_query"\n                }\n            }\n        }\n    }\n}\'\n',yt='curl -X POST "https://api.clarifai.com/v2/users/meta/apps/Llama-3/models/Llama-3_2-3B-Instruct/versions/52528868e11d431fa0450f00b22af18c/outputs/generate" \\\n  -H "Authorization: Key YOUR_PAT_HERE" \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "inputs": [\n      {\n        "data": {\n          "text": {\n            "raw": "What is the future of AI?"\n          }\n        }\n      }\n    ]\n  }\'\n',wt={description:"Generate predictions using our older method",sidebar_position:3,toc_max_heading_level:4},St="Legacy Inference via API",Rt={},Pt=[{value:"Legacy Inference via Compute Orchestration",id:"legacy-inference-via-compute-orchestration",level:2},{value:"Unary-Unary Predict Call",id:"unary-unary-predict-call",level:3},{value:"Unary-Stream Predict Call",id:"unary-stream-predict-call",level:3},{value:"Stream-Stream Predict Call",id:"stream-stream-predict-call",level:3},{value:"Legacy Inference via Traditional Methods",id:"legacy-inference-via-traditional-methods",level:2},{value:"Image as Input",id:"image-as-input",level:3},{value:"Visual Classifier",id:"visual-classifier",level:4},{value:"Predict via URL",id:"predict-via-url",level:5},{value:"Predict via Bytes",id:"predict-via-bytes",level:5},{value:"Predict Multiple Inputs",id:"predict-multiple-inputs",level:4},{value:"Visual Detector - Image",id:"visual-detector---image",level:4},{value:"Visual Segmenter",id:"visual-segmenter",level:4},{value:"Image-to-Text",id:"image-to-text",level:4},{value:"Image-to-Image",id:"image-to-image",level:4},{value:"Visual Embedder",id:"visual-embedder",level:4},{value:"Video as Input",id:"video-as-input",level:3},{value:"Visual Detector - Video",id:"visual-detector---video",level:4},{value:"Predict via URL",id:"predict-via-url-1",level:5},{value:"Predict via Bytes",id:"predict-via-bytes-1",level:5},{value:"Text as Input",id:"text-as-input",level:3},{value:"Text Classifier",id:"text-classifier",level:4},{value:"Predict via URL",id:"predict-via-url-2",level:5},{value:"Predict via Local Files",id:"predict-via-local-files",level:5},{value:"Predict via Raw Text",id:"predict-via-raw-text",level:5},{value:"Text Generation Using LLMs",id:"text-generation-using-llms",level:4},{value:"Set Inference Parameters",id:"set-inference-parameters",level:4},{value:"Text Classification Using LLMs",id:"text-classification-using-llms",level:4},{value:"Text-to-Image",id:"text-to-image",level:4},{value:"Text-to-Audio",id:"text-to-audio",level:4},{value:"Text Embedder",id:"text-embedder",level:4},{value:"Audio as Input",id:"audio-as-input",level:3},{value:"Audio-to-Text",id:"audio-to-text",level:4},{value:"Predict via URL",id:"predict-via-url-3",level:5},{value:"Predict via Bytes",id:"predict-via-bytes-2",level:5},{value:"MultiModal as Input",id:"multimodal-as-input",level:3},{value:"[Image,Text]-to-Text",id:"imagetext-to-text",level:4},{value:"Predict via Image URL",id:"predict-via-image-url",level:5},{value:"Predict via Local Image",id:"predict-via-local-image",level:5},{value:"Use Third-Party API Keys",id:"use-third-party-api-keys",level:3}];function vt(e){const n={a:"a",admonition:"admonition",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",h5:"h5",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components},{Details:t}=n;return t||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"legacy-inference-via-api",children:"Legacy Inference via API"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Generate predictions using our older method"})}),"\n",(0,i.jsx)("hr",{}),"\n",(0,i.jsx)(n.p,{children:"The legacy inference technique uses our previous API structure and is best suited for models built using the older techniques."}),"\n",(0,i.jsxs)(n.p,{children:["While this method remains functional, we recommend transitioning to the ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/compute/models/inference/api",children:"new inference method"})," for improved efficiency, scalability, and access to the latest features."]}),"\n",(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsxs)(n.p,{children:["Before using the ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/additional-resources/api-overview/python-sdk",children:"Python SDK"}),", ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/additional-resources/api-overview/cli",children:"CLI"}),", ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/additional-resources/api-overview/nodejs-sdk",children:"Node.js SDK"}),", or any of our ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/additional-resources/api-overview/grpc-clients",children:"gRPC clients"}),", ensure they are properly installed on your machine. Refer to their respective installation guides for instructions on how to install and initialize them."]})}),"\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n",(0,i.jsx)(n.h2,{id:"legacy-inference-via-compute-orchestration",children:"Legacy Inference via Compute Orchestration"}),"\n",(0,i.jsx)(n.admonition,{type:"note",children:(0,i.jsxs)(n.p,{children:["Before making a prediction using our Compute Orchestration capabilities,  ensure that your model has been ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/compute/deployments/deploy-model",children:"deployed"}),", ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/compute/models/inference/",children:"as explained here"}),"."]})}),"\n",(0,i.jsx)(n.h3,{id:"unary-unary-predict-call",children:"Unary-Unary Predict Call"}),"\n",(0,i.jsx)(n.p,{children:"This is the simplest type of prediction. In this method, a single input is sent to the model, and it returns a single response. This is ideal for tasks where a quick, non-streaming prediction is required, such as classifying an image."}),"\n",(0,i.jsx)(n.p,{children:"It supports the following prediction methods:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"predict_by_url"}),"  \u2014 Use a publicly accessible URL for the input."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"predict_by_bytes"})," \u2014 Pass raw input data directly."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"predict_by_filepath"})," \u2014 Provide the local file path for the input."]}),"\n"]}),"\n",(0,i.jsxs)(o.A,{groupId:"code",children:[(0,i.jsx)(r.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:c})}),(0,i.jsx)(r.A,{value:"bash",label:"CLI",children:(0,i.jsx)(l.A,{className:"language-yaml",children:p})})]}),"\n",(0,i.jsxs)(t,{children:[(0,i.jsx)("summary",{children:"Example Output"}),(0,i.jsx)(l.A,{className:"language-text",children:h})]}),"\n",(0,i.jsx)(n.h3,{id:"unary-stream-predict-call",children:"Unary-Stream Predict Call"}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.strong,{children:"Unary-Stream"})," predict call processes a single input, but returns a stream of responses. It is particularly useful for tasks where multiple outputs are generated from a single input, such as generating text completions from a prompt."]}),"\n",(0,i.jsx)(n.p,{children:"It supports the following prediction methods:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"generate_by_url"}),"  \u2014 Provide a publicly accessible URL and handle the streamed responses iteratively."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"generate_by_bytes"})," \u2014 Use raw input data."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"generate_by_filepath"})," \u2014 Use a local file path for the input."]}),"\n"]}),"\n",(0,i.jsxs)(o.A,{groupId:"code",children:[(0,i.jsx)(r.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:u})}),(0,i.jsx)(r.A,{value:"curl",label:"cURL",children:(0,i.jsx)(l.A,{className:"language-bash",children:yt})})]}),"\n",(0,i.jsxs)(t,{children:[(0,i.jsx)("summary",{children:"Example Output"}),(0,i.jsx)(l.A,{className:"language-text",children:m})]}),"\n",(0,i.jsx)(n.h3,{id:"stream-stream-predict-call",children:"Stream-Stream Predict Call"}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.strong,{children:"stream-stream"})," predict call enables bidirectional streaming of both inputs and outputs, making it highly effective for processing large datasets or real-time applications."]}),"\n",(0,i.jsx)(n.p,{children:"In this setup, multiple inputs can be continuously sent to the model, and the corresponding multiple predictions are streamed back in real-time. This is ideal for tasks like real-time video processing/predictions or live sensor data analysis."}),"\n",(0,i.jsx)(n.p,{children:"It supports the following prediction methods:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"stream_by_url"})," \u2014 Stream a list of publicly accessible URLs and receive a stream of predictions. It takes an iterator of inputs and returns a stream of predictions."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"stream_by_bytes"})," \u2014 Stream raw input data."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"stream_by_filepath"})," \u2014 Stream inputs from local file paths."]}),"\n"]}),"\n",(0,i.jsx)(o.A,{groupId:"code",children:(0,i.jsx)(r.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:d})})}),"\n",(0,i.jsx)(n.h2,{id:"legacy-inference-via-traditional-methods",children:"Legacy Inference via Traditional Methods"}),"\n",(0,i.jsx)(n.h3,{id:"image-as-input",children:"Image as Input"}),"\n",(0,i.jsxs)(n.admonition,{type:"tip",children:[(0,i.jsxs)(n.p,{children:["When you take an image with a digital device (such as a smartphone camera) the image's meta-information (such as the orientation value for how the camera is held) is stored in the image's ",(0,i.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Exif",children:"Exif's data"}),". And when you use a photo viewer to check the image on your computer, the photo viewer will respect that orientation value and automatically rotate the image to present it the way it was viewed. This allows you to see a correctly-oriented image no matter how the camera was held."]}),(0,i.jsx)(n.p,{children:"So, when you want to make predictions from an image taken with a digital device, you need to strip the Exif data from the image. Since the Clarifai platform does not account for the Exif data, removing it allows you to make accurate predictions using images in their desired rotation."})]}),"\n",(0,i.jsx)(n.h4,{id:"visual-classifier",children:"Visual Classifier"}),"\n",(0,i.jsxs)(n.p,{children:["You can use a ",(0,i.jsx)(n.a,{href:"https://clarifai.com/explore/models?page=1&perPage=24&filterData=%5B%7B%22field%22%3A%22model_type_id%22%2C%22value%22%3A%5B%22visual-classifier%22%5D%7D%5D",children:"visual classifier"})," model to categorize images into predefined classes based on their visual content. You can provide image data either through URLs or by uploading files."]}),"\n",(0,i.jsx)(n.h5,{id:"predict-via-url",children:"Predict via URL"}),"\n",(0,i.jsx)(n.admonition,{type:"note",children:(0,i.jsxs)(n.p,{children:["You can send up to 128 images in a single API call, with each image file sized under 20MB. Learn more ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/create-manage/inputs/upload/#upload-limits",children:"here"}),"."]})}),"\n",(0,i.jsxs)(o.A,{groupId:"code",children:[(0,i.jsx)(r.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:_})}),(0,i.jsx)(r.A,{value:"typescript",label:"Node.js SDK",children:(0,i.jsx)(l.A,{className:"language-typescript",children:f})}),(0,i.jsx)(r.A,{value:"bash",label:"CLI",children:(0,i.jsx)(l.A,{className:"language-bash",children:N})}),(0,i.jsx)(r.A,{value:"python2",label:"Python (gRPC)",children:(0,i.jsx)(l.A,{className:"language-python",children:De})}),(0,i.jsx)(r.A,{value:"js_rest",label:"JavaScript (REST)",children:(0,i.jsx)(l.A,{className:"language-javascript",children:Se})}),(0,i.jsx)(r.A,{value:"nodejs",label:"Node.js (gRPC)",children:(0,i.jsx)(l.A,{className:"language-javascript",children:Ce})}),(0,i.jsx)(r.A,{value:"java",label:"Java (gRPC)",children:(0,i.jsx)(l.A,{className:"language-java",children:je})}),(0,i.jsx)(r.A,{value:"php",label:"PHP (gRPC)",children:(0,i.jsx)(l.A,{className:"language-php",children:$e})}),(0,i.jsx)(r.A,{value:"curl",label:"cURL",children:(0,i.jsx)(l.A,{className:"language-bash",children:Xe})})]}),"\n",(0,i.jsxs)(t,{children:[(0,i.jsx)("summary",{children:"Output"}),(0,i.jsx)(l.A,{className:"language-text",children:P})]}),"\n",(0,i.jsx)(n.h5,{id:"predict-via-bytes",children:"Predict via Bytes"}),"\n",(0,i.jsx)(n.p,{children:"Below is an example of how you would send the bytes of an image and receive model predictions."}),"\n",(0,i.jsxs)(o.A,{groupId:"code",children:[(0,i.jsx)(r.A,{value:"python",label:"Python (gRPC)",children:(0,i.jsx)(l.A,{className:"language-python",children:Oe})}),(0,i.jsx)(r.A,{value:"js_rest",label:"JavaScript (REST)",children:(0,i.jsx)(l.A,{className:"language-javascript",children:Re})}),(0,i.jsx)(r.A,{value:"nodejs",label:"Node.js (gRPC)",children:(0,i.jsx)(l.A,{className:"language-javascript",children:Le})}),(0,i.jsx)(r.A,{value:"java",label:"Java (gRPC)",children:(0,i.jsx)(l.A,{className:"language-java",children:He})}),(0,i.jsx)(r.A,{value:"php",label:"PHP (gRPC)",children:(0,i.jsx)(l.A,{className:"language-php",children:Fe})}),(0,i.jsx)(r.A,{value:"curl",label:"cURL",children:(0,i.jsx)(l.A,{className:"language-bash",children:Qe})})]}),"\n",(0,i.jsx)(n.h4,{id:"predict-multiple-inputs",children:"Predict Multiple Inputs"}),"\n",(0,i.jsx)(n.p,{children:"To predict multiple inputs at once and avoid the need for numerous API calls, you can use the following approach."}),"\n",(0,i.jsx)(n.p,{children:"Note that these examples are provided for cURL and Python, but the same concept is applicable to any supported programming language."}),"\n",(0,i.jsxs)(o.A,{groupId:"code",children:[(0,i.jsx)(r.A,{value:"curl",label:"cURL",children:(0,i.jsx)(l.A,{className:"language-bash",children:ze})}),(0,i.jsx)(r.A,{value:"python",label:"Python (gRPC)",children:(0,i.jsx)(l.A,{className:"language-python",children:Te})})]}),"\n",(0,i.jsx)(n.h4,{id:"visual-detector---image",children:"Visual Detector - Image"}),"\n",(0,i.jsxs)(n.p,{children:["Unlike image classification, which assigns a single label to an entire image, a ",(0,i.jsx)(n.a,{href:"https://clarifai.com/explore/models?page=1&perPage=24&filterData=%5B%7B%22field%22%3A%22model_type_id%22%2C%22value%22%3A%5B%22visual-detector%22%5D%7D%5D",children:"visual detector"})," model identifies and outlines multiple objects or regions within an image, associating each with specific classes or labels."]}),"\n",(0,i.jsx)(n.p,{children:"You can provide input images either through URLs or by uploading files."}),"\n",(0,i.jsxs)(o.A,{groupId:"code",children:[(0,i.jsx)(r.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:g})}),(0,i.jsx)(r.A,{value:"typescript",label:"Node.js SDK",children:(0,i.jsx)(l.A,{className:"language-typescript",children:I})}),(0,i.jsx)(r.A,{value:"bash",label:"CLI",children:(0,i.jsx)(l.A,{className:"language-bash",children:M})}),(0,i.jsx)(r.A,{value:"python2",label:"Python (gRPC)",children:(0,i.jsx)(l.A,{className:"language-python",children:be})}),(0,i.jsx)(r.A,{value:"js_rest",label:"JavaScript (REST)",children:(0,i.jsx)(l.A,{className:"language-javascript",children:Pe})}),(0,i.jsx)(r.A,{value:"nodejs",label:"Node.js (gRPC)",children:(0,i.jsx)(l.A,{className:"language-javascript",children:Ue})}),(0,i.jsx)(r.A,{value:"java",label:"Java  (gRPC)",children:(0,i.jsx)(l.A,{className:"language-java",children:Be})}),(0,i.jsx)(r.A,{value:"php",label:"PHP  (gRPC)",children:(0,i.jsx)(l.A,{className:"language-php",children:Ge})}),(0,i.jsx)(r.A,{value:"curl",label:"cURL",children:(0,i.jsx)(l.A,{className:"language-bash",children:Ke})})]}),"\n",(0,i.jsxs)(t,{children:[(0,i.jsx)("summary",{children:"Output"}),(0,i.jsx)(l.A,{className:"language-text",children:v})]}),"\n",(0,i.jsx)(n.h4,{id:"visual-segmenter",children:"Visual Segmenter"}),"\n",(0,i.jsxs)(n.p,{children:["You can use a ",(0,i.jsx)(n.a,{href:"https://clarifai.com/explore/models?page=1&perPage=24&filterData=%5B%7B%22field%22%3A%22model_type_id%22%2C%22value%22%3A%5B%22visual-segmenter%22%5D%7D%5D",children:"segmentation model"})," to generate segmentation masks by providing an image as input. This enables detailed analysis by identifying distinct regions within the image and associating them with specific concepts."]}),"\n",(0,i.jsxs)(o.A,{groupId:"code",children:[(0,i.jsxs)(r.A,{value:"python",label:"Python SDK",children:[(0,i.jsx)(l.A,{className:"language-python",children:D}),(0,i.jsxs)(t,{children:[(0,i.jsx)("summary",{children:"Output"}),(0,i.jsx)(l.A,{className:"language-text",children:C})]})]}),(0,i.jsx)(r.A,{value:"typescript",label:"Node.js SDK",children:(0,i.jsx)(l.A,{className:"language-typescript",children:T})}),(0,i.jsx)(r.A,{value:"bash",label:"CLI",children:(0,i.jsx)(l.A,{className:"language-bash",children:H})}),(0,i.jsx)(r.A,{value:"python2 ",label:"Python (gRPC)",children:(0,i.jsx)(l.A,{className:"language-python",children:ye})}),(0,i.jsx)(r.A,{value:"js_rest",label:"JavaScript (REST)",children:(0,i.jsx)(l.A,{className:"language-javascript",children:ve})}),(0,i.jsx)(r.A,{value:"nodejs",label:"Node.js (gRPC)",children:(0,i.jsx)(l.A,{className:"language-javascript",children:Ne})}),(0,i.jsx)(r.A,{value:"java",label:"Java (gRPC)",children:(0,i.jsx)(l.A,{className:"language-java",children:Ye})}),(0,i.jsx)(r.A,{value:"php",label:"PHP (gRPC)",children:(0,i.jsx)(l.A,{className:"language-php",children:Ve})}),(0,i.jsx)(r.A,{value:"curl",label:"cURL",children:(0,i.jsx)(l.A,{className:"language-bash",children:We})})]}),"\n",(0,i.jsx)(n.h4,{id:"image-to-text",children:"Image-to-Text"}),"\n",(0,i.jsxs)(n.p,{children:["You can use an ",(0,i.jsx)(n.a,{href:"https://clarifai.com/explore/models?page=1&perPage=24&filterData=%5B%7B%22field%22%3A%22model_type_id%22%2C%22value%22%3A%5B%22image-to-text%22%5D%7D%5D",children:"image-to-text"})," model to generate meaningful textual descriptions from images."]}),"\n",(0,i.jsxs)(o.A,{groupId:"code",children:[(0,i.jsxs)(r.A,{value:"python",label:"Python SDK",children:[(0,i.jsx)(l.A,{className:"language-python",children:O}),(0,i.jsxs)(t,{children:[(0,i.jsx)("summary",{children:"Output"}),(0,i.jsx)(l.A,{className:"language-text",children:L})]})]}),(0,i.jsx)(r.A,{value:"typescript",label:"Node.js SDK",children:(0,i.jsx)(l.A,{className:"language-typescript",children:b})}),(0,i.jsx)(r.A,{value:"bash",label:"CLI",children:(0,i.jsx)(l.A,{className:"language-bash",children:B})}),(0,i.jsx)(r.A,{value:"python2",label:"Python (gRPC)",children:(0,i.jsx)(l.A,{className:"language-python",children:we})}),(0,i.jsx)(r.A,{value:"js_rest",label:"JavaScript (REST)",children:(0,i.jsx)(l.A,{className:"language-javascript",children:xe})}),(0,i.jsx)(r.A,{value:"nodejs",label:"Node.js (gRPC)",children:(0,i.jsx)(l.A,{className:"language-javascript",children:Me})}),(0,i.jsx)(r.A,{value:"java",label:"Java (gRPC)",children:(0,i.jsx)(l.A,{className:"language-java",children:ke})}),(0,i.jsx)(r.A,{value:"php",label:"PHP (gRPC)",children:(0,i.jsx)(l.A,{className:"language-php",children:qe})}),(0,i.jsx)(r.A,{value:"curl",label:"cURL",children:(0,i.jsx)(l.A,{className:"language-bash",children:Je})})]}),"\n",(0,i.jsx)(n.h4,{id:"image-to-image",children:"Image-to-Image"}),"\n",(0,i.jsxs)(n.p,{children:["You can use an upscaling ",(0,i.jsx)(n.a,{href:"https://clarifai.com/explore/models?page=1&perPage=24&filterData=%5B%7B%22field%22%3A%22model_type_id%22%2C%22value%22%3A%5B%22image-to-image%22%5D%7D%5D",children:"image-to-image"})," model to improve the quality of an image."]}),"\n",(0,i.jsxs)(o.A,{groupId:"code",children:[(0,i.jsx)(r.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:y})}),(0,i.jsx)(r.A,{value:"typescript",label:"Node.js SDK",children:(0,i.jsx)(l.A,{className:"language-typescript",children:w})}),(0,i.jsx)(r.A,{value:"bash",label:"CLI",children:(0,i.jsx)(l.A,{className:"language-bash",children:Y})})]}),"\n",(0,i.jsx)(n.h4,{id:"visual-embedder",children:"Visual Embedder"}),"\n",(0,i.jsxs)(n.p,{children:["You can use an ",(0,i.jsx)(n.a,{href:"https://clarifai.com/explore/models?page=1&perPage=24&filterData=%5B%7B%22field%22%3A%22model_type_id%22%2C%22value%22%3A%5B%22visual-embedder%22%5D%7D%5D",children:"embedding model"})," to generate embeddings from an image. Image embeddings are vector representations that capture the semantic content of an image, providing a powerful foundation for applications like similarity search, recommendation systems, and more."]}),"\n",(0,i.jsxs)(o.A,{groupId:"code",children:[(0,i.jsxs)(r.A,{value:"python",label:"Python SDK",children:[(0,i.jsx)(l.A,{className:"language-python",children:S}),(0,i.jsxs)(t,{children:[(0,i.jsx)("summary",{children:"Output"}),(0,i.jsx)(l.A,{className:"language-text",children:U})]})]}),(0,i.jsx)(r.A,{value:"typescript",label:"Node.js CLI",children:(0,i.jsx)(l.A,{className:"language-typescript",children:R})}),(0,i.jsx)(r.A,{value:"bash",label:"CLI",children:(0,i.jsx)(l.A,{className:"language-bash",children:k})})]}),"\n",(0,i.jsx)(n.h3,{id:"video-as-input",children:"Video as Input"}),"\n",(0,i.jsxs)(n.admonition,{title:"Configure FPS",type:"note",children:[(0,i.jsxs)(n.p,{children:["When processing a video input, the API returns a list of predicted concepts for each frame. By default, the video is analyzed at 1 frame per second (FPS), which corresponds to one prediction every 1000 milliseconds. This rate can be adjusted by setting the ",(0,i.jsx)(n.code,{children:"sample_ms"})," parameter in your prediction request."]}),(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"sample_ms"})," defines the time interval, in milliseconds, between frames selected for inference. It must be a value between 100 and 60000."]}),(0,i.jsxs)(n.p,{children:["It is calculated as: FPS = 1000 / ",(0,i.jsx)(n.code,{children:"sample_ms"})]}),(0,i.jsxs)(n.p,{children:["For example, setting ",(0,i.jsx)(n.code,{children:"sample_ms"})," to 1000 results in 1 FPS, which is the default rate."]})]}),"\n",(0,i.jsx)(n.h4,{id:"visual-detector---video",children:"Visual Detector - Video"}),"\n",(0,i.jsxs)(n.p,{children:["You can use a ",(0,i.jsx)(n.a,{href:"https://clarifai.com/explore/models?filterData=%5B%7B%22field%22%3A%22model_type_id%22%2C%22value%22%3A%5B%22visual-detector%22%5D%7D%5D&page=2&perPage=24",children:"visual detector"})," model to get predictions for every frame when processing a video input. You can also fine-tune your requests by adjusting parameters, such as the number of frames processed per second, giving you greater control over the speed and depth of the analysis."]}),"\n",(0,i.jsx)(n.p,{children:"You can provide video inputs either through URLs or by uploading files."}),"\n",(0,i.jsx)(n.admonition,{type:"note",children:(0,i.jsxs)(n.p,{children:["When uploading via URL, videos must be no longer than 10 minutes in duration or 300MB in size. Learn more ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/create-manage/inputs/upload/#videos",children:"here"}),"."]})}),"\n",(0,i.jsx)(n.h5,{id:"predict-via-url-1",children:"Predict via URL"}),"\n",(0,i.jsx)(n.p,{children:"Below is an example of how you would send video URLs and receive predictions."}),"\n",(0,i.jsxs)(o.A,{groupId:"code",children:[(0,i.jsxs)(r.A,{value:"python",label:"Python SDK",children:[(0,i.jsx)(l.A,{className:"language-python",children:A}),(0,i.jsxs)(t,{children:[(0,i.jsx)("summary",{children:"Output"}),(0,i.jsx)(l.A,{className:"language-text",children:x})]})]}),(0,i.jsx)(r.A,{value:"typescript",label:"Node.js SDK",children:(0,i.jsx)(l.A,{className:"language-typescript",children:E})}),(0,i.jsx)(r.A,{value:"bash",label:"CLI",children:(0,i.jsx)(l.A,{className:"language-bash",children:j})}),(0,i.jsx)(r.A,{value:"python2",label:"Python (gRPC)",children:(0,i.jsx)(l.A,{className:"language-python",children:Ze})}),(0,i.jsx)(r.A,{value:"js_rest",label:"JavaScript (REST)",children:(0,i.jsx)(l.A,{className:"language-javascript",children:nn})}),(0,i.jsx)(r.A,{value:"nodejs",label:"Node.js (gRPC) ",children:(0,i.jsx)(l.A,{className:"language-javascript",children:an})}),(0,i.jsx)(r.A,{value:"java",label:"Java (gRPC)",children:(0,i.jsx)(l.A,{className:"language-java",children:on})}),(0,i.jsx)(r.A,{value:"php",label:"PHP (gRPC)",children:(0,i.jsx)(l.A,{className:"language-php",children:ln})}),(0,i.jsx)(r.A,{value:"curl",label:"cURL",children:(0,i.jsx)(l.A,{className:"language-bash",children:pn})})]}),"\n",(0,i.jsx)(n.h5,{id:"predict-via-bytes-1",children:"Predict via Bytes"}),"\n",(0,i.jsx)(n.p,{children:"Below is an example of how you would send the bytes of a video and receive predictions."}),"\n",(0,i.jsxs)(o.A,{groupId:"code",children:[(0,i.jsx)(r.A,{value:"python",label:"Python (gRPC)",children:(0,i.jsx)(l.A,{className:"language-python",children:en})}),(0,i.jsx)(r.A,{value:"js_rest",label:"JavaScript (REST)",children:(0,i.jsx)(l.A,{className:"language-javascript",children:tn})}),(0,i.jsx)(r.A,{value:"nodejs",label:"Node.js (gRPC)",children:(0,i.jsx)(l.A,{className:"language-javascript",children:sn})}),(0,i.jsx)(r.A,{value:"java",label:"Java (gRPC)",children:(0,i.jsx)(l.A,{className:"language-java",children:rn})}),(0,i.jsx)(r.A,{value:"php",label:"PHP (gRPC)",children:(0,i.jsx)(l.A,{className:"language-php",children:cn})}),(0,i.jsx)(r.A,{value:"curl",label:"cURL",children:(0,i.jsx)(l.A,{className:"language-bash",children:un})})]}),"\n",(0,i.jsx)(n.h3,{id:"text-as-input",children:"Text as Input"}),"\n",(0,i.jsx)(n.h4,{id:"text-classifier",children:"Text Classifier"}),"\n",(0,i.jsxs)(n.p,{children:["You can use a ",(0,i.jsx)(n.a,{href:"https://clarifai.com/explore/models?page=1&perPage=24&filterData=%5B%7B%22field%22%3A%22model_type_id%22%2C%22value%22%3A%5B%22text-classifier%22%5D%7D%5D",children:"text classifier"})," model to automatically categorize text into predefined categories based on its content."]}),"\n",(0,i.jsx)(n.p,{children:"You can provide the text data via URLs, file uploads, or by entering raw text directly."}),"\n",(0,i.jsx)(n.admonition,{type:"note",children:(0,i.jsxs)(n.p,{children:["The file size of each text input should be less than 20MB. Learn more ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/create-manage/inputs/upload/#text-files",children:"here"}),"."]})}),"\n",(0,i.jsx)(n.h5,{id:"predict-via-url-2",children:"Predict via URL"}),"\n",(0,i.jsx)(n.p,{children:"Below is an example of how you would make predictions on passages of text hosted on the web."}),"\n",(0,i.jsxs)(o.A,{groupId:"code",children:[(0,i.jsx)(r.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:$})}),(0,i.jsx)(r.A,{value:"typescript",label:"Node.js SDK",children:(0,i.jsx)(l.A,{className:"language-typescript",children:F})}),(0,i.jsx)(r.A,{value:"bash",label:"CLI",children:(0,i.jsx)(l.A,{className:"language-bash",children:ne})}),(0,i.jsx)(r.A,{value:"python2",label:"Python (gRPC)",children:(0,i.jsx)(l.A,{className:"language-python",children:dn})}),(0,i.jsx)(r.A,{value:"js_rest",label:"JavaScript (REST)",children:(0,i.jsx)(l.A,{className:"language-javascript",children:gn})}),(0,i.jsx)(r.A,{value:"nodejs",label:"Node.js (gRPC)",children:(0,i.jsx)(l.A,{className:"language-javascript",children:Tn})}),(0,i.jsx)(r.A,{value:"java",label:"Java (gRPC)",children:(0,i.jsx)(l.A,{className:"language-java",children:Sn})}),(0,i.jsx)(r.A,{value:"php",label:"PHP (gRPC)",children:(0,i.jsx)(l.A,{className:"language-php",children:Cn})}),(0,i.jsx)(r.A,{value:"curl",label:"cURL",children:(0,i.jsx)(l.A,{className:"language-bash",children:jn})})]}),"\n",(0,i.jsxs)(t,{children:[(0,i.jsx)("summary",{children:"Output"}),(0,i.jsx)(l.A,{className:"language-text",children:le})]}),"\n",(0,i.jsx)(n.h5,{id:"predict-via-local-files",children:"Predict via Local Files"}),"\n",(0,i.jsx)(n.p,{children:"Below is an example of how you would provide text inputs via local text files and receive predictions."}),"\n",(0,i.jsxs)(o.A,{groupId:"code",children:[(0,i.jsx)(r.A,{value:"python",label:"Python (gRPC)",children:(0,i.jsx)(l.A,{className:"language-python",children:hn})}),(0,i.jsx)(r.A,{value:"js_rest",label:"JavaScript (REST)",children:(0,i.jsx)(l.A,{className:"language-javascript",children:In})}),(0,i.jsx)(r.A,{value:"nodejs",label:"Node.js (gRPC)",children:(0,i.jsx)(l.A,{className:"language-javascript",children:On})}),(0,i.jsx)(r.A,{value:"java",label:"Java (gRPC)",children:(0,i.jsx)(l.A,{className:"language-java",children:Rn})}),(0,i.jsx)(r.A,{value:"php",label:"PHP (gRPC)",children:(0,i.jsx)(l.A,{className:"language-php",children:Ln})}),(0,i.jsx)(r.A,{value:"curl",label:"cURL",children:(0,i.jsx)(l.A,{className:"language-bash",children:Hn})})]}),"\n",(0,i.jsx)(n.h5,{id:"predict-via-raw-text",children:"Predict via Raw Text"}),"\n",(0,i.jsx)(n.p,{children:"Below is an example of how you would provide raw text inputs and receive predictions."}),"\n",(0,i.jsxs)(o.A,{groupId:"code",children:[(0,i.jsx)(r.A,{value:"python",label:"Python (gRPC)",children:(0,i.jsx)(l.A,{className:"language-python",children:mn})}),(0,i.jsx)(r.A,{value:"js_rest",label:"JavaScript (REST)",children:(0,i.jsx)(l.A,{className:"language-javascript",children:An})}),(0,i.jsx)(r.A,{value:"nodejs",label:"Node.js (gRPC)",children:(0,i.jsx)(l.A,{className:"language-javascript",children:bn})}),(0,i.jsx)(r.A,{value:"java",label:"Java (gRPC)",children:(0,i.jsx)(l.A,{className:"language-java",children:Pn})}),(0,i.jsx)(r.A,{value:"php",label:"PHP (gRPC)",children:(0,i.jsx)(l.A,{className:"language-php",children:Un})}),(0,i.jsx)(r.A,{value:"curl",label:"cURL",children:(0,i.jsx)(l.A,{className:"language-bash",children:Yn})})]}),"\n",(0,i.jsx)(n.h4,{id:"text-generation-using-llms",children:"Text Generation Using LLMs"}),"\n",(0,i.jsxs)(n.p,{children:["You can use ",(0,i.jsx)(n.a,{href:"https://clarifai.com/explore/models?page=1&perPage=24&filterData=%5B%7B%22field%22%3A%22model_type_id%22%2C%22value%22%3A%5B%22text-to-text%22%5D%7D%5D",children:"text generation"})," models to dynamically create textual content based on user-defined prompts."]}),"\n",(0,i.jsxs)(o.A,{groupId:"code",children:[(0,i.jsx)(r.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:G})}),(0,i.jsx)(r.A,{value:"typescript",label:"Node.js SDK",children:(0,i.jsx)(l.A,{className:"language-typescript",children:q})}),(0,i.jsx)(r.A,{value:"bash",label:"CLI",children:(0,i.jsx)(l.A,{className:"language-bash",children:ae})}),(0,i.jsx)(r.A,{value:"python2",label:"Python (gRPC)",children:(0,i.jsx)(l.A,{className:"language-python",children:Fn})}),(0,i.jsx)(r.A,{value:"js_rest",label:"JavaScript (REST)",children:(0,i.jsx)(l.A,{className:"language-javascript",children:Gn})}),(0,i.jsx)(r.A,{value:"nodejs",label:"Node,js (gRPC)",children:(0,i.jsx)(l.A,{className:"language-javascript",children:Vn})}),(0,i.jsx)(r.A,{value:"java",label:"Java (gRPC)",children:(0,i.jsx)(l.A,{className:"language-java",children:qn})}),(0,i.jsx)(r.A,{value:"php",label:"PHP (gRPC)",children:(0,i.jsx)(l.A,{className:"language-php",children:Xn})}),(0,i.jsx)(r.A,{value:"curl",label:"cURL",children:(0,i.jsx)(l.A,{className:"language-bash",children:Qn})})]}),"\n",(0,i.jsxs)(t,{children:[(0,i.jsx)("summary",{children:"Output"}),(0,i.jsx)(l.A,{className:"language-text",children:ce})]}),"\n",(0,i.jsx)(n.h4,{id:"set-inference-parameters",children:"Set Inference Parameters"}),"\n",(0,i.jsx)(n.p,{children:"When making predictions using LLMs on our platform, some models offer the ability to specify various inference parameters to influence their output."}),"\n",(0,i.jsx)(n.p,{children:"These parameters control the behavior of the model during the generation process, affecting aspects like creativity, coherence, and the diversity of the generated text."}),"\n",(0,i.jsxs)(n.p,{children:["You can learn more about them ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/compute/inference/advanced",children:"here"}),"."]}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Note:"})," You can also find various examples of how to set inference parameters throughout this guide."]}),"\n"]}),"\n",(0,i.jsxs)(o.A,{groupId:"code",children:[(0,i.jsx)(r.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:V})}),(0,i.jsx)(r.A,{value:"bash",label:"CLI",children:(0,i.jsx)(l.A,{className:"language-bash",children:te})}),(0,i.jsx)(r.A,{value:"python2",label:"Python (gRPC)",children:(0,i.jsx)(l.A,{className:"language-python",children:Kn})}),(0,i.jsx)(r.A,{value:"js_rest",label:"JavaScript (REST)",children:(0,i.jsx)(l.A,{className:"language-javascript",children:Wn})}),(0,i.jsx)(r.A,{value:"nodejs",label:"Node.js (gRPC) ",children:(0,i.jsx)(l.A,{className:"language-javascript",children:zn})}),(0,i.jsx)(r.A,{value:"java",label:"Java (gRPC)",children:(0,i.jsx)(l.A,{className:"language-java",children:Jn})}),(0,i.jsx)(r.A,{value:"php",label:"PHP (gRPC)",children:(0,i.jsx)(l.A,{className:"language-php",children:Zn})}),(0,i.jsx)(r.A,{value:"curl",label:"cURL",children:(0,i.jsx)(l.A,{className:"language-bash",children:et})})]}),"\n",(0,i.jsx)(n.h4,{id:"text-classification-using-llms",children:"Text Classification Using LLMs"}),"\n",(0,i.jsxs)(n.p,{children:["You can leverage ",(0,i.jsx)(n.a,{href:"https://clarifai.com/explore/models?filterData=%5B%7B%22field%22%3A%22use_cases%22%2C%22value%22%3A%5B%22llm%22%5D%7D%5D&page=1&perPage=24",children:"LLMs"})," to categorize text using carefully crafted prompts."]}),"\n",(0,i.jsxs)(o.A,{groupId:"code",children:[(0,i.jsxs)(r.A,{value:"python",label:"Python SDK",children:[(0,i.jsx)(l.A,{className:"language-python",children:X}),(0,i.jsxs)(t,{children:[(0,i.jsx)("summary",{children:"Output"}),(0,i.jsx)(l.A,{className:"language-text",children:pe})]})]}),(0,i.jsx)(r.A,{value:"typescript",label:"Node.js SDK",children:(0,i.jsx)(l.A,{className:"language-typescript",children:Q})}),(0,i.jsx)(r.A,{value:"bash",label:"CLI",children:(0,i.jsx)(l.A,{className:"language-bash",children:ie})})]}),"\n",(0,i.jsx)(n.h4,{id:"text-to-image",children:"Text-to-Image"}),"\n",(0,i.jsxs)(n.p,{children:["You can use a ",(0,i.jsx)(n.a,{href:"https://clarifai.com/explore/models?page=1&perPage=24&filterData=%5B%7B%22field%22%3A%22model_type_id%22%2C%22value%22%3A%5B%22text-to-image%22%5D%7D%5D",children:"text-to-image"})," model to transform textual input into vibrant and expressive images."]}),"\n",(0,i.jsxs)(o.A,{groupId:"code",children:[(0,i.jsx)(r.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:K})}),(0,i.jsx)(r.A,{value:"typescript",label:"Node.js SDK",children:(0,i.jsx)(l.A,{className:"language-typescript",children:W})}),(0,i.jsx)(r.A,{value:"bash",label:"CLI",children:(0,i.jsx)(l.A,{className:"language-bash",children:se})}),(0,i.jsx)(r.A,{value:"python2",label:"Python (gRPC)",children:(0,i.jsx)(l.A,{className:"language-python",children:_n})}),(0,i.jsx)(r.A,{value:"js_rest",label:"JavaScript (REST)",children:(0,i.jsx)(l.A,{className:"language-javascript",children:En})}),(0,i.jsx)(r.A,{value:"nodejs",label:"Node.js (gRPC)",children:(0,i.jsx)(l.A,{className:"language-javascript",children:yn})}),(0,i.jsx)(r.A,{value:"java",label:"Java (gRPC)",children:(0,i.jsx)(l.A,{className:"language-java",children:vn})}),(0,i.jsx)(r.A,{value:"php",label:"PHP (gRPC)",children:(0,i.jsx)(l.A,{className:"language-php",children:Nn})}),(0,i.jsx)(r.A,{value:"curl",label:"cURL",children:(0,i.jsx)(l.A,{className:"language-bash",children:Bn})})]}),"\n",(0,i.jsxs)(t,{children:[(0,i.jsx)("summary",{children:"Output"}),(0,i.jsx)("img",{src:"/img/python-sdk/text_to_image.png"})]}),"\n",(0,i.jsx)(n.h4,{id:"text-to-audio",children:"Text-to-Audio"}),"\n",(0,i.jsxs)(n.p,{children:["You can use a ",(0,i.jsx)(n.a,{href:"https://clarifai.com/explore/models?page=1&perPage=24&filterData=%5B%7B%22field%22%3A%22input_fields%22%2C%22value%22%3A%5B%22text%22%5D%7D%2C%7B%22field%22%3A%22use_cases%22%2C%22value%22%3A%5B%22speech-synthesis%22%2C%22text-to-speech%22%5D%7D%5D",children:"text-to-audio"})," model to convert written text into natural, expressive speech."]}),"\n",(0,i.jsxs)(o.A,{groupId:"code",children:[(0,i.jsx)(r.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:z})}),(0,i.jsx)(r.A,{value:"typescript",label:"Node.js SDK",children:(0,i.jsx)(l.A,{className:"language-typescript",children:J})}),(0,i.jsx)(r.A,{value:"bash",label:"CLI",children:(0,i.jsx)(l.A,{className:"language-bash",children:oe})}),(0,i.jsx)(r.A,{value:"python2",label:"Python (gRPC)",children:(0,i.jsx)(l.A,{className:"language-python",children:fn})}),(0,i.jsx)(r.A,{value:"js_rest",label:"JavaScript (REST)",children:(0,i.jsx)(l.A,{className:"language-javascript",children:Dn})}),(0,i.jsx)(r.A,{value:"nodejs",label:"Node.js (gRPC)",children:(0,i.jsx)(l.A,{className:"language-javascript",children:wn})}),(0,i.jsx)(r.A,{value:"java",label:"Java (gRPC)",children:(0,i.jsx)(l.A,{className:"language-java",children:xn})}),(0,i.jsx)(r.A,{value:"php",label:"PHP (gRPC)",children:(0,i.jsx)(l.A,{className:"language-php",children:Mn})}),(0,i.jsx)(r.A,{value:"curl",label:"cURL",children:(0,i.jsx)(l.A,{className:"language-bash",children:kn})})]}),"\n",(0,i.jsx)(n.h4,{id:"text-embedder",children:"Text Embedder"}),"\n",(0,i.jsxs)(n.p,{children:["You can use an ",(0,i.jsx)(n.a,{href:"https://clarifai.com/explore/models?page=1&perPage=24&filterData=%5B%7B%22field%22%3A%22model_type_id%22%2C%22value%22%3A%5B%22text-embedder%22%5D%7D%5D",children:"embedding model"})," to generate embeddings from text. These embeddings are vector representations that capture the semantic meaning of the text, making them ideal for applications such as similarity search, recommendation systems, document clustering, and more."]}),"\n",(0,i.jsxs)(n.admonition,{type:"note",children:[(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.a,{href:"https://clarifai.com/cohere/embed/models/cohere-embed-english-v3_0",children:"Cohere Embed-v3"})," model requires an ",(0,i.jsx)(n.code,{children:"input_type"})," parameter to be specified, which can be set using one of the following values:"]}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"search_document"})," (default): For texts (documents) intended to be stored in a vector database."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"search_query"}),": For search queries to find the most relevant documents in a vector database."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"classification"}),": If the embeddings are used as input for a classification system."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"clustering"}),": If the embeddings are used for text clustering."]}),"\n"]})]}),"\n",(0,i.jsxs)(o.A,{groupId:"code",children:[(0,i.jsx)(r.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:Z})}),(0,i.jsx)(r.A,{value:"typescript",label:"Node.js SDK",children:(0,i.jsx)(l.A,{className:"language-typescript",children:ee})}),(0,i.jsx)(r.A,{value:"bash",label:"CLI",children:(0,i.jsx)(l.A,{className:"language-bash",children:re})}),(0,i.jsx)(r.A,{value:"python2",label:"Python (gRPC)",children:(0,i.jsx)(l.A,{className:"language-python",children:At})}),(0,i.jsx)(r.A,{value:"js_rest",label:"JavaScript (REST)",children:(0,i.jsx)(l.A,{className:"language-javascript",children:Et})}),(0,i.jsx)(r.A,{value:"nodejs",label:"Node.js (gRPC)",children:(0,i.jsx)(l.A,{className:"language-javascript",children:Dt})}),(0,i.jsx)(r.A,{value:"java",label:"Java (gRPC)",children:(0,i.jsx)(l.A,{className:"language-java",children:Tt})}),(0,i.jsx)(r.A,{value:"php",label:"PHP (gRPC)",children:(0,i.jsx)(l.A,{className:"language-php",children:Ot})}),(0,i.jsx)(r.A,{value:"curl",label:"cURL",children:(0,i.jsx)(l.A,{className:"language-bash",children:bt})})]}),"\n",(0,i.jsxs)(t,{children:[(0,i.jsx)("summary",{children:"Output"}),(0,i.jsx)(l.A,{className:"language-text",children:ue})]}),"\n",(0,i.jsx)(n.h3,{id:"audio-as-input",children:"Audio as Input"}),"\n",(0,i.jsx)(n.h4,{id:"audio-to-text",children:"Audio-to-Text"}),"\n",(0,i.jsxs)(n.p,{children:["You can use an ",(0,i.jsx)(n.a,{href:"https://clarifai.com/explore/models?page=1&perPage=24&filterData=%5B%7B%22field%22%3A%22model_type_id%22%2C%22value%22%3A%5B%22audio-to-text%22%5D%7D%5D",children:"audio-to-text"})," model to convert audio files into text. This enables the transcription of spoken words for a variety of use cases, including transcription services, voice command processing, and more."]}),"\n",(0,i.jsx)(n.h5,{id:"predict-via-url-3",children:"Predict via URL"}),"\n",(0,i.jsxs)(o.A,{groupId:"code",children:[(0,i.jsx)(r.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:de})}),(0,i.jsx)(r.A,{value:"typescript",label:"Node.js SDK",children:(0,i.jsx)(l.A,{className:"language-typescript",children:he})}),(0,i.jsx)(r.A,{value:"bash",label:"CLI",children:(0,i.jsx)(l.A,{className:"language-bash",children:me})}),(0,i.jsx)(r.A,{value:"python2",label:"Python (gRPC)",children:(0,i.jsx)(l.A,{className:"language-python",children:nt})}),(0,i.jsx)(r.A,{value:"js_rest",label:"JavaScript (REST)",children:(0,i.jsx)(l.A,{className:"language-javascript",children:at})}),(0,i.jsx)(r.A,{value:"nodejs",label:"Node.js (gRPC)",children:(0,i.jsx)(l.A,{className:"language-javascript",children:st})}),(0,i.jsx)(r.A,{value:"java",label:"Java (gRPC)",children:(0,i.jsx)(l.A,{className:"language-java",children:rt})}),(0,i.jsx)(r.A,{value:"php",label:"PHP (gRPC)",children:(0,i.jsx)(l.A,{className:"language-php",children:ct})}),(0,i.jsx)(r.A,{value:"curl",label:"cURL",children:(0,i.jsx)(l.A,{className:"language-bash",children:ut})})]}),"\n",(0,i.jsxs)(t,{children:[(0,i.jsx)("summary",{children:"Output"}),(0,i.jsx)(l.A,{className:"language-text",children:_e})]}),"\n",(0,i.jsx)(n.h5,{id:"predict-via-bytes-2",children:"Predict via Bytes"}),"\n",(0,i.jsxs)(o.A,{groupId:"code",children:[(0,i.jsx)(r.A,{value:"python",label:"Python (gRPC)",children:(0,i.jsx)(l.A,{className:"language-python",children:tt})}),(0,i.jsx)(r.A,{value:"js_rest",label:"JavaScript (REST)",children:(0,i.jsx)(l.A,{className:"language-javascript",children:it})}),(0,i.jsx)(r.A,{value:"nodejs",label:"Node.js (gRPC)",children:(0,i.jsx)(l.A,{className:"language-javascript",children:ot})}),(0,i.jsx)(r.A,{value:"java",label:"Java (gRPC)",children:(0,i.jsx)(l.A,{className:"language-java",children:lt})}),(0,i.jsx)(r.A,{value:"php",label:"PHP (gRPC)",children:(0,i.jsx)(l.A,{className:"language-php",children:pt})}),(0,i.jsx)(r.A,{value:"curl",label:"cURL",children:(0,i.jsx)(l.A,{className:"language-bash",children:dt})})]}),"\n",(0,i.jsx)(n.h3,{id:"multimodal-as-input",children:"MultiModal as Input"}),"\n",(0,i.jsx)(n.h4,{id:"imagetext-to-text",children:"[Image,Text]-to-Text"}),"\n",(0,i.jsx)(n.p,{children:"You can process multimodal inputs \u2014 combining multiple modalities, such as text, images, and/or other types of data \u2014 to generate accurate predictions."}),"\n",(0,i.jsx)(n.p,{children:"Below is an example of how you can send both image and text inputs to a model."}),"\n",(0,i.jsx)(n.h5,{id:"predict-via-image-url",children:"Predict via Image URL"}),"\n",(0,i.jsxs)(o.A,{groupId:"code",children:[(0,i.jsx)(r.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:fe})}),(0,i.jsx)(r.A,{value:"typescript",label:"Node.js SDK",children:(0,i.jsx)(l.A,{className:"language-typescript",children:ge})}),(0,i.jsx)(r.A,{value:"python2",label:"Python (gRPC)",children:(0,i.jsx)(l.A,{className:"language-python",children:ht})}),(0,i.jsx)(r.A,{value:"js_rest",label:"JavaScript (REST)",children:(0,i.jsx)(l.A,{className:"language-javascript",children:mt})}),(0,i.jsx)(r.A,{value:"nodejs",label:"Node.js (gRPC)",children:(0,i.jsx)(l.A,{className:"language-javascript",children:_t})}),(0,i.jsx)(r.A,{value:"java",label:"Java (gRPC)",children:(0,i.jsx)(l.A,{className:"language-java",children:ft})}),(0,i.jsx)(r.A,{value:"php",label:"PHP (gRPC)",children:(0,i.jsx)(l.A,{className:"language-php",children:gt})}),(0,i.jsx)(r.A,{value:"curl",label:"cURL",children:(0,i.jsx)(l.A,{className:"language-bash",children:It})})]}),"\n",(0,i.jsxs)(t,{children:[(0,i.jsx)("summary",{children:"Output"}),(0,i.jsx)(l.A,{className:"language-text",children:Ee})]}),"\n",(0,i.jsx)(n.h5,{id:"predict-via-local-image",children:"Predict via Local Image"}),"\n",(0,i.jsxs)(o.A,{groupId:"code",children:[(0,i.jsxs)(r.A,{value:"python",label:"Python SDK",children:[(0,i.jsx)(l.A,{className:"language-python",children:Ie}),(0,i.jsxs)(t,{children:[(0,i.jsx)("summary",{children:"Output"}),(0,i.jsx)(l.A,{className:"language-text",children:Ee})]})]}),(0,i.jsx)(r.A,{value:"typescript",label:"Node.js SDK",children:(0,i.jsx)(l.A,{className:"language-typescript",children:Ae})})]}),"\n",(0,i.jsx)(n.h3,{id:"use-third-party-api-keys",children:"Use Third-Party API Keys"}),"\n",(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsxs)(n.p,{children:["The ability to use third-party API keys is currently exclusively available to Enterprise users. Learn more ",(0,i.jsx)(n.a,{href:"https://www.clarifai.com/pricing",children:"here"}),"."]})}),"\n",(0,i.jsx)(n.p,{children:"For the third-party models we've wrapped into our platform, like those provided by OpenAI, Anthropic, Cohere, and others, you can also choose to utilize their API keys as an option\u2014in addition to using the default Clarifai keys."}),"\n",(0,i.jsx)(n.p,{children:"This Bring Your Own Key (BYOK) flexibility allows you to integrate your preferred services and APIs into your workflow, enhancing the versatility of our platform."}),"\n",(0,i.jsxs)(n.p,{children:["Here is an example of how to add an OpenAI API key for ",(0,i.jsx)(n.a,{href:"https://clarifai.com/openai/dall-e/models/dall-e-3",children:"Dalle-3"})," for text-to-image tasks."]}),"\n",(0,i.jsx)(o.A,{groupId:"code",children:(0,i.jsx)(r.A,{value:"curl",label:"cURL",children:(0,i.jsx)(l.A,{className:"language-bash",children:$n})})})]})}function xt(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(vt,{...e})}):vt(e)}}}]);
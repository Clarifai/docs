"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[9368],{3905:function(e,t,n){n.d(t,{Zo:function(){return p},kt:function(){return f}});var o=n(67294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function s(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,o)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?s(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):s(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function i(e,t){if(null==e)return{};var n,o,a=function(e,t){if(null==e)return{};var n,o,a={},s=Object.keys(e);for(o=0;o<s.length;o++)n=s[o],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);for(o=0;o<s.length;o++)n=s[o],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var l=o.createContext({}),u=function(e){var t=o.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},p=function(e){var t=u(e.components);return o.createElement(l.Provider,{value:t},e.children)},c={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},d=o.forwardRef((function(e,t){var n=e.components,a=e.mdxType,s=e.originalType,l=e.parentName,p=i(e,["components","mdxType","originalType","parentName"]),d=u(n),f=a,m=d["".concat(l,".").concat(f)]||d[f]||c[f]||s;return n?o.createElement(m,r(r({ref:t},p),{},{components:n})):o.createElement(m,r({ref:t},p))}));function f(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var s=n.length,r=new Array(s);r[0]=d;var i={};for(var l in t)hasOwnProperty.call(t,l)&&(i[l]=t[l]);i.originalType=e,i.mdxType="string"==typeof e?e:a,r[1]=i;for(var u=2;u<s;u++)r[u]=n[u];return o.createElement.apply(null,r)}return o.createElement.apply(null,n)}d.displayName="MDXCreateElement"},58215:function(e,t,n){var o=n(67294);t.Z=function(e){var t=e.children,n=e.hidden,a=e.className;return o.createElement("div",{role:"tabpanel",hidden:n,className:a},t)}},26396:function(e,t,n){n.d(t,{Z:function(){return d}});var o=n(87462),a=n(67294),s=n(72389),r=n(79443);var i=function(){var e=(0,a.useContext)(r.Z);if(null==e)throw new Error('"useUserPreferencesContext" is used outside of "Layout" component.');return e},l=n(53810),u=n(86010),p="tabItem_vU9c";function c(e){var t,n,s,r=e.lazy,c=e.block,d=e.defaultValue,f=e.values,m=e.groupId,h=e.className,_=a.Children.map(e.children,(function(e){if((0,a.isValidElement)(e)&&void 0!==e.props.value)return e;throw new Error("Docusaurus error: Bad <Tabs> child <"+("string"==typeof e.type?e.type:e.type.name)+'>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.')})),b=null!=f?f:_.map((function(e){var t=e.props;return{value:t.value,label:t.label,attributes:t.attributes}})),w=(0,l.lx)(b,(function(e,t){return e.value===t.value}));if(w.length>0)throw new Error('Docusaurus error: Duplicate values "'+w.map((function(e){return e.value})).join(", ")+'" found in <Tabs>. Every value needs to be unique.');var g=null===d?d:null!=(t=null!=d?d:null==(n=_.find((function(e){return e.props.default})))?void 0:n.props.value)?t:null==(s=_[0])?void 0:s.props.value;if(null!==g&&!b.some((function(e){return e.value===g})))throw new Error('Docusaurus error: The <Tabs> has a defaultValue "'+g+'" but none of its children has the corresponding value. Available values are: '+b.map((function(e){return e.value})).join(", ")+". If you intend to show no default tab, use defaultValue={null} instead.");var k=i(),v=k.tabGroupChoices,y=k.setTabGroupChoices,T=(0,a.useState)(g),N=T[0],D=T[1],I=[],C=(0,l.o5)().blockElementScrollPositionUntilNextRender;if(null!=m){var O=v[m];null!=O&&O!==N&&b.some((function(e){return e.value===O}))&&D(O)}var P=function(e){var t=e.currentTarget,n=I.indexOf(t),o=b[n].value;o!==N&&(C(t),D(o),null!=m&&y(m,o))},E=function(e){var t,n=null;switch(e.key){case"ArrowRight":var o=I.indexOf(e.currentTarget)+1;n=I[o]||I[0];break;case"ArrowLeft":var a=I.indexOf(e.currentTarget)-1;n=I[a]||I[I.length-1]}null==(t=n)||t.focus()};return a.createElement("div",{className:"tabs-container"},a.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,u.Z)("tabs",{"tabs--block":c},h)},b.map((function(e){var t=e.value,n=e.label,s=e.attributes;return a.createElement("li",(0,o.Z)({role:"tab",tabIndex:N===t?0:-1,"aria-selected":N===t,key:t,ref:function(e){return I.push(e)},onKeyDown:E,onFocus:P,onClick:P},s,{className:(0,u.Z)("tabs__item",p,null==s?void 0:s.className,{"tabs__item--active":N===t})}),null!=n?n:t)}))),r?(0,a.cloneElement)(_.filter((function(e){return e.props.value===N}))[0],{className:"margin-vert--md"}):a.createElement("div",{className:"margin-vert--md"},_.map((function(e,t){return(0,a.cloneElement)(e,{key:t,hidden:e.props.value!==N})}))))}function d(e){var t=(0,s.Z)();return a.createElement(c,(0,o.Z)({key:String(t)},e))}},12686:function(e,t,n){n.r(t),n.d(t,{frontMatter:function(){return u},contentTitle:function(){return p},metadata:function(){return c},toc:function(){return d},default:function(){return m}});var o=n(87462),a=n(63366),s=(n(67294),n(3905)),r=n(26396),i=n(58215),l=["components"],u={description:"Use facial recognition to identify individual people.",sidebar_position:3},p="Custom KNN Face Classifier Workflow",c={unversionedId:"api-guide/workflows/common-workflows/knn-face-classifier-workflow-walkthrough",id:"api-guide/workflows/common-workflows/knn-face-classifier-workflow-walkthrough",title:"Custom KNN Face Classifier Workflow",description:"Use facial recognition to identify individual people.",source:"@site/docs/api-guide/workflows/common-workflows/knn-face-classifier-workflow-walkthrough.md",sourceDirName:"api-guide/workflows/common-workflows",slug:"/api-guide/workflows/common-workflows/knn-face-classifier-workflow-walkthrough",permalink:"/docs-new/api-guide/workflows/common-workflows/knn-face-classifier-workflow-walkthrough",editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/api-guide/workflows/common-workflows/knn-face-classifier-workflow-walkthrough.md",tags:[],version:"current",sidebarPosition:3,frontMatter:{description:"Use facial recognition to identify individual people.",sidebar_position:3},sidebar:"tutorialSidebar",previous:{title:"Auto Annotation",permalink:"/docs-new/api-guide/workflows/common-workflows/auto-annotation-walkthrough"},next:{title:"Visual Text Recognition",permalink:"/docs-new/api-guide/workflows/common-workflows/visual-text-recognition-walkthrough"}},d=[{value:"Create a new application",id:"create-a-new-application",children:[],level:2},{value:"Add images",id:"add-images",children:[],level:2},{value:"Wait for upload &amp; map IDs to URLs",id:"wait-for-upload--map-ids-to-urls",children:[],level:2},{value:"List the annotations",id:"list-the-annotations",children:[],level:2},{value:"Post new annotations",id:"post-new-annotations",children:[],level:2},{value:"Create a KNN model",id:"create-a-knn-model",children:[],level:2},{value:"Create a workflow",id:"create-a-workflow",children:[],level:2},{value:"Predict",id:"predict",children:[],level:2}],f={toc:d};function m(e){var t=e.components,n=(0,a.Z)(e,l);return(0,s.kt)("wrapper",(0,o.Z)({},f,n,{components:t,mdxType:"MDXLayout"}),(0,s.kt)("h1",{id:"custom-knn-face-classifier-workflow"},"Custom KNN Face Classifier Workflow"),(0,s.kt)("p",null,"Let's say you want to build a face recognition system that is able to differentiate between persons of whom you only have a few samples ","(","per person",")",". Machine learning models generally require a large inputs dataset to be able to classify the inputs well."),(0,s.kt)("p",null,"When a large dataset is the luxury you do not have, we recommend using our ",(0,s.kt)("strong",{parentName:"p"},"KNN Classifier Model")," which uses K nearest neighbor search and plurality voting amongst the nearest neighbors to classify new instances. It's recommended when you only have a small dataset like one input per concept."),(0,s.kt)("p",null,"In this walkthorugh, you'll learn how to create a KNN classifier that's going to work based off the Clarifai's base Face model. The whole process below is going to be done programmatically, using the Clarifai's powerful API."),(0,s.kt)("blockquote",null,(0,s.kt)("p",{parentName:"blockquote"},"Note: Each of the steps below can also be done manually on ",(0,s.kt)("a",{parentName:"p",href:"https://portal.clarifai.com/"},"the Clarifai Portal"),".")),(0,s.kt)("h2",{id:"create-a-new-application"},"Create a new application"),(0,s.kt)("p",null,"The first step is manual: in the Clarifai Portal, ",(0,s.kt)("a",{parentName:"p",href:"https://docs.clarifai.com/clarifai-basics/applications/create-an-application#create-an-application-in-portal"},"create an new application")," with ",(0,s.kt)("strong",{parentName:"p"},"Face")," selected as the Base Workflow."),(0,s.kt)("p",null,"Afterward, copy the newly-created application's ",(0,s.kt)("em",{parentName:"p"},"API key")," and set it as metadata ","(","see the initialization code",")",". This variable is going to be used, for authorization purposes, by all Clarifai API calls that follow."),(0,s.kt)("h2",{id:"add-images"},"Add images"),(0,s.kt)("p",null,"Add images that contain the faces you want to use as a training set."),(0,s.kt)("p",null,"Since the application's base model is Face, after adding an image, faces are automatically located and are available to be annotated."),(0,s.kt)(r.Z,{mdxType:"Tabs"},(0,s.kt)(i.Z,{value:"grpc_python",label:"gRPC Python",mdxType:"TabItem"},(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},'import time\n\n# Insert here the initialization code as outlined on this page:\n# https://docs.clarifai.com/api-guide/api-overview/api-clients#client-installation-instructions\n\n# Insert here the URLs of the images\nimage_urls = [\n    "{YOUR_IMAGE_URL_1}",\n    "{YOUR_IMAGE_URL_2}"\n]\npost_inputs_response = stub.PostInputs(\n    service_pb2.PostInputsRequest(\n        user_app_id=userDataObject,  # The userDataObject is created in the overview and is required when using a PAT\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    image=resources_pb2.Image(url=url)\n                )\n            )\n            for url in image_urls\n        ]\n    ),\n    metadata=metadata\n)\n\n\nif post_inputs_response.status.code != status_code_pb2.SUCCESS:\n    raise Exception("Failed response, status: " + str(post_inputs_response.status))\n')))),(0,s.kt)("h2",{id:"wait-for-upload--map-ids-to-urls"},"Wait for upload & map IDs to URLs"),(0,s.kt)("p",null,"Now we'll wait for all the images to finish uploading, and then create a dictionary mapping from an input ID to the URL. This will help us to annotate the proper image in the next step."),(0,s.kt)(r.Z,{mdxType:"Tabs"},(0,s.kt)(i.Z,{value:"grpc_python",label:"gRPC Python",mdxType:"TabItem"},(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},'while True:\n    list_inputs_response = stub.ListInputs(\n        service_pb2.ListInputsRequest(\n            user_app_id=userDataObject,  # The userDataObject is created in the overview and is required when using a PAT\n            page=1, \n            per_page=100\n        ),\n        metadata=metadata\n    )\n\n    if list_inputs_response.status.code != status_code_pb2.SUCCESS:\n        raise Exception("Failed response, status: " + str(list_inputs_response.status))\n\n    for the_input in list_inputs_response.inputs:\n        input_status_code = the_input.status.code\n        if input_status_code == status_code_pb2.INPUT_DOWNLOAD_SUCCESS:\n            continue\n        elif input_status_code in (status_code_pb2.INPUT_DOWNLOAD_PENDING, status_code_pb2.INPUT_DOWNLOAD_IN_PROGRESS):\n            print("Not all inputs have been downloaded yet. Checking again shortly.")\n            break\n        else:\n            error_message = (\n                    str(input_status_code) + " " +\n                    the_input.status.description + " " +\n                    the_input.status.details\n            )\n            raise Exception(\n                f"Expected inputs to download, but got {error_message}. Full response: {list_inputs_response}"\n            )\n    else:\n        # Once all inputs have been successfully downloaded, break the while True loop.\n        print("All inputs have been successfully downloaded.")\n        break\n    time.sleep(2)\n\n\ninput_id_to_url = {inp.id: inp.data.image.url for inp in list_inputs_response.inputs}\n')))),(0,s.kt)("h2",{id:"list-the-annotations"},"List the annotations"),(0,s.kt)("p",null,"Let's now print all the regions that the Face base model detected on our images."),(0,s.kt)("p",null,"The code below prints the annotations together with the input ID and region ID. These two IDs will be needed in the next step to annotate using our custom concepts. We'll also need the base Face model ID which is the one where ",(0,s.kt)("inlineCode",{parentName:"p"},"model_version_id")," equals to ",(0,s.kt)("inlineCode",{parentName:"p"},"embedding_model_version_id"),"."),(0,s.kt)(r.Z,{mdxType:"Tabs"},(0,s.kt)(i.Z,{value:"grpc_python",label:"gRPC Python",mdxType:"TabItem"},(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},'list_annotations_response = stub.ListAnnotations(\n    service_pb2.ListAnnotationsRequest(\n        user_app_id=userDataObject,  # The userDataObject is created in the overview and is required when using a PAT\n        list_all_annotations=True, \n        page=1, \n        per_page=100\n    ),\n    metadata=metadata\n)\n\nif list_annotations_response.status.code != status_code_pb2.SUCCESS:\n    raise Exception("Failed response, status: " + str(list_annotations_response.status))\n\n\nfor annotation in list_annotations_response.annotations:\n    if not annotation.data or not annotation.data.regions:\n        continue\n    # Display results only for the base Face model.\n    if annotation.model_version_id != annotation.embed_model_version_id:\n        continue\n    for region in annotation.data.regions:\n        bbox = region.region_info.bounding_box\n        print(f"Face was detected on input ID {annotation.input_id} (URL: {input_id_to_url[annotation.input_id]})")\n        print(f"\\tRegion ID: {region.id}")\n        print(f"\\tRegion location: left={bbox.left_col:.4f}, top={bbox.top_row:.4f}, right={bbox.right_col:.4f}, bottom={bbox.bottom_row:.4f}")\n        print(f"\\tConfidence: {region.value:.2f}")\n        print(f"\\tBase Face model version ID: {annotation.embed_model_version_id}")\n        print()\n')))),(0,s.kt)("h2",{id:"post-new-annotations"},"Post new annotations"),(0,s.kt)("p",null,"Let's use the above information to add annotations, in the form of a concept, to the detected face regions."),(0,s.kt)("p",null,"Input below the IDs from the previous call, and choose your concept ID and name that you want to annotate the region with ","(","you may want to use e.g. person's name",")","."),(0,s.kt)(r.Z,{mdxType:"Tabs"},(0,s.kt)(i.Z,{value:"grpc_python",label:"gRPC Python",mdxType:"TabItem"},(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},'post_annotations_response = stub.PostAnnotations(\n    service_pb2.PostAnnotationsRequest(\n        user_app_id=userDataObject,  # The userDataObject is created in the overview and is required when using a PAT\n        annotations=[\n            resources_pb2.Annotation(\n                input_id="{MY_INPUT_ID}",\n                embed_model_version_id="{MY_EMBED_MODEL_VERSION_ID}",\n                data=resources_pb2.Data(\n                    regions=[\n                        resources_pb2.Region(\n                            id="{MY_REGION_ID}",\n                            data=resources_pb2.Data(\n                                concepts=[\n                                    resources_pb2.Concept(\n                                        id="{MY_CONCEPT_ID}",\n                                        name="{MY_CONCEPT_NAME}",\n                                        value=1\n                                    )\n                                ]\n                            )\n                        )\n                    ]\n                )\n\n            )\n        ],\n    ),\n    metadata=metadata\n)\n\nif post_annotations_response.status.code != status_code_pb2.SUCCESS:\n    raise Exception("Failed response, status: " + str(post_annotations_response.status))\n')))),(0,s.kt)("h2",{id:"create-a-knn-model"},"Create a KNN model"),(0,s.kt)("p",null,"Let's now create a KNN model using the concept IDs that were added above. The model type ID should be set to ",(0,s.kt)("inlineCode",{parentName:"p"},"knn-concept"),"."),(0,s.kt)(r.Z,{mdxType:"Tabs"},(0,s.kt)(i.Z,{value:"grpc_python",label:"gRPC Python",mdxType:"TabItem"},(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},'post_models_response = stub.PostModels(\n    service_pb2.PostModelsRequest(\n        user_app_id=userDataObject,  # The userDataObject is created in the overview and is required when using a PAT\n        models=[\n            resources_pb2.Model(\n                id="my-knn-face-classifier-model",\n                model_type_id="knn-concept",\n                output_info=resources_pb2.OutputInfo(\n                    data=resources_pb2.Data(\n                        concepts=[\n                            resources_pb2.Concept(id="{MY_CONCEPT_ID_1}"),\n                            resources_pb2.Concept(id="{MY_CONCEPT_ID_2}"),\n                        ]\n                    )\n                )\n            )\n        ]\n    ),\n    metadata=metadata\n)\n\nif post_models_response.status.code != status_code_pb2.SUCCESS:\n    raise Exception("Failed response, status: " + str(post_models_response.status))\n')))),(0,s.kt)("h2",{id:"create-a-workflow"},"Create a workflow"),(0,s.kt)("p",null,"One last step before being able to do predictions: create a workflow that's going to map from the base Face model to our custom KNN model."),(0,s.kt)(r.Z,{mdxType:"Tabs"},(0,s.kt)(i.Z,{value:"grpc_python",label:"gRPC Python",mdxType:"TabItem"},(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},'post_workflows_response = stub.PostWorkflows(\n    service_pb2.PostWorkflowsRequest(\n        user_app_id=userDataObject,  # The userDataObject is created in the overview and is required when using a PAT\n        workflows=[\n            resources_pb2.Workflow(\n                id="detect-knn-workflow",\n                nodes=[\n                    resources_pb2.WorkflowNode(\n                        id="face-v1.3-embed",\n                        model=resources_pb2.Model(\n                            id="d02b4508df58432fbb84e800597b8959",  # This is the base Face model ID.\n                            model_version=resources_pb2.ModelVersion(\n                                id="{EMBEDDING_MODEL_VERSION_ID}"  # This is the base Face model version ID.\n                            )\n                        )\n                    ),\n                    resources_pb2.WorkflowNode(\n                        id="knn-classifier",\n                        model=resources_pb2.Model(\n                            id="my-knn-face-classifier-model",\n                            model_version=resources_pb2.ModelVersion(\n                                id="{YOUR_MODEL_VERSION_ID}"\n                            )\n                        )\n                    ),\n                ]\n            )\n        ]\n    ),\n    metadata=metadata\n)\n\nif post_workflows_response.status.code != status_code_pb2.SUCCESS:\n    raise Exception("Failed response, status: " + str(post_workflows_response.status))\n')))),(0,s.kt)("h2",{id:"predict"},"Predict"),(0,s.kt)("p",null,"We're going to run a prediction on the workflow created above."),(0,s.kt)(r.Z,{mdxType:"Tabs"},(0,s.kt)(i.Z,{value:"grpc_python",label:"gRPC Python",mdxType:"TabItem"},(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},'post_workflow_results_response = stub.PostWorkflowResults(\n    service_pb2.PostWorkflowResultsRequest(\n        user_app_id=userDataObject,  # The userDataObject is created in the overview and is required when using a PAT\n        workflow_id="detect-knn-workflow",\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    image=resources_pb2.Image(\n                        url="{MY_URL_TO_PREDICT_FACES_ON}"\n                    )\n                )\n            )\n        ]\n    ),\n    metadata=metadata\n)\n\n# We get back one result per input. Since there\'s one input above, one input was returned.\nresult = post_workflow_results_response.results[0]\n\nfor output in result.outputs:\n    # At this point, two outputs will be returned: one for the base Face model, and the other for our custom model.\n    # At this moment, we are only interested in the results of the latter model (if you want, you may also see the\n    # half-baked results of the base Face model, which is not a list of concepts, but an embedding vector).\n    if output.model.id != "my-knn-face-classifier-model":\n        continue\n    print(f"The prediction of the model ID `{output.model.id}` is:")\n    for concept in output.data.concepts:\n        print(f"\\t{concept.name} (id: {concept.id}): {concept.value:.4f}")\n')))))}m.isMDXComponent=!0}}]);
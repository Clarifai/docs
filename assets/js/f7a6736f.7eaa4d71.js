"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[1333],{15680:(e,t,r)=>{r.d(t,{xA:()=>u,yg:()=>L});var n=r(96540);function i(e,t,r){return t in e?Object.defineProperty(e,t,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[t]=r,e}function a(e,t){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),r.push.apply(r,n)}return r}function o(e){for(var t=1;t<arguments.length;t++){var r=null!=arguments[t]?arguments[t]:{};t%2?a(Object(r),!0).forEach((function(t){i(e,t,r[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):a(Object(r)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(r,t))}))}return e}function l(e,t){if(null==e)return{};var r,n,i=function(e,t){if(null==e)return{};var r,n,i={},a=Object.keys(e);for(n=0;n<a.length;n++)r=a[n],t.indexOf(r)>=0||(i[r]=e[r]);return i}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(n=0;n<a.length;n++)r=a[n],t.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(i[r]=e[r])}return i}var s=n.createContext({}),c=function(e){var t=n.useContext(s),r=t;return e&&(r="function"==typeof e?e(t):o(o({},t),e)),r},u=function(e){var t=c(e.components);return n.createElement(s.Provider,{value:t},e.children)},f="mdxType",p={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},g=n.forwardRef((function(e,t){var r=e.components,i=e.mdxType,a=e.originalType,s=e.parentName,u=l(e,["components","mdxType","originalType","parentName"]),f=c(r),g=i,L=f["".concat(s,".").concat(g)]||f[g]||p[g]||a;return r?n.createElement(L,o(o({ref:t},u),{},{components:r})):n.createElement(L,o({ref:t},u))}));function L(e,t){var r=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var a=r.length,o=new Array(a);o[0]=g;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[f]="string"==typeof e?e:i,o[1]=l;for(var c=2;c<a;c++)o[c]=r[c];return n.createElement.apply(null,o)}return n.createElement.apply(null,r)}g.displayName="MDXCreateElement"},51424:(e,t,r)=>{r.r(t),r.d(t,{assets:()=>s,contentTitle:()=>o,default:()=>p,frontMatter:()=>a,metadata:()=>l,toc:()=>c});var n=r(58168),i=(r(96540),r(15680));const a={},o="LiteLLM Clarifai Integration",l={unversionedId:"integrations/LiteLLM/README",id:"integrations/LiteLLM/README",title:"LiteLLM Clarifai Integration",description:"Learn how to use LiteLLM along with Clarifai SDKs",source:"@site/docs/integrations/LiteLLM/README.mdx",sourceDirName:"integrations/LiteLLM",slug:"/integrations/LiteLLM/",permalink:"/integrations/LiteLLM/",draft:!1,editUrl:"https://github.com/Clarifai/docs/blob/main/docs/integrations/LiteLLM/README.mdx",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Multistage RAG Pipeline With DSPy",permalink:"/integrations/DSPy/multistage-dspy"},next:{title:"Using Clarifai Models In LiteLLM",permalink:"/integrations/LiteLLM/clarifai-litellm"}},s={},c=[],u={toc:c},f="wrapper";function p(e){let{components:t,...r}=e;return(0,i.yg)(f,(0,n.A)({},u,r,{components:t,mdxType:"MDXLayout"}),(0,i.yg)("h1",{id:"litellm-clarifai-integration"},"LiteLLM Clarifai Integration"),(0,i.yg)("p",null,(0,i.yg)("strong",{parentName:"p"},"Learn how to use LiteLLM along with Clarifai SDKs")),(0,i.yg)("hr",null),(0,i.yg)("p",null,(0,i.yg)("a",{parentName:"p",href:"https://www.litellm.ai/"},"LiteLLM")," takes complex large language models and makes them easier to use for everyone. LiteLLM works with many different language models, so you can find the perfect one for your task. LiteLLM offers functionalities like load balancing, fallback mechanisms, and cost tracking, making it a user-friendly and efficient solution for developers working with LLMs. It also runs efficiently, so you don't need a supercomputer to use it. Basically, it gives you the power of these advanced language models without all the hassle. By integrating Clarifai into LiteLLM, users gain access to Clarifai's various ready-to-use community LLM models."))}p.isMDXComponent=!0}}]);
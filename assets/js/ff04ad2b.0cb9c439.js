"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[1984],{19259:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>d,default:()=>h,frontMatter:()=>t,metadata:()=>l,toc:()=>a});var i=s(74848),r=s(28453);const t={description:"Learn about the instance types we support",sidebar_position:4,pagination_next:null},d="Supported Cloud Instances",l={id:"portal-guide/compute-orchestration/cloud-instances",title:"Supported Cloud Instances",description:"Learn about the instance types we support",source:"@site/docs/portal-guide/compute-orchestration/cloud-instances.md",sourceDirName:"portal-guide/compute-orchestration",slug:"/portal-guide/compute-orchestration/cloud-instances",permalink:"/portal-guide/compute-orchestration/cloud-instances",draft:!1,unlisted:!1,editUrl:"https://github.com/Clarifai/docs/blob/main/docs/portal-guide/compute-orchestration/cloud-instances.md",tags:[],version:"current",sidebarPosition:4,frontMatter:{description:"Learn about the instance types we support",sidebar_position:4,pagination_next:null},sidebar:"tutorialSidebar",previous:{title:"Managing Your Compute",permalink:"/portal-guide/compute-orchestration/manage"}},c={},a=[{value:"t3a Instances",id:"t3a-instances",level:2},{value:"g4dn Instances",id:"g4dn-instances",level:2},{value:"g5 Instances",id:"g5-instances",level:2},{value:"g6 Instances",id:"g6-instances",level:2}];function o(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",li:"li",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h1,{id:"supported-cloud-instances",children:"Supported Cloud Instances"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Learn about the instance types we support"})}),"\n",(0,i.jsx)("hr",{}),"\n",(0,i.jsx)(n.p,{children:"We offer a range of Amazon Web Services (AWS) instance types, designed to handle a variety of machine learning workloads. These instances vary in their CPU, RAM (Random Access Memory), and GPU configurations, which allow you to orchestrate the right balance of performance and cost for your use case."}),"\n",(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsxs)(n.p,{children:["You can ",(0,i.jsx)(n.a,{href:"https://www.clarifai.com/explore/contact-us",children:"contact us"})," to unlock access to our most powerful instances, including NVIDIA A100 and H100 GPUs."]})}),"\n",(0,i.jsx)(n.h2,{id:"t3a-instances",children:"t3a Instances"}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"t3a"})," series is designed for cost-effective, general-purpose workloads that do not require GPU acceleration. It offers a balanced combination of CPU and memory, making it ideal for lightweight applications."]}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Instance Type"}),(0,i.jsx)(n.th,{children:"GPUs"}),(0,i.jsx)(n.th,{children:"Total GPU RAM"}),(0,i.jsx)(n.th,{children:"CPU"}),(0,i.jsx)(n.th,{children:"RAM"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"t3a.medium"})}),(0,i.jsx)(n.td,{children:"-"}),(0,i.jsx)(n.td,{children:"-"}),(0,i.jsx)(n.td,{children:"2x CPU"}),(0,i.jsx)(n.td,{children:"4GiB"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"t3a.large"})}),(0,i.jsx)(n.td,{children:"-"}),(0,i.jsx)(n.td,{children:"-"}),(0,i.jsx)(n.td,{children:"2x CPU"}),(0,i.jsx)(n.td,{children:"8GiB"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"t3a.xlarge"})}),(0,i.jsx)(n.td,{children:"-"}),(0,i.jsx)(n.td,{children:"-"}),(0,i.jsx)(n.td,{children:"4x CPU"}),(0,i.jsx)(n.td,{children:"16GiB"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"t3a.2xlarge"})}),(0,i.jsx)(n.td,{children:"-"}),(0,i.jsx)(n.td,{children:"-"}),(0,i.jsx)(n.td,{children:"8x CPU"}),(0,i.jsx)(n.td,{children:"32GiB"})]})]})]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Key Features"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["vCPUs (virtual CPUs) \u2014 Burstable performance for intermittent, compute-heavy tasks. Ideal for CPU-intensive operations like running traditional models or pre-processing pipelines.  For example, ",(0,i.jsx)(n.code,{children:"t3a.medium"})," offers two vCPUs, while ",(0,i.jsx)(n.code,{children:"t3a.2xlarge"})," offers eight vCPUs."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"RAM \u2014 Determines the capacity for handling data in memory. It ranges from 4 GiB to 32 GiB, allowing you to handle lightweight, data-intensive workloads without requiring GPU acceleration."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Example Use Case"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Running simple models for classification tasks."}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"g4dn-instances",children:"g4dn Instances"}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"g4dn"})," series is designed for moderate GPU-accelerated workloads, making it suitable for small-to-medium-scale machine learning tasks."]}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Instance Type"}),(0,i.jsx)(n.th,{children:"GPUs"}),(0,i.jsx)(n.th,{children:"Total GPU RAM"}),(0,i.jsx)(n.th,{children:"CPU"}),(0,i.jsx)(n.th,{children:"RAM"})]})}),(0,i.jsx)(n.tbody,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"g4dn.xlarge"})}),(0,i.jsx)(n.td,{children:"1x T4"}),(0,i.jsx)(n.td,{children:"16GiB"}),(0,i.jsx)(n.td,{children:"4x CPU"}),(0,i.jsx)(n.td,{children:"16GiB"})]})})]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Key Features"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"NVIDIA T4 GPUs \u2014 Optimized for inference and light model training, offering a balance of performance and cost."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"vCPUs and RAM \u2014 Includes four vCPUs and 16 GiB of RAM for data processing and workload orchestration."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Example Use Cases"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Inference workloads, such as running NLP models like BERT-base for text summarization and question answering."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Fine-tuning pre-trained models for specific tasks like object detection or sentiment analysis."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"g5-instances",children:"g5 Instances"}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"g5"})," series delivers enhanced GPU capabilities and is designed for tasks requiring higher memory and computational power, such as large-scale deep learning model training."]}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Instance Type"}),(0,i.jsx)(n.th,{children:"GPUs"}),(0,i.jsx)(n.th,{children:"Total GPU RAM"}),(0,i.jsx)(n.th,{children:"CPU"}),(0,i.jsx)(n.th,{children:"RAM"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"g5.xlarge"})}),(0,i.jsx)(n.td,{children:"1x A10G"}),(0,i.jsx)(n.td,{children:"24GiB"}),(0,i.jsx)(n.td,{children:"4x CPU"}),(0,i.jsx)(n.td,{children:"16GiB"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"g5.2xlarge"})}),(0,i.jsx)(n.td,{children:"1x A10G"}),(0,i.jsx)(n.td,{children:"24GiB"}),(0,i.jsx)(n.td,{children:"8x CPU"}),(0,i.jsx)(n.td,{children:"32GiB"})]})]})]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Key Features"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"NVIDIA A10G GPUs \u2014 High memory bandwidth and compute power for complex deep learning models and advanced workloads."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"vCPUs and RAM \u2014 Increased CPU and memory for tasks involving heavy data processing alongside GPU computation."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Example Use Cases"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Training mid-sized NLP models like GPT-2 or T5 for text generation, or training image segmentation models like UNet or Mask R-CNN for medical imaging."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Running object tracking or pose estimation workflows in real-time video analysis."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"g6-instances",children:"g6 Instances"}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"g6"})," series offers next-generation GPU technologies and is designed for the most demanding machine learning workloads, including large-scale model training and high-performance simulations. Each instance type in the ",(0,i.jsx)(n.code,{children:"g6"})," series is tailored to specific workloads."]}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Instance Type"}),(0,i.jsx)(n.th,{children:"GPUs"}),(0,i.jsx)(n.th,{children:"Total GPU RAM"}),(0,i.jsx)(n.th,{children:"CPU"}),(0,i.jsx)(n.th,{children:"RAM"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"g6.xlarge"})}),(0,i.jsx)(n.td,{children:"1x L4"}),(0,i.jsx)(n.td,{children:"24GiB"}),(0,i.jsx)(n.td,{children:"4x CPU"}),(0,i.jsx)(n.td,{children:"16GiB"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"g6.2xlarge"})}),(0,i.jsx)(n.td,{children:"1x L4"}),(0,i.jsx)(n.td,{children:"24GiB"}),(0,i.jsx)(n.td,{children:"8x CPU"}),(0,i.jsx)(n.td,{children:"32GiB"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"g6e.xlarge"})}),(0,i.jsx)(n.td,{children:"1x L40S"}),(0,i.jsx)(n.td,{children:"48GiB"}),(0,i.jsx)(n.td,{children:"4x CPU"}),(0,i.jsx)(n.td,{children:"32GiB"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"g6e.12xlarge"})}),(0,i.jsx)(n.td,{children:"4x L40S"}),(0,i.jsx)(n.td,{children:"192GiB"}),(0,i.jsx)(n.td,{children:"48x CPU"}),(0,i.jsx)(n.td,{children:"384GiB"})]})]})]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Key Features"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Next-Gen GPUs \u2014 NVIDIA L4 and L40S GPUs deliver exceptional performance for training and inference tasks, with GPU memory scaling from 24 GiB to 192 GiB."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"High vCPU & RAM Configurations \u2014  Ideal for handling massive datasets and parallel processing for complex workflows."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Example Use Cases"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"g6.xlarge"})," and ",(0,i.jsx)(n.code,{children:"g6.2xlarge"})," instances support mid-tier workloads, such as fine-tuning the BERT-large model or running computer vision tasks like text-to-image generation."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"g6e.xlarge"})," and ",(0,i.jsx)(n.code,{children:"g6e.12xlarge"})," instances support high-end workloads, such as training large-scale language models like GPT-4 or T5-XL for multi-modal tasks."]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(o,{...e})}):o(e)}},28453:(e,n,s)=>{s.d(n,{R:()=>d,x:()=>l});var i=s(96540);const r={},t=i.createContext(r);function d(e){const n=i.useContext(t);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:d(e.components),i.createElement(t.Provider,{value:n},e.children)}}}]);
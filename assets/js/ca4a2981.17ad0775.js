"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[1832],{11470:(e,n,t)=>{t.d(n,{A:()=>L});var a=t(96540),l=t(18215),i=t(17559),o=t(23104),s=t(56347),r=t(205),c=t(57485),d=t(31682),u=t(70679);function h(e){return a.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,a.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function p(e){const{values:n,children:t}=e;return(0,a.useMemo)(()=>{const e=n??function(e){return h(e).map(({props:{value:e,label:n,attributes:t,default:a}})=>({value:e,label:n,attributes:t,default:a}))}(t);return function(e){const n=(0,d.XI)(e,(e,n)=>e.value===n.value);if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[n,t])}function m({value:e,tabValues:n}){return n.some(n=>n.value===e)}function f({queryString:e=!1,groupId:n}){const t=(0,s.W6)(),l=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,c.aZ)(l),(0,a.useCallback)(e=>{if(!l)return;const n=new URLSearchParams(t.location.search);n.set(l,e),t.replace({...t.location,search:n.toString()})},[l,t])]}function g(e){const{defaultValue:n,queryString:t=!1,groupId:l}=e,i=p(e),[o,s]=(0,a.useState)(()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!m({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const t=n.find(e=>e.default)??n[0];if(!t)throw new Error("Unexpected error: 0 tabValues");return t.value}({defaultValue:n,tabValues:i})),[c,d]=f({queryString:t,groupId:l}),[h,g]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[t,l]=(0,u.Dv)(n);return[t,(0,a.useCallback)(e=>{n&&l.set(e)},[n,l])]}({groupId:l}),x=(()=>{const e=c??h;return m({value:e,tabValues:i})?e:null})();(0,r.A)(()=>{x&&s(x)},[x]);return{selectedValue:o,selectValue:(0,a.useCallback)(e=>{if(!m({value:e,tabValues:i}))throw new Error(`Can't select invalid tab value=${e}`);s(e),d(e),g(e)},[d,g,i]),tabValues:i}}var x=t(92303);const y={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var b=t(74848);function v({className:e,block:n,selectedValue:t,selectValue:a,tabValues:i}){const s=[],{blockElementScrollPositionUntilNextRender:r}=(0,o.a_)(),c=e=>{const n=e.currentTarget,l=s.indexOf(n),o=i[l].value;o!==t&&(r(n),a(o))},d=e=>{let n=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const t=s.indexOf(e.currentTarget)+1;n=s[t]??s[0];break}case"ArrowLeft":{const t=s.indexOf(e.currentTarget)-1;n=s[t]??s[s.length-1];break}}n?.focus()};return(0,b.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,l.A)("tabs",{"tabs--block":n},e),children:i.map(({value:e,label:n,attributes:a})=>(0,b.jsx)("li",{role:"tab",tabIndex:t===e?0:-1,"aria-selected":t===e,ref:e=>{s.push(e)},onKeyDown:d,onClick:c,...a,className:(0,l.A)("tabs__item",y.tabItem,a?.className,{"tabs__item--active":t===e}),children:n??e},e))})}function j({lazy:e,children:n,selectedValue:t}){const i=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=i.find(e=>e.props.value===t);return e?(0,a.cloneElement)(e,{className:(0,l.A)("margin-top--md",e.props.className)}):null}return(0,b.jsx)("div",{className:"margin-top--md",children:i.map((e,n)=>(0,a.cloneElement)(e,{key:n,hidden:e.props.value!==t}))})}function A(e){const n=g(e);return(0,b.jsxs)("div",{className:(0,l.A)(i.G.tabs.container,"tabs-container",y.tabList),children:[(0,b.jsx)(v,{...n,...e}),(0,b.jsx)(j,{...n,...e})]})}function L(e){const n=(0,x.A)();return(0,b.jsx)(A,{...e,children:h(e.children)},String(n))}},19365:(e,n,t)=>{t.d(n,{A:()=>o});t(96540);var a=t(18215);const l={tabItem:"tabItem_Ymn6"};var i=t(74848);function o({children:e,hidden:n,className:t}){return(0,i.jsx)("div",{role:"tabpanel",className:(0,a.A)(l.tabItem,t),hidden:n,children:e})}},64374:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>x,contentTitle:()=>g,default:()=>v,frontMatter:()=>f,metadata:()=>a,toc:()=>y});const a=JSON.parse('{"id":"compute/inference/litellm","title":"LiteLLM","description":"Run inferences on Clarifai models using LiteLLM","source":"@site/docs/compute/inference/litellm.md","sourceDirName":"compute/inference","slug":"/compute/inference/litellm","permalink":"/compute/inference/litellm","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"description":"Run inferences on Clarifai models using LiteLLM","sidebar_position":3,"toc_max_heading_level":4},"sidebar":"tutorialSidebar","previous":{"title":"OpenAI","permalink":"/compute/inference/open-ai"},"next":{"title":"Vercel AI SDK","permalink":"/compute/inference/vercel"}}');var l=t(74848),i=t(28453),o=t(11470),s=t(19365),r=t(88149);const c='import os\nfrom litellm import completion\n\nresponse = completion(\n    model="clarifai/openai.chat-completion.gpt-oss-120b",\n    api_key=os.environ["CLARIFAI_PAT"],  # Ensure CLARIFAI_PAT is set as an environment variable\n\n    # Message formatting follows OpenAI\'s schema: {"role": ..., "content": ...}\n    messages=[\n        {"role": "system", "content": "You are a friendly assistant."},\n        {"role": "user", "content": "Hey, how\'s it going?"}\n    ],\n\n    # Optional OpenAI-compatible parameters\n    temperature=0.7,  # Controls randomness\n    max_tokens=100    # Limits response length\n)\n\nprint(response[\'choices\'][0][\'message\'][\'content\'])\n',d='import os\nfrom litellm import completion\n\n# Define tools the model can call\ntools = [\n    {\n        "type": "function",\n        "function": {\n            "name": "get_weather",\n            "description": "Get the current weather in a given location",\n            "parameters": {\n                "type": "object",\n                "properties": {\n                    "location": {\n                        "type": "string",\n                        "description": "City and state, e.g., \'San Francisco, CA\'"\n                    },\n                    "unit": {\n                        "type": "string",\n                        "enum": ["celsius", "fahrenheit"]\n                    }\n                },\n                "required": ["location", "unit"]\n            }\n        }\n    }\n]\n\n# Make the completion request via LiteLLM\nresponse = completion(\n    model="clarifai/openai.chat-completion.gpt-oss-120b",\n    api_key=os.environ["CLARIFAI_PAT"],  # Ensure CLARIFAI_PAT is set as an environment variable\n    messages=[{"role": "user", "content": "What\'s the weather like in San Francisco?"}],\n    tools=tools\n)\n\n# Print any tool calls suggested by the model\ntool_calls = response.choices[0].message.tool_calls\nif tool_calls:\n    print("Tool call suggested by the model:", tool_calls)\nelse:\n    print("No tool call was made by the model.")\n',u='import os\nimport litellm\n\nfor chunk in litellm.completion(\n    model="clarifai/openai.chat-completion.gpt-oss-120b",\n    api_key=os.environ["CLARIFAI_PAT"],  # Ensure CLARIFAI_PAT is set as an environment variable\n\n    # Message formatting follows OpenAI\'s schema: {"role": ..., "content": ...}\n    messages=[\n        {"role": "system", "content": "You are a friendly assistant."},\n        {"role": "user", "content": "Tell me a fun fact about space."}\n    ],\n\n    stream=True,       # Enable streaming responses\n    # Optional OpenAI-compatible parameters\n    temperature=0.7,   # Controls randomness\n    max_tokens=100     # Limits response length\n):\n    # Print incremental text as it arrives\n    print(chunk.choices[0].delta.get("content", ""), end="", flush=True)\n',h='import os\nimport json\nfrom litellm import completion\n\n# -----------------------------------------\n# Step 1: Define the tool schema\n# -----------------------------------------\ntools = [\n    {\n        "type": "function",\n        "function": {\n            "name": "get_weather",\n            "description": "Retrieve the current temperature for a given location.",\n            "parameters": {\n                "type": "object",\n                "properties": {\n                    "location": {\n                        "type": "string",\n                        "description": "City and country, e.g., \'Tokyo, Japan\'"\n                    }\n                },\n                "required": ["location"],\n                "additionalProperties": False\n            }\n        }\n    }\n]\n\n# -----------------------------------------\n# Step 2: Implement the tool logic\n# -----------------------------------------\ndef get_weather(location: str) -> str:\n    # In a real app, you\'d call a weather API here\n    return f"The current temperature in {location} is 22\xb0C."\n\n# -----------------------------------------\n# Step 3: Request a model completion that may trigger a tool call\n# -----------------------------------------\nresponse = completion(\n    model="clarifai/openai.chat-completion.gpt-oss-120b",\n    api_key=os.environ["CLARIFAI_PAT"],  # Ensure CLARIFAI_PAT is set\n    messages=[\n        {"role": "user", "content": "What is the weather in Paris today?"}\n    ],\n    tools=tools\n)\n\ntool_calls = response.choices[0].message.tool_calls\n\n# -----------------------------------------\n# Step 4: Parse and execute the tool call\n# -----------------------------------------\nif tool_calls:\n    for tool_call in tool_calls:\n        tool_name = tool_call.function.name\n        arguments = json.loads(tool_call.function.arguments)\n\n        if tool_name == "get_weather":\n            result = get_weather(arguments["location"])\n\n            # -----------------------------------------\n            # Step 5: Send tool result back to the model\n            # -----------------------------------------\n            follow_up = completion(\n                model="clarifai/openai.chat-completion.gpt-oss-120b",\n                api_key=os.environ["CLARIFAI_PAT"],\n                messages=[\n                    {"role": "user", "content": "What is the weather in Paris today?"},\n                    {"role": "assistant", "tool_calls": [tool_call]},\n                    {\n                        "role": "tool",\n                        "tool_call_id": tool_call.id,\n                        "content": result\n                    }\n                ]\n            )\n\n            # Print the assistant\'s final response\n            print(follow_up.choices[0].message.content)\nelse:\n    print("No tool was called.")\n',p='import openai\n\nclient = openai.OpenAI(\n    api_key="anything",  # LiteLLM proxy accepts any key\n    base_url="http://0.0.0.0:4000"\n)\n\nresponse = client.chat.completions.create(\n    model="clarifai-model",\n    messages=[\n        {\n            "role": "user",\n            "content": "this is a test request, write a short poem"\n        }\n    ]\n)\n\nprint(response)',m='curl --location \'http://0.0.0.0:4000/chat/completions\' \\\n--header \'Content-Type: application/json\' \\\n--data \'{\n  "model": "clarifai-model",\n  "messages": [\n    {"role": "user", "content": "what llm are you"}\n  ]\n}\'',f={description:"Run inferences on Clarifai models using LiteLLM",sidebar_position:3,toc_max_heading_level:4},g="LiteLLM",x={},y=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Install LiteLLM",id:"install-litellm",level:3},{value:"Get a PAT Key",id:"get-a-pat-key",level:3},{value:"Get a Clarifai Model",id:"get-a-clarifai-model",level:3},{value:"Chat Completions",id:"chat-completions",level:2},{value:"Streaming",id:"streaming",level:2},{value:"Tool Calling",id:"tool-calling",level:2},{value:"Usage with LiteLLM Proxy",id:"usage-with-litellm-proxy",level:2},{value:"Install LiteLLM with Proxy Support",id:"install-litellm-with-proxy-support",level:3},{value:"Set Key",id:"set-key",level:3},{value:"Start the Proxy",id:"start-the-proxy",level:3},{value:"Test the Proxy",id:"test-the-proxy",level:3}];function b(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",p:"p",pre:"pre",strong:"strong",...(0,i.R)(),...e.components},{Details:t}=n;return t||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(n.header,{children:(0,l.jsx)(n.h1,{id:"litellm",children:"LiteLLM"})}),"\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.strong,{children:"Run inferences on Clarifai models using LiteLLM"})}),"\n",(0,l.jsx)("hr",{}),"\n",(0,l.jsx)(n.p,{children:"LiteLLM provides a universal interface that simplifies working with LLMs across multiple providers. It offers a single, consistent API for making inferences, allowing you to interact with a wide range of models using the same method, regardless of the underlying provider."}),"\n",(0,l.jsxs)(n.p,{children:["LiteLLM natively supports OpenAI-compatible APIs, making it easy to run inferences on ",(0,l.jsx)(n.a,{href:"https://docs.litellm.ai/docs/providers/clarifai",children:"Clarifai-hosted"})," models with minimal setup."]}),"\n",(0,l.jsx)(n.admonition,{type:"tip",children:(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.a,{href:"https://github.com/Clarifai/examples/tree/main/models/model_predict",children:"Click here"})," for additional examples on how to perform model predictions using various SDKs \u2014 such as the Clarifai SDK, OpenAI client, and LiteLLM. The examples demonstrate various model types and include both streaming and non-streaming modes, as well as tool calling capabilities."]})}),"\n","\n","\n",(0,l.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,l.jsx)(n.h3,{id:"install-litellm",children:"Install LiteLLM"}),"\n",(0,l.jsxs)(n.p,{children:["Install the ",(0,l.jsx)(n.code,{children:"litellm"})," package."]}),"\n",(0,l.jsx)(o.A,{groupId:"code",children:(0,l.jsx)(s.A,{value:"bash",label:"Python",children:(0,l.jsx)(r.A,{className:"language-bash",children:"pip install litellm"})})}),"\n",(0,l.jsx)(n.h3,{id:"get-a-pat-key",children:"Get a PAT Key"}),"\n",(0,l.jsxs)(n.p,{children:["You need a ",(0,l.jsx)(n.a,{href:"https://docs.clarifai.com/control/authentication/pat",children:"PAT"})," key to authenticate your connection to the Clarifai platform. You can get one by navigating to ",(0,l.jsx)(n.strong,{children:"Settings"})," in the collapsible left sidebar, selecting ",(0,l.jsx)(n.strong,{children:"Secrets"}),", and creating or copying an existing token from there."]}),"\n",(0,l.jsxs)(n.p,{children:["You can then set the PAT as an environment variable using ",(0,l.jsx)(n.code,{children:"CLARIFAI_PAT"}),":"]}),"\n",(0,l.jsxs)(o.A,{groupId:"code",children:[(0,l.jsx)(s.A,{value:"bash",label:"Unix-Like Systems",children:(0,l.jsx)(r.A,{className:"language-bash",children:"export CLARIFAI_PAT=YOUR_PERSONAL_ACCESS_TOKEN_HERE"})}),(0,l.jsx)(s.A,{value:"bash2",label:"Windows",children:(0,l.jsx)(r.A,{className:"language-bash",children:"set CLARIFAI_PAT=YOUR_PERSONAL_ACCESS_TOKEN_HERE"})})]}),"\n",(0,l.jsx)(n.h3,{id:"get-a-clarifai-model",children:"Get a Clarifai Model"}),"\n",(0,l.jsxs)(n.p,{children:["Go to the Clarifai ",(0,l.jsx)(n.a,{href:"https://clarifai.com/explore",children:"Community"})," platform and select the model you want to use for predictions. LiteLLM supports all models in the Clarifai community."]}),"\n",(0,l.jsx)(n.admonition,{type:"note",children:(0,l.jsxs)(n.p,{children:["When using a Clarifai model with LiteLLM, reference it using the ",(0,l.jsx)(n.code,{children:"clarifai/"}),"-prefixed model ID in the following format: ",(0,l.jsx)(n.code,{children:"clarifai/<user_id>.<app_id>.<model_id>"}),". For example: ",(0,l.jsx)(n.code,{children:"clarifai/openai.chat-completion.gpt-oss-20b"}),"."]})}),"\n",(0,l.jsx)(n.h2,{id:"chat-completions",children:"Chat Completions"}),"\n",(0,l.jsxs)(n.p,{children:["In LiteLLM, the ",(0,l.jsx)(n.a,{href:"https://docs.litellm.ai/docs/completion",children:(0,l.jsx)(n.code,{children:"completion()"})})," function is the primary method for interacting with language models that follow the OpenAI Chat API format. It supports both traditional completions and chat-based interactions by accepting a list of messages \u2014 similar to OpenAI\u2019s ",(0,l.jsx)(n.a,{href:"https://docs.clarifai.com/compute/inference/open-ai/#chat-completions-api",children:(0,l.jsx)(n.code,{children:"chat.completions.create()"})}),"."]}),"\n",(0,l.jsx)(o.A,{children:(0,l.jsx)(s.A,{value:"python",label:"Python SDK",children:(0,l.jsx)(r.A,{className:"language-python",children:c})})}),"\n",(0,l.jsxs)(t,{children:[(0,l.jsx)("summary",{children:"Example Output"}),(0,l.jsx)(r.A,{className:"language-text",children:"Hey there! I'm doing great\u2014thanks for asking. How about you? Anything fun or interesting on your mind today?"})]}),"\n",(0,l.jsx)(n.h2,{id:"streaming",children:"Streaming"}),"\n",(0,l.jsxs)(n.p,{children:["When ",(0,l.jsx)(n.a,{href:"https://docs.litellm.ai/docs/completion/stream",children:"streaming"})," is enabled by setting ",(0,l.jsx)(n.code,{children:"stream=True"}),", the ",(0,l.jsx)(n.code,{children:"completion"})," method returns an iterator that yields partial responses in real time as the model generates them, instead of a single complete dictionary."]}),"\n",(0,l.jsx)(o.A,{children:(0,l.jsx)(s.A,{value:"python",label:"Python SDK",children:(0,l.jsx)(r.A,{className:"language-python",children:u})})}),"\n",(0,l.jsx)(n.h2,{id:"tool-calling",children:"Tool Calling"}),"\n",(0,l.jsxs)(n.p,{children:["Clarifai models accessed via LiteLLM fully support ",(0,l.jsx)(n.a,{href:"https://docs.clarifai.com/compute/inference/open-ai/#tool-calling",children:"tool calling"}),", enabling advanced interactions such as function execution during a conversation."]}),"\n",(0,l.jsx)(o.A,{children:(0,l.jsx)(s.A,{value:"python",label:"Python SDK",children:(0,l.jsx)(r.A,{className:"language-python",children:d})})}),"\n",(0,l.jsxs)(t,{children:[(0,l.jsx)("summary",{children:" Tool Calling Implementation Example"}),(0,l.jsx)(r.A,{className:"language-python",children:h})]}),"\n",(0,l.jsx)(n.h2,{id:"usage-with-litellm-proxy",children:"Usage with LiteLLM Proxy"}),"\n",(0,l.jsx)(n.p,{children:"Here\u2019s how to call Clarifai models through the LiteLLM Proxy Server."}),"\n",(0,l.jsx)(n.h3,{id:"install-litellm-with-proxy-support",children:"Install LiteLLM with Proxy Support"}),"\n",(0,l.jsx)(o.A,{groupId:"code",children:(0,l.jsx)(s.A,{value:"bash",label:"Python",children:(0,l.jsx)(r.A,{className:"language-bash",children:"pip install 'litellm[proxy]'"})})}),"\n",(0,l.jsx)(n.h3,{id:"set-key",children:"Set Key"}),"\n",(0,l.jsxs)(n.p,{children:["Set your Clarifai PAT as an environment variable, as illustrated ",(0,l.jsx)(n.a,{href:"#get-a-pat-key",children:"above"}),"."]}),"\n",(0,l.jsx)(n.h3,{id:"start-the-proxy",children:"Start the Proxy"}),"\n",(0,l.jsxs)(n.p,{children:["Create a ",(0,l.jsx)(n.code,{children:"config.yaml"}),"."]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-yaml",children:"model_list:\n  - model_name: clarifai-model\n    litellm_params:\n      model: clarifai/openai.chat-completion.gpt-oss-20b\n      api_key: ${CLARIFAI_PAT}\n"})}),"\n",(0,l.jsx)(n.p,{children:"Then, start the LiteLLM proxy:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"litellm --config /path/to/config.yaml\n"})}),"\n",(0,l.jsx)(n.p,{children:"The server will run at:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:"http://0.0.0.0:4000\n"})}),"\n",(0,l.jsx)(n.h3,{id:"test-the-proxy",children:"Test the Proxy"}),"\n",(0,l.jsxs)(o.A,{groupId:"code",children:[(0,l.jsx)(s.A,{value:"curl",label:"cURL",children:(0,l.jsx)(r.A,{className:"language-python",children:m})}),(0,l.jsx)(s.A,{value:"openai",label:"Python (OpenAI)",children:(0,l.jsx)(r.A,{className:"language-php",children:p})})]})]})}function v(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,l.jsx)(n,{...e,children:(0,l.jsx)(b,{...e})}):b(e)}}}]);
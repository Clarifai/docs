"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[2757],{29032:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>N,contentTitle:()=>D,default:()=>S,frontMatter:()=>P,metadata:()=>o,toc:()=>E});const o=JSON.parse('{"id":"compute/models/upload/README","title":"Model Uploading","description":"Import custom models, including from external sources like Hugging Face and OpenAI","source":"@site/docs/compute/models/upload/README.mdx","sourceDirName":"compute/models/upload","slug":"/compute/models/upload/","permalink":"/compute/models/upload/","draft":false,"unlisted":false,"editUrl":"https://github.com/Clarifai/docs/blob/main/docs/compute/models/upload/README.mdx","tags":[],"version":"current","frontMatter":{"description":"Import custom models, including from external sources like Hugging Face and OpenAI","toc_min_heading_level":2,"toc_max_heading_level":5},"sidebar":"tutorialSidebar","previous":{"title":"Advanced Inference Options","permalink":"/compute/models/inference/advanced"},"next":{"title":"Test Models Locally","permalink":"/compute/models/upload/test-locally"}}');var i=t(74848),r=t(28453),s=t(65537),l=t(79329),a=t(58069);const d='from clarifai.runners.models.model_class import ModelClass\nfrom clarifai.runners.utils.data_types import Text\nfrom typing import Iterator\n\n\nclass MyModel(ModelClass):\n  """A custom runner that adds "Hello World" to the end of the text."""\n\n  def load_model(self):\n    """Load the model here."""\n\n  @ModelClass.method\n  def predict(self, text1: Text = "") -> Text:\n    """This is the method that will be called when the runner is run. It takes in an input and\n    returns an output.\n    """\n\n    output_text = text1.text + " Hello World!"\n\n    return Text(output_text)\n\n  @ModelClass.method\n  def generate(self, text1: Text = Text("")) -> Iterator[Text]:\n    """Example yielding a whole batch of streamed stuff back."""\n\n    for i in range(10):  # fake something iterating generating 10 times.\n      output_text = text1.text + f"Generate Hello World {i}"\n      yield Text(output_text)\n\n  @ModelClass.method\n  def stream(self, input_iterator: Iterator[Text]) -> Iterator[Text]:\n    """Example yielding a whole batch of streamed stuff back."""\n\n    for i, input in enumerate(input_iterator):\n      output_text = input.text + f"Stream Hello World {i}"\n      yield Text(output_text)',c='model:\n  id: "my-uploaded-model"\n  user_id: "YOUR_USER_ID_HERE"\n  app_id: "YOUR_APP_ID_HERE"\n  model_type_id: "text-to-text"\n\nbuild_info:\n  python_version: "3.11"\n\ninference_compute_info:\n  cpu_limit: "1"\n  cpu_memory: "13Gi"\n  num_accelerators: 1\n  accelerator_type: ["NVIDIA-*"]\n  accelerator_memory: "15Gi"',m='checkpoints:\n  type: "huggingface"\n  repo_id: "meta-llama/Meta-Llama-3-8B-Instruct"\n  when: "runtime"\n  hf_token: "your_hf_token" # Required for private models',p="concepts:\n  - id: '0'\n    name: bus\n  - id: '1'\n    name: person\n  - id: '2'\n    name: bicycle\n  - id: '3'\n    name: car",u='import os\nimport tempfile\nfrom typing import List, Iterator\nfrom io import BytesIO\nimport cv2\nimport torch\nfrom transformers import AutoModelForImageClassification, ViTImageProcessor\n\nfrom clarifai.runners.models.model_class import ModelClass\nfrom clarifai.runners.utils.data_types import Image,  Concept, Video\nfrom clarifai.runners.models.model_builder import ModelBuilder\n\nfrom PIL import Image as PILImage\n\n\ndef video_to_frames(video_bytes):\n  """Convert video bytes to frames."""\n  frames = []\n  with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as temp_video_file:\n    temp_video_file.write(video_bytes)\n    temp_video_path = temp_video_file.name\n\n    video = cv2.VideoCapture(temp_video_path)\n    while video.isOpened():\n      ret, frame = video.read()\n      if not ret:\n        break\n      frame_bytes = cv2.imencode(\'.jpg\', frame)[1].tobytes()\n      frames.append(frame_bytes)\n    video.release()\n  return frames\n\ndef preprocess_image(image_bytes):\n  """Convert image bytes into RGB format suitable for model processing\n  Args:\n      image_bytes: Raw image data in bytes format\n  Returns:\n      PIL Image object in RGB format ready for model input\n  """\n  return PILImage.open(BytesIO(image_bytes)).convert("RGB")\n\ndef process_concepts( logits, model_labels):\n  """Process logits and map them to concepts."""\n  outputs = []\n  for logit in logits:\n    probs = torch.softmax(logit, dim=-1)\n    sorted_indices = torch.argsort(probs, dim=-1, descending=True)\n    output_concepts = []\n    for idx in sorted_indices:\n      concept = Concept(id = model_labels[idx.item()],name=model_labels[idx.item()], value=probs[idx].item())\n      output_concepts.append(concept)\n    outputs.append(output_concepts)\n  return outputs\n\n\nclass ImageClassifierModel(ModelClass):\n  """A custom runner that classifies images and outputs concepts."""\n\n  def load_model(self):\n    """Load the model and processor."""\n\n    model_path = os.path.dirname(os.path.dirname(__file__))\n    builder = ModelBuilder(model_path, download_validation_only=True)\n    checkpoints = builder.download_checkpoints(stage="runtime")\n\n    self.device = \'cuda\' if torch.cuda.is_available() else \'cpu\'\n\n    self.model = AutoModelForImageClassification.from_pretrained(checkpoints,).to(self.device)\n    self.model_labels = self.model.config.id2label\n    self.processor = ViTImageProcessor.from_pretrained(checkpoints)\n\n  @ModelClass.method\n  def predict(self, image: Image) -> List[List[Concept]]:\n    """Predict concepts for a list of images."""\n    pil_image = preprocess_image(image.bytes)\n    inputs = self.processor(images=pil_image, return_tensors="pt")\n    inputs = {name: tensor.to(self.device) for name, tensor in inputs.items()}\n    with torch.no_grad():\n      logits = self.model(**inputs).logits\n    return process_concepts(logits, self.model_labels)\n\n  @ModelClass.method\n  def generate(self, video: Video) -> Iterator[List[Concept]]:\n      """Generate concepts for frames extracted from a video."""\n      video_bytes = video.bytes\n      frame_generator = video_to_frames(video_bytes)\n      for frame in frame_generator:\n          image = preprocess_image(frame)\n          inputs = self.processor(images=image, return_tensors="pt")\n          inputs = {name: tensor.to(self.device) for name, tensor in inputs.items()}\n          with torch.no_grad():\n              logits = self.model(**inputs).logits\n              yield process_concepts(logits, self.model_labels)  # Yield concepts for each frame\n\n\n  @ModelClass.method\n  def stream_image(self, image_stream: Iterator[Image]) -> Iterator[List[Concept]]:\n      """Stream process image inputs."""\n      for image in image_stream:\n          result = self.predict(image)\n          yield result\n\n  @ModelClass.method\n  def stream_video(self, video_stream: Iterator[Video]) -> Iterator[List[Concept]]:\n      """Stream process video inputs."""\n      for video in video_stream:\n          for frame_result in self.generate(video):\n              yield frame_result',h="torch==2.5.1\ntransformers>=4.47.0\npillow==10.4.0\nrequests==2.32.3\ntimm==1.0.12\nopencv-python-headless==4.10.0.84\nnumpy\naiohttp\nclarifai>=11.3.0\nclarifai-protocol>=0.0.20",f="model:\n  id: model_id\n  user_id: user_id\n  app_id: app_id\n  model_type_id: visual-classifier\nbuild_info:\n  python_version: '3.11'\ninference_compute_info:\n  cpu_limit: '2'\n  cpu_memory: 2Gi\n  num_accelerators: 1\n  accelerator_type:\n  - NVIDIA-A10G\n  accelerator_memory: 3Gi\ncheckpoints:\n  type: huggingface\n  repo_id: Falconsai/nsfw_image_detection\n  hf_token: hf_token",g='# Standard library imports\nimport os\nimport tempfile\nimport time\nfrom io import BytesIO\nfrom typing import List, Dict, Any, Iterator\n\n# Third-party imports\nimport cv2\nimport torch\nfrom PIL import Image as PILImage\nfrom transformers import DetrForObjectDetection, DetrImageProcessor\n\n# Clarifai imports\nfrom clarifai.runners.models.model_class import ModelClass\nfrom clarifai.runners.models.model_builder import ModelBuilder\nfrom clarifai.runners.utils.data_types import Concept, Image, Video, Region\nfrom clarifai.utils.logging import logger\n\n\ndef preprocess_image(image_bytes: bytes) -> PILImage:\n    """Convert image bytes into RGB format suitable for model processing.\n\n    Args:\n        image_bytes: Raw image data in bytes format\n\n    Returns:\n        PIL Image object in RGB format ready for model input\n    """\n    return PILImage.open(BytesIO(image_bytes)).convert("RGB")\n\n\ndef detect_objects(\n    images: List[PILImage],\n    model: DetrForObjectDetection,\n    processor: DetrImageProcessor,\n    device: str\n) -> Dict[str, Any]:\n    """Process images through the DETR model to detect objects.\n\n    Args:\n        images: List of preprocessed images\n        model: DETR model instance\n        processor: Image processor for DETR\n        device: Computation device (CPU/GPU)\n\n    Returns:\n        Detection results from the model\n    """\n    model_inputs = processor(images=images, return_tensors="pt").to(device)\n    model_inputs = {name: tensor.to(device) for name, tensor in model_inputs.items()}\n    model_output = model(**model_inputs)\n    results = processor.post_process_object_detection(model_output)\n    return results\n\n\ndef process_detections(\n    results: List[Dict[str, torch.Tensor]],\n    images: List[PILImage],\n    threshold: float,\n    model_labels: Dict[int, str]\n) -> List[List[Region]]:\n    """Convert model outputs into a structured format of detections.\n\n    Args:\n        results: Raw detection results from model\n        images: Original input images\n        threshold: Confidence threshold for detections\n        model_labels: Dictionary mapping label indices to names\n\n    Returns:\n        List of lists containing Region objects for each detection\n    """\n    outputs = []\n    for i, result in enumerate(results):\n        image = images[i]\n        detections = []\n        for score, label_idx, box in zip(result["scores"], result["labels"], result["boxes"]):\n            if score > threshold:\n                label = model_labels[label_idx.item()]\n                detections.append(\n                    Region(\n                        box=box.tolist(),\n                        concepts=[Concept(id=label, name=label, value=score.item())]\n                    )\n                )\n        outputs.append(detections)\n    return outputs\n\n\ndef video_to_frames(video_bytes: bytes) -> Iterator[bytes]:\n    """Convert video bytes to frames.\n\n    Args:\n        video_bytes: Raw video data in bytes\n\n    Yields:\n        JPEG encoded frame data as bytes\n    """\n    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as temp_video_file:\n        temp_video_file.write(video_bytes)\n        temp_video_path = temp_video_file.name\n        logger.info(f"temp_video_path: {temp_video_path}")\n\n        video = cv2.VideoCapture(temp_video_path)\n        logger.info(f"video opened: {video.isOpened()}")\n        \n        while video.isOpened():\n            ret, frame = video.read()\n            if not ret:\n                break\n            frame_bytes = cv2.imencode(\'.jpg\', frame)[1].tobytes()\n            yield frame_bytes\n            \n        video.release()\n        os.unlink(temp_video_path)\n\n\nclass MyRunner(ModelClass):\n    """A custom runner for DETR object detection model that processes images and videos"""\n\n    def load_model(self):\n        """Load the model here."""\n        model_path = os.path.dirname(os.path.dirname(__file__))\n        builder = ModelBuilder(model_path, download_validation_only=True)\n        checkpoint_path = builder.download_checkpoints(stage="runtime")\n        \n        self.device = \'cuda\' if torch.cuda.is_available() else \'cpu\'\n        logger.info(f"Running on device: {self.device}")\n\n        self.model = DetrForObjectDetection.from_pretrained(checkpoint_path).to(self.device)\n        self.processor = DetrImageProcessor.from_pretrained(checkpoint_path)\n        self.model.eval()\n        self.threshold = 0.9\n        self.model_labels = self.model.config.id2label\n\n        logger.info("Done loading!")\n\n    @ModelClass.method \n    def predict(self, image: Image) -> List[Region]:\n        """Process a single image and return detected objects."""\n        image_bytes = image.bytes\n        image = preprocess_image(image_bytes)\n        \n        with torch.no_grad():\n            results = detect_objects([image], self.model, self.processor, self.device)\n            outputs = process_detections(results, [image], self.threshold, self.model_labels)\n            return outputs[0]  # Return detections for single image\n\n    @ModelClass.method\n    def generate(self, video: Video) -> Iterator[List[Region]]:\n        """Process video frames and yield detected objects for each frame."""\n        video_bytes = video.bytes\n        frame_generator = video_to_frames(video_bytes)\n        for frame in frame_generator:\n            image = preprocess_image(frame)\n            with torch.no_grad():\n                results = detect_objects([image], self.model, self.processor, self.device)\n                outputs = process_detections(results, [image], self.threshold, self.model_labels)\n                yield outputs[0]  # Yield detections for each frame\n\n    @ModelClass.method\n    def stream_image(self, image_stream: Iterator[Image]) -> Iterator[List[Region]]:\n        """Stream process image inputs."""\n        logger.info("Starting stream processing for images")\n        for image in image_stream:\n            start_time = time.time()\n            result = self.predict(image)\n            yield result\n            logger.info(f"Processing time: {time.time() - start_time:.3f}s")\n\n    @ModelClass.method\n    def stream_video(self, video_stream: Iterator[Video]) -> Iterator[List[Region]]:\n        """Stream process video inputs."""\n        logger.info("Starting stream processing for videos")\n        for video in video_stream:\n            start_time = time.time()\n            for frame_result in self.generate(video):\n                yield frame_result\n            logger.info(f"Processing time: {time.time() - start_time:.3f}s")\n        \n    def test(self):\n        """Test the model functionality."""\n        import requests  # Import moved here as it\'s only used for testing\n        \n        # Test configuration\n        TEST_URLS = {\n            "images": [\n                "https://samples.clarifai.com/metro-north.jpg",\n                "https://samples.clarifai.com/dog.tiff"\n            ],\n            "video": "https://samples.clarifai.com/beer.mp4"\n        }\n\n        def get_test_data(url):\n            return Image(bytes=requests.get(url).content)\n\n        def get_test_video():\n            return Video(bytes=requests.get(TEST_URLS["video"]).content)\n\n        def run_test(name, test_fn):\n            logger.info(f"\\nTesting {name}...")\n            try:\n                test_fn()\n                logger.info(f"{name} test completed successfully")\n            except Exception as e:\n                logger.error(f"Error in {name} test: {e}")\n\n        # Test predict\n        def test_predict():\n            result = self.predict(get_test_data(TEST_URLS["images"][0]))\n            logger.info(f"Predict result: {result}")\n\n        # Test generate\n        def test_generate():\n            for detections in self.generate(get_test_video()):\n                logger.info(f"First frame detections: {detections}")\n                break\n\n        # Test stream\n        def test_stream():\n            # Split into two separate test functions for clarity\n            def test_stream_image():\n                images = [get_test_data(url) for url in TEST_URLS["images"]]\n                for result in self.stream_image(iter(images)):\n                    logger.info(f"Image stream result: {result}")\n\n            def test_stream_video():\n                for result in self.stream_video(iter([get_test_video()])):\n                    logger.info(f"Video stream result: {result}")\n                    break  # Just test first frame\n\n            logger.info("\\nTesting image streaming...")\n            test_stream_image()\n            logger.info("\\nTesting video streaming...")\n            test_stream_video()\n\n        # Run all tests\n        for test_name, test_fn in [\n            ("predict", test_predict),\n            ("generate", test_generate),\n            ("stream", test_stream)\n        ]:\n            run_test(test_name, test_fn)',x="torch==2.5.1\ntransformers>=4.47.0\npillow==10.4.0\nrequests==2.32.3\ntimm==1.0.12\nopencv-python-headless==4.10.0.84\nclarifai>=11.3.0",y='# This is the sample config file for the image-detection model.\n\nmodel:\n  id: "detr-resnet-50"\n  user_id: "user_id"\n  app_id: "app_id"\n  model_type_id: "visual-detector"\n\nbuild_info:\n  python_version: "3.11"\n\ninference_compute_info:\n  cpu_limit: "4"\n  cpu_memory: "2Gi"\n  num_accelerators: 1\n  accelerator_type: ["NVIDIA-*"]\n  accelerator_memory: "5Gi"\n\n\ncheckpoints:\n  type: "huggingface"\n  repo_id: "facebook/detr-resnet-50"\n  hf_token: "hf_token"',_='from typing import List, Iterator\nfrom threading import Thread\nimport os\nimport torch\n\nfrom clarifai.runners.models.model_class import ModelClass\nfrom clarifai.utils.logging import logger\nfrom clarifai.runners.models.model_builder import ModelBuilder\nfrom clarifai.runners.utils.openai_convertor import openai_response\nfrom transformers import (AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer)\n\n\nclass MyModel(ModelClass):\n  """A custom runner for llama-3.2-1b-instruct llm that integrates with the Clarifai platform"""\n\n  def load_model(self):\n    """Load the model here."""\n    self.device = \'cuda\' if torch.cuda.is_available() else \'cpu\'\n    logger.info(f"Running on device: {self.device}")\n\n    # Load checkpoints\n    model_path = os.path.dirname(os.path.dirname(__file__))\n    builder = ModelBuilder(model_path, download_validation_only=True)\n    self.checkpoints = builder.download_checkpoints(stage="runtime")\n    \n    # Load model and tokenizer\n    self.tokenizer = AutoTokenizer.from_pretrained(self.checkpoints,)\n    self.tokenizer.pad_token = self.tokenizer.eos_token  # Set pad token to eos token\n    self.model = AutoModelForCausalLM.from_pretrained(\n        self.checkpoints,\n        low_cpu_mem_usage=True,\n        device_map=self.device,\n        torch_dtype=torch.bfloat16,\n    )\n    self.streamer = TextIteratorStreamer(tokenizer=self.tokenizer,)\n    self.chat_template = None\n    logger.info("Done loading!")\n\n  @ModelClass.method\n  def predict(self,\n              prompt: str ="",\n              chat_history: List[dict] = None,\n              max_tokens: int = 512,\n              temperature: float = 0.7,\n              top_p: float = 0.8) -> str:\n    """\n    Predict the response for the given prompt and chat history using the model.\n    """\n    # Construct chat-style messages\n    messages = chat_history if chat_history else []\n    if prompt:\n        messages.append({\n            "role": "user",\n            "content": [{"type": "text", "text": prompt}]\n        })\n\n    inputs = self.tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors="pt").to(self.model.device)\n\n    generation_kwargs = {\n        "input_ids": inputs["input_ids"],\n        "do_sample": True,\n        "max_new_tokens": max_tokens,\n        "temperature": temperature,\n        "top_p": top_p,\n        "eos_token_id": self.tokenizer.eos_token_id,\n    }\n\n    output = self.model.generate(**generation_kwargs)\n    generated_tokens = output[0][inputs["input_ids"].shape[-1]:]\n    return self.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n\n  @ModelClass.method\n  def generate(self,\n              prompt: str="",\n              chat_history: List[dict] = None,\n              max_tokens: int = 512,\n              temperature: float = 0.7,\n              top_p: float = 0.8) -> Iterator[str]:\n      """Stream generated text tokens from a prompt + optional chat history."""\n\n      # Construct chat-style messages\n      messages = chat_history if chat_history else []\n      if prompt:\n          messages.append({\n              "role": "user",\n              "content": [{"type": "text", "text": prompt}]\n          })\n      \n      response = self.chat(\n          messages=messages,\n          max_tokens=max_tokens,\n          temperature=temperature,\n          top_p=top_p\n      )\n      for each in response:\n        yield each[\'choices\'][0][\'delta\'][\'content\']\n\n\n  @ModelClass.method\n  def chat(self,\n          messages: List[dict],\n          max_tokens: int = 512,\n          temperature: float = 0.7,\n          top_p: float = 0.8) -> Iterator[dict]:\n      """\n      Stream back JSON dicts for assistant messages.\n      Example return format:\n      {"role": "assistant", "content": [{"type": "text", "text": "response here"}]}\n      """\n\n      # Tokenize using chat template\n      inputs = self.tokenizer.apply_chat_template(\n          messages,\n          tokenize=True,\n          add_generation_prompt=True,\n          return_tensors="pt"\n      ).to(self.model.device)\n\n      generation_kwargs = {\n          "input_ids": inputs["input_ids"],\n          "do_sample": True,\n          "max_new_tokens": max_tokens,\n          "temperature": temperature,\n          "top_p": top_p,\n          "eos_token_id": self.tokenizer.eos_token_id,\n          "streamer": self.streamer\n      }\n\n      thread = Thread(target=self.model.generate, kwargs=generation_kwargs)\n      thread.start()\n\n      # Accumulate response text\n      for token_text in self.streamer:\n         yield openai_response(token_text)\n\n      thread.join()\n\n\n  def test(self):\n    """Test the model here."""\n    try:\n      print("Testing predict...")\n      # Test predict\n      print(self.predict(prompt="What is the capital of India?",))\n    except Exception as e:\n      print("Error in predict", e)\n\n    try:\n      print("Testing generate...")\n      # Test generate\n      for each in self.generate(prompt="What is the capital of India?",):\n        print(each, end="")\n      print()\n    except Exception as e:\n      print("Error in generate", e)\n\n    try:\n      print("Testing chat...")\n      messages = [\n        {"role": "system", "content": "You are an helpful assistant."},\n        {"role": "user", "content": "What is the capital of India?"},\n      ]\n      for each in self.chat(messages=messages,):\n        print(each, end="")\n      print()\n    except Exception as e:\n      print("Error in generate", e)',j="torch==2.5.1\ntokenizers>=0.21.0\ntransformers>=4.47.0\naccelerate>=1.2.0\nscipy==1.10.1\noptimum>=1.23.3\nprotobuf==5.27.3\neinops>=0.8.0\nrequests==2.32.3\nclarifai>=11.3.0",b='model:\n  id: "llama_3_2_1b_instruct"\n  user_id: "user_id"\n  app_id: "app_id"\n  model_type_id: "text-to-text"\n\nbuild_info:\n  python_version: "3.11"\n\ninference_compute_info:\n  cpu_limit: "1"\n  cpu_memory: "13Gi"\n  num_accelerators: 1\n  accelerator_type: ["NVIDIA-*"]\n  accelerator_memory: "18Gi"\n\ncheckpoints:\n  type: "huggingface"\n  repo_id: "unsloth/Llama-3.2-1B-Instruct"\n  hf_token: "hf_token"\n  when: "runtime"',v='[INFO] 13:21:18.571215 Validating folder: """" |  thread=15892\n[INFO] 13:21:19.635009 No checkpoints specified in the config file |  thread=15892\n[INFO] 13:21:19.644012 Using Python version 3.11 from the config file to build the Dockerfile |  thread=15892\n[INFO] 13:21:19.977325 New model will be created at https://clarifai.com/alfrick/docs-demos/models/my-uploaded-model with it\'s first version. |  thread=15892\nPress Enter to continue...\n[INFO] 13:21:24.984514 Uploading file... |  thread=10284\n[INFO] 13:21:24.985517 Upload complete! |  thread=10284\n   Status: Upload done, Progress: 0% - Completed upload of files, initiating model version image build..  request_id:\n   Status: Model image is currently being built., Progress: 0% - Model version image is being built.  request_id:\n[INFO] 13:21:25.791835 Created Model Version ID: 959b32947f0f4061b598f56b8ffc152f |  thread=15892\n[INFO] 13:21:25.791835 Full url to that version is: https://clarifai.com/alfrick/docs-demos/models/my-uploaded-model |  thread=15892\n[INFO] 13:21:31.140198 2025-05-07 10:21:26.135234 INFO: Downloading uploaded model from storage... |  thread=15892\n[INFO] 13:21:37.939478 2025-05-07 10:21:31.839495 INFO: Done downloading model\n\n2025-05-07 10:21:31.842088 INFO: Extracting upload...\n\n2025-05-07 10:21:31.846218 INFO: Done extracting upload\n\n2025-05-07 10:21:31.848309 INFO: Parsing requirements file for model version ID ****f56b8ffc152f\n\n2025-05-07 10:21:31.869731 INFO: Dockerfile found at /shared/context/Dockerfile\n\ncat: /shared/context/downloader/hf_token: No such file or directory\n\n2025-05-07 10:21:32.520135 INFO: Setting up credentials\n\namazon-ecr-credential-helper\n\nVersion:    0.8.0\n\nGit commit: ********\n\n2025-05-07 10:21:32.523522 INFO: Building image...\n\n#1 \\[internal] load build definition from Dockerfile\n\n#1 DONE 0.0s\n\n\n\n#1 \\[internal] load build definition from Dockerfile\n\n#1 transferring dockerfile: 2.61kB done\n\n#1 WARN: FromAsCasing: \'as\' and \'FROM\' keywords\' casing do not match (line 2)\n\n#1 DONE 0.0s\n\n\n\n#2 resolve image config for docker-image://docker.io/docker/dockerfile:1.13-labs\n\n#2 DONE 0.1s\n\n\n\n#3 docker-image://docker.io/docker/dockerfile:1.13-labs@sha256:************18b8\n\n#3 resolve docker.io/docker/dockerfile:1.13-labs@sha256:************18b8 done\n\n#3 CACHED\n\n\n\n#4 \\[internal] load metadata for public.ecr.aws/clarifai-models/python-base:3.11-********\n\n#4 DONE 0.1s\n\n\n\n#5 \\[internal] load .dockerignore\n\n#5 transferring context: 2B done\n\n#5 DONE 0.0s\n\n\n\n#6 \\[internal] load build context\n\n#6 transferring context: 2.66kB done\n\n#6 DONE 0.0s\n\n\n\n#7 \\[final 1/8] FROM public.ecr.aws/clarifai-models/python-base:3.11-********@sha256:************6ab0\n\n#7 resolve public.ecr.aws/clarifai-models/python-base:3.11-********@sha256:************6ab0 done\n\n#7 DONE 0.0s\n\n\n\n#8 \\[final 5/8] COPY --chown=nonroot:nonroot downloader/unused.yaml /home/nonroot/main/1/checkpoints/.cache/unused.yaml\n\n#8 CACHED\n\n\n\n#9 \\[final 4/8] RUN ["pip", "show", "clarifai"]\n\n#9 CACHED\n\n\n\n#10 \\[final 2/8] COPY --link requirements.txt /home/nonroot/requirements.txt\n\n#10 CACHED\n\n\n\n#11 \\[final 3/8] RUN ["pip", "install", "--no-cache-dir", "-r", "/home/nonroot/requirements.txt"]\n\n#11 CACHED\n\n\n\n#12 \\[final 6/8] RUN  ["python", "-m", "clarifai.cli", "model", "download-checkpoints", "/home/nonroot/main", "--out_path", "/home/nonroot/main/1/checkpoints", "--stage", "build"]\n\n#12 CACHED\n\n\n\n#13 \\[final 7/8] COPY --link=true 1 /home/nonroot/main/1\n\n#13 DONE 0.0s\n\n\n\n#14 \\[final 8/8] COPY --link=true requirements.txt config.yaml /home/nonroot/main/\n\n#14 DONE 0.0s\n\n\n\n#15 \\[auth] sharing credentials for 891377382885.dkr.ecr.us-east-1.amazonaws.com\n\n#15 DONE 0.0s\n\n\n\n#16 exporting to image\n\n#16 exporting layers done\n\n#16 exporting manifest sha256:************4cc5 done\n\n#16 exporting config sha256:************bd5a done\n\n#16 pushing layers\n\n#16 pushing layers 1.0s done\n\n#16 pushing manifest for ****/prod/python:****f56b8ffc152f@sha256:************4cc5\n\n#16 pushing manifest for ****/prod/python:****f56b8ffc152f@sha256:************4cc5 0.4s done\n\n#16 DONE 1.4s\n\n2025-05-07 10:21:34.241532 INFO: Done building image!!! |  thread=15892\n[INFO] 13:21:39.758911 Model build complete! |  thread=15892\n[INFO] 13:21:39.760236 Build time elapsed 14.0s) |  thread=15892\n[INFO] 13:21:39.760236 Check out the model at https://clarifai.com/alfrick/docs-demos/models/my-uploaded-model version: 959b32947f0f4061b598f56b8ffc152f |  thread=15892',k='import os\nfrom clarifai.client import Model\n\n# Set your Personal Access Token (PAT)\nos.environ["CLARIFAI_PAT"] = "YOUR_PAT_HERE"\n\n# Initialize with model URL\nmodel = Model(url="https://clarifai.com/alfrick/docs-demos/models/my-uploaded-model")\n\nresponse = model.predict("Yes, I uploaded it!")\n\nprint(response)',I='import os\nfrom clarifai.client import Model\n\n# Set your Personal Access Token (PAT)\nos.environ["CLARIFAI_PAT"] = "YOUR_PAT_HERE"\n\n# Initialize with model URL\nmodel = Model(url="https://clarifai.com/alfrick/docs-demos/models/my-uploaded-model")\n\nfor response in model.generate("Yes, I uploaded it! "):\n    print(response.text)',w='import os\nfrom clarifai.client import Model\nfrom clarifai.runners.utils.data_types import Text\n\n# Set your Personal Access Token (PAT)\nos.environ["CLARIFAI_PAT"] = "YOUR_PAT_HERE"\n\n# Initialize with model URL\nmodel = Model(url="https://clarifai.com/alfrick/docs-demos/models/my-uploaded-model", deployment_id="YOUR_DEPLOYMENT_ID_HERE")\n\n# Create a list of input Texts to simulate a stream\ninput_texts = iter([\n    Text(text="First input."),\n    Text(text="Second input."),\n    Text(text="Third input.")\n])\n\n# Call the stream method and process outputs\nresponse_iterator = model.stream(input_texts)\n\n# Print streamed results\nprint("Streaming output:\\n")\nfor response in response_iterator:\n    print(response.text)\n',T="Text(text='Yes, I uploaded it! Hello World!', url=None)",A="Yes, I uploaded it! Generate Hello World 0\nYes, I uploaded it! Generate Hello World 1\nYes, I uploaded it! Generate Hello World 2\nYes, I uploaded it! Generate Hello World 3\nYes, I uploaded it! Generate Hello World 4\nYes, I uploaded it! Generate Hello World 5\nYes, I uploaded it! Generate Hello World 6\nYes, I uploaded it! Generate Hello World 7\nYes, I uploaded it! Generate Hello World 8\nYes, I uploaded it! Generate Hello World 9",C="Streaming output:\n\nFirst input.Stream Hello World 0\nSecond input.Stream Hello World 1\nThird input.Stream Hello World 2",P={description:"Import custom models, including from external sources like Hugging Face and OpenAI",toc_min_heading_level:2,toc_max_heading_level:5},D="Model Uploading",N={},E=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Set up Docker or a Virtual Environment",id:"set-up-docker-or-a-virtual-environment",level:3},{value:"Install Clarifai Package",id:"install-clarifai-package",level:3},{value:"Set a PAT Key",id:"set-a-pat-key",level:3},{value:"Create Project Directory",id:"create-project-directory",level:3},{value:"How to Upload a Model",id:"how-to-upload-a-model",level:2},{value:"Step 1: Prepare the <code>model.py</code> File",id:"step-1-prepare-the-modelpy-file",level:3},{value:"a. <code>load_model</code> Method",id:"a-load_model-method",level:4},{value:"b. Prediction Methods",id:"b-prediction-methods",level:4},{value:"Step 2: Prepare the <code>config.yaml</code> File",id:"step-2-prepare-the-configyaml-file",level:3},{value:"Model Info",id:"model-info",level:4},{value:"Build Info",id:"build-info",level:4},{value:"Compute Resources",id:"compute-resources",level:4},{value:"Hugging Face Model Checkpoints",id:"hugging-face-model-checkpoints",level:4},{value:"Model Concepts or Labels",id:"model-concepts-or-labels",level:4},{value:"Step 3: Define Dependencies in <code>requirements.txt</code>",id:"step-3-define-dependencies-in-requirementstxt",level:3},{value:"Step 4: Test the Model Locally",id:"step-4-test-the-model-locally",level:3},{value:"Step 5: Upload the Model to Clarifai",id:"step-5-upload-the-model-to-clarifai",level:3},{value:"Step 6: Predict With Model",id:"step-6-predict-with-model",level:3},{value:"Unary-Unary Predict Call",id:"unary-unary-predict-call",level:4},{value:"Unary-Stream Predict Call",id:"unary-stream-predict-call",level:4},{value:"Stream-Stream Predict Call",id:"stream-stream-predict-call",level:4},{value:"Additional Examples",id:"additional-examples",level:2},{value:"Llama-3.2-1B-Instruct",id:"llama-32-1b-instruct",level:3},{value:"NSFW Image Classifier",id:"nsfw-image-classifier",level:3},{value:"DETR Resnet Image Detector",id:"detr-resnet-image-detector",level:3}];function M(e){const n={a:"a",admonition:"admonition",blockquote:"blockquote",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",mdxAdmonitionTitle:"mdxAdmonitionTitle",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components},{Details:t}=n;return t||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"model-uploading",children:"Model Uploading"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Import custom models, including from external sources like Hugging Face and OpenAI"})}),"\n",(0,i.jsx)("hr",{}),"\n",(0,i.jsx)("div",{style:{position:"relative",width:"100%",overflow:"hidden","padding-top":"56.25%"},children:(0,i.jsx)("iframe",{width:"900",height:"500",style:{position:"absolute",top:"0",left:"0",bottom:"0",right:"0",width:"100%",height:"100%"},src:"https://www.youtube.com/embed/SpIDmDtf7UE",title:"Upload Custom Models to Clarifai Platform Using Python SDK",frameborder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share",allowfullscreen:!0})}),"\n",(0,i.jsx)("br",{}),"\n",(0,i.jsx)("br",{}),"\n",(0,i.jsx)(n.p,{children:"The Clarifai Python SDK allows you to upload custom models easily. Whether you're working with a pre-trained model from an external source like Hugging Face or OpenAI, or one you've built from scratch, Clarifai allows seamless integration of your models, enabling you to take advantage of the platform\u2019s powerful capabilities."}),"\n",(0,i.jsx)(n.p,{children:"Once imported to our platform, your model can be utilized alongside Clarifai's vast suite of AI tools. It will be automatically deployed and ready to be evaluated, combined with other models and agent operators in a workflow, or used to serve inference requests as it is."}),"\n",(0,i.jsx)(n.admonition,{title:"Objective",type:"info",children:(0,i.jsxs)(n.p,{children:["Let\u2019s walk through how to build and upload a custom model to the Clarifai platform. This example model appends the phrase ",(0,i.jsx)(n.code,{children:"Hello World"})," to any input text and also supports streaming responses.\nYou can test the already uploaded model ",(0,i.jsx)(n.a,{href:"https://clarifai.com/alfrick/docs-demos/models/my-uploaded-model",children:"here"}),"."]})}),"\n",(0,i.jsx)(n.admonition,{type:"tip",children:(0,i.jsxs)(n.p,{children:["You can explore ",(0,i.jsx)(n.a,{href:"https://github.com/Clarifai/runners-examples",children:"this repository"})," for examples on uploading different model types."]})}),"\n","\n","\n","\n","\n","\n","\n","\n","\n",(0,i.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsx)(n.h3,{id:"set-up-docker-or-a-virtual-environment",children:"Set up Docker or a Virtual Environment"}),"\n",(0,i.jsx)(n.p,{children:"To test, run, and upload your model, you need to set up either a Docker container or a Python virtual environment. This ensures proper dependency management and prevents conflicts in your project."}),"\n",(0,i.jsxs)(n.p,{children:["Both options allow you to work with different Python versions. For example, you can use Python 3.11 for uploading one model and Python 3.12 for another \u2014 configured via the ",(0,i.jsx)(n.a,{href:"#build-info",children:(0,i.jsx)(n.code,{children:"config.yaml"})})," file."]}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsx)(n.p,{children:"If Docker is installed on your system, it is highly recommended to use it for running the model. Docker provides better isolation and a fully portable environment, including for Python and system libraries."}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["You should ensure your local environment has sufficient memory and compute resources to handle model loading and execution, especially during ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/sdk/compute-orchestration/test-models-locally",children:"testing"}),"."]}),"\n",(0,i.jsx)(n.h3,{id:"install-clarifai-package",children:"Install Clarifai Package"}),"\n",(0,i.jsxs)(n.p,{children:["Install the latest version of the ",(0,i.jsx)(n.code,{children:"clarifai"})," Python package. This will also install the Clarifai ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/additional-resources/api-overview/cli/",children:"Command Line Interface"})," (CLI), which we'll use for testing and uploading the model."]}),"\n",(0,i.jsx)(s.A,{children:(0,i.jsx)(l.A,{value:"bash",label:"Bash",children:(0,i.jsx)(a.A,{className:"language-bash",children:" pip install --upgrade clarifai "})})}),"\n",(0,i.jsx)(n.h3,{id:"set-a-pat-key",children:"Set a PAT Key"}),"\n",(0,i.jsxs)(n.p,{children:["You need to set the ",(0,i.jsx)(n.code,{children:"CLARIFAI_PAT"})," (Personal Access Token) as an environment variable. You can generate the PAT key in your personal settings page by navigating to the ",(0,i.jsx)(n.a,{href:"https://clarifai.com/settings/security",children:"Security section"}),"."]}),"\n",(0,i.jsx)(n.p,{children:"This token is essential for authenticating your connection to the Clarifai platform."}),"\n",(0,i.jsxs)(s.A,{children:[(0,i.jsx)(l.A,{value:"bash",label:"Unix-Like Systems",children:(0,i.jsx)(a.A,{className:"language-bash",children:" export CLARIFAI_PAT=YOUR_PERSONAL_ACCESS_TOKEN_HERE "})}),(0,i.jsx)(l.A,{value:"bash2",label:"Windows",children:(0,i.jsx)(a.A,{className:"language-bash",children:" set CLARIFAI_PAT=YOUR_PERSONAL_ACCESS_TOKEN_HERE "})})]}),"\n",(0,i.jsx)(n.admonition,{title:"tip",type:"note",children:(0,i.jsxs)(n.p,{children:["On Windows, the Clarifai Python SDK expects a ",(0,i.jsx)(n.code,{children:"HOME"})," environment variable, which isn\u2019t set by default. To ensure compatibility with file paths used by the SDK, set ",(0,i.jsx)(n.code,{children:"HOME"})," to the value of your ",(0,i.jsx)(n.code,{children:"USERPROFILE"}),". You can set it in your Command Prompt this way: ",(0,i.jsx)(n.code,{children:"set HOME=%USERPROFILE%"}),"."]})}),"\n",(0,i.jsx)(n.h3,{id:"create-project-directory",children:"Create Project Directory"}),"\n",(0,i.jsx)(n.p,{children:"Create a project directory and organize your files as indicated below to fit the requirements of uploading models to the Clarifai platform."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",children:"your_model_directory/\n\u251c\u2500\u2500 1/\n\u2502   \u2514\u2500\u2500 model.py\n\u251c\u2500\u2500 requirements.txt\n\u2514\u2500\u2500 config.yaml\n"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"your_model_directory/"})," \u2013 The root directory containing all files related to your custom model.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"1/"})," \u2013 A subdirectory that holds the model file (",(0,i.jsxs)(n.em,{children:["Note that the folder is named as ",(0,i.jsx)(n.strong,{children:"1"})]}),").","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"model.py"})," \u2013 Contains the code that defines your model, including loading the model and running inference."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"requirements.txt"})," \u2013 Lists the Python dependencies required to run your model."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"config.yaml"})," \u2013 Contains model metadata and configuration details necessary for building the Docker image, defining compute resources, and uploading the model to Clarifai."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"how-to-upload-a-model",children:"How to Upload a Model"}),"\n",(0,i.jsx)(n.p,{children:"Let's talk about the general steps you'd follow to upload any type of model to the Clarifai platform."}),"\n",(0,i.jsxs)(n.h3,{id:"step-1-prepare-the-modelpy-file",children:["Step 1: Prepare the ",(0,i.jsx)(n.code,{children:"model.py"})," File"]}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"model.py"})," file contains the core logic for your model, including how the model is loaded and how predictions are made. This file must define a custom class that inherits from ",(0,i.jsx)(n.code,{children:"ModelClass"})," and implements the required methods."]}),"\n",(0,i.jsxs)(n.p,{children:["This is the ",(0,i.jsx)(n.code,{children:"model.py"})," file for the custom model we want to upload:"]}),"\n",(0,i.jsx)(s.A,{children:(0,i.jsx)(l.A,{value:"python",label:"Python",children:(0,i.jsx)(a.A,{className:"language-python",children:d})})}),"\n",(0,i.jsx)(n.p,{children:"Let\u2019s break down what each part of the file does."}),"\n",(0,i.jsxs)(n.h4,{id:"a-load_model-method",children:["a. ",(0,i.jsx)(n.code,{children:"load_model"})," Method"]}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"load_model"})," method is optional but recommended, as it prepares the model for inference by handling resource-heavy initializations. It is particularly useful for:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"One-time setup of heavy resources, such as loading trained models or initializing data transformations."}),"\n",(0,i.jsx)(n.li,{children:"Executing tasks during model container startup to reduce runtime latency."}),"\n",(0,i.jsx)(n.li,{children:"Loading essential components like tokenizers, pipelines, and other model-related assets."}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Here is an example:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def load_model(self):\n  self.tokenizer = AutoTokenizer.from_pretrained("model/")\n  self.pipeline = transformers.pipeline(...)\n'})}),"\n",(0,i.jsx)(n.h4,{id:"b-prediction-methods",children:"b. Prediction Methods"}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"model.py"})," file must include at least one method decorated with ",(0,i.jsx)(n.code,{children:"@ModelClass.method"})," to define the prediction endpoints."]}),"\n",(0,i.jsxs)(n.p,{children:["In the example model we want to upload, we defined a method that appends the phrase ",(0,i.jsx)(n.code,{children:"Hello World"})," to any input text and added support for different types of ",(0,i.jsx)(n.a,{href:"#step-6-predict-with-model",children:"streaming responses"}),"."]}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Note:"})," The structure of prediction methods on the client side directly mirrors the method signatures defined in your ",(0,i.jsx)(n.code,{children:"model.py"})," file. This one-to-one mapping provides flexibility in defining prediction methods with varying names and arguments."]}),"\n"]}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsx)(n.p,{children:"Here are some examples of method mapping:"}),"\n"]}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsxs)(n.th,{children:[(0,i.jsx)(n.code,{children:"model.py"})," Model Implementation"]}),(0,i.jsx)(n.th,{children:"Client-Side Usage Pattern"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"@ModelClass.method def predict(...)"})}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"model.predict(...)"})})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"@ModelClass.method def generate(...)"})}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"model.generate(...)"})})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"@ModelClass.method def stream(...)"})}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"model.stream(...)"})})]})]})]}),"\n"]}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:["You can learn more about the structure of prediction methods ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/compute/models/inference/#structure-of-prediction-methods",children:"here"}),"."]}),"\n"]}),"\n",(0,i.jsxs)(n.admonition,{type:"warning",children:[(0,i.jsx)(n.mdxAdmonitionTitle,{children:(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/compute/models/model-upload/data-types/",children:"Supported Input and Output Data Types"})}),(0,i.jsxs)(n.p,{children:["Each parameter in the class methods must be annotated with a type, and the return type must also be specified. Clarifai's model framework supports rich data typing for both inputs and outputs. Supported types include ",(0,i.jsx)(n.code,{children:"Text"}),", ",(0,i.jsx)(n.code,{children:"Image"}),", ",(0,i.jsx)(n.code,{children:"Audio"}),", ",(0,i.jsx)(n.code,{children:"Video"}),", and more."]})]}),"\n",(0,i.jsxs)(n.h3,{id:"step-2-prepare-the-configyaml-file",children:["Step 2: Prepare the ",(0,i.jsx)(n.code,{children:"config.yaml"})," File"]}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"config.yaml"})," file is essential for specifying the model\u2019s metadata, compute resource requirements, and model checkpoints."]}),"\n",(0,i.jsxs)(n.p,{children:["This is the ",(0,i.jsx)(n.code,{children:"config.yaml"})," file for the custom model we want to upload:"]}),"\n",(0,i.jsx)(s.A,{children:(0,i.jsx)(l.A,{value:"yaml",label:"YAML",children:(0,i.jsx)(a.A,{className:"language-yaml",children:c})})}),"\n",(0,i.jsx)(n.p,{children:"Let\u2019s break down what each part of the file does."}),"\n",(0,i.jsx)(n.h4,{id:"model-info",children:"Model Info"}),"\n",(0,i.jsx)(n.p,{children:"This section defines your model ID, Clarifai user ID, and Clarifai app ID, which will determine where the model is uploaded on the Clarifai platform."}),"\n",(0,i.jsx)(n.h4,{id:"build-info",children:"Build Info"}),"\n",(0,i.jsxs)(n.p,{children:["This section specifies details about the environment used to build or run the model. You can include the ",(0,i.jsx)(n.code,{children:"python_version"}),", which is useful for ensuring compatibility between the model and its runtime environment, as different Python versions may have varying dependencies, library support, and performance characteristics."]}),"\n",(0,i.jsx)(n.admonition,{type:"note",children:(0,i.jsx)(n.p,{children:"We currently support Python 3.11 and Python 3.12 (default)."})}),"\n",(0,i.jsx)(n.h4,{id:"compute-resources",children:"Compute Resources"}),"\n",(0,i.jsx)(n.p,{children:"You must define the minimum compute resources required for running your model, including CPU, memory, and optional GPU specifications."}),"\n",(0,i.jsx)(n.p,{children:"These are some parameters you can define:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"cpu_limit"})}),' \u2013 Number of CPUs allocated for the model (follows Kubernetes notation, e.g., "1", "2").']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"cpu_memory"})}),' \u2013 Minimum memory required for the CPU (uses Kubernetes notation, e.g., "1Gi", "1500Mi", "3Gi").']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"num_accelerators"})})," \u2013 Number of GPUs or TPUs to use for inference."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"accelerator_type"})})," \u2013 Specifies the type of hardware ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/compute/deployments/cloud-instances/",children:"accelerators"}),' (e.g., GPU or TPU) supported by the model (e.g., "NVIDIA-A10G"). ',(0,i.jsxs)(n.em,{children:["Note that instead of specifying an exact accelerator type, you can use a wildcard ",(0,i.jsx)(n.code,{children:"(*)"})," to automatically match all available accelerators that fit your use case. For example, using ",(0,i.jsx)(n.code,{children:'["NVIDIA-*"]'})," will enable the system to choose from all NVIDIA options compatible with your model."]})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"accelerator_memory"})})," \u2013 Minimum memory required for the GPU or TPU."]}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"hugging-face-model-checkpoints",children:"Hugging Face Model Checkpoints"}),"\n",(0,i.jsx)(n.p,{children:"If you're using a model from Hugging Face, you can automatically download its checkpoints by specifying the appropriate configuration in this section. For private or restricted Hugging Face repositories, include an access token."}),"\n",(0,i.jsxs)(n.p,{children:["See the ",(0,i.jsx)(n.a,{href:"#llama-32-1b-instruct",children:"example below"})," for how to define Hugging Face checkpoints."]}),"\n",(0,i.jsx)(s.A,{children:(0,i.jsx)(l.A,{value:"yaml",label:"YAML",children:(0,i.jsx)(a.A,{className:"language-yaml",children:m})})}),"\n",(0,i.jsxs)(n.admonition,{type:"note",children:[(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"when"})," parameter in the ",(0,i.jsx)(n.code,{children:"checkpoints"})," section determines when model checkpoints should be downloaded and stored. It must be set to one of the following options:"]}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"runtime"})," (",(0,i.jsx)(n.em,{children:"default"}),") \u2013 Downloads checkpoints when loading the model in the ",(0,i.jsx)(n.code,{children:"load_model"})," method."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"build"})," \u2013 Downloads checkpoints during the image build process."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"upload"})," \u2013 Downloads checkpoints before uploading the model."]}),"\n"]}),(0,i.jsxs)(n.p,{children:["For larger models, we highly recommend downloading checkpoints at ",(0,i.jsx)(n.code,{children:"runtime"}),". Doing so prevents unnecessary increases in Docker image size, which has some advantages:"]}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Smaller image sizes"}),"\n",(0,i.jsx)(n.li,{children:"Faster build times"}),"\n",(0,i.jsx)(n.li,{children:"Quicker uploads and inference on the Clarifai platform"}),"\n"]}),(0,i.jsxs)(n.p,{children:["Downloading checkpoints at ",(0,i.jsx)(n.code,{children:"build"})," or ",(0,i.jsx)(n.code,{children:"upload"})," time can significantly increase image size, resulting in longer upload times and increased cold start latency."]})]}),"\n",(0,i.jsx)(n.h4,{id:"model-concepts-or-labels",children:"Model Concepts or Labels"}),"\n",(0,i.jsxs)(n.p,{children:["This section is required if your model outputs concepts or labels and is not being directly loaded from Hugging Face. So, you must define a ",(0,i.jsx)(n.code,{children:"concepts"})," section in the ",(0,i.jsx)(n.code,{children:"config.yaml"})," file."]}),"\n",(0,i.jsx)(n.p,{children:"The following model types output concepts or labels:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"visual-classifier"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"visual-detector"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"visual-segmenter"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"text-classifier"})}),"\n"]}),"\n",(0,i.jsx)(s.A,{children:(0,i.jsx)(l.A,{value:"yaml",label:"YAML",children:(0,i.jsx)(a.A,{className:"language-yaml",children:p})})}),"\n",(0,i.jsxs)(n.admonition,{type:"note",children:[(0,i.jsx)(n.mdxAdmonitionTitle,{}),(0,i.jsxs)(n.p,{children:["If you're using a model from Hugging Face and the ",(0,i.jsx)(n.code,{children:"checkpoints"})," section is defined, the Clarifai platform will automatically infer concepts. In this case, you don\u2019t need to manually specify them."]})]}),"\n",(0,i.jsxs)(n.h3,{id:"step-3-define-dependencies-in-requirementstxt",children:["Step 3: Define Dependencies in ",(0,i.jsx)(n.code,{children:"requirements.txt"})]}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"requirements.txt"})," file lists all the Python dependencies your model needs."]}),"\n",(0,i.jsxs)(n.p,{children:["This is the ",(0,i.jsx)(n.code,{children:"requirements.txt"})," file for the custom model we want to upload:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",children:"clarifai>=11.3.0\n"})}),"\n",(0,i.jsx)(n.p,{children:"If your model requires Torch, we provide optimized pre-built Torch images as the base for machine learning and inference tasks."}),"\n",(0,i.jsx)(n.p,{children:"These images include all necessary dependencies, ensuring efficient execution. The available pre-built Torch images are:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"2.4.1-py3.11-cuda124"})," \u2014 Based on PyTorch 2.4.1, Python 3.11, and CUDA 12.4."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"2.5.1-py3.11-cuda124"})," \u2014 Based on PyTorch 2.5.1, Python 3.11, and CUDA 12.4."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"2.4.1-py3.12-cuda124"})," \u2014 Based on PyTorch 2.4.1, Python 3.12, and CUDA 12.4."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"2.5.1-py3.12-cuda124"})," \u2014 Based on PyTorch 2.5.1, Python 3.12, and CUDA 12.4."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"To use a specific Torch version, define it in your requirements.txt file like this:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",children:"torch==2.5.1\n"})}),"\n",(0,i.jsx)(n.p,{children:"This ensures the correct pre-built image is pulled from Clarifai's container registry, ensuring the correct environment is used. This minimizes cold start times and speeds up model uploads and runtime execution \u2014 avoiding the overhead of building images from scratch or pulling and configuring them from external sources."}),"\n",(0,i.jsxs)(n.p,{children:["We recommend using either ",(0,i.jsx)(n.code,{children:"torch==2.5.1"})," or ",(0,i.jsx)(n.code,{children:"torch==2.4.1"}),". If your model requires a different Torch version, you can specify it in requirements.txt, but this may slightly increase the model upload time."]}),"\n",(0,i.jsx)(n.h3,{id:"step-4-test-the-model-locally",children:"Step 4: Test the Model Locally"}),"\n",(0,i.jsx)(n.p,{children:"Before uploading your model to the Clarifai platform, it's important to test it locally to catch any typos or misconfigurations in the code."}),"\n",(0,i.jsxs)(n.p,{children:["Learn how to test your models locally ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/compute/models/model-upload/test-models-locally/",children:"here"}),"."]}),"\n",(0,i.jsx)(n.h3,{id:"step-5-upload-the-model-to-clarifai",children:"Step 5: Upload the Model to Clarifai"}),"\n",(0,i.jsx)(n.p,{children:"Once your model is ready, you can upload it to the platform using Clarifai CLI."}),"\n",(0,i.jsx)(n.p,{children:"To upload your model, run the following command in your terminal:"}),"\n",(0,i.jsx)(s.A,{children:(0,i.jsx)(l.A,{value:"bash",label:"Bash",children:(0,i.jsx)(a.A,{className:"language-bash",children:" clarifai model upload ./your/model/path/here "})})}),"\n",(0,i.jsx)(n.p,{children:"Alternatively, navigate to the directory containing your custom model and run the command without specifying the directory path:"}),"\n",(0,i.jsx)(s.A,{children:(0,i.jsx)(l.A,{value:"bash",label:"Bash",children:(0,i.jsx)(a.A,{className:"language-bash",children:" clarifai model upload "})})}),"\n",(0,i.jsx)(n.p,{children:"This command builds the model\u2019s Docker image using the defined compute resources and uploads it to Clarifai, where it can be served in production.\nThe build logs will be displayed in your terminal, which helps you troubleshoot any upload issues."}),"\n",(0,i.jsxs)(t,{children:[(0,i.jsx)("summary",{children:"Build Logs Example"}),(0,i.jsx)(a.A,{className:"language-text",children:v})]}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Note"}),": If you make any changes to your model and upload it again to the Clarifai platform, a new version of the model will be created automatically."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"step-6-predict-with-model",children:"Step 6: Predict With Model"}),"\n",(0,i.jsx)(n.p,{children:"Once the model is successfully uploaded to Clarifai, you can start making predictions with it."}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Note:"})," If you want to make a prediction request with our Compute Orchestration capabilities, you need to first ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/compute/deployments/deploy-model",children:"deploy"})," it into a cluster and nodepool you've created."]}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"unary-unary-predict-call",children:"Unary-Unary Predict Call"}),"\n",(0,i.jsxs)(n.p,{children:["You can make a ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/compute/models/inference/api/#unary-unary-predict-call",children:"unary-unary"})," predict call using the uploaded model."]}),"\n",(0,i.jsx)(s.A,{children:(0,i.jsx)(l.A,{value:"python",label:"Python",children:(0,i.jsx)(a.A,{className:"language-python",children:k})})}),"\n",(0,i.jsxs)(t,{children:[(0,i.jsx)("summary",{children:"Example Output"}),(0,i.jsx)(a.A,{className:"language-text",children:T})]}),"\n",(0,i.jsx)(n.h4,{id:"unary-stream-predict-call",children:"Unary-Stream Predict Call"}),"\n",(0,i.jsxs)(n.p,{children:["You can make a ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/compute/models/inference/api/#unary-stream-predict-call",children:"unary-stream"})," predict call using the uploaded model."]}),"\n",(0,i.jsx)(s.A,{children:(0,i.jsx)(l.A,{value:"python",label:"Python",children:(0,i.jsx)(a.A,{className:"language-python",children:I})})}),"\n",(0,i.jsxs)(t,{children:[(0,i.jsx)("summary",{children:"Example Output"}),(0,i.jsx)(a.A,{className:"language-text",children:A})]}),"\n",(0,i.jsx)(n.h4,{id:"stream-stream-predict-call",children:"Stream-Stream Predict Call"}),"\n",(0,i.jsxs)(n.p,{children:["You can make a ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/compute/models/inference/api/#stream-stream-predict-call",children:"stream-stream"})," predict call using the uploaded model."]}),"\n",(0,i.jsx)(s.A,{children:(0,i.jsx)(l.A,{value:"python",label:"Python",children:(0,i.jsx)(a.A,{className:"language-python",children:w})})}),"\n",(0,i.jsxs)(t,{children:[(0,i.jsx)("summary",{children:"Example Output"}),(0,i.jsx)(a.A,{className:"language-text",children:C})]}),"\n",(0,i.jsx)(n.h2,{id:"additional-examples",children:"Additional Examples"}),"\n",(0,i.jsx)(n.admonition,{type:"tip",children:(0,i.jsxs)(n.p,{children:["You can find various up-to-date model upload examples ",(0,i.jsx)(n.a,{href:"https://github.com/Clarifai/runners-examples",children:"here"}),", which demonstrate different use cases and optimizations."]})}),"\n",(0,i.jsx)(n.h3,{id:"llama-32-1b-instruct",children:"Llama-3.2-1B-Instruct"}),"\n",(0,i.jsxs)(t,{children:[(0,i.jsx)("summary",{children:"model.py"}),(0,i.jsx)(a.A,{className:"language-text",children:_})]}),"\n",(0,i.jsxs)(t,{children:[(0,i.jsx)("summary",{children:"config.yaml"}),(0,i.jsx)(a.A,{className:"language-text",children:b})]}),"\n",(0,i.jsxs)(t,{children:[(0,i.jsx)("summary",{children:"requirements.txt"}),(0,i.jsx)(a.A,{className:"language-text",children:j})]}),"\n",(0,i.jsx)(n.h3,{id:"nsfw-image-classifier",children:"NSFW Image Classifier"}),"\n",(0,i.jsxs)(t,{children:[(0,i.jsx)("summary",{children:"model.py"}),(0,i.jsx)(a.A,{className:"language-text",children:u})]}),"\n",(0,i.jsxs)(t,{children:[(0,i.jsx)("summary",{children:"config.yaml"}),(0,i.jsx)(a.A,{className:"language-text",children:f})]}),"\n",(0,i.jsxs)(t,{children:[(0,i.jsx)("summary",{children:"requirements.txt"}),(0,i.jsx)(a.A,{className:"language-text",children:h})]}),"\n",(0,i.jsx)(n.h3,{id:"detr-resnet-image-detector",children:"DETR Resnet Image Detector"}),"\n",(0,i.jsxs)(t,{children:[(0,i.jsx)("summary",{children:"model.py"}),(0,i.jsx)(a.A,{className:"language-text",children:g})]}),"\n",(0,i.jsxs)(t,{children:[(0,i.jsx)("summary",{children:"config.yaml"}),(0,i.jsx)(a.A,{className:"language-text",children:y})]}),"\n",(0,i.jsxs)(t,{children:[(0,i.jsx)("summary",{children:"requirements.txt"}),(0,i.jsx)(a.A,{className:"language-text",children:x})]})]})}function S(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(M,{...e})}):M(e)}},65537:(e,n,t)=>{t.d(n,{A:()=>v});var o=t(96540),i=t(18215),r=t(65627),s=t(56347),l=t(50372),a=t(30604),d=t(11861),c=t(78749);function m(e){return o.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,o.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function p(e){const{values:n,children:t}=e;return(0,o.useMemo)((()=>{const e=n??function(e){return m(e).map((e=>{let{props:{value:n,label:t,attributes:o,default:i}}=e;return{value:n,label:t,attributes:o,default:i}}))}(t);return function(e){const n=(0,d.XI)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,t])}function u(e){let{value:n,tabValues:t}=e;return t.some((e=>e.value===n))}function h(e){let{queryString:n=!1,groupId:t}=e;const i=(0,s.W6)(),r=function(e){let{queryString:n=!1,groupId:t}=e;if("string"==typeof n)return n;if(!1===n)return null;if(!0===n&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:n,groupId:t});return[(0,a.aZ)(r),(0,o.useCallback)((e=>{if(!r)return;const n=new URLSearchParams(i.location.search);n.set(r,e),i.replace({...i.location,search:n.toString()})}),[r,i])]}function f(e){const{defaultValue:n,queryString:t=!1,groupId:i}=e,r=p(e),[s,a]=(0,o.useState)((()=>function(e){let{defaultValue:n,tabValues:t}=e;if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(n){if(!u({value:n,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${n}" but none of its children has the corresponding value. Available values are: ${t.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return n}const o=t.find((e=>e.default))??t[0];if(!o)throw new Error("Unexpected error: 0 tabValues");return o.value}({defaultValue:n,tabValues:r}))),[d,m]=h({queryString:t,groupId:i}),[f,g]=function(e){let{groupId:n}=e;const t=function(e){return e?`docusaurus.tab.${e}`:null}(n),[i,r]=(0,c.Dv)(t);return[i,(0,o.useCallback)((e=>{t&&r.set(e)}),[t,r])]}({groupId:i}),x=(()=>{const e=d??f;return u({value:e,tabValues:r})?e:null})();(0,l.A)((()=>{x&&a(x)}),[x]);return{selectedValue:s,selectValue:(0,o.useCallback)((e=>{if(!u({value:e,tabValues:r}))throw new Error(`Can't select invalid tab value=${e}`);a(e),m(e),g(e)}),[m,g,r]),tabValues:r}}var g=t(9136);const x={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var y=t(74848);function _(e){let{className:n,block:t,selectedValue:o,selectValue:s,tabValues:l}=e;const a=[],{blockElementScrollPositionUntilNextRender:d}=(0,r.a_)(),c=e=>{const n=e.currentTarget,t=a.indexOf(n),i=l[t].value;i!==o&&(d(n),s(i))},m=e=>{let n=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const t=a.indexOf(e.currentTarget)+1;n=a[t]??a[0];break}case"ArrowLeft":{const t=a.indexOf(e.currentTarget)-1;n=a[t]??a[a.length-1];break}}n?.focus()};return(0,y.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,i.A)("tabs",{"tabs--block":t},n),children:l.map((e=>{let{value:n,label:t,attributes:r}=e;return(0,y.jsx)("li",{role:"tab",tabIndex:o===n?0:-1,"aria-selected":o===n,ref:e=>{a.push(e)},onKeyDown:m,onClick:c,...r,className:(0,i.A)("tabs__item",x.tabItem,r?.className,{"tabs__item--active":o===n}),children:t??n},n)}))})}function j(e){let{lazy:n,children:t,selectedValue:r}=e;const s=(Array.isArray(t)?t:[t]).filter(Boolean);if(n){const e=s.find((e=>e.props.value===r));return e?(0,o.cloneElement)(e,{className:(0,i.A)("margin-top--md",e.props.className)}):null}return(0,y.jsx)("div",{className:"margin-top--md",children:s.map(((e,n)=>(0,o.cloneElement)(e,{key:n,hidden:e.props.value!==r})))})}function b(e){const n=f(e);return(0,y.jsxs)("div",{className:(0,i.A)("tabs-container",x.tabList),children:[(0,y.jsx)(_,{...n,...e}),(0,y.jsx)(j,{...n,...e})]})}function v(e){const n=(0,g.A)();return(0,y.jsx)(b,{...e,children:m(e.children)},String(n))}},79329:(e,n,t)=>{t.d(n,{A:()=>s});t(96540);var o=t(18215);const i={tabItem:"tabItem_Ymn6"};var r=t(74848);function s(e){let{children:n,hidden:t,className:s}=e;return(0,r.jsx)("div",{role:"tabpanel",className:(0,o.A)(i.tabItem,s),hidden:t,children:n})}}}]);
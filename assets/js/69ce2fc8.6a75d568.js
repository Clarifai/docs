"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[3629],{31652:(e,t,i)=>{i.r(t),i.d(t,{assets:()=>A,contentTitle:()=>_,default:()=>N,frontMatter:()=>k,metadata:()=>v,toc:()=>C});var n=i(74848),r=i(28453),s=i(11470),o=i(19365),l=i(21432);const a='from clarifai.client.user import User\n\n# Your PAT (Personal Access Token) can be found in the Account\'s Security section\n# Specify the correct user_id/app_id pairings\n# Since you\'re making inferences outside your app\'s scope\nUSER_ID = "user_id"\nAPP_ID = "app_id"\n\napp = User(user_id=USER_ID, pat="YOUR_PAT").create_app(\n    app_id=APP_ID, base_workflow="Empty"\n)\n\n\n# create a yaml file specifying the workflow structure\n# eg:\n"""configs/prompter_llm.yml\nworkflow:\n  id: wf-prompter-llm\n  nodes:\n    - id: prompter\n      model:\n          model_id: prompter\n          model_type_id: prompter\n          description: \'Prompter Model\'\n          output_info:\n            params:\n              prompt_template: \'Classify sentiment between postive and negative for the text {data.text.raw}\'\n\n    - id: llm\n      model:\n          user_id: mistralai\n          model_id: mistral-7B-Instruct\n          app_id: completion\n\n      node_inputs:\n        - node_id: prompter\n\n"""\n\n# create the workflow\nprompter_llm_workflow = app.create_workflow(config_filepath="configs/prompter_llm.yml")',d='from clarifai.client.app import App\n\n# Your PAT (Personal Access Token) can be found in the Account\'s Security section\napp = App(app_id="APP_ID", user_id="USER_ID", pat="YOUR_PAT")\n\nfor workflow in app.list_workflows(page_no=1,per_page=7):\n    print("Workflow ID: ", workflow.id)',c='from clarifai.client.app import App\n\n# Your PAT (Personal Access Token) can be found in the Account\'s Security section\napp = App(app_id="APP_ID", user_id="USER_ID", pat="YOUR_PAT")\n# Delete the workflow within an application\napp.delete_workflow(workflow_id="workflow-id")',h='from clarifai.client.workflow import Workflow\n\n# Your PAT (Personal Access Token) can be found in the Account\'s Security section\n\nworkflow_url = "https://clarifai.com/clarifai/main/workflows/Demographics"\ndemographics_workflow = Workflow(\n    url=workflow_url , pat="YOUR_PAT"\n)\ndemographics_workflow.export("demographics_workflow.yml")\n"""\nNow the parameters of each model can be changed and new workflow can be easily created by app.create_workflow().\n\nHere we change the margin parameter of the image cropper node to be 1.5\n\n- id: image-crop\n    model:\n      model_id: margin-100-image-crop-custom\n          model_type_id: image-crop\n          description: Custom crop model\n          output_info:\n            params:\n              margin: 1.5\n\n\n"""\n',m="workflow:\n  id: asr-sentiment\n  nodes:\n    - id: audio-speech-recognition\n      model:\n          model_id: asr-wav2vec2-large-robust-ft-swbd-300h-english\n          user_id: facebook\n          app_id: asr\n\n    - id: text-sentiment-classification\n      model:\n          model_id: sentiment-analysis-twitter-roberta-base\n          user_id: erfan\n          app_id: text-classification\n\n      node_inputs:\n        - node_id: audio-speech-recognition",p="workflow:\n  id: Demographics\n  nodes:\n    - id: detect-concept\n      model:\n        model_id: face-detection\n        model_version_id: 45fb9a671625463fa646c3523a3087d5\n\n    - id: image-crop\n      model:\n        model_id: margin-110-image-crop\n        model_version_id: b9987421b40a46649566826ef9325303\n      node_inputs:\n        - node_id: detect-concept\n\n    - id: demographics-race\n      model:\n        model_id: ethnicity-demographics-recognition\n        model_version_id: b2897edbda314615856039fb0c489796\n      node_inputs:\n        - node_id: image-crop\n\n    - id: demographics-gender\n      model:\n        model_id: gender-demographics-recognition\n        model_version_id: ff83d5baac004aafbe6b372ffa6f8227\n      node_inputs:\n        - node_id: image-crop\n\n    - id: demographics-age\n      model:\n        model_id: age-demographics-recognition\n        model_version_id: fb9f10339ac14e23b8e960e74984401b\n      node_inputs:\n        - node_id: image-crop\n",g="workflow:\n  id: Face-Search\n  nodes:\n    - id: face-detect\n      model:\n        model_id: face-detection\n        model_version_id: fe995da8cb73490f8556416ecf25cea3\n\n    - id: crop\n      model:\n        model_id: margin-100-image-crop\n        model_version_id: 0af5cd8ad40e43ef92154e4f4bc76bef\n      node_inputs:\n        - node_id: face-detect\n\n    - id: face-landmarks\n      model:\n        model_id: face-landmarks\n        model_version_id: 98ace9ca45e64339be94b06011557e2a\n      node_inputs:\n        - node_id: crop\n\n    - id: face-alignment\n      model:\n        model_id: landmarks-align\n        model_version_id: 4bc8b83a327247829ec638c78cde5f8b\n      node_inputs:\n        - node_id: face-landmarks\n\n    - id: face-embed\n      model:\n        model_id: face-identification-transfer-learn\n        model_version_id: fc3b8814fbe54533a3d80a1896dc9884\n      node_inputs:\n        - node_id: face-alignment\n\n    - id: face-cluster\n      model:\n        model_id: face-clustering\n        model_version_id: 621d74074a5443d7ad9dc1503fba9ff0\n      node_inputs:\n        - node_id: face-embed\n",f="workflow:\n  id: Face-Sentiment\n  nodes:\n    - id: face-det\n      model:\n        model_id: face-detection\n        model_version_id: 6dc7e46bc9124c5c8824be4822abe105\n\n    - id: margin-110\n      model:\n        model_id: margin-110-image-crop\n        model_version_id: b9987421b40a46649566826ef9325303\n      node_inputs:\n        - node_id: face-det\n\n    - id: face-sentiment\n      model:\n        model_id: face-sentiment-recognition\n        model_version_id: a5d7776f0c064a41b48c3ce039049f65\n      node_inputs:\n        - node_id: margin-110\n",x="workflow:\n  id: General\n  nodes:\n    - id: general-v1.5-concept\n      model:\n          model_id: aaa03c23b3724a16a56b629203edc62c\n          model_version_id: aa7f35c01e0642fda5cf400f543e7c40\n\n    - id: general-v1.5-embed\n      model:\n          model_id: bbb5f41425b8468d9b7a554ff10f8581\n          model_version_id: bb186755eda04f9cbb6fe32e816be104\n\n    - id: general-v1.5-cluster\n      model:\n          model_id: cccbe437d6e54e2bb911c6aa292fb072\n          model_version_id: cc2074cff6dc4c02b6f4e1b8606dcb54\n      node_inputs:\n        - node_id: general-v1.5-embed\n",u='workflow:\n  id: wf-ocr\n  nodes:\n    - id: ocr-workflow\n      model:\n          model_id: language-aware-multilingual-ocr-multiplex\n\n    - id: text-aggregator\n      model:\n          model_id: text-aggregation\n          model_type_id: text-aggregation-operator\n          output_info:\n            params:\n              avg_word_width_window_factor: 2.0\n              avg_word_height_window_factor: 1.0\n\n      node_inputs:\n        - node_id: ocr-workflow\n\n    - id: language-id-operator\n      model:\n          model_id: language-id\n          model_type_id: language-id-operator\n          output_info:\n            params:\n              library: "fasttext"\n              topk: 1\n              threshold:  0.1\n              lowercase: true\n\n      node_inputs:\n        - node_id: text-aggregator\n',j="workflow:\n  id: wf-prompter-llm\n  nodes:\n    - id: prompter\n      model:\n          model_id: prompter\n          model_type_id: prompter\n          description: 'Prompter Model'\n          output_info:\n            params:\n              prompt_template: 'Classify sentiment between postive and negative for the text {data.text.raw}'\n\n    - id: llm\n      model:\n          user_id: mistralai\n          model_id: mistral-7B-Instruct\n          app_id: completion\n\n      node_inputs:\n        - node_id: prompter",w="workflow:\n  id: wf-prompter-llm\n  nodes:\n    - id: rag-prompter\n      model:\n          model_id: rag-prompter\n          model_type_id: rag-prompter\n          description: 'RAG Prompter Model'\n\n    - id: llm\n      model:\n          user_id: mistralai\n          model_id: mistral-7B-Instruct\n          app_id: completion\n\n      node_inputs:\n        - node_id: rag-prompter",b="Workflow ID:  object_track\n\nWorkflow ID:  video_track\n\nWorkflow ID:  multimodal_to_text\n\nWorkflow ID:  text_to_audio\n\nWorkflow ID:  upscale_workflow\n\nWorkflow ID:  image_generation\n\nWorkflow ID:  text_generation",y='2024-01-18 16:34:46 INFO     clarifai.client.app:                                                        app.py:653\n\n                             Workflow Deleted                                                                      \n\n                             code: SUCCESS                                                                         \n\n                             description: "Ok"                                                                     \n\n                             req_id: "a979f35e9c826bb9046f4d92879c6b7c" ',k={sidebar_position:7},_="Building Workflow Graphs",v={id:"sdk/Building-Workflow-Graphs/README",title:"Building Workflow Graphs",description:"Learn how to build workflows using Clarifai SDKs",source:"@site/docs/sdk/Building-Workflow-Graphs/README.mdx",sourceDirName:"sdk/Building-Workflow-Graphs",slug:"/sdk/Building-Workflow-Graphs/",permalink:"/sdk/Building-Workflow-Graphs/",draft:!1,unlisted:!1,editUrl:"https://github.com/Clarifai/docs/blob/main/docs/sdk/Building-Workflow-Graphs/README.mdx",tags:[],version:"current",sidebarPosition:7,frontMatter:{sidebar_position:7},sidebar:"tutorialSidebar",previous:{title:"Advanced Inference Options",permalink:"/sdk/Inference-from-AI-Models/Advance-Inference-Options"},next:{title:"Inference from Workflows",permalink:"/sdk/Building-Workflow-Graphs/Inference-from-Workflows/"}},A={},C=[{value:"Create workflow",id:"create-workflow",level:2},{value:"Examples for YAML based Workflows",id:"examples-for-yaml-based-workflows",level:2},{value:"ASR Sentiment",id:"asr-sentiment",level:3},{value:"Demographics",id:"demographics",level:3},{value:"Face Search",id:"face-search",level:3},{value:"Face Sentiment",id:"face-sentiment",level:3},{value:"General",id:"general",level:3},{value:"Language Aware OCR",id:"language-aware-ocr",level:3},{value:"Prompter LLM",id:"prompter-llm",level:3},{value:"RAG Prompter LLM",id:"rag-prompter-llm",level:3},{value:"List workflows",id:"list-workflows",level:2},{value:"Delete workflows",id:"delete-workflows",level:2},{value:"Export workflow",id:"export-workflow",level:2}];function T(e){const t={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,r.R)(),...e.components},{Details:i}=t;return i||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(t.h1,{id:"building-workflow-graphs",children:"Building Workflow Graphs"}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.strong,{children:"Learn how to build workflows using Clarifai SDKs"})}),"\n",(0,n.jsx)("hr",{}),"\n",(0,n.jsx)(t.p,{children:"Unlock the potential of efficient data processing pipelines tailored precisely to your unique requirements with this empowering feature. As you embark on the journey of constructing workflow graphs, you gain unprecedented flexibility to seamlessly connect and orchestrate a diverse array of models, operators, and processing steps. Whether your focus is on integrating sophisticated image classification, precise object detection, or custom operators, our SDK offers an intuitive and user-friendly interface. Experience the freedom to effortlessly create, modify, and optimize workflow graphs, empowering you to design and implement highly specialized data processing pipelines that align seamlessly with your specific needs."}),"\n",(0,n.jsx)(t.h2,{id:"create-workflow",children:"Create workflow"}),"\n",(0,n.jsx)(t.p,{children:"Designing and implementing workflows is a crucial aspect of building a robust and flexible inference pipeline. Workflows  offer a modular architecture that serves as the foundation for both sophisticated machine learning ensemble modeling and seamless integration of business logic. These workflows empower you to streamline your processes, enhance model performance, and effectively handle diverse tasks in your machine learning endeavors. Workflow can be created with the SDK using a YAML specification."}),"\n",(0,n.jsxs)(t.p,{children:["To know more about workflows visit this ",(0,n.jsx)(t.a,{href:"https://docs.clarifai.com/portal-guide/workflows/",children:"link"}),"."]}),"\n",(0,n.jsx)(s.A,{children:(0,n.jsx)(o.A,{value:"python",label:"Python",children:(0,n.jsx)(l.A,{className:"language-python",children:a})})}),"\n",(0,n.jsxs)(i,{children:[(0,n.jsx)("summary",{children:"Output"}),(0,n.jsx)("img",{src:"/img/python-sdk/create_workflow.png",width:"500",height:"500"})]}),"\n",(0,n.jsx)(t.h2,{id:"examples-for-yaml-based-workflows",children:"Examples for YAML based Workflows"}),"\n",(0,n.jsxs)(t.table,{children:[(0,n.jsx)(t.thead,{children:(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.th,{children:"Node Name"}),(0,n.jsx)(t.th,{children:"Input & Output"}),(0,n.jsx)(t.th,{children:"Description"}),(0,n.jsx)(t.th,{style:{textAlign:"center"},children:"Example Usage"})]})}),(0,n.jsxs)(t.tbody,{children:[(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"audio-to-text"}),(0,n.jsx)(t.td,{children:"Audio -> Text"}),(0,n.jsx)(t.td,{children:"Classify audio signal into string of text."}),(0,n.jsx)(t.td,{style:{textAlign:"center"},children:(0,n.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Predict/audio-to-text.yml",children:[(0,n.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"barcode-operator"}),(0,n.jsx)(t.td,{children:"Image -> Text"}),(0,n.jsx)(t.td,{children:"Operator that detects and recognizes barcodes from the image. It assigns regions with barcode text for each detected barcode. Supports EAN/UPC, Code 128, Code 39, Interleaved 2 of 5 and QR Code."}),(0,n.jsx)(t.td,{style:{textAlign:"center"},children:(0,n.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Predict/barcode-operator.yml",children:[(0,n.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"Centroid Tracker"}),(0,n.jsx)(t.td,{children:"Frames -> Track ID"}),(0,n.jsx)(t.td,{children:"Centroid trackers rely on the Euclidean distance between centroids of regions in different video frames to assign the same track ID to detections of the same object."}),(0,n.jsx)(t.td,{style:{textAlign:"center"},children:(0,n.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Predict/centroid-tracker.yml",children:[(0,n.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"Clusterer"}),(0,n.jsx)(t.td,{children:"Embeddings -> Clusters"}),(0,n.jsx)(t.td,{children:"Cluster semantically similar images and video frames together in embedding space. This is the basis for good visual search within your app at scale or for grouping your data together without the need for annotated concepts"}),(0,n.jsx)(t.td,{style:{textAlign:"center"},children:(0,n.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Predict/clusterer.yml",children:[(0,n.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"embeddings-classifier"}),(0,n.jsx)(t.td,{children:"Embeddings -> Concepts"}),(0,n.jsx)(t.td,{children:"Classify images or texts based on the embedding model that has indexed them in your app. Transfer learning leverages feature representations from a pre-trained model based on massive amounts of data, so you don't have to train a new model from scratch and can learn new things very quickly with minimal training data"}),(0,n.jsx)(t.td,{style:{textAlign:"center"},children:(0,n.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Predict/embeddings-classifier.yml",children:[(0,n.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"image-color-recognizer"}),(0,n.jsx)(t.td,{children:"Image -> Colors"}),(0,n.jsx)(t.td,{children:"Recognize standard color formats and the proportion each color that covers an image"}),(0,n.jsx)(t.td,{style:{textAlign:"center"},children:(0,n.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Predict/image-color-recognizer.yml",children:[(0,n.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"image-to-text"}),(0,n.jsx)(t.td,{children:"Image -> Text"}),(0,n.jsx)(t.td,{children:"Takes in cropped regions with text in them and returns the text it sees."}),(0,n.jsx)(t.td,{style:{textAlign:"center"},children:(0,n.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Predict/image-to-text.yml",children:[(0,n.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"kalman-filter-tracker"}),(0,n.jsx)(t.td,{children:"Frames -> Track ID"}),(0,n.jsx)(t.td,{children:"Kalman Filter trackers rely on the Kalman Filter algorithm to estimate the next position of an object based on its position and velocity in previous frames. Then detections are matched to predictions by using the Hungarian algorithm"}),(0,n.jsx)(t.td,{style:{textAlign:"center"},children:(0,n.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Predict/kalman-filter-tracker.yml",children:[(0,n.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"kalman-reid-tracker"}),(0,n.jsx)(t.td,{children:"Frames -> Track ID"}),(0,n.jsx)(t.td,{children:"Kalman reid tracker is a kalman filter tracker that expects the Embedding proto field to be populated for detections, and reassigns track IDs based off of embedding distance"}),(0,n.jsx)(t.td,{style:{textAlign:"center"},children:(0,n.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Predict/kalman-reid-tracker.yml",children:[(0,n.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"neural-lite-tracker"}),(0,n.jsx)(t.td,{children:"Frames -> Track ID"}),(0,n.jsx)(t.td,{children:"Neural Lite Tracker uses light-weight trainable graphical models to infer states of tracks and perform associations using hybrid similairty of lou and centroid distance"}),(0,n.jsx)(t.td,{style:{textAlign:"center"},children:(0,n.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Predict/neural-lite-tracker.yml",children:[(0,n.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"neural-tracker"}),(0,n.jsx)(t.td,{children:"Frames -> Track ID"}),(0,n.jsx)(t.td,{children:"Neural Tracker uses neural probabilistic models to perform filtering and association"}),(0,n.jsx)(t.td,{style:{textAlign:"center"},children:(0,n.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Predict/neural-tracker.yml",children:[(0,n.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"optical-character-recognizer"}),(0,n.jsx)(t.td,{children:"Image -> Text"}),(0,n.jsx)(t.td,{children:"Detect bounding box regions in images or video frames where text is present and then output the text read with the score"}),(0,n.jsx)(t.td,{style:{textAlign:"center"},children:(0,n.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Predict/optical-character-recognizer.yml",children:[(0,n.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"tesseract-operator"}),(0,n.jsx)(t.td,{children:"Image -> Text"}),(0,n.jsx)(t.td,{children:"Operator for Optical Character Recognition using the Tesseract libraries"}),(0,n.jsx)(t.td,{style:{textAlign:"center"},children:(0,n.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Predict/tesseract-operator.yml",children:[(0,n.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"text-classifier"}),(0,n.jsx)(t.td,{children:"Text -> Concepts"}),(0,n.jsx)(t.td,{children:"Classify text into a set of concepts"}),(0,n.jsx)(t.td,{style:{textAlign:"center"},children:(0,n.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Predict/text-classifier.yml",children:[(0,n.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"text-embedder"}),(0,n.jsx)(t.td,{children:"Text -> Embeddings"}),(0,n.jsx)(t.td,{children:"Embed text into a vector representing a high level understanding from our Al models.  These embeddings enable similarity search and training on top of them"}),(0,n.jsx)(t.td,{style:{textAlign:"center"},children:(0,n.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Predict/text-embedder.yml",children:[(0,n.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"text-token-classifier"}),(0,n.jsx)(t.td,{children:"Text -> Concepts"}),(0,n.jsx)(t.td,{children:"Classify tokens from a set of entity classes"}),(0,n.jsx)(t.td,{style:{textAlign:"center"},children:(0,n.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Predict/text-token-classifier.yml",children:[(0,n.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"visual-classifier"}),(0,n.jsx)(t.td,{children:"Image -> Concepts"}),(0,n.jsx)(t.td,{children:"Classify images and videos frames into set of concepts"}),(0,n.jsx)(t.td,{style:{textAlign:"center"},children:(0,n.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Predict/visual-classifier.yml",children:[(0,n.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"visual-detector"}),(0,n.jsx)(t.td,{children:"Image -> Bounding Box"}),(0,n.jsx)(t.td,{children:"Detect bounding box regions in images or video frames where things and then classify objects, descriptive words or topics within the boxes"}),(0,n.jsx)(t.td,{style:{textAlign:"center"},children:(0,n.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Predict/visual-detector.yml",children:[(0,n.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"visual-embedder"}),(0,n.jsx)(t.td,{children:"Image -> Embeddings"}),(0,n.jsx)(t.td,{children:"Embed images and videos frames into a vector representing a high level understanding from our Al models. These embeddings enable visual search and training on top of them"}),(0,n.jsx)(t.td,{style:{textAlign:"center"},children:(0,n.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Predict/visual-embedder.yml",children:[(0,n.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"visual-segmenter"}),(0,n.jsx)(t.td,{children:"Image -> Concepts"}),(0,n.jsx)(t.td,{children:"Segment a per-pixel mask in images where things are and then classify objects, descriptive words or topics within the masks"}),(0,n.jsx)(t.td,{style:{textAlign:"center"},children:(0,n.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Predict/visual-segmenter.yml",children:[(0,n.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"concept-thresholder"}),(0,n.jsx)(t.td,{children:"Concepts -> Concpets"}),(0,n.jsx)(t.td,{children:'Threshold input concepts according to both a threshold and an operator (>, >=, =, <=, or <). For example, assume the " > " threshold type is set for the model, then if the input concept value is greater than the threshold for that concept, the input concept will be output from this model, otherwise it will not be output by the model'}),(0,n.jsx)(t.td,{style:{textAlign:"center"},children:(0,n.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Filter/concept-thresholder.yml",children:[(0,n.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"random-sample"}),(0,n.jsx)(t.td,{children:"Any -> Any"}),(0,n.jsx)(t.td,{children:"Randomly sample allowing the input to pass to the output. This is done with the conditional keep_fraction > rand() where keep_fraction is the fraction to allow through on average"}),(0,n.jsx)(t.td,{style:{textAlign:"center"},children:(0,n.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Filter/random-sample.yml",children:[(0,n.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"region-thresholder"}),(0,n.jsx)(t.td,{children:"Concepts -> Concepts"}),(0,n.jsx)(t.td,{children:'Threshold regions based on the concepts that they contain using a threshold per concept and an overall operator (>, >=, =, <=, or <). For example, assume the " > " threshold type is set for the model, then if the input regions[...].data.concepts.value is greater than the threshold for that concept, the input concept will be output from this model, otherwise it will not be output by the model. If the entire list of concepts at regions[...].data.concepts is filtered out then the overall region will also be removed'}),(0,n.jsx)(t.td,{style:{textAlign:"center"},children:(0,n.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Filter/region-thresholder.yml",children:[(0,n.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"byte-tracker"}),(0,n.jsx)(t.td,{children:"Frame -> Track ID"}),(0,n.jsx)(t.td,{children:"Uses byte tracking algorithm for tracking objects"}),(0,n.jsx)(t.td,{style:{textAlign:"center"},children:(0,n.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Edit/byte-tracker.yml",children:[(0,n.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"concept-synonym-mapper"}),(0,n.jsx)(t.td,{children:"Concept -> Concept"}),(0,n.jsx)(t.td,{children:"Map the input concepts to output concepts by following synonym concept relations in the knowledge graph of your app"}),(0,n.jsx)(t.td,{style:{textAlign:"center"},children:(0,n.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Edit/concept-synonym-mapper.yml",children:[(0,n.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"image-align"}),(0,n.jsx)(t.td,{children:"Image -> Image"}),(0,n.jsx)(t.td,{children:"Aligns images using keypoints"}),(0,n.jsx)(t.td,{style:{textAlign:"center"},children:(0,n.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Edit/image-align.yml",children:[(0,n.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"image-crop"}),(0,n.jsx)(t.td,{children:"Image -> Image"}),(0,n.jsx)(t.td,{children:"Crop the input image according to each input region that is present in the input. When used in a workflow this model can look back along the graph of the workflow to find the input image if the preceding model does not output an image itself so that you can do image -> detector -> cropper type of workflow easily"}),(0,n.jsx)(t.td,{style:{textAlign:"center"},children:(0,n.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Edit/image-crop.yml",children:[(0,n.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"image-tiling-operator"}),(0,n.jsx)(t.td,{children:"Image -> Image"}),(0,n.jsx)(t.td,{children:"Operator for tiling images into a fixed number of equal sized images"}),(0,n.jsx)(t.td,{style:{textAlign:"center"},children:(0,n.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Edit/image-tiling-operator.yml",children:[(0,n.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"image-to-image"}),(0,n.jsx)(t.td,{children:"Image -> Image"}),(0,n.jsx)(t.td,{children:"Given an image, apply a transformation on the input and return the post-processed image as output"}),(0,n.jsx)(t.td,{style:{textAlign:"center"},children:(0,n.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Edit/image-to-image.yml",children:[(0,n.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"input-filter"}),(0,n.jsx)(t.td,{children:"Any -> Any"}),(0,n.jsx)(t.td,{children:"If the input going through this model does not match those we are filtering for, it will not be passed on in the workflow branch"}),(0,n.jsx)(t.td,{style:{textAlign:"center"},children:(0,n.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Edit/input-filter.yml",children:[(0,n.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"input-searcher"}),(0,n.jsx)(t.td,{children:"Concepts,Images,Text -> Hits"}),(0,n.jsx)(t.td,{children:"Triggers a visual search in another app based on the model configs if concept(s) are found in images and returns the matched search hits as regions."}),(0,n.jsx)(t.td,{style:{textAlign:"center"},children:(0,n.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Edit/keyword-filter-operator.yml",children:[(0,n.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"keyword-filter-operator"}),(0,n.jsx)(t.td,{children:"Text -> Concepts"}),(0,n.jsx)(t.td,{children:"This operator is initialized with a set of words, and then determines which are found in the input text."}),(0,n.jsx)(t.td,{style:{textAlign:"center"},children:(0,n.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Edit/input-searcher.yml",children:[(0,n.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"language-id-operator"}),(0,n.jsx)(t.td,{children:"Text -> Concepts"}),(0,n.jsx)(t.td,{children:"Operator for language identification using the langdetect library"}),(0,n.jsx)(t.td,{style:{textAlign:"center"},children:(0,n.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Edit/language-id-operator.yml",children:[(0,n.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"multimodal-embedder"}),(0,n.jsx)(t.td,{children:"Any -> Embeddings"}),(0,n.jsx)(t.td,{children:"Embed text or image into a vector representing a high level understanding from our Al models, e.g. CLIP. These embeddings enable similarity search and training on top of them."}),(0,n.jsx)(t.td,{style:{textAlign:"center"},children:(0,n.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Edit/multimodal-embedder.yml",children:[(0,n.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"multimodal-to-text"}),(0,n.jsx)(t.td,{children:"Any -> Text"}),(0,n.jsx)(t.td,{children:"Generate text from either text or images or both as input, allowing it to understand and respond to questions about those images"}),(0,n.jsx)(t.td,{style:{textAlign:"center"},children:(0,n.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Edit/multimodal-to-text.yml",children:[(0,n.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"prompter"}),(0,n.jsx)(t.td,{children:"Text -> Text"}),(0,n.jsx)(t.td,{children:"Prompt template where inputted text will be inserted into placeholders marked with (data.text.raw)."}),(0,n.jsx)(t.td,{style:{textAlign:"center"},children:(0,n.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Edit/prompter.yml",children:[(0,n.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"rag-prompter"}),(0,n.jsx)(t.td,{children:"Text -> Text"}),(0,n.jsxs)(t.td,{children:["A prompt template where we will perform a semantic search in the app with the incoming text. The inputted text will be inserted into placeholders marked with '(data.text.raw)' and search results will be inserted into placeholders with '",(0,n.jsx)(t.code,{children:"{data.hits}"}),"', which will be new line separated"]}),(0,n.jsx)(t.td,{style:{textAlign:"center"},children:(0,n.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Edit/rag-prompter.yml",children:[(0,n.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"regex-based-classifier"}),(0,n.jsx)(t.td,{children:"Text -> Concepts"}),(0,n.jsx)(t.td,{children:"Classifies text using regex. If the regex matches, the text is classified as the provided concepts."}),(0,n.jsx)(t.td,{style:{textAlign:"center"},children:(0,n.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Edit/regex-based-classifier.yml",children:[(0,n.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"text-to-audio"}),(0,n.jsx)(t.td,{children:"Text -> Audio"}),(0,n.jsx)(t.td,{children:"Given text input, this model produces an audio file containing the spoken version of the input"}),(0,n.jsx)(t.td,{style:{textAlign:"center"},children:(0,n.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Edit/text-to-audio.yml",children:[(0,n.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"text-to-image"}),(0,n.jsx)(t.td,{children:"Text -> Image"}),(0,n.jsx)(t.td,{children:"Takes in a prompt and generates an image"}),(0,n.jsx)(t.td,{style:{textAlign:"center"},children:(0,n.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Edit/text-to-image.yml",children:[(0,n.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"tiling-region-aggregator-operator"}),(0,n.jsx)(t.td,{children:"Frames -> Concepts,Bounding Box"}),(0,n.jsx)(t.td,{children:"Operator to be used as a follow up to the image-tiling-operator and visual detector. This operator will transform the detections on each of tiles back to the original image and perform non-maximum suppression. Only the top class prediction for each box is considered"}),(0,n.jsx)(t.td,{style:{textAlign:"center"},children:(0,n.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Edit/tiling-region-aggregator-operator.yml",children:[(0,n.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"visual-keypointer"}),(0,n.jsx)(t.td,{children:"Image -> Keypoints"}),(0,n.jsx)(t.td,{children:"This model detects keypoints in images or video frames."}),(0,n.jsx)(t.td,{style:{textAlign:"center"},children:(0,n.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Edit/visual-keypointer.yml",children:[(0,n.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"isolation-operator"}),(0,n.jsx)(t.td,{children:"Concepts,BoundingBox -> Concepts,BoundingBox"}),(0,n.jsx)(t.td,{children:"Operator that computes distance between detections and assigns isolation label"}),(0,n.jsx)(t.td,{style:{textAlign:"center"},children:(0,n.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Aggregate/isolation-operator.yml",children:[(0,n.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"object-counter"}),(0,n.jsx)(t.td,{children:"Concepts -> Metadata"}),(0,n.jsx)(t.td,{children:"count number of regions that match this model's active concepts frame by frame"}),(0,n.jsx)(t.td,{style:{textAlign:"center"},children:(0,n.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Aggregate/object-counter.yml",children:[(0,n.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"text-aggregation-operator"}),(0,n.jsx)(t.td,{children:"Text -> Text"}),(0,n.jsx)(t.td,{children:"Operator that combines text detections into text body for the whole image. Detections are sorted from left to right first and then top to bottom, using the top-left corner of the bounding box as reference"}),(0,n.jsx)(t.td,{style:{textAlign:"center"},children:(0,n.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Aggregate/text-aggregation-operator.yml",children:[(0,n.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"tokens-to-entity-operator"}),(0,n.jsx)(t.td,{children:"Text,Concepts -> Text,Concepts"}),(0,n.jsx)(t.td,{children:"Operator that combines text tokens into entities, e.g. New' + 'York' -> New York"}),(0,n.jsx)(t.td,{style:{textAlign:"center"},children:(0,n.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Aggregate/tokens-to-entity-operator.yml",children:[(0,n.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"annotation-writer"}),(0,n.jsx)(t.td,{children:"Any -> Any"}),(0,n.jsx)(t.td,{children:"Write the input data to the database in the form of an annotation with a specified status as if a specific user created the annotation"}),(0,n.jsx)(t.td,{style:{textAlign:"center"},children:(0,n.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Action/annotation-writer.yml",children:[(0,n.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"aws-lambda"}),(0,n.jsx)(t.td,{children:"Any -> Any"}),(0,n.jsx)(t.td,{children:"This model sends data to an AWS lambda function so you can implement any arbitrary logic to be handled within a model predict or workflow. The request our API sends is a PostModelOutputsRequest in the 'request' field and the response we expect is a MultiOutputResponse response in the 'response' field"}),(0,n.jsx)(t.td,{style:{textAlign:"center"},children:(0,n.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Action/aws-lambda.yml",children:[(0,n.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"email"}),(0,n.jsx)(t.td,{children:"Any -> Any"}),(0,n.jsx)(t.td,{children:"Email alert model will send an email if there are any data fields input to this model"}),(0,n.jsx)(t.td,{style:{textAlign:"center"},children:(0,n.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Action/email.yml",children:[(0,n.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"results-push"}),(0,n.jsx)(t.td,{children:"Any -> Any"}),(0,n.jsx)(t.td,{children:"This model pushes clarifai prediction results in an external format"}),(0,n.jsx)(t.td,{style:{textAlign:"center"},children:(0,n.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Action/results-push.yml",children:[(0,n.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"sms"}),(0,n.jsx)(t.td,{children:"Any -> Any"}),(0,n.jsx)(t.td,{children:"SMS alert model will send a SMS if there are any data fields input to this model"}),(0,n.jsx)(t.td,{style:{textAlign:"center"},children:(0,n.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Action/sms.yml",children:[(0,n.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"status-push"}),(0,n.jsx)(t.td,{children:"Any -> Any"}),(0,n.jsx)(t.td,{children:"This model pushes processing status of a batch of inputs ingested through vendor/inputs endpoint in one request"}),(0,n.jsx)(t.td,{style:{textAlign:"center"},children:(0,n.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Action/status-push.yml",children:[(0,n.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]})]})]}),"\n",(0,n.jsx)(t.h3,{id:"asr-sentiment",children:"ASR Sentiment"}),"\n",(0,n.jsx)(t.p,{children:"Automatic Speech Recognition (ASR) sentiment analysis refers to the process of analyzing the emotional tone or sentiment expressed in spoken language using ASR model."}),"\n",(0,n.jsx)(l.A,{className:"language-text",children:m}),"\n",(0,n.jsx)(t.h3,{id:"demographics",children:"Demographics"}),"\n",(0,n.jsx)(t.p,{children:"This is a Multi-model workflow that detects, crops, and recognizes demographic characteristics of faces. Visually classifies age, gender, and multi-culture."}),"\n",(0,n.jsx)(l.A,{className:"language-text",children:p}),"\n",(0,n.jsx)(t.h3,{id:"face-search",children:"Face Search"}),"\n",(0,n.jsx)(t.p,{children:"A workflow that combines detection, recognition, and embedding to generate face landmarks and enable visual search using detected faces's embeddings."}),"\n",(0,n.jsx)(l.A,{className:"language-text",children:g}),"\n",(0,n.jsx)(t.h3,{id:"face-sentiment",children:"Face Sentiment"}),"\n",(0,n.jsx)(t.p,{children:"Multi-model workflow that combines face detection and sentiment classification of 7 concepts: anger, disgust, fear, neutral, happiness, sadness, contempt."}),"\n",(0,n.jsx)(l.A,{className:"language-text",children:f}),"\n",(0,n.jsx)(t.h3,{id:"general",children:"General"}),"\n",(0,n.jsx)(t.p,{children:"A general image detection workflow that detects a variety of common objects, and enable visual search using general embeddings on detected regions."}),"\n",(0,n.jsx)(l.A,{className:"language-text",children:x}),"\n",(0,n.jsx)(t.h3,{id:"language-aware-ocr",children:"Language Aware OCR"}),"\n",(0,n.jsx)(t.p,{children:"A workflows that performs OCR in different languages."}),"\n",(0,n.jsx)(l.A,{className:"language-text",children:u}),"\n",(0,n.jsx)(t.h3,{id:"prompter-llm",children:"Prompter LLM"}),"\n",(0,n.jsx)(t.p,{children:"A workflow that uses a prompt template with LLM."}),"\n",(0,n.jsx)(l.A,{className:"language-text",children:j}),"\n",(0,n.jsx)(t.h3,{id:"rag-prompter-llm",children:"RAG Prompter LLM"}),"\n",(0,n.jsx)(t.p,{children:"This workflow uses LLM with a RAG prompter template."}),"\n",(0,n.jsx)(l.A,{className:"language-text",children:w}),"\n",(0,n.jsxs)(t.p,{children:["Visit this ",(0,n.jsx)(t.a,{href:"https://github.com/Clarifai/examples/tree/main/workflows/configs",children:"link"})," to view some YAML examples."]}),"\n",(0,n.jsx)(t.h2,{id:"list-workflows",children:"List workflows"}),"\n",(0,n.jsx)(t.p,{children:"The Clarifai SDKs provides an API that empowers users to effortlessly retrieve a comprehensive list of all workflows accessible within an app. This functionality enables seamless exploration and management of workflows, facilitating a streamlined experience for developers to interact with and leverage the power of Clarifai's workflow capabilities."}),"\n",(0,n.jsxs)(t.p,{children:["To know more about workflows visit this ",(0,n.jsx)(t.a,{href:"https://docs.clarifai.com/portal-guide/workflows/",children:"link"}),"."]}),"\n",(0,n.jsx)(s.A,{children:(0,n.jsx)(o.A,{value:"python",label:"Python",children:(0,n.jsx)(l.A,{className:"language-python",children:d})})}),"\n",(0,n.jsxs)(i,{children:[(0,n.jsx)("summary",{children:"Output"}),(0,n.jsx)(l.A,{className:"language-text",children:b})]}),"\n",(0,n.jsx)(t.h2,{id:"delete-workflows",children:"Delete workflows"}),"\n",(0,n.jsx)(t.p,{children:"The Clarifai SDKs empowers users to seamlessly delete workflows through the API by specifying the unique identifier of the workflow. This functionality provides a straightforward method for users to manage and remove workflows as needed within their applications or projects. By utilizing the provided workflow id parameter, developers can efficiently interact with the Clarifai platform to streamline workflow management processes."}),"\n",(0,n.jsxs)(t.p,{children:["To know more about workflows visit this ",(0,n.jsx)(t.a,{href:"https://docs.clarifai.com/portal-guide/workflows/",children:"link"}),"."]}),"\n",(0,n.jsx)(t.admonition,{type:"caution",children:(0,n.jsx)(t.p,{children:"Be certain that you want to delete a particular workflow as the operation cannot be undone."})}),"\n",(0,n.jsx)(s.A,{children:(0,n.jsx)(o.A,{value:"python",label:"Python",children:(0,n.jsx)(l.A,{className:"language-python",children:c})})}),"\n",(0,n.jsxs)(i,{children:[(0,n.jsx)("summary",{children:"Output"}),(0,n.jsx)(l.A,{className:"language-text",children:y})]}),"\n",(0,n.jsx)(t.h2,{id:"export-workflow",children:"Export workflow"}),"\n",(0,n.jsx)(t.p,{children:"The Clarifai SDKs provides a powerful feature for initiating or swiftly modifying existing Clarifai community workflows through an intuitive YAML configuration. With the export functionality, users can effortlessly download a YAML file representing the entire workflow. This file serves as a local copy, allowing seamless edits and providing the flexibility to create and manage new workflows with ease."}),"\n",(0,n.jsxs)(t.p,{children:["To know more about workflows visit this ",(0,n.jsx)(t.a,{href:"https://docs.clarifai.com/portal-guide/workflows/",children:"link"}),"."]}),"\n",(0,n.jsx)(s.A,{children:(0,n.jsx)(o.A,{value:"python",label:"Python",children:(0,n.jsx)(l.A,{className:"language-python",children:h})})})]})}function N(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(T,{...e})}):T(e)}},19365:(e,t,i)=>{i.d(t,{A:()=>o});i(96540);var n=i(18215);const r={tabItem:"tabItem_Ymn6"};var s=i(74848);function o(e){let{children:t,hidden:i,className:o}=e;return(0,s.jsx)("div",{role:"tabpanel",className:(0,n.A)(r.tabItem,o),hidden:i,children:t})}},11470:(e,t,i)=>{i.d(t,{A:()=>k});var n=i(96540),r=i(18215),s=i(23104),o=i(56347),l=i(205),a=i(57485),d=i(31682),c=i(70679);function h(e){return n.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,n.isValidElement)(e)&&function(e){const{props:t}=e;return!!t&&"object"==typeof t&&"value"in t}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function m(e){const{values:t,children:i}=e;return(0,n.useMemo)((()=>{const e=t??function(e){return h(e).map((e=>{let{props:{value:t,label:i,attributes:n,default:r}}=e;return{value:t,label:i,attributes:n,default:r}}))}(i);return function(e){const t=(0,d.X)(e,((e,t)=>e.value===t.value));if(t.length>0)throw new Error(`Docusaurus error: Duplicate values "${t.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[t,i])}function p(e){let{value:t,tabValues:i}=e;return i.some((e=>e.value===t))}function g(e){let{queryString:t=!1,groupId:i}=e;const r=(0,o.W6)(),s=function(e){let{queryString:t=!1,groupId:i}=e;if("string"==typeof t)return t;if(!1===t)return null;if(!0===t&&!i)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return i??null}({queryString:t,groupId:i});return[(0,a.aZ)(s),(0,n.useCallback)((e=>{if(!s)return;const t=new URLSearchParams(r.location.search);t.set(s,e),r.replace({...r.location,search:t.toString()})}),[s,r])]}function f(e){const{defaultValue:t,queryString:i=!1,groupId:r}=e,s=m(e),[o,a]=(0,n.useState)((()=>function(e){let{defaultValue:t,tabValues:i}=e;if(0===i.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(t){if(!p({value:t,tabValues:i}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${t}" but none of its children has the corresponding value. Available values are: ${i.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return t}const n=i.find((e=>e.default))??i[0];if(!n)throw new Error("Unexpected error: 0 tabValues");return n.value}({defaultValue:t,tabValues:s}))),[d,h]=g({queryString:i,groupId:r}),[f,x]=function(e){let{groupId:t}=e;const i=function(e){return e?`docusaurus.tab.${e}`:null}(t),[r,s]=(0,c.Dv)(i);return[r,(0,n.useCallback)((e=>{i&&s.set(e)}),[i,s])]}({groupId:r}),u=(()=>{const e=d??f;return p({value:e,tabValues:s})?e:null})();(0,l.A)((()=>{u&&a(u)}),[u]);return{selectedValue:o,selectValue:(0,n.useCallback)((e=>{if(!p({value:e,tabValues:s}))throw new Error(`Can't select invalid tab value=${e}`);a(e),h(e),x(e)}),[h,x,s]),tabValues:s}}var x=i(92303);const u={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var j=i(74848);function w(e){let{className:t,block:i,selectedValue:n,selectValue:o,tabValues:l}=e;const a=[],{blockElementScrollPositionUntilNextRender:d}=(0,s.a_)(),c=e=>{const t=e.currentTarget,i=a.indexOf(t),r=l[i].value;r!==n&&(d(t),o(r))},h=e=>{let t=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const i=a.indexOf(e.currentTarget)+1;t=a[i]??a[0];break}case"ArrowLeft":{const i=a.indexOf(e.currentTarget)-1;t=a[i]??a[a.length-1];break}}t?.focus()};return(0,j.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,r.A)("tabs",{"tabs--block":i},t),children:l.map((e=>{let{value:t,label:i,attributes:s}=e;return(0,j.jsx)("li",{role:"tab",tabIndex:n===t?0:-1,"aria-selected":n===t,ref:e=>a.push(e),onKeyDown:h,onClick:c,...s,className:(0,r.A)("tabs__item",u.tabItem,s?.className,{"tabs__item--active":n===t}),children:i??t},t)}))})}function b(e){let{lazy:t,children:i,selectedValue:r}=e;const s=(Array.isArray(i)?i:[i]).filter(Boolean);if(t){const e=s.find((e=>e.props.value===r));return e?(0,n.cloneElement)(e,{className:"margin-top--md"}):null}return(0,j.jsx)("div",{className:"margin-top--md",children:s.map(((e,t)=>(0,n.cloneElement)(e,{key:t,hidden:e.props.value!==r})))})}function y(e){const t=f(e);return(0,j.jsxs)("div",{className:(0,r.A)("tabs-container",u.tabList),children:[(0,j.jsx)(w,{...t,...e}),(0,j.jsx)(b,{...t,...e})]})}function k(e){const t=(0,x.A)();return(0,j.jsx)(y,{...e,children:h(e.children)},String(t))}}}]);
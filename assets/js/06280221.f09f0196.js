"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[2057],{85162:(n,e,t)=>{t.d(e,{Z:()=>i});var a=t(67294),s=t(86010);const o={tabItem:"tabItem_Ymn6"};function i(n){let{children:e,hidden:t,className:i}=n;return a.createElement("div",{role:"tabpanel",className:(0,s.Z)(o.tabItem,i),hidden:t},e)}},74866:(n,e,t)=>{t.d(e,{Z:()=>k});var a=t(87462),s=t(67294),o=t(86010),i=t(12466),l=t(16550),r=t(91980),u=t(67392),d=t(50012);function c(n){return function(n){return s.Children.map(n,(n=>{if(!n||(0,s.isValidElement)(n)&&function(n){const{props:e}=n;return!!e&&"object"==typeof e&&"value"in e}(n))return n;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof n.type?n.type:n.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}(n).map((n=>{let{props:{value:e,label:t,attributes:a,default:s}}=n;return{value:e,label:t,attributes:a,default:s}}))}function p(n){const{values:e,children:t}=n;return(0,s.useMemo)((()=>{const n=e??c(t);return function(n){const e=(0,u.l)(n,((n,e)=>n.value===e.value));if(e.length>0)throw new Error(`Docusaurus error: Duplicate values "${e.map((n=>n.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(n),n}),[e,t])}function _(n){let{value:e,tabValues:t}=n;return t.some((n=>n.value===e))}function f(n){let{queryString:e=!1,groupId:t}=n;const a=(0,l.k6)(),o=function(n){let{queryString:e=!1,groupId:t}=n;if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:e,groupId:t});return[(0,r._X)(o),(0,s.useCallback)((n=>{if(!o)return;const e=new URLSearchParams(a.location.search);e.set(o,n),a.replace({...a.location,search:e.toString()})}),[o,a])]}function m(n){const{defaultValue:e,queryString:t=!1,groupId:a}=n,o=p(n),[i,l]=(0,s.useState)((()=>function(n){let{defaultValue:e,tabValues:t}=n;if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!_({value:e,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${t.map((n=>n.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const a=t.find((n=>n.default))??t[0];if(!a)throw new Error("Unexpected error: 0 tabValues");return a.value}({defaultValue:e,tabValues:o}))),[r,u]=f({queryString:t,groupId:a}),[c,m]=function(n){let{groupId:e}=n;const t=function(n){return n?`docusaurus.tab.${n}`:null}(e),[a,o]=(0,d.Nk)(t);return[a,(0,s.useCallback)((n=>{t&&o.set(n)}),[t,o])]}({groupId:a}),h=(()=>{const n=r??c;return _({value:n,tabValues:o})?n:null})();(0,s.useLayoutEffect)((()=>{h&&l(h)}),[h]);return{selectedValue:i,selectValue:(0,s.useCallback)((n=>{if(!_({value:n,tabValues:o}))throw new Error(`Can't select invalid tab value=${n}`);l(n),u(n),m(n)}),[u,m,o]),tabValues:o}}var h=t(72389);const w={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};function I(n){let{className:e,block:t,selectedValue:l,selectValue:r,tabValues:u}=n;const d=[],{blockElementScrollPositionUntilNextRender:c}=(0,i.o5)(),p=n=>{const e=n.currentTarget,t=d.indexOf(e),a=u[t].value;a!==l&&(c(e),r(a))},_=n=>{let e=null;switch(n.key){case"Enter":p(n);break;case"ArrowRight":{const t=d.indexOf(n.currentTarget)+1;e=d[t]??d[0];break}case"ArrowLeft":{const t=d.indexOf(n.currentTarget)-1;e=d[t]??d[d.length-1];break}}e?.focus()};return s.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,o.Z)("tabs",{"tabs--block":t},e)},u.map((n=>{let{value:e,label:t,attributes:i}=n;return s.createElement("li",(0,a.Z)({role:"tab",tabIndex:l===e?0:-1,"aria-selected":l===e,key:e,ref:n=>d.push(n),onKeyDown:_,onClick:p},i,{className:(0,o.Z)("tabs__item",w.tabItem,i?.className,{"tabs__item--active":l===e})}),t??e)})))}function E(n){let{lazy:e,children:t,selectedValue:a}=n;const o=(Array.isArray(t)?t:[t]).filter(Boolean);if(e){const n=o.find((n=>n.props.value===a));return n?(0,s.cloneElement)(n,{className:"margin-top--md"}):null}return s.createElement("div",{className:"margin-top--md"},o.map(((n,e)=>(0,s.cloneElement)(n,{key:e,hidden:n.props.value!==a}))))}function b(n){const e=m(n);return s.createElement("div",{className:(0,o.Z)("tabs-container",w.tabList)},s.createElement(I,(0,a.Z)({},n,e)),s.createElement(E,(0,a.Z)({},n,e)))}function k(n){const e=(0,h.Z)();return s.createElement(b,(0,a.Z)({key:String(e)},n))}},69065:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>C,contentTitle:()=>N,default:()=>W,frontMatter:()=>S,metadata:()=>P,toc:()=>U});var a=t(87462),s=(t(67294),t(3905)),o=t(74866),i=t(85162),l=t(90814);const r="########################################################################################\n# In this section, we set the user authentication, app ID, and the details of the new\n# custom workflow we want to create. Change these strings to run your own example.\n########################################################################################\n\nUSER_ID = 'YOUR_USER_ID_HERE'\n# Your PAT (Personal Access Token) can be found in the portal under Authentification\nPAT = 'YOUR_PAT_HERE'\nAPP_ID = 'YOUR_APP_ID_HERE'\n# Change these to create your own custom workflow\nWORKFLOW_ID = 'my-custom-workflow'\nNODE_ID_1 = 'audio-to-text'\nMODEL_ID_1 = 'asr-wav2vec2-base-960h-english'\nMODEL_VERSION_ID_1 = 'f4deae70a473492a8e2f9b7bb1dbee85'\n\nNODE_ID_2 = 'sentiment-analysis'\nMODEL_ID_2 = 'sentiment-analysis-distilbert-english'\nMODEL_VERSION_ID_2 = 'c0b09e606db94d9bae7eb40c626192fc'\n\n##########################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n##########################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (('authorization', 'Key ' + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID) # The userDataObject is required when using a PAT\n\npost_workflows_response = stub.PostWorkflows(\n    service_pb2.PostWorkflowsRequest(\n      user_app_id=userDataObject,  \n      workflows=[\n        resources_pb2.Workflow(\n          id=WORKFLOW_ID,\n          nodes=[\n            resources_pb2.WorkflowNode(\n              id=NODE_ID_1,\n              model=resources_pb2.Model(\n                id=MODEL_ID_1,\n                model_version=resources_pb2.ModelVersion(\n                  id=MODEL_VERSION_ID_1\n                )\n              )\n            ),\n            resources_pb2.WorkflowNode(\n              id=NODE_ID_2,\n              model=resources_pb2.Model(\n                id=MODEL_ID_2,\n                model_version=resources_pb2.ModelVersion(\n                  id=MODEL_VERSION_ID_2\n                )\n              ),\n              node_inputs=[\n                resources_pb2.NodeInput(node_id=NODE_ID_1)\n                ]\n            ),\n          ]\n        )\n      ]\n    ),\n    metadata=metadata\n)               \n\nif post_workflows_response.status.code != status_code_pb2.SUCCESS:\n    print(post_workflows_response.status)\n    raise Exception(\"Post workflows failed, status: \" + post_workflows_response.status.description) \n\n",u="#############################################################################\n# In this section, we set the user authentication, app ID, workflow ID, and  \n# audio URL. Change these strings to run your own example.\n##############################################################################\n\nUSER_ID = 'YOUR_USER_ID_HERE'\n# Your PAT (Personal Access Token) can be found in the portal under Authentification\nPAT = 'YOUR_PAT_HERE'\nAPP_ID = 'YOUR_APP_ID_HERE'\n# Change these to make your own predictions\nWORKFLOW_ID = 'my-custom-workflow'\nAUDIO_URL = 'https://samples.clarifai.com/negative_sentence_1.wav'\n\n##########################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n##########################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (('authorization', 'Key ' + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID) # The userDataObject is required when using a PAT\n\npost_workflow_results_response = stub.PostWorkflowResults(\n    service_pb2.PostWorkflowResultsRequest(\n        user_app_id=userDataObject,  \n        workflow_id=WORKFLOW_ID,\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    audio=resources_pb2.Audio(\n                        url=AUDIO_URL\n                    )\n                )\n            )\n        ]\n    ),\n    metadata=metadata\n)\nif post_workflow_results_response.status.code != status_code_pb2.SUCCESS:\n    print(post_workflow_results_response.status)\n    raise Exception(\"Post workflow results failed, status: \" + post_workflow_results_response.status.description)\n\n# We'll get one WorkflowResult for each input we used above. Because of one input, we have here one WorkflowResult\nresults = post_workflow_results_response.results[0]\n\n# Each model we have in the workflow will produce its output\nfor output in results.outputs:    \n    model = output.model    \n    print(\"Output for the model: `%s`\" % model.id)   \n    for concept in output.data.concepts:        \n        print(\"\\t%s %.2f\" % (concept.name, concept.value)) \n    print(output.data.text.raw)     \n\n# Uncomment this line to print the full Response JSON\n#print(results) \n",d="##############################################################################\n# In this section, we set the user authentication, app ID, workflow ID, and  \n# audio file location. Change these strings to run your own example.\n##############################################################################\n\nUSER_ID = 'YOUR_USER_ID_HERE'\n# Your PAT (Personal Access Token) can be found in the portal under Authentification\nPAT = 'YOUR_PAT_HERE'\nAPP_ID = 'YOUR_APP_ID_HERE'\n# Change these to make your own predictions\nWORKFLOW_ID = 'my-custom-workflow'\nAUDIO_FILE_LOCATION = 'YOUR_AUDIO_FILE_LOCATION_HERE' \n\n##########################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n##########################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (('authorization', 'Key ' + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID) # The userDataObject is required when using a PAT\n\nwith open(AUDIO_FILE_LOCATION, \"rb\") as f:\n    file_bytes = f.read()\n\npost_workflow_results_response = stub.PostWorkflowResults(\n    service_pb2.PostWorkflowResultsRequest(\n        user_app_id=userDataObject,  \n        workflow_id=WORKFLOW_ID,\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    audio=resources_pb2.Audio(\n                        base64=file_bytes\n                    )\n                )\n            )\n        ]\n    ),\n    metadata=metadata\n)\nif post_workflow_results_response.status.code != status_code_pb2.SUCCESS:\n    print(post_workflow_results_response.status)\n    raise Exception(\"Post workflow results failed, status: \" + post_workflow_results_response.status.description)\n\n# We'll get one WorkflowResult for each input we used above. Because of one input, we have here one WorkflowResult\nresults = post_workflow_results_response.results[0]\n\n# Each model we have in the workflow will produce its output\nfor output in results.outputs:    \n    model = output.model    \n    print(\"Output for the model: `%s`\" % model.id)   \n    for concept in output.data.concepts:        \n        print(\"\\t%s %.2f\" % (concept.name, concept.value)) \n    print(output.data.text.raw)     \n\n# Uncomment this line to print the full Response JSON\n#print(results) \n",c='\x3c!--index.html file--\x3e\n\n<script>\n    //////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, app ID, and the details of the new\n    // custom workflow we want to create. Change these strings to run your own example.\n    /////////////////////////////////////////////////////////////////////////////////////////\n\n    const USER_ID = \'YOUR_USER_ID_HERE\';\n    // Your PAT (Personal Access Token) can be found in the portal under Authentification\n    const PAT = \'YOUR_PAT_HERE\';\n    const APP_ID = \'YOUR_APP_ID_HERE\';\n    // Change these to create your own custom workflow\n    const WORKFLOW_ID = \'my-custom-workflow\';\n    const NODE_ID_1 = \'audio-to-text\';\n    const MODEL_ID_1 = \'asr-wav2vec2-base-960h-english\';\n    const MODEL_VERSION_ID_1 = \'f4deae70a473492a8e2f9b7bb1dbee85\';\n\n    const NODE_ID_2 = \'sentiment-analysis\';\n    const MODEL_ID_2 = \'sentiment-analysis-distilbert-english\';\n    const MODEL_VERSION_ID_2 = \'c0b09e606db94d9bae7eb40c626192fc\';\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////   \n\n    const raw = JSON.stringify({\n        "user_app_id": {\n            "user_id": USER_ID,\n            "app_id": APP_ID\n        },\n        "workflows": [{\n            "id": WORKFLOW_ID,\n            "nodes": [\n                {\n                    "id": NODE_ID_1,\n                    "model": {\n                        "id": MODEL_ID_1,\n                        "model_version": {\n                            "id": MODEL_VERSION_ID_1\n                        }\n                    }\n                },\n                {\n                    "id": NODE_ID_2,\n                    "model": {\n                        "id": MODEL_ID_2,\n                        "model_version": {\n                            "id": MODEL_VERSION_ID_2\n                        }\n                    },\n                        "node_inputs": [\n                            {\n                                "node_id": NODE_ID_1\n                            }\n                        ]\n                }\n            ]\n        }]\n    });\n\n    const requestOptions = {\n        method: \'POST\',\n        headers: {\n            \'Accept\': \'application/json\',\n            \'Authorization\': \'Key \' + PAT\n        },\n        body: raw\n    };\n\n    fetch(`https://api.clarifai.com/v2/workflows`, requestOptions)\n        .then(response => response.text())\n        .then(result => console.log(result))\n        .catch(error => console.log(\'error\', error));\n\n<\/script>',p='\x3c!--index.html file--\x3e\n\n<script>\n    ///////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, app ID, workflow ID, and\n    // audio URL. Change these strings to run your own example.\n    ///////////////////////////////////////////////////////////////////////////////////\n  \n    const USER_ID = "YOUR_USER_ID_HERE";\n    // Your PAT (Personal Access Token) can be found in the portal under Authentification\n    const PAT = "YOUR_PAT_HERE";\n    const APP_ID = "YOUR_APP_ID_HERE";\n    // Change these to make your own predictions\n    const WORKFLOW_ID = "my-custom-workflow";\n    const AUDIO_URL = "https://samples.clarifai.com/negative_sentence_1.wav";\n  \n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    /////////////////////////////////////////////////////////////////////////////////// \n  \n    const raw = JSON.stringify({\n      "user_app_id": {\n        "user_id": USER_ID,\n        "app_id": APP_ID\n      },\n      "inputs": [\n        {\n          "data": {\n            "audio": {\n              "url": AUDIO_URL\n            }\n          }\n        }\n      ]\n    });\n  \n    const requestOptions = {\n      method: \'POST\',\n      headers: {\n        \'Accept\': \'application/json\',\n        \'Authorization\': \'Key \' + PAT\n      },\n      body: raw\n    };\n  \n    fetch(`https://api.clarifai.com/v2/workflows/${WORKFLOW_ID}/results`, requestOptions)\n      .then(response => response.text())\n      .then(result => console.log(result))\n      .catch(error => console.log(\'error\', error));\n  <\/script>',_='\x3c!--index.html file--\x3e\n\n<script>\n    //////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, app ID, workflow ID, and bytes of\n    // the audio we want as an input. Change these strings to run your own example.\n    /////////////////////////////////////////////////////////////////////////////////////////\n  \n    const USER_ID = "YOUR_USER_ID_HERE";\n    // Your PAT (Personal Access Token) can be found in the portal under Authentification\n    const PAT = "YOUR_PAT_HERE";\n    const APP_ID = "YOUR_APP_ID_HERE";\n    // Change these to make your own predictions\n    const WORKFLOW_ID = "my-custom-workflow";\n    const AUDIO_BYTES_STRING = "YOUR_BYTES_STRING_HERE";\n  \n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    /////////////////////////////////////////////////////////////////////////////////// \n  \n    const raw = JSON.stringify({\n      "user_app_id": {\n        "user_id": USER_ID,\n        "app_id": APP_ID\n      },\n      "inputs": [\n        {\n          "data": {\n            "audio": {\n              "base64": AUDIO_BYTES_STRING\n            }\n          }\n        }\n      ]\n    });\n  \n    const requestOptions = {\n      method: \'POST\',\n      headers: {\n        \'Accept\': \'application/json\',\n        \'Authorization\': \'Key \' + PAT\n      },\n      body: raw\n    };\n  \n    fetch(`https://api.clarifai.com/v2/workflows/${WORKFLOW_ID}/results`, requestOptions)\n      .then(response => response.text())\n      .then(result => console.log(result))\n      .catch(error => console.log(\'error\', error));\n  <\/script>',f="//index.js file\n\n//////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, app ID, and the details of the new\n// custom workflow we want to create. Change these strings to run your own example.\n/////////////////////////////////////////////////////////////////////////////////////////\n\nconst USER_ID = 'YOUR_USER_ID_HERE';\n// Your PAT (Personal Access Token) can be found in the portal under Authentification\nconst PAT = 'YOUR_PAT_HERE';\nconst APP_ID = 'YOUR_APP_ID_HERE';\n// Change these to create your own custom workflow\nconst WORKFLOW_ID = 'my-custom-workflow';\nconst NODE_ID_1 = 'audio-to-text';\nconst MODEL_ID_1 = 'asr-wav2vec2-base-960h-english';\nconst MODEL_VERSION_ID_1 = 'f4deae70a473492a8e2f9b7bb1dbee85';\n\nconst NODE_ID_2 = 'sentiment-analysis';\nconst MODEL_ID_2 = 'sentiment-analysis-distilbert-english';\nconst MODEL_VERSION_ID_2 = 'c0b09e606db94d9bae7eb40c626192fc';\n\n/////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n/////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require(\"clarifai-nodejs-grpc\");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set(\"authorization\", \"Key \" + PAT);\n\nstub.PostWorkflows(\n    {\n        user_app_id: {\n            \"user_id\": USER_ID,\n            \"app_id\": APP_ID\n        },\n        workflows: [\n            {\n                id: WORKFLOW_ID,\n                nodes: [\n                    {\n                        id: NODE_ID_1,\n                        model: {\n                            id: MODEL_ID_1,\n                            model_version: {\n                                id: MODEL_VERSION_ID_1\n                            }\n                        }\n                    },\n                    {\n                        id: NODE_ID_2,\n                        model: {\n                            id: MODEL_ID_2,\n                            model_version: {\n                                id: MODEL_VERSION_ID_2\n                            }\n                        },\n                        node_inputs: [\n                            {\n                                node_id: NODE_ID_1 \n                            }\n                        ]\n                    }\n                ]\n            }\n        ]\n    },\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            throw new Error(\"Post workflows failed, status: \" + response.status.description);\n        }\n    }\n);",m='//index.js file\n\n///////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, app ID, workflow ID, and\n// audio URL. Change these strings to run your own example.\n///////////////////////////////////////////////////////////////////////////////////\n\nconst USER_ID = \'YOUR_USER_ID_HERE\';\n// Your PAT (Personal Access Token) can be found in the portal under Authentification\nconst PAT = \'YOUR_PAT_HERE\';\nconst APP_ID = \'YOUR_APP_ID_HERE\';\n// Change these to make your own predictions\nconst WORKFLOW_ID = "my-custom-workflow";\nconst AUDIO_URL = "https://samples.clarifai.com/negative_sentence_1.wav";\n\n/////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n/////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\nstub.PostWorkflowResults(\n  {\n    user_app_id: {\n      "user_id": USER_ID,\n      "app_id": APP_ID,\n    },\n    workflow_id: WORKFLOW_ID,\n    inputs: [{ data: { audio: { url: AUDIO_URL } } }],\n  },\n  metadata,\n  (err, response) => {\n    if (err) {\n      throw new Error(err);\n    }\n\n    if (response.status.code !== 10000) {\n      throw new Error(\n        "Post workflow results failed, status: " + response.status.description\n      );\n    }\n\n    // We\'ll get one WorkflowResult for each input we used above. Because of one input, we have here \n    // one WorkflowResult\n    const results = response.results[0];\n\n    // Each model we have in the workflow will produce its output   \n    for (const output of results.outputs) {\n      const model = output.model;\n      console.log("Output for the model: `" + model.id + "`");        \n      for (const concept of output.data.concepts){    \n        console.log("\\t" + concept.name + " " + concept.value);        \n      } \n      if(output.data.text){\n      console.log(output.data.text.raw);        \n      }               \n    }\n  }\n);\n',h='//index.js file\n\n///////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, app ID, workflow ID, and\n// audio file location. Change these strings to run your own example.\n///////////////////////////////////////////////////////////////////////////////////\n\nconst USER_ID = \'YOUR_USER_ID_HERE\';\n// Your PAT (Personal Access Token) can be found in the portal under Authentification\nconst PAT = \'YOUR_PAT_HERE\';\nconst APP_ID = \'YOUR_APP_ID_HERE\';\n// Change these to make your own predictions\nconst WORKFLOW_ID = "my-custom-workflow";\nconst AUDIO_FILE_LOCATION = "YOUR_AUDIO_FILE_LOCATION_HERE";\n\n/////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n/////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\nconst fs = require("fs");\nconst audioBytes = fs.readFileSync(AUDIO_FILE_LOCATION);\n\nstub.PostWorkflowResults(\n  {\n    user_app_id: {\n      "user_id": USER_ID,\n      "app_id": APP_ID,\n    },\n    workflow_id: WORKFLOW_ID,\n    inputs: [{ data: { audio: { base64: audioBytes } } }],\n  },\n  metadata,\n  (err, response) => {\n    if (err) {\n      throw new Error(err);\n    }\n\n    if (response.status.code !== 10000) {\n      throw new Error(\n        "Post workflow results failed, status: " + response.status.description\n      );\n    }\n\n    // We\'ll get one WorkflowResult for each input we used above. Because of one input, we have here \n    // one WorkflowResult\n    const results = response.results[0];\n\n    // Each model we have in the workflow will produce its output   \n    for (const output of results.outputs) {\n      const model = output.model;\n      console.log("Output for the model: `" + model.id + "`");        \n      for (const concept of output.data.concepts){    \n        console.log("\\t" + concept.name + " " + concept.value);        \n      } \n      if(output.data.text){\n      console.log(output.data.text.raw);        \n      }               \n    }\n  }\n);\n',w='package com.clarifai.example;\n\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.grpc.api.status.StatusCode;\n\npublic class ClarifaiExample {\n\n    //////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, app ID, and the details of the new\n    // custom workflow we want to create. Change these strings to run your own example.\n    /////////////////////////////////////////////////////////////////////////////////////////\n\n    static final String USER_ID = "YOUR_USER_ID_HERE";\n    //Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    static final String APP_ID = "YOUR_APP_ID_HERE";\n    // Change these to create your own custom workflow\n    static final String WORKFLOW_ID = "my-custom-workflow";\n    static final String NODE_ID_1 = "audio-to-text";\n    static final String MODEL_ID_1 = "asr-wav2vec2-base-960h-english";\n    static final String MODEL_VERSION_ID_1 = "f4deae70a473492a8e2f9b7bb1dbee85";\n\n    static final String NODE_ID_2 = "sentiment-analysis";\n    static final String MODEL_ID_2 = "sentiment-analysis-distilbert-english";\n    static final String MODEL_VERSION_ID_2 = "c0b09e606db94d9bae7eb40c626192fc";\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    public static void main(String[] args) {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n            .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        MultiWorkflowResponse postWorkflowsResponse = stub.postWorkflows(\n            PostWorkflowsRequest.newBuilder()\n            .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n            .addWorkflows(\n                Workflow.newBuilder()\n                .setId(WORKFLOW_ID)\n                .addNodes(\n                    WorkflowNode.newBuilder()\n                    .setId(NODE_ID_1)\n                    .setModel(\n                        Model.newBuilder()\n                        .setId(MODEL_ID_1)\n                        .setModelVersion(ModelVersion.newBuilder().setId(MODEL_VERSION_ID_1))\n                    )\n                )\n                .addNodes(\n                    WorkflowNode.newBuilder()\n                    .setId(NODE_ID_2)\n                    .setModel(\n                        Model.newBuilder()\n                        .setId(MODEL_ID_2)\n                        .setModelVersion(ModelVersion.newBuilder().setId(MODEL_VERSION_ID_2))\n                    )\n                    .addNodeInputs(NodeInput.newBuilder().setNodeId(NODE_ID_1))\n                )\n            ).build()\n        );\n\n        if (postWorkflowsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("Post workflows failed, status: " + postWorkflowsResponse.getStatus());\n        }\n\n    }\n\n}',I='package com.clarifai.example;\n\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.grpc.api.status.StatusCode;\n\npublic class ClarifaiExample {\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, app ID, workflow ID, and\n    // audio URL. Change these strings to run your own example.\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    static final String USER_ID = "YOUR_USER_ID_HERE";\n    //Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    static final String APP_ID = "YOUR_APP_ID_HERE";\n    // Change these to make your own predictions\n    static final String WORKFLOW_ID = "my-custom-workflow";\n    static final String AUDIO_URL = "https://samples.clarifai.com/negative_sentence_1.wav";\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    public static void main(String[] args) {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n            .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        PostWorkflowResultsResponse postWorkflowResultsResponse = stub.postWorkflowResults(\n            PostWorkflowResultsRequest.newBuilder()\n            .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n            .setWorkflowId(WORKFLOW_ID)\n            .addInputs(\n                Input.newBuilder().setData(\n                    Data.newBuilder().setAudio(\n                        Audio.newBuilder().setUrl(AUDIO_URL)\n                    )\n                )\n            )\n            .build()\n        );\n\n        if (postWorkflowResultsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("Post workflow results failed, status: " + postWorkflowResultsResponse.getStatus());\n        }\n\n        // We\'ll get one WorkflowResult for each input we used above. Because of one input, we have here\n        // one WorkflowResult\n        WorkflowResult results = postWorkflowResultsResponse.getResults(0);\n\n        // Each model we have in the workflow will produce its output\n        for (Output output: results.getOutputsList()) {\n            Model model = output.getModel();\n            System.out.println("Output for the model: `" + model.getId() + "`");   \n            for (Concept concept: output.getData().getConceptsList()) {       \n            \tSystem.out.printf("\\t%s %.2f%n",concept.getName(), concept.getValue());                 \n            }\n            System.out.println(output.getData().getText().getRaw());\n        }\n\n    }\n\n}',E='package com.clarifai.example;\n\nimport java.io.File;\nimport java.io.IOException;\nimport java.nio.file.Files;\n\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.grpc.api.status.StatusCode;\nimport com.google.protobuf.ByteString;\n\npublic class ClarifaiExample {\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, app ID, workflow ID, and\n    // audio file location. Change these strings to run your own example.\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    static final String USER_ID = "YOUR_USER_ID_HERE";\n    //Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    static final String APP_ID = "YOUR_APP_ID_HERE";\n    // Change these to make your own predictions\n    static final String WORKFLOW_ID = "my-custom-workflow";\n    static final String AUDIO_FILE_LOCATION = "YOUR_AUDIO_FILE_LOCATION_HERE";\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    public static void main(String[] args) throws IOException {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n            .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        PostWorkflowResultsResponse postWorkflowResultsResponse = stub.postWorkflowResults(\n            PostWorkflowResultsRequest.newBuilder()\n            .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n            .setWorkflowId(WORKFLOW_ID)\n            .addInputs(\n                Input.newBuilder().setData(\n                    Data.newBuilder().setAudio(\n                        Audio.newBuilder().setBase64(ByteString.copyFrom(Files.readAllBytes(\n                                new File(AUDIO_FILE_LOCATION).toPath()\n                                )))\n                    )\n                )\n            )\n            .build()\n        );\n\n        if (postWorkflowResultsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("Post workflow results failed, status: " + postWorkflowResultsResponse.getStatus());\n        }\n\n        // We\'ll get one WorkflowResult for each input we used above. Because of one input, we have here\n        // one WorkflowResult\n        WorkflowResult results = postWorkflowResultsResponse.getResults(0);\n\n        // Each model we have in the workflow will produce its output\n        for (Output output: results.getOutputsList()) {\n            Model model = output.getModel();\n            System.out.println("Output for the model: `" + model.getId() + "`");   \n            for (Concept concept: output.getData().getConceptsList()) {       \n            \tSystem.out.printf("\\t%s %.2f%n",concept.getName(), concept.getValue());                 \n            }\n            System.out.println(output.getData().getText().getRaw());\n        }\n\n    }\n\n}',b='<?php\n\nrequire __DIR__ . "/vendor/autoload.php";\n\n//////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, app ID, and the details of the new\n// custom workflow we want to create. Change these strings to run your own example.\n/////////////////////////////////////////////////////////////////////////////////////////\n\n$USER_ID = "YOUR_USER_ID_HERE";\n// Your PAT (Personal Access Token) can be found in the portal under Authentification\n$PAT = "YOUR_PAT_HERE";\n$APP_ID = "YOUR_APP_ID_HERE";\n// Change these to create your own custom workflow\n$WORKFLOW_ID = "my-custom-workflow";\n$NODE_ID_1 = "audio-to-text";\n$MODEL_ID_1 = "asr-wav2vec2-base-960h-english";\n$MODEL_VERSION_ID_1 = "f4deae70a473492a8e2f9b7bb1dbee85";\n\n$NODE_ID_2 = "sentiment-analysis";\n$MODEL_ID_2 = "sentiment-analysis-distilbert-english";\n$MODEL_VERSION_ID_2 = "c0b09e606db94d9bae7eb40c626192fc";\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\PostWorkflowsRequest;\nuse Clarifai\\Api\\Workflow;\nuse Clarifai\\Api\\WorkflowNode;\nuse Clarifai\\Api\\NodeInput;\nuse Clarifai\\Api\\Model;\nuse Clarifai\\Api\\ModelVersion;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ["Authorization" => ["Key " . $PAT]]; \n\n$userDataObject = new UserAppIDSet([\n    "user_id" => $USER_ID,\n    "app_id" => $APP_ID\n]);\n\n// Let\'s make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client->PostWorkflows(\n        // The request object carries the request along with the request status and other metadata related to the request itself\n        new PostWorkflowsRequest([\n            "user_app_id" => $userDataObject,\n            "workflows" => [\n                new Workflow([\n                    "id"=> $WORKFLOW_ID,\n                    "nodes" => [\n                        new WorkflowNode([\n                            "id" => $NODE_ID_1,\n                            "model" => new Model([\n                                "id" => $MODEL_ID_1,\n                                "model_version" => new ModelVersion([\n                                    "id" => $MODEL_VERSION_ID_1\n                                ])\n                            ])\n\n                        ]),\n                        new WorkflowNode([\n                            "id" => $NODE_ID_2,\n                            "model"=> new Model([\n                                "id" => $MODEL_ID_2,\n                                "model_version" => new ModelVersion([\n                                    "id" => $MODEL_VERSION_ID_2\n                                ])\n                            ]),\n                            "node_inputs" => [\n                                new NodeInput([\n                                    "node_id"=> $NODE_ID_1\n                                ])\n                            ]\n                        ])                       \n                    ]\n                ])\n            ]\n        ]),\n        $metadata\n    )->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception("Error: {$status->details}");\n}\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure \n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    print $response->getStatus()->getDetails();\n    throw new Exception("Failure response: " . $response->getStatus()->getDescription());\n}\n\n?>\n',k='<?php\n\nrequire __DIR__ . "/vendor/autoload.php";\n\n///////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, app ID, workflow ID, and\n// audio URL. Change these strings to run your own example.\n///////////////////////////////////////////////////////////////////////////////////\n\n$USER_ID = "YOUR_USER_ID_HERE";\n// Your PAT (Personal Access Token) can be found in the portal under Authentification\n$PAT = "YOUR_PAT_HERE";\n$APP_ID = "YOUR_APP_ID_HERE";\n// Change these to make your own predictions\n$WORKFLOW_ID = "my-custom-workflow";\n$AUDIO_URL = "https://samples.clarifai.com/negative_sentence_1.wav";\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\Api\\Audio;\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\PostWorkflowResultsRequest;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ["Authorization" => ["Key " . $PAT]]; \n\n$userDataObject = new UserAppIDSet([\n    "user_id" => $USER_ID,\n    "app_id" => $APP_ID\n]);\n\n// Let\'s make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client->PostWorkflowResults(\n        // The request object carries the request along with the request status and other metadata related to the request itself\n        new PostWorkflowResultsRequest([\n            "user_app_id" => $userDataObject,\n            "workflow_id" => $WORKFLOW_ID,\n            "inputs" => [\n                new Input([\n                    "data" => new Data([\n                        "audio" => new Audio([\n                            "url" => $AUDIO_URL\n                        ])\n                    ])\n                ])\n            ]          \n        ]),\n        $metadata\n    )->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception("Error: {$status->details}");\n}\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure \n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    print $response->getStatus()->getDetails();\n    throw new Exception("Failure response: " . $response->getStatus()->getDescription());\n}\n\n// We\'ll get one WorkflowResult for each input we used above. Because of one input, we have here one WorkflowResult\n$results = $response->getResults()[0];\n\n// Each model we have in the workflow will produce its output\nforeach ($results->getOutputs() as $output){\n    $model = $output->getModel();\n    print "Output for the model: `" . $model->getId() . "`" . "<br>";\n    foreach ($output->getData()->getConcepts() as $concept){\n        print $concept->getName() . " " . number_format($concept->getValue(),2) . "<br>";\n    }\n    print $output->getData()->getText()->getRaw() . "<br>";\n}\n\n?>\n',g='<?php\n\nrequire __DIR__ . "/vendor/autoload.php";\n\n//////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, app ID, workflow ID, and bytes of\n// the audio we want as an input. Change these strings to run your own example.\n/////////////////////////////////////////////////////////////////////////////////////////\n\n$USER_ID = "YOUR_USER_ID_HERE";\n// Your PAT (Personal Access Token) can be found in the portal under Authentification\n$PAT = "YOUR_PAT_HERE";\n$APP_ID = "YOUR_APP_ID_HERE";\n// Change these to make your own predictions\n$WORKFLOW_ID = "my-custom-workflow";\n$AUDIO_BYTES_STRING = "YOUR_AUDIO_FILE_LOCATION_HERE";\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\Api\\Audio;\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\PostWorkflowResultsRequest;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ["Authorization" => ["Key " . $PAT]]; \n\n$userDataObject = new UserAppIDSet([\n    "user_id" => $USER_ID,\n    "app_id" => $APP_ID\n]);\n\n$audioData = file_get_contents($AUDIO_BYTES_STRING); // Get the audio bytes data from the location\n\n// Let\'s make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client->PostWorkflowResults(\n        // The request object carries the request along with the request status and other metadata related to the request itself\n        new PostWorkflowResultsRequest([\n            "user_app_id" => $userDataObject,\n            "workflow_id" => $WORKFLOW_ID,\n            "inputs" => [\n                new Input([\n                    "data" => new Data([\n                        "audio" => new Audio([\n                            "base64" => $audioData\n                        ])\n                    ])\n                ])\n            ]          \n        ]),\n        $metadata\n    )->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception("Error: {$status->details}");\n}\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure \n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    print $response->getStatus()->getDetails();\n    throw new Exception("Failure response: " . $response->getStatus()->getDescription());\n}\n\n// We\'ll get one WorkflowResult for each input we used above. Because of one input, we have here one WorkflowResult\n$results = $response->getResults()[0];\n\n// Each model we have in the workflow will produce its output\nforeach ($results->getOutputs() as $output){\n    $model = $output->getModel();\n    print "Output for the model: `" . $model->getId() . "`" . "<br>";\n    foreach ($output->getData()->getConcepts() as $concept){\n        print $concept->getName() . " " . number_format($concept->getValue(),2) . "<br>";\n    }\n    print $output->getData()->getText()->getRaw() . "<br>";\n}\n\n?>\n',v='curl -X POST "https://api.clarifai.com/v2/users/YOUR_USER_ID_HERE/apps/YOUR_APP_ID_HERE/workflows" \\\n    -H "Content-Type: application/json" \\\n    -H "Authorization: Key YOUR_PAT_HERE" \\\n    --data-raw \'{\n      "workflows": [{\n        "id": "my-custom-workflow",\n        "nodes": [\n          {\n            "id": "audio-to-text",\n            "model": {\n              "id": "asr-wav2vec2-base-960h-english",\n              "model_version": {\n                "id": "f4deae70a473492a8e2f9b7bb1dbee85"\n              }\n            }\n          },\n          {\n            "id": "sentiment-analysis",\n            "model": {\n              "id": "sentiment-analysis-distilbert-english",\n              "model_version": {\n                "id": "c0b09e606db94d9bae7eb40c626192fc"\n              }\n            },\n              "node_inputs": [\n                {\n                  "node_id": "audio-to-text"\n                }\n              ]\n          }\n        ]\n      }]\n    }\'',O='curl -X POST "https://api.clarifai.com/v2/users/YOUR_USER_ID_HERE/apps/YOUR_APP_ID_HERE/workflows/YOUR_WORKFLOW_ID_HERE/results" \\\n  -H "authorization: Key YOUR_PAT_HERE" \\\n  -H "content-type: application/json" \\\n  -d \'{\n    "inputs": [\n        {\n          "data": {\n            "audio": {\n              "url": "https://samples.clarifai.com/negative_sentence_1.wav"\n          }\n        }\n      }\n    ]\n}\'',D='curl -X POST "https://api.clarifai.com/v2/users/YOUR_USER_ID_HERE/apps/YOUR_APP_ID_HERE/workflows/YOUR_WORKFLOW_ID_HERE/results" \\\n  -H "authorization: Key YOUR_PAT_HERE" \\\n  -H "content-type: application/json" \\\n  -d \'{\n    "inputs": [\n        {\n          "data": {\n            "audio": {\n              "base64": "YOUR_BYTES_STRING_HERE"\n          }\n        }\n      }\n    ]\n}\'',R="Output for the model: `asr-wav2vec2-base-960h-english`\nI AM NOT FLYING TO ENGLAND\nOutput for the model: `sentiment-analysis-distilbert-english`\n\tNEGATIVE 1.00\n\tPOSITIVE 0.00",T='status {\n  code: SUCCESS\n  description: "Ok"\n}\ninput {\n  id: "9c39fec1ac9d4d58b7c1353c20cf8538"\n  data {\n    audio {\n      url: "https://samples.clarifai.com/negative_sentence_1.wav"\n    }\n  }\n}\noutputs {\n  id: "e04117c61f0b46fdb537b6663f235300"\n  status {\n    code: SUCCESS\n    description: "Ok"\n  }\n  created_at {\n    seconds: 1664555464\n    nanos: 495787635\n  }\n  model {\n    id: "asr-wav2vec2-base-960h-english"\n    name: "facebook/wav2vec2-base-960h"\n    created_at {\n      seconds: 1634677145\n      nanos: 661061000\n    }\n    app_id: "asr"\n    output_info {\n      output_config {\n      }\n      message: "Show output_info with: GET /models/{model_id}/output_info"\n      fields_map {\n        fields {\n          key: "text"\n          value {\n            string_value: "text"\n          }\n        }\n      }\n    }\n    model_version {\n      id: "f4deae70a473492a8e2f9b7bb1dbee85"\n      created_at {\n        seconds: 1634677145\n        nanos: 668993000\n      }\n      status {\n        code: MODEL_TRAINED\n        description: "Model is trained and ready"\n      }\n      visibility {\n        gettable: PUBLIC\n      }\n      app_id: "asr"\n      user_id: "facebook"\n      metadata {\n      }\n      license: "Apache-2.0"\n    }\n    user_id: "facebook"\n    input_info {\n      fields_map {\n        fields {\n          key: "audio"\n          value {\n            string_value: "audio"\n          }\n        }\n      }\n    }\n    train_info {\n    }\n    model_type_id: "audio-to-text"\n    visibility {\n      gettable: PUBLIC\n    }\n    modified_at {\n      seconds: 1658884251\n      nanos: 743015000\n    }\n    import_info {\n    }\n  }\n  data {\n    text {\n      raw: "I AM NOT FLYING TO ENGLAND"\n      text_info {\n        encoding: "UnknownTextEnc"\n      }\n    }\n  }\n}\noutputs {\n  id: "2f37022e94e74ac7a7a5f5e927bbcf66"\n  status {\n    code: SUCCESS\n    description: "Ok"\n  }\n  created_at {\n    seconds: 1664555464\n    nanos: 504551013\n  }\n  model {\n    id: "sentiment-analysis-distilbert-english"\n    name: "sentiment-analysis-distilbert-english"\n    created_at {\n      seconds: 1656524917\n      nanos: 269700000\n    }\n    app_id: "text-classification"\n    output_info {\n      output_config {\n      }\n      message: "Show output_info with: GET /models/{model_id}/output_info"\n      fields_map {\n        fields {\n          key: "concepts"\n          value {\n            string_value: "softmax"\n          }\n        }\n      }\n      params {\n        fields {\n          key: "max_concepts"\n          value {\n            number_value: 20.0\n          }\n        }\n        fields {\n          key: "min_value"\n          value {\n            number_value: 0.0\n          }\n        }\n        fields {\n          key: "select_concepts"\n          value {\n            list_value {\n            }\n          }\n        }\n      }\n    }\n    model_version {\n      id: "c0b09e606db94d9bae7eb40c626192fc"\n      created_at {\n        seconds: 1656524917\n        nanos: 276685000\n      }\n      status {\n        code: MODEL_TRAINED\n        description: "Model is trained and ready"\n      }\n      visibility {\n        gettable: PUBLIC\n      }\n      app_id: "text-classification"\n      user_id: "erfan"\n      metadata {\n        fields {\n          key: "Model version logs zipped"\n          value {\n            string_value: "https://s3.amazonaws.com/clarifai-temp/prod/c0b09e606db94d9bae7eb40c626192fc.zip"\n          }\n        }\n      }\n    }\n    user_id: "erfan"\n    input_info {\n      fields_map {\n        fields {\n          key: "text"\n          value {\n            string_value: "text"\n          }\n        }\n      }\n    }\n    train_info {\n      params {\n        fields {\n          key: "model_config"\n          value {\n            struct_value {\n              fields {\n                key: "_name_or_path"\n                value {\n                  string_value: "distilbert-base-uncased-finetuned-sst-2-english"\n                }\n              }\n              fields {\n                key: "activation"\n                value {\n                  string_value: "gelu"\n                }\n              }\n              fields {\n                key: "architectures"\n                value {\n                  list_value {\n                    values {\n                      string_value: "DistilBertForSequenceClassification"\n                    }\n                  }\n                }\n              }\n              fields {\n                key: "attention_dropout"\n                value {\n                  number_value: 0.1\n                }\n              }\n              fields {\n                key: "dim"\n                value {\n                  number_value: 768.0\n                }\n              }\n              fields {\n                key: "dropout"\n                value {\n                  number_value: 0.1\n                }\n              }\n              fields {\n                key: "finetuning_task"\n                value {\n                  string_value: "sst-2"\n                }\n              }\n              fields {\n                key: "hidden_dim"\n                value {\n                  number_value: 3072.0\n                }\n              }\n              fields {\n                key: "id2label"\n                value {\n                  struct_value {\n                    fields {\n                      key: "0"\n                      value {\n                        string_value: "NEGATIVE"\n                      }\n                    }\n                    fields {\n                      key: "1"\n                      value {\n                        string_value: "POSITIVE"\n                      }\n                    }\n                  }\n                }\n              }\n              fields {\n                key: "initializer_range"\n                value {\n                  number_value: 0.02\n                }\n              }\n              fields {\n                key: "label2id"\n                value {\n                  struct_value {\n                    fields {\n                      key: "NEGATIVE"\n                      value {\n                        number_value: 0.0\n                      }\n                    }\n                    fields {\n                      key: "POSITIVE"\n                      value {\n                        number_value: 1.0\n                      }\n                    }\n                  }\n                }\n              }\n              fields {\n                key: "max_position_embeddings"\n                value {\n                  number_value: 512.0\n                }\n              }\n              fields {\n                key: "model_type"\n                value {\n                  string_value: "distilbert"\n                }\n              }\n              fields {\n                key: "n_heads"\n                value {\n                  number_value: 12.0\n                }\n              }\n              fields {\n                key: "n_layers"\n                value {\n                  number_value: 6.0\n                }\n              }\n              fields {\n                key: "output_past"\n                value {\n                  bool_value: true\n                }\n              }\n              fields {\n                key: "pad_token_id"\n                value {\n                  number_value: 0.0\n                }\n              }\n              fields {\n                key: "qa_dropout"\n                value {\n                  number_value: 0.1\n                }\n              }\n              fields {\n                key: "seq_classif_dropout"\n                value {\n                  number_value: 0.2\n                }\n              }\n              fields {\n                key: "sinusoidal_pos_embds"\n                value {\n                  bool_value: false\n                }\n              }\n              fields {\n                key: "tie_weights_"\n                value {\n                  bool_value: true\n                }\n              }\n              fields {\n                key: "torch_dtype"\n                value {\n                  string_value: "float32"\n                }\n              }\n              fields {\n                key: "transformers_version"\n                value {\n                  string_value: "4.20.0"\n                }\n              }\n              fields {\n                key: "vocab_size"\n                value {\n                  number_value: 30522.0\n                }\n              }\n            }\n          }\n        }\n        fields {\n          key: "tokenizer_config"\n          value {\n            struct_value {\n              fields {\n                key: "cls_token"\n                value {\n                  string_value: "[CLS]"\n                }\n              }\n              fields {\n                key: "do_basic_tokenize"\n                value {\n                  bool_value: true\n                }\n              }\n              fields {\n                key: "do_lower_case"\n                value {\n                  bool_value: true\n                }\n              }\n              fields {\n                key: "mask_token"\n                value {\n                  string_value: "[MASK]"\n                }\n              }\n              fields {\n                key: "model_max_length"\n                value {\n                  number_value: 512.0\n                }\n              }\n              fields {\n                key: "name_or_path"\n                value {\n                  string_value: "distilbert-base-uncased-finetuned-sst-2-english"\n                }\n              }\n              fields {\n                key: "never_split"\n                value {\n                  null_value: NULL_VALUE\n                }\n              }\n              fields {\n                key: "pad_token"\n                value {\n                  string_value: "[PAD]"\n                }\n              }\n              fields {\n                key: "sep_token"\n                value {\n                  string_value: "[SEP]"\n                }\n              }\n              fields {\n                key: "special_tokens_map_file"\n                value {\n                  null_value: NULL_VALUE\n                }\n              }\n              fields {\n                key: "strip_accents"\n                value {\n                  null_value: NULL_VALUE\n                }\n              }\n              fields {\n                key: "tokenize_chinese_chars"\n                value {\n                  bool_value: true\n                }\n              }\n              fields {\n                key: "tokenizer_class"\n                value {\n                  string_value: "DistilBertTokenizer"\n                }\n              }\n              fields {\n                key: "unk_token"\n                value {\n                  string_value: "[UNK]"\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    model_type_id: "text-classifier"\n    visibility {\n      gettable: PUBLIC\n    }\n    modified_at {\n      seconds: 1656525047\n      nanos: 842099000\n    }\n    import_info {\n      params {\n        fields {\n          key: "model_name"\n          value {\n            string_value: "distilbert-base-uncased-finetuned-sst-2-english"\n          }\n        }\n        fields {\n          key: "pipeline_name"\n          value {\n            string_value: "text-classification"\n          }\n        }\n        fields {\n          key: "tokenizer_name"\n          value {\n            string_value: "distilbert-base-uncased-finetuned-sst-2-english"\n          }\n        }\n        fields {\n          key: "toolkit"\n          value {\n            string_value: "HuggingFace"\n          }\n        }\n      }\n    }\n  }\n  data {\n    concepts {\n      id: "NEGATIVE"\n      name: "NEGATIVE"\n      value: 0.9991872906684875\n      app_id: "c4660162651d490285bcbfc5aff50bf0"\n    }\n    concepts {\n      id: "POSITIVE"\n      name: "POSITIVE"\n      value: 0.0008126832544803619\n      app_id: "c4660162651d490285bcbfc5aff50bf0"\n    }\n  }\n}',y="Output for the model: `asr-wav2vec2-base-960h-english`\nI AM NOT FLYING TO ENGLAND\nOutput for the model: `sentiment-analysis-distilbert-english`\n\tNEGATIVE 1.00\n\tPOSITIVE 0.00",A='status {\n    code: SUCCESS\n    description: "Ok"\n  }\n  input {\n    id: "c4e45044f96a4fc9b1a4a1ddbd24ee26"\n    data {\n      audio {\n        url: "https://samples.clarifai.com/placeholder.gif"\n        base64: "true"\n      }\n    }\n  }\n  outputs {\n    id: "557b489f3031436bbe5501f92d03461f"\n    status {\n      code: SUCCESS\n      description: "Ok"\n    }\n    created_at {\n      seconds: 1664805906\n      nanos: 332458633\n    }\n    model {\n      id: "asr-wav2vec2-base-960h-english"\n      name: "facebook/wav2vec2-base-960h"\n      created_at {\n        seconds: 1634677145\n        nanos: 661061000\n      }\n      app_id: "asr"\n      output_info {\n        output_config {\n        }\n        message: "Show output_info with: GET /models/{model_id}/output_info"\n        fields_map {\n          fields {\n            key: "text"\n            value {\n              string_value: "text"\n            }\n          }\n        }\n      }\n      model_version {\n        id: "f4deae70a473492a8e2f9b7bb1dbee85"\n        created_at {\n          seconds: 1634677145\n          nanos: 668993000\n        }\n        status {\n          code: MODEL_TRAINED\n          description: "Model is trained and ready"\n        }\n        visibility {\n          gettable: PUBLIC\n        }\n        app_id: "asr"\n        user_id: "facebook"\n        metadata {\n        }\n        license: "Apache-2.0"\n      }\n      user_id: "facebook"\n      input_info {\n        fields_map {\n          fields {\n            key: "audio"\n            value {\n              string_value: "audio"\n            }\n          }\n        }\n      }\n      train_info {\n      }\n      model_type_id: "audio-to-text"\n      visibility {\n        gettable: PUBLIC\n      }\n      modified_at {\n        seconds: 1658884251\n        nanos: 743015000\n      }\n      import_info {\n      }\n    }\n    data {\n      text {\n        raw: "I AM NOT FLYING TO ENGLAND"\n        text_info {\n          encoding: "UnknownTextEnc"\n        }\n      }\n    }\n  }\n  outputs {\n    id: "e7d1bf12c487435bb005504dd402642e"\n    status {\n      code: SUCCESS\n      description: "Ok"\n    }\n    created_at {\n      seconds: 1664805906\n      nanos: 336852253\n    }\n    model {\n      id: "sentiment-analysis-distilbert-english"\n      name: "sentiment-analysis-distilbert-english"\n      created_at {\n        seconds: 1656524917\n        nanos: 269700000\n      }\n      app_id: "text-classification"\n      output_info {\n        output_config {\n        }\n        message: "Show output_info with: GET /models/{model_id}/output_info"\n        fields_map {\n          fields {\n            key: "concepts"\n            value {\n              string_value: "softmax"\n            }\n          }\n        }\n        params {\n          fields {\n            key: "max_concepts"\n            value {\n              number_value: 20.0\n            }\n          }\n          fields {\n            key: "min_value"\n            value {\n              number_value: 0.0\n            }\n          }\n          fields {\n            key: "select_concepts"\n            value {\n              list_value {\n              }\n            }\n          }\n        }\n      }\n      model_version {\n        id: "c0b09e606db94d9bae7eb40c626192fc"\n        created_at {\n          seconds: 1656524917\n          nanos: 276685000\n        }\n        status {\n          code: MODEL_TRAINED\n          description: "Model is trained and ready"\n        }\n        visibility {\n          gettable: PUBLIC\n        }\n        app_id: "text-classification"\n        user_id: "erfan"\n        metadata {\n          fields {\n            key: "Model version logs zipped"\n            value {\n              string_value: "https://s3.amazonaws.com/clarifai-temp/prod/c0b09e606db94d9bae7eb40c626192fc.zip"\n            }\n          }\n        }\n      }\n      user_id: "erfan"\n      input_info {\n        fields_map {\n          fields {\n            key: "text"\n            value {\n              string_value: "text"\n            }\n          }\n        }\n      }\n      train_info {\n        params {\n          fields {\n            key: "model_config"\n            value {\n              struct_value {\n                fields {\n                  key: "_name_or_path"\n                  value {\n                    string_value: "distilbert-base-uncased-finetuned-sst-2-english"\n                  }\n                }\n                fields {\n                  key: "activation"\n                  value {\n                    string_value: "gelu"\n                  }\n                }\n                fields {\n                  key: "architectures"\n                  value {\n                    list_value {\n                      values {\n                        string_value: "DistilBertForSequenceClassification"\n                      }\n                    }\n                  }\n                }\n                fields {\n                  key: "attention_dropout"\n                  value {\n                    number_value: 0.1\n                  }\n                }\n                fields {\n                  key: "dim"\n                  value {\n                    number_value: 768.0\n                  }\n                }\n                fields {\n                  key: "dropout"\n                  value {\n                    number_value: 0.1\n                  }\n                }\n                fields {\n                  key: "finetuning_task"\n                  value {\n                    string_value: "sst-2"\n                  }\n                }\n                fields {\n                  key: "hidden_dim"\n                  value {\n                    number_value: 3072.0\n                  }\n                }\n                fields {\n                  key: "id2label"\n                  value {\n                    struct_value {\n                      fields {\n                        key: "0"\n                        value {\n                          string_value: "NEGATIVE"\n                        }\n                      }\n                      fields {\n                        key: "1"\n                        value {\n                          string_value: "POSITIVE"\n                        }\n                      }\n                    }\n                  }\n                }\n                fields {\n                  key: "initializer_range"\n                  value {\n                    number_value: 0.02\n                  }\n                }\n                fields {\n                  key: "label2id"\n                  value {\n                    struct_value {\n                      fields {\n                        key: "NEGATIVE"\n                        value {\n                          number_value: 0.0\n                        }\n                      }\n                      fields {\n                        key: "POSITIVE"\n                        value {\n                          number_value: 1.0\n                        }\n                      }\n                    }\n                  }\n                }\n                fields {\n                  key: "max_position_embeddings"\n                  value {\n                    number_value: 512.0\n                  }\n                }\n                fields {\n                  key: "model_type"\n                  value {\n                    string_value: "distilbert"\n                  }\n                }\n                fields {\n                  key: "n_heads"\n                  value {\n                    number_value: 12.0\n                  }\n                }\n                fields {\n                  key: "n_layers"\n                  value {\n                    number_value: 6.0\n                  }\n                }\n                fields {\n                  key: "output_past"\n                  value {\n                    bool_value: true\n                  }\n                }\n                fields {\n                  key: "pad_token_id"\n                  value {\n                    number_value: 0.0\n                  }\n                }\n                fields {\n                  key: "qa_dropout"\n                  value {\n                    number_value: 0.1\n                  }\n                }\n                fields {\n                  key: "seq_classif_dropout"\n                  value {\n                    number_value: 0.2\n                  }\n                }\n                fields {\n                  key: "sinusoidal_pos_embds"\n                  value {\n                    bool_value: false\n                  }\n                }\n                fields {\n                  key: "tie_weights_"\n                  value {\n                    bool_value: true\n                  }\n                }\n                fields {\n                  key: "torch_dtype"\n                  value {\n                    string_value: "float32"\n                  }\n                }\n                fields {\n                  key: "transformers_version"\n                  value {\n                    string_value: "4.20.0"\n                  }\n                }\n                fields {\n                  key: "vocab_size"\n                  value {\n                    number_value: 30522.0\n                  }\n                }\n              }\n            }\n          }\n          fields {\n            key: "tokenizer_config"\n            value {\n              struct_value {\n                fields {\n                  key: "cls_token"\n                  value {\n                    string_value: "[CLS]"\n                  }\n                }\n                fields {\n                  key: "do_basic_tokenize"\n                  value {\n                    bool_value: true\n                  }\n                }\n                fields {\n                  key: "do_lower_case"\n                  value {\n                    bool_value: true\n                  }\n                }\n                fields {\n                  key: "mask_token"\n                  value {\n                    string_value: "[MASK]"\n                  }\n                }\n                fields {\n                  key: "model_max_length"\n                  value {\n                    number_value: 512.0\n                  }\n                }\n                fields {\n                  key: "name_or_path"\n                  value {\n                    string_value: "distilbert-base-uncased-finetuned-sst-2-english"\n                  }\n                }\n                fields {\n                  key: "never_split"\n                  value {\n                    null_value: NULL_VALUE\n                  }\n                }\n                fields {\n                  key: "pad_token"\n                  value {\n                    string_value: "[PAD]"\n                  }\n                }\n                fields {\n                  key: "sep_token"\n                  value {\n                    string_value: "[SEP]"\n                  }\n                }\n                fields {\n                  key: "special_tokens_map_file"\n                  value {\n                    null_value: NULL_VALUE\n                  }\n                }\n                fields {\n                  key: "strip_accents"\n                  value {\n                    null_value: NULL_VALUE\n                  }\n                }\n                fields {\n                  key: "tokenize_chinese_chars"\n                  value {\n                    bool_value: true\n                  }\n                }\n                fields {\n                  key: "tokenizer_class"\n                  value {\n                    string_value: "DistilBertTokenizer"\n                  }\n                }\n                fields {\n                  key: "unk_token"\n                  value {\n                    string_value: "[UNK]"\n                  }\n                }\n              }\n            }\n          }\n        }\n      }\n      model_type_id: "text-classifier"\n      visibility {\n        gettable: PUBLIC\n      }\n      modified_at {\n        seconds: 1656525047\n        nanos: 842099000\n      }\n      import_info {\n        params {\n          fields {\n            key: "model_name"\n            value {\n              string_value: "distilbert-base-uncased-finetuned-sst-2-english"\n            }\n          }\n          fields {\n            key: "pipeline_name"\n            value {\n              string_value: "text-classification"\n            }\n          }\n          fields {\n            key: "tokenizer_name"\n            value {\n              string_value: "distilbert-base-uncased-finetuned-sst-2-english"\n            }\n          }\n          fields {\n            key: "toolkit"\n            value {\n              string_value: "HuggingFace"\n            }\n          }\n        }\n      }\n    }\n    data {\n      concepts {\n        id: "NEGATIVE"\n        name: "NEGATIVE"\n        value: 0.9991872906684875\n        app_id: "c4660162651d490285bcbfc5aff50bf0"\n      }\n      concepts {\n        id: "POSITIVE"\n        name: "POSITIVE"\n        value: 0.0008126832544803619\n        app_id: "c4660162651d490285bcbfc5aff50bf0"\n      }\n    }\n  }',S={description:"Make predictions on passages of text",sidebar_position:5},N="Audio",P={unversionedId:"api-guide/predict/audio",id:"api-guide/predict/audio",title:"Audio",description:"Make predictions on passages of text",source:"@site/docs/api-guide/predict/audio.md",sourceDirName:"api-guide/predict",slug:"/api-guide/predict/audio",permalink:"/api-guide/predict/audio",draft:!1,editUrl:"https://github.com/Clarifai/docs/blob/main/docs/api-guide/predict/audio.md",tags:[],version:"current",lastUpdatedAt:1698924850,formattedLastUpdatedAt:"Nov 2, 2023",sidebarPosition:5,frontMatter:{description:"Make predictions on passages of text",sidebar_position:5},sidebar:"tutorialSidebar",previous:{title:"Large Language Models (LLMs)",permalink:"/api-guide/predict/llms"},next:{title:"Prediction Parameters",permalink:"/api-guide/predict/prediction-parameters"}},C={},U=[{value:"Create a Workflow",id:"create-a-workflow",level:2},{value:"Predict via URL",id:"predict-via-url",level:2},{value:"Predict via Bytes",id:"predict-via-bytes",level:2}],L={toc:U},x="wrapper";function W(n){let{components:e,...t}=n;return(0,s.kt)(x,(0,a.Z)({},L,t,{components:e,mdxType:"MDXLayout"}),(0,s.kt)("h1",{id:"audio"},"Audio"),(0,s.kt)("p",null,(0,s.kt)("strong",{parentName:"p"},"Make predictions on audio inputs")),(0,s.kt)("hr",null),(0,s.kt)("p",null,"To make predictions on audio inputs, you need to use a ",(0,s.kt)("a",{parentName:"p",href:"https://docs.clarifai.com/api-guide/workflows/create-get-update-delete"},"workflow"),". Workflows is a useful Clarifai's feature that allows you to combine multiple models and carry out different operations. You can use Clarifai's built-in models or your own custom models."),(0,s.kt)("p",null,"In this case, we'll create a workflow that first does audio-to-text transcription and then makes predictions from the transcribed texts. "),(0,s.kt)("p",null,"The file size of each audio input should be less than 20MB."),(0,s.kt)("admonition",{type:"info"},(0,s.kt)("p",{parentName:"admonition"},"The initialization code used in the following examples is outlined in detail on the ",(0,s.kt)("a",{parentName:"p",href:"https://docs.clarifai.com/api-guide/api-overview/api-clients/#client-installation-instructions"},"client installation page."))),(0,s.kt)("h2",{id:"create-a-workflow"},"Create a Workflow"),(0,s.kt)("p",null,"In this example, we'll create a simple workflow that first converts an audio file to text and then analyses the sentiment of the transcribed text. "),(0,s.kt)("p",null,"We'll connect the following two models to achieve our objective:"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},"The ",(0,s.kt)("a",{parentName:"li",href:"https://clarifai.com/facebook/asr/models/asr-wav2vec2-base-960h-english"},"asr-wav2vec2-base-960h-english"),", which converts English audio to English text. "),(0,s.kt)("li",{parentName:"ul"},"The ",(0,s.kt)("a",{parentName:"li",href:"https://clarifai.com/erfan/text-classification/models/sentiment-analysis-distilbert-english"},"sentiment-analysis-distilbert-english"),", which predicts whether the sentiment of an English text is positive or negative. ")),(0,s.kt)("p",null,"We'll specify the IDs of the models and their versions\u2014since a model can have several versions."),(0,s.kt)(o.Z,{mdxType:"Tabs"},(0,s.kt)(i.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,s.kt)(l.Z,{className:"language-python",mdxType:"CodeBlock"},r)),(0,s.kt)(i.Z,{value:"js_rest",label:"JavaScript (REST)",mdxType:"TabItem"},(0,s.kt)(l.Z,{className:"language-javascript",mdxType:"CodeBlock"},c)),(0,s.kt)(i.Z,{value:"nodejs",label:"NodeJS",mdxType:"TabItem"},(0,s.kt)(l.Z,{className:"language-javascript",mdxType:"CodeBlock"},f)),(0,s.kt)(i.Z,{value:"java",label:"Java",mdxType:"TabItem"},(0,s.kt)(l.Z,{className:"language-java",mdxType:"CodeBlock"},w)),(0,s.kt)(i.Z,{value:"php",label:"PHP",mdxType:"TabItem"},(0,s.kt)(l.Z,{className:"language-php",mdxType:"CodeBlock"},b)),(0,s.kt)(i.Z,{value:"curl",label:"cURL",mdxType:"TabItem"},(0,s.kt)(l.Z,{className:"language-bash",mdxType:"CodeBlock"},v))),(0,s.kt)("h2",{id:"predict-via-url"},"Predict via URL"),(0,s.kt)("p",null,"After creating the workflow, let's now use it to convert this ",(0,s.kt)("a",{parentName:"p",href:"https://samples.clarifai.com/negative_sentence_1.wav"},"audio file")," to text and then get the sentiment of the generated text. "),(0,s.kt)("p",null,"The response will contain the predictions each model in the workflow returns for the audio input."),(0,s.kt)(o.Z,{mdxType:"Tabs"},(0,s.kt)(i.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,s.kt)(l.Z,{className:"language-python",mdxType:"CodeBlock"},u)),(0,s.kt)(i.Z,{value:"js_rest",label:"JavaScript (REST)",mdxType:"TabItem"},(0,s.kt)(l.Z,{className:"language-javascript",mdxType:"CodeBlock"},p)),(0,s.kt)(i.Z,{value:"nodejs",label:"NodeJS",mdxType:"TabItem"},(0,s.kt)(l.Z,{className:"language-javascript",mdxType:"CodeBlock"},m)),(0,s.kt)(i.Z,{value:"java",label:"Java",mdxType:"TabItem"},(0,s.kt)(l.Z,{className:"language-java",mdxType:"CodeBlock"},I)),(0,s.kt)(i.Z,{value:"php",label:"PHP",mdxType:"TabItem"},(0,s.kt)(l.Z,{className:"language-php",mdxType:"CodeBlock"},k)),(0,s.kt)(i.Z,{value:"curl",label:"cURL",mdxType:"TabItem"},(0,s.kt)(l.Z,{className:"language-bash",mdxType:"CodeBlock"},O))),(0,s.kt)("details",null,(0,s.kt)("summary",null,"Text Output Example"),(0,s.kt)(l.Z,{className:"language-text",mdxType:"CodeBlock"},R)),(0,s.kt)("details",null,(0,s.kt)("summary",null,"JSON Output Example"),(0,s.kt)(l.Z,{className:"language-javascript",mdxType:"CodeBlock"},T)),(0,s.kt)("h2",{id:"predict-via-bytes"},"Predict via Bytes"),(0,s.kt)("p",null,"Below is an example of how you would send the bytes of an audio and receive predictions from the above-mentioned workflow."),(0,s.kt)(o.Z,{mdxType:"Tabs"},(0,s.kt)(i.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,s.kt)(l.Z,{className:"language-python",mdxType:"CodeBlock"},d)),(0,s.kt)(i.Z,{value:"js_rest",label:"JavaScript (REST)",mdxType:"TabItem"},(0,s.kt)(l.Z,{className:"language-javascript",mdxType:"CodeBlock"},_)),(0,s.kt)(i.Z,{value:"nodejs",label:"NodeJS",mdxType:"TabItem"},(0,s.kt)(l.Z,{className:"language-javascript",mdxType:"CodeBlock"},h)),(0,s.kt)(i.Z,{value:"java",label:"Java",mdxType:"TabItem"},(0,s.kt)(l.Z,{className:"language-java",mdxType:"CodeBlock"},E)),(0,s.kt)(i.Z,{value:"php",label:"PHP",mdxType:"TabItem"},(0,s.kt)(l.Z,{className:"language-php",mdxType:"CodeBlock"},g)),(0,s.kt)(i.Z,{value:"curl",label:"cURL",mdxType:"TabItem"},(0,s.kt)(l.Z,{className:"language-bash",mdxType:"CodeBlock"},D))),(0,s.kt)("details",null,(0,s.kt)("summary",null,"Text Output Example"),(0,s.kt)(l.Z,{className:"language-text",mdxType:"CodeBlock"},y)),(0,s.kt)("details",null,(0,s.kt)("summary",null,"JSON Output Example"),(0,s.kt)(l.Z,{className:"language-javascript",mdxType:"CodeBlock"},A)))}W.isMDXComponent=!0}}]);
"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[558],{32027:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>M,contentTitle:()=>j,default:()=>$,frontMatter:()=>N,metadata:()=>a,toc:()=>U});const a=JSON.parse('{"id":"api-guide/predict/llms","title":"Large Language Models (LLMs)","description":"Make predictions using LLMs","source":"@site/docs/api-guide/predict/llms.md","sourceDirName":"api-guide/predict","slug":"/api-guide/predict/llms","permalink":"/api-guide/predict/llms","draft":false,"unlisted":false,"editUrl":"https://github.com/Clarifai/docs/blob/main/docs/api-guide/predict/llms.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"description":"Make predictions using LLMs","sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Text","permalink":"/api-guide/predict/text"},"next":{"title":"Audio","permalink":"/api-guide/predict/audio"}}');var s=n(74848),o=n(28453),i=n(65537),r=n(79329),l=n(58069);const u="######################################################################################################\n# In this section, we set the user authentication, user and app ID, model details, and the URL of \n# the text we want as a prompt. Change these strings to run your own example.\n######################################################################################################\n\n# Your PAT (Personal Access Token) can be found in the Account's Security section\nPAT = 'YOUR_PAT_HERE'\n# Specify the correct user_id/app_id pairings\n# Since you're making inferences outside your app's scope\nUSER_ID = 'meta'\nAPP_ID = 'Llama-2'\n# Change these to whatever model and text URL you want to use\nMODEL_ID = 'llama2-7b-chat'\nMODEL_VERSION_ID = 'e52af5d6bc22445aa7a6761f327f7129'\nTEXT_FILE_URL = 'https://samples.clarifai.com/negative_sentence_12.txt'\n\n############################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n############################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (('authorization', 'Key ' + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\npost_model_outputs_response = stub.PostModelOutputs(\n    service_pb2.PostModelOutputsRequest(\n        user_app_id=userDataObject,  # The userDataObject is created in the overview and is required when using a PAT\n        model_id=MODEL_ID,\n        version_id=MODEL_VERSION_ID,  # This is optional. Defaults to the latest model version\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    text=resources_pb2.Text(\n                        url=TEXT_FILE_URL\n                    )\n                )\n            )\n        ]\n    ),\n    metadata=metadata\n)\nif post_model_outputs_response.status.code != status_code_pb2.SUCCESS:\n    print(post_model_outputs_response.status)\n    raise Exception(f\"Post model outputs failed, status: {post_model_outputs_response.status.description}\")\n\n# Since we have one input, one output will exist here\noutput = post_model_outputs_response.outputs[0]\n\nprint(\"Completion:\\n\")\nprint(output.data.text.raw)",c="\x3c!--index.html file--\x3e\n\n<script>        \n    ////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, and the URL\n    // of the text we want as a prompt. Change these strings to run your own example.\n    ///////////////////////////////////////////////////////////////////////////////////////////////////\n\n    // Your PAT (Personal Access Token) can be found in the Account's Security section\n    const PAT = 'YOUR_PAT_HERE';\n    // Specify the correct user_id/app_id pairings\n    // Since you're making inferences outside your app's scope\n    const USER_ID = 'meta';    \n    const APP_ID = 'Llama-2';\n    // Change these to whatever model and text URL you want to use  \n    const MODEL_ID = 'llama2-7b-chat';\n    const MODEL_VERSION_ID = 'e52af5d6bc22445aa7a6761f327f7129';    \n    const TEXT_FILE_URL = 'https://samples.clarifai.com/negative_sentence_12.txt';\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    const raw = JSON.stringify({\n        \"user_app_id\": {\n            \"user_id\": USER_ID,\n            \"app_id\": APP_ID\n        },\n        \"inputs\": [\n            {\n                \"data\": {\n                    \"text\": {\n                        \"url\": TEXT_FILE_URL\n                    }\n                }\n            }\n        ]\n    });\n\n    const requestOptions = {\n        method: 'POST',\n        headers: {\n            'Accept': 'application/json',\n            'Authorization': 'Key ' + PAT\n        },\n        body: raw\n    };\n\n    // NOTE: MODEL_VERSION_ID is optional, you can also call prediction with the MODEL_ID only\n    // https://api.clarifai.com/v2/models/{YOUR_MODEL_ID}/outputs\n    // this will default to the latest version_id\n\n    fetch(\"https://api.clarifai.com/v2/models/\" + MODEL_ID + \"/versions/\" + MODEL_VERSION_ID + \"/outputs\", requestOptions)\n        .then((response) => {\n            return response.json();\n        })\n        .then((data) => {\n            if(data.status.code != 10000) console.log(data.status);\n            else console.log(data['outputs'][0]['data']['text']['raw']);\n        }).catch(error => console.log('error', error));\n\n<\/script>",p="//index.js file\n\n////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, and the URL\n// of the text we want as a prompt. Change these strings to run your own example.\n/////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account's Security section\nconst PAT = 'YOUR_PAT_HERE';\n// Specify the correct user_id/app_id pairings\n// Since you're making inferences outside your app's scope\nconst USER_ID = 'meta';    \nconst APP_ID = 'Llama-2';\n// Change these to whatever model and text URL you want to use\nconst MODEL_ID = 'llama2-7b-chat';\nconst MODEL_VERSION_ID = 'e52af5d6bc22445aa7a6761f327f7129';   \nconst TEXT_URL = 'https://samples.clarifai.com/negative_sentence_12.txt';\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require(\"clarifai-nodejs-grpc\");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set(\"authorization\", \"Key \" + PAT);\n\nstub.PostModelOutputs(\n    {\n        user_app_id: {\n            \"user_id\": USER_ID,\n            \"app_id\": APP_ID\n        },\n        model_id: MODEL_ID,\n        version_id: MODEL_VERSION_ID, // This is optional. Defaults to the latest model version.\n        inputs: [\n            { data: { text: { url: TEXT_URL, allow_duplicate_url: true } } }\n        ]\n    },\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            console.log(response)\n            throw new Error(\"Post model outputs failed, status: \" + response.status.description);\n        }\n\n        // Since we have one input, one output will exist here.\n        const output = response.outputs[0];\n\n        console.log(\"Completion:\\n\");\n        console.log(output.data.text.raw);\n    }\n\n);\n",d='package com.clarifai.example;\n\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.status.StatusCode;\n\npublic class ClarifaiExample {\n\n    ///////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, and the URL\n    // of the text we want as a prompt. Change these strings to run your own example.\n    //////////////////////////////////////////////////////////////////////////////////////////////////\n\n    //Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    static final String USER_ID = "meta";    \n    static final String APP_ID = "Llama-2";\n    // Change these to whatever model and text URL you want to use\n    static final String MODEL_ID = "llama2-7b-chat"; \n    static final String MODEL_VERSION_ID = "e52af5d6bc22445aa7a6761f327f7129";   \n    static final String TEXT_URL = "https://samples.clarifai.com/negative_sentence_12.txt";\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\t\n\n    public static void main(String[] args) {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n            .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        MultiOutputResponse postModelOutputsResponse = stub.postModelOutputs(\n            PostModelOutputsRequest.newBuilder()\n            .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n            .setModelId(MODEL_ID)\n            .setVersionId(MODEL_VERSION_ID) // This is optional. Defaults to the latest model version\n            .addInputs(\n                Input.newBuilder().setData(\n                    Data.newBuilder().setText(\n                        Text.newBuilder().setUrl(TEXT_URL)\n                    )\n                )\n            )\n            .build()\n        );\n\n        if (postModelOutputsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("Post model outputs failed, status: " + postModelOutputsResponse.getStatus());\n        }\n\n        // Since we have one input, one output will exist here\n        Output output = postModelOutputsResponse.getOutputs(0);\n\n        System.out.println("Completion:");\n        System.out.println(output.getData().getText().getRaw());\n        \n    }\n\n}\n',h="<?php\n\nrequire __DIR__ . '/vendor/autoload.php';\n\n////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, and the URL\n// of the text we want as a prompt. Change these strings to run your own example.\n////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account's Security section\n$PAT = 'YOUR_PAT_HERE';\n// Specify the correct user_id/app_id pairings\n// Since you're making inferences outside your app's scope\n$USER_ID = 'meta';\n$APP_ID = 'Llama-2';\n// Change these to whatever model and text URL you want to use\n$MODEL_ID = 'llama2-7b-chat';\n$MODEL_VERSION_ID = 'e52af5d6bc22445aa7a6761f327f7129';\n$TEXT_URL = 'https://samples.clarifai.com/negative_sentence_12.txt';\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Text;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\PostModelOutputsRequest;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ['Authorization' => ['Key ' . $PAT ]];\n\n$userDataObject = new UserAppIDSet([\n    'user_id' => $USER_ID, \n    'app_id' => $APP_ID \n]);\n\n// Let's make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client->PostModelOutputs(\n    // The request object carries the request along with the request status and other metadata related to the request itself\n    new PostModelOutputsRequest([\n        'user_app_id' => $userDataObject,\n        'model_id' => $MODEL_ID,  \n        'version_id' => $MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n        'inputs' => [\n            new Input([ // The Input object wraps the Data object in order to meet the API specification                \n                'data' => new Data([ // The Data object is constructed around the Text object. It offers a container that has additional text independent\n                                    // metadata. In this particular use case, no other metadata is needed to be specified\n                    'text' => new Text([ // In the Clarifai platform, a text is defined by a special Text object\n                        'url' => $TEXT_URL \n                    ])\n                ])\n            ])\n        ]        \n    ]),\n    $metadata\n)->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception(\"Error: {$status->details}\");\n}\n\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure \n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    throw new Exception(\"Failure response: \" . $response->getStatus()->getDescription() . \" \" .\n        $response->getStatus()->getDetails());\n}\n\n// The output of a successful call can be used in many ways. In this example, we loop through all of the predicted concepts and print them out along with\n// their numerical prediction value (confidence)\necho \"Completion: </br>\";\necho $response->getOutputs()[0]->getData()->getText()->getRaw()\n\n?>\n",m='curl -X POST "https://api.clarifai.com/v2/users/meta/apps/Llama-2/models/llama2-7b-chat/versions/e52af5d6bc22445aa7a6761f327f7129/outputs" \\\n    -H "Authorization: Key YOUR_PAT_HERE" \\\n    -H "Content-Type: application/json" \\\n    -d \'{\n      "inputs": [\n        {\n          "data": {\n            "text": {\n              "url": "https://samples.clarifai.com/negative_sentence_12.txt"\n            }\n          }\n        }\n      ]\n    }\'\n   ',_="######################################################################################################\n# In this section, we set the user authentication, user and app ID, model details, and the location of \n# the text we want as a prompt. Change these strings to run your own example.\n######################################################################################################\n\n# Your PAT (Personal Access Token) can be found in the Account's Security section\nPAT = 'YOUR_PAT_HERE'\n# Specify the correct user_id/app_id pairings\n# Since you're making inferences outside your app's scope\nUSER_ID = 'meta'\nAPP_ID = 'Llama-2'\n# Change these to whatever model and text you want to use\nMODEL_ID = 'llama2-7b-chat'\nMODEL_VERSION_ID = 'e52af5d6bc22445aa7a6761f327f7129'\nTEXT_FILE_LOCATION = 'YOUR_TEXT_FILE_LOCATION_HERE'\n\n############################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n############################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (('authorization', 'Key ' + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\nwith open(TEXT_FILE_LOCATION, \"rb\") as f:\n    file_bytes = f.read()\n\npost_model_outputs_response = stub.PostModelOutputs(\n    service_pb2.PostModelOutputsRequest(\n        user_app_id=userDataObject,  # The userDataObject is created in the overview and is required when using a PAT\n        model_id=MODEL_ID,\n        version_id=MODEL_VERSION_ID,  # This is optional. Defaults to the latest model version\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    text=resources_pb2.Text(\n                        raw=file_bytes\n                    )\n                )\n            )\n        ]\n    ),\n    metadata=metadata\n)\nif post_model_outputs_response.status.code != status_code_pb2.SUCCESS:\n    print(post_model_outputs_response.status)\n    raise Exception(f\"Post model outputs failed, status: {post_model_outputs_response.status.description}\")\n\n# Since we have one input, one output will exist here\noutput = post_model_outputs_response.outputs[0]\n\nprint(\"Completion:\\n\")\nprint(output.data.text.raw)",f="\x3c!--index.html file--\x3e\n\n<script>        \n    ///////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, and the location\n    // of the text we want as a prompt. Change these strings to run your own example.\n    //////////////////////////////////////////////////////////////////////////////////////////////////////\n\n    // Your PAT (Personal Access Token) can be found in the Account's Security section\n    const PAT = 'YOUR_PAT_HERE';\n    // Specify the correct user_id/app_id pairings\n    // Since you're making inferences outside your app's scope\n    const USER_ID = 'meta';    \n    const APP_ID = 'Llama-2';\n    // Change these to whatever model and text you want to use  \n    const MODEL_ID = 'llama2-7b-chat';\n    const MODEL_VERSION_ID = 'e52af5d6bc22445aa7a6761f327f7129';    \n    const TEXT_FILE_BYTES = 'YOUR_TEXT_FILE_BYTES_HERE';\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    const raw = JSON.stringify({\n        \"user_app_id\": {\n            \"user_id\": USER_ID,\n            \"app_id\": APP_ID\n        },\n        \"inputs\": [\n            {\n                \"data\": {\n                    \"text\": {\n                        \"raw\": TEXT_FILE_BYTES\n                    }\n                }\n            }\n        ]\n    });\n\n    const requestOptions = {\n        method: 'POST',\n        headers: {\n            'Accept': 'application/json',\n            'Authorization': 'Key ' + PAT\n        },\n        body: raw\n    };\n\n    // NOTE: MODEL_VERSION_ID is optional, you can also call prediction with the MODEL_ID only\n    // https://api.clarifai.com/v2/models/{YOUR_MODEL_ID}/outputs\n    // this will default to the latest version_id\n\n    fetch(\"https://api.clarifai.com/v2/models/\" + MODEL_ID + \"/versions/\" + MODEL_VERSION_ID + \"/outputs\", requestOptions)\n        .then((response) => {\n            return response.json();\n        })\n        .then((data) => {\n            if(data.status.code != 10000) console.log(data.status);\n            else console.log(data['outputs'][0]['data']['text']['raw']);\n        }).catch(error => console.log('error', error));\n\n<\/script>",E='//index.js file\n\n//////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, and the location\n// of the text we want as a prompt. Change these strings to run your own example.\n///////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\nconst PAT = \'YOUR_PAT_HERE\';\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\nconst USER_ID = \'meta\';    \nconst APP_ID = \'Llama-2\';\n// Change these to whatever model and text you want to use\nconst MODEL_ID = \'llama2-7b-chat\';\nconst MODEL_VERSION_ID = \'e52af5d6bc22445aa7a6761f327f7129\';   \nconst TEXT_FILE_LOCATION = \'YOUR_TEXT_FILE_LOCATION_HERE\';\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\nconst fs = require("fs");\nconst fileBytes = fs.readFileSync(TEXT_FILE_LOCATION);\n\nstub.PostModelOutputs(\n    {\n        user_app_id: {\n            "user_id": USER_ID,\n            "app_id": APP_ID\n        },\n        model_id: MODEL_ID,\n        version_id: MODEL_VERSION_ID, // This is optional. Defaults to the latest model version.\n        inputs: [\n            { data: { text: { raw: fileBytes } } }\n        ]\n    },\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            console.log(response)\n            throw new Error("Post model outputs failed, status: " + response.status.description);\n        }\n\n        // Since we have one input, one output will exist here.\n        const output = response.outputs[0];\n\n        console.log("Completion:\\n");\n        console.log(output.data.text.raw);\n    }\n\n);\n',g='package com.clarifai.example;\n\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.status.StatusCode;\nimport com.google.protobuf.ByteString;\nimport java.io.File;\nimport java.io.IOException;\nimport java.nio.file.Files;\n\npublic class ClarifaiExample {\n\n    /////////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, and the location\n    // of the text we want as a prompt. Change these strings to run your own example.\n    /////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n    //Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    static final String USER_ID = "meta";    \n    static final String APP_ID = "Llama-2";\n    // Change these to whatever model and text input you want to use\n    static final String MODEL_ID = "llama2-7b-chat"; \n    static final String MODEL_VERSION_ID = "e52af5d6bc22445aa7a6761f327f7129";   \n    static final String TEXT_FILE_LOCATION = "YOUR_TEXT_FILE_LOCATION_HERE";\n   \n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\t\n\n    public static void main(String[] args) throws IOException {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n            .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        MultiOutputResponse postModelOutputsResponse = stub.postModelOutputs(\n            PostModelOutputsRequest.newBuilder()\n            .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n            .setModelId(MODEL_ID)\n            .setVersionId(MODEL_VERSION_ID) // This is optional. Defaults to the latest model version\n            .addInputs(\n                Input.newBuilder().setData(\n                    Data.newBuilder().setText(\n                        Text.newBuilder()\n                        .setRawBytes(ByteString.copyFrom(Files.readAllBytes(\n                            new File(TEXT_FILE_LOCATION).toPath()\n                        )))\n                    )\n                )\n            )\n            .build()\n        );\n\n        if (postModelOutputsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("Post model outputs failed, status: " + postModelOutputsResponse.getStatus());\n        }\n\n        // Since we have one input, one output will exist here\n        Output output = postModelOutputsResponse.getOutputs(0);\n\n        System.out.println("Completion:");\n        System.out.println(output.getData().getText().getRaw());\n\n    }\n\n}',I="<?php\n\nrequire __DIR__ . '/vendor/autoload.php';\n\n////////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, and the location\n// of the text we want as a prompt. Change these strings to run your own example.\n///////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account's Security section\n$PAT = 'YOUR_PAT_HERE';\n// Specify the correct user_id/app_id pairings\n// Since you're making inferences outside your app's scope\n$USER_ID = 'meta';\n$APP_ID = 'Llama-2';\n// Change these to whatever model and text you want to use\n$MODEL_ID = 'llama2-7b-chat';\n$MODEL_VERSION_ID = 'e52af5d6bc22445aa7a6761f327f7129';\n$TEXT_FILE_LOCATION = 'YOUR_TEXT_FILE_LOCATION_HERE';\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Text;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\PostModelOutputsRequest;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ['Authorization' => ['Key ' . $PAT ]];\n\n$userDataObject = new UserAppIDSet([\n    'user_id' => $USER_ID, \n    'app_id' => $APP_ID \n]);\n\n$textData = file_get_contents($TEXT_FILE_LOCATION); // Get the text bytes data from the location\n\n// Let's make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client->PostModelOutputs(\n    // The request object carries the request along with the request status and other metadata related to the request itself\n    new PostModelOutputsRequest([\n        'user_app_id' => $userDataObject,\n        'model_id' => $MODEL_ID,  \n        'version_id' => $MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n        'inputs' => [\n            new Input([ // The Input object wraps the Data object in order to meet the API specification                \n                'data' => new Data([ // The Data object is constructed around the Text object. It offers a container that has additional text independent\n                                    // metadata. In this particular use case, no other metadata is needed to be specified\n                    'text' => new Text([ // In the Clarifai platform, a text is defined by a special Text object\n                        'raw' => $textData \n                    ])\n                ])\n            ])\n        ]        \n    ]),\n    $metadata\n)->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception(\"Error: {$status->details}\");\n}\n\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure \n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    throw new Exception(\"Failure response: \" . $response->getStatus()->getDescription() . \" \" .\n        $response->getStatus()->getDetails());\n}\n\n// The output of a successful call can be used in many ways. In this example, we loop through all of the predicted concepts and print them out along with\n// their numerical prediction value (confidence)\necho \"Completion: </br>\";\necho $response->getOutputs()[0]->getData()->getText()->getRaw()\n\n?>\n",T='curl -X POST "https://api.clarifai.com/v2/users/meta/apps/Llama-2/models/llama2-7b-chat/versions/e52af5d6bc22445aa7a6761f327f7129/outputs" \\\n    -H "Authorization: Key YOUR_PAT_HERE" \\\n    -H "Content-Type: application/json" \\\n    -d \'{\n      "inputs": [\n        {\n          "data": {\n            "text": {\n              "raw": "YOUR_TEXT_FILE_BYTES_HERE"\n            }\n          }\n        }\n      ]\n    }\'\n   ',w="######################################################################################################\n# In this section, we set the user authentication, user and app ID, model details, and the raw\n# text we want as a prompt. Change these strings to run your own example.\n######################################################################################################\n\n# Your PAT (Personal Access Token) can be found in the Account's Security section\nPAT = 'YOUR_PAT_HERE'\n# Specify the correct user_id/app_id pairings\n# Since you're making inferences outside your app's scope\nUSER_ID = 'meta'\nAPP_ID = 'Llama-2'\n# Change these to whatever model and text you want to use\nMODEL_ID = 'llama2-7b-chat'\nMODEL_VERSION_ID = 'e52af5d6bc22445aa7a6761f327f7129'\nRAW_TEXT = 'I love your product very much'\n\n############################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n############################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (('authorization', 'Key ' + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\npost_model_outputs_response = stub.PostModelOutputs(\n    service_pb2.PostModelOutputsRequest(\n        user_app_id=userDataObject,  # The userDataObject is created in the overview and is required when using a PAT\n        model_id=MODEL_ID,\n        version_id=MODEL_VERSION_ID,  # This is optional. Defaults to the latest model version\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    text=resources_pb2.Text(\n                        raw=RAW_TEXT\n                    )\n                )\n            )\n        ]\n    ),\n    metadata=metadata\n)\nif post_model_outputs_response.status.code != status_code_pb2.SUCCESS:\n    print(post_model_outputs_response.status)\n    raise Exception(f\"Post model outputs failed, status: {post_model_outputs_response.status.description}\")\n\n# Since we have one input, one output will exist here\noutput = post_model_outputs_response.outputs[0]\n\nprint(\"Completion:\\n\")\nprint(output.data.text.raw)",O="\x3c!--index.html file--\x3e\n\n<script>        \n    ////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, and the raw\n    // text we want as a prompt. Change these strings to run your own example.\n    ///////////////////////////////////////////////////////////////////////////////////////////////////\n\n    // Your PAT (Personal Access Token) can be found in the Account's Security section\n    const PAT = 'YOUR_PAT_HERE';\n    // Specify the correct user_id/app_id pairings\n    // Since you're making inferences outside your app's scope\n    const USER_ID = 'meta';    \n    const APP_ID = 'Llama-2';\n    // Change these to whatever model and text you want to use  \n    const MODEL_ID = 'llama2-7b-chat';\n    const MODEL_VERSION_ID = 'e52af5d6bc22445aa7a6761f327f7129';    \n    const RAW_TEXT = 'I love your product very much';\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    const raw = JSON.stringify({\n        \"user_app_id\": {\n            \"user_id\": USER_ID,\n            \"app_id\": APP_ID\n        },\n        \"inputs\": [\n            {\n                \"data\": {\n                    \"text\": {\n                        \"raw\": RAW_TEXT\n                    }\n                }\n            }\n        ]\n    });\n\n    const requestOptions = {\n        method: 'POST',\n        headers: {\n            'Accept': 'application/json',\n            'Authorization': 'Key ' + PAT\n        },\n        body: raw\n    };\n\n    // NOTE: MODEL_VERSION_ID is optional, you can also call prediction with the MODEL_ID only\n    // https://api.clarifai.com/v2/models/{YOUR_MODEL_ID}/outputs\n    // this will default to the latest version_id\n\n    fetch(\"https://api.clarifai.com/v2/models/\" + MODEL_ID + \"/versions/\" + MODEL_VERSION_ID + \"/outputs\", requestOptions)\n        .then((response) => {\n            return response.json();\n        })\n        .then((data) => {\n            if(data.status.code != 10000) console.log(data.status);\n            else console.log(data['outputs'][0]['data']['text']['raw']);\n        }).catch(error => console.log('error', error));\n\n<\/script>",D="//index.js file\n\n////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, and the raw\n// text we want as a prompt. Change these strings to run your own example.\n/////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account's Security section\nconst PAT = 'YOUR_PAT_HERE';\n// Specify the correct user_id/app_id pairings\n// Since you're making inferences outside your app's scope\nconst USER_ID = 'meta';    \nconst APP_ID = 'Llama-2';\n// Change these to whatever model and text you want to use\nconst MODEL_ID = 'llama2-7b-chat';\nconst MODEL_VERSION_ID = 'e52af5d6bc22445aa7a6761f327f7129';   \nconst RAW_TEXT = 'I love your product very much';\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require(\"clarifai-nodejs-grpc\");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set(\"authorization\", \"Key \" + PAT);\n\nstub.PostModelOutputs(\n    {\n        user_app_id: {\n            \"user_id\": USER_ID,\n            \"app_id\": APP_ID\n        },\n        model_id: MODEL_ID,\n        version_id: MODEL_VERSION_ID, // This is optional. Defaults to the latest model version.\n        inputs: [\n            { data: { text: { raw: RAW_TEXT } } }\n        ]\n    },\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            console.log(response)\n            throw new Error(\"Post model outputs failed, status: \" + response.status.description);\n        }\n\n        // Since we have one input, one output will exist here.\n        const output = response.outputs[0];\n\n        console.log(\"Completion:\\n\");\n        console.log(output.data.text.raw);\n    }\n\n);\n",A='package com.clarifai.example;\n\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.status.StatusCode;\n\npublic class ClarifaiExample {\n\n    ///////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, and the raw\n    // text we want as a prompt. Change these strings to run your own example.\n    //////////////////////////////////////////////////////////////////////////////////////////////////\n\n    //Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    static final String USER_ID = "meta";    \n    static final String APP_ID = "Llama-2";\n    // Change these to whatever model and text you want to use\n    static final String MODEL_ID = "llama2-7b-chat"; \n    static final String MODEL_VERSION_ID = "e52af5d6bc22445aa7a6761f327f7129";   \n    static final String RAW_TEXT = "I love your product very much";\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\t\n\n    public static void main(String[] args) {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n            .withCallCredentials(new ClarifaiCallCredentials(PAT));\n        \n        MultiOutputResponse postModelOutputsResponse = stub.postModelOutputs(\n            PostModelOutputsRequest.newBuilder()\n            .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n            .setModelId(MODEL_ID)\n            .setVersionId(MODEL_VERSION_ID) // This is optional. Defaults to the latest model version\n            .addInputs(\n                Input.newBuilder().setData(\n                    Data.newBuilder().setText(\n                        Text.newBuilder().setRaw(RAW_TEXT)\n                    )\n                )\n            )\n            .build()\n        );\n\n        if (postModelOutputsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("Post model outputs failed, status: " + postModelOutputsResponse.getStatus());\n        }\n\n        // Since we have one input, one output will exist here\n        Output output = postModelOutputsResponse.getOutputs(0);\n\n        System.out.println("Completion:");\n        System.out.println(output.getData().getText().getRaw());\n        \n    }\n\n}\n',S="<?php\n\nrequire __DIR__ . '/vendor/autoload.php';\n\n////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, and the raw\n// text we want as a prompt. Change these strings to run your own example.\n////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account's Security section\n$PAT = 'YOUR_PAT_HERE';\n// Specify the correct user_id/app_id pairings\n// Since you're making inferences outside your app's scope\n$USER_ID = 'meta';\n$APP_ID = 'Llama-2';\n// Change these to whatever model and text you want to use\n$MODEL_ID = 'llama2-7b-chat';\n$MODEL_VERSION_ID = 'e52af5d6bc22445aa7a6761f327f7129';\n$RAW_TEXT = 'I love your product very much';\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Text;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\PostModelOutputsRequest;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ['Authorization' => ['Key ' . $PAT ]];\n\n$userDataObject = new UserAppIDSet([\n    'user_id' => $USER_ID, \n    'app_id' => $APP_ID \n]);\n\n// Let's make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client->PostModelOutputs(\n    // The request object carries the request along with the request status and other metadata related to the request itself\n    new PostModelOutputsRequest([\n        'user_app_id' => $userDataObject,\n        'model_id' => $MODEL_ID,  \n        'version_id' => $MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n        'inputs' => [\n            new Input([ // The Input object wraps the Data object in order to meet the API specification                \n                'data' => new Data([ // The Data object is constructed around the Text object. It offers a container that has additional text independent\n                                    // metadata. In this particular use case, no other metadata is needed to be specified\n                    'text' => new Text([ // In the Clarifai platform, a text is defined by a special Text object\n                        'raw' => $RAW_TEXT\n                    ])\n                ])\n            ])\n        ]        \n    ]),\n    $metadata\n)->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception(\"Error: {$status->details}\");\n}\n\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure \n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    throw new Exception(\"Failure response: \" . $response->getStatus()->getDescription() . \" \" .\n        $response->getStatus()->getDetails());\n}\n\n// The output of a successful call can be used in many ways. In this example, we loop through all of the predicted concepts and print them out along with\n// their numerical prediction value (confidence)\necho \"Completion: </br>\";\necho $response->getOutputs()[0]->getData()->getText()->getRaw()\n\n?>\n",b='curl -X POST "https://api.clarifai.com/v2/users/meta/apps/Llama-2/models/llama2-7b-chat/versions/e52af5d6bc22445aa7a6761f327f7129/outputs" \\\n    -H "Authorization: Key YOUR_PAT_HERE" \\\n    -H "Content-Type: application/json" \\\n    -d \'{\n      "inputs": [\n        {\n          "data": {\n            "text": {\n              "raw": "I love your product very much"\n            }\n          }\n        }\n      ]\n    }\'\n   ',x="#####################################################################################################################\n# In this section, we set the user authentication, user and app ID, model details, raw text we want as a prompt,\n# and the parameters. Change these values to run your own example.\n#####################################################################################################################\n\n# Your PAT (Personal Access Token) can be found in the Account's Security section\nPAT = 'YOUR_PAT_HERE'\n# Specify the correct user_id/app_id pairings\n# Since you're making inferences outside your app's scope\nUSER_ID = 'meta'\nAPP_ID = 'Llama-2'\n# Change these to whatever model and text you want to use\nMODEL_ID = 'llama2-7b-chat'\nMODEL_VERSION_ID = 'e52af5d6bc22445aa7a6761f327f7129'\nRAW_TEXT = 'I love your product very much'\n\n############################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n############################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\nfrom google.protobuf.struct_pb2 import Struct\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (('authorization', 'Key ' + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\nparams = Struct()\nparams.update({\n    \"temperature\": 0.5,\n    \"max_tokens\": 2048,\n    \"top_k\": 0.95\n})\n\npost_model_outputs_response = stub.PostModelOutputs(\n    service_pb2.PostModelOutputsRequest(\n        user_app_id=userDataObject,  # The userDataObject is created in the overview and is required when using a PAT\n        model_id=MODEL_ID,\n        version_id=MODEL_VERSION_ID,  # This is optional. Defaults to the latest model version\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    text=resources_pb2.Text(\n                        raw=RAW_TEXT\n                    )\n                )\n            )\n        ],\n        model=resources_pb2.Model(\n            model_version=resources_pb2.ModelVersion(\n                output_info=resources_pb2.OutputInfo(\n                    params=params\n                )\n            )\n        )\n\n    ),\n    metadata=metadata\n)\nif post_model_outputs_response.status.code != status_code_pb2.SUCCESS:\n    print(post_model_outputs_response.status)\n    raise Exception(f\"Post model outputs failed, status: {post_model_outputs_response.status.description}\")\n\n# Since we have one input, one output will exist here\noutput = post_model_outputs_response.outputs[0]\n\nprint(\"Completion:\\n\")\nprint(output.data.text.raw)",v="\x3c!--index.html file--\x3e\n\n<script>\n    /////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, raw text we want as a prompt,\n    // and the parameters. Change these values to run your own example.\n    /////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n    // Your PAT (Personal Access Token) can be found in the Account's Security section\n    const PAT = 'YOUR_PAT_HERE';    \n    // Specify the correct user_id/app_id pairings\n    // Since you're making inferences outside your app's scope\n    const USER_ID = 'meta';\n    const APP_ID = 'Llama-2';\n    // Change these to whatever model and text you want to use\n    const MODEL_ID = 'llama2-7b-chat';\n    const MODEL_VERSION_ID = 'e52af5d6bc22445aa7a6761f327f7129';\n    const RAW_TEXT = 'I love your product very much'\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    const raw = JSON.stringify({  \n        \"inputs\": [\n        {\n            \"data\": {\n                \"text\": {\n                    \"raw\": RAW_TEXT\n                }\n            }\n        }\n    ],\n    \"model\": {\n        \"model_version\": {\n            \"output_info\": {\n                \"params\": {\n                    \"temperature\": 0.5,\n                    \"max_tokens\": 2048,\n                    \"top_k\": 0.95\n                }\n            }\n        }\n    }      \n    });\n\n    const requestOptions = {\n        method: 'POST',\n        headers: {\n            'Accept': 'application/json',\n            'Authorization': 'Key ' + PAT\n        },\n        body: raw\n    };\n\n    fetch(`https://api.clarifai.com/v2/users/${USER_ID}/apps/${APP_ID}/models/${MODEL_ID}/versions/${MODEL_VERSION_ID}/outputs`, requestOptions)\n        .then(response => response.text())\n        .then(result => console.log(result))\n        .catch(error => console.log('error', error));\n\n<\/script>",C='//index.js file\n\n//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, raw text we want as a prompt,\n// and the parameters. Change these values to run your own example.\n//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\nconst PAT = \'YOUR_PAT_HERE\';    \n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\nconst USER_ID = \'meta\';\nconst APP_ID = \'Llama-2\';\n// Change these to whatever model and text you want to use\nconst MODEL_ID = \'llama2-7b-chat\';\nconst MODEL_VERSION_ID = \'e52af5d6bc22445aa7a6761f327f7129\';\nconst RAW_TEXT = \'I love your product very much\'\n\n/////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n/////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\nstub.PostModelOutputs(\n    {\n        user_app_id: {\n            "user_id": USER_ID,\n            "app_id": APP_ID\n        },\n        model_id: MODEL_ID,\n        version_id: MODEL_VERSION_ID,  // This is optional. Defaults to the latest model version\n        inputs: [\n            {\n                "data": {\n                    "text": {\n                        "raw": RAW_TEXT\n                    }\n                }\n            }\n        ],\n        model: {\n            "model_version": {\n                "output_info": {\n                    "params": {\n                        "temperature": 0.5,\n                        "max_tokens": 2048,\n                        "top_k": 0.95\n                    }\n                }\n            }\n        } \n    },\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            throw new Error("Post models failed, status: " + response.status.description);\n        }\n        // Since we have one input, one output will exist here.\n        const output = response.outputs[0];\n\n        console.log("Completion:\\n");\n        console.log(output.data.text.raw);\n    }\n);',y='package com.clarifai.example;\n\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.status.StatusCode;\nimport java.io.FileOutputStream;\nimport com.google.protobuf.Struct;\nimport com.google.protobuf.Value;\n\nimport com.google.protobuf.ByteString;\nimport java.io.File;\nimport java.io.IOException;\nimport java.nio.file.Files;\n\npublic class ClarifaiExample {\n\n    /////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, and raw text we want as a prompt.\n    // Change these values to run your own example.\n    /////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n    // Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    static final String USER_ID = "meta";\n    static final String APP_ID = "Llama-2";\n    // Change these to whatever model you want to use\n    static final String MODEL_ID = "llama2-7b-chat";\n    static final String MODEL_VERSION_ID = "e52af5d6bc22445aa7a6761f327f7129";\n    static final String RAW_TEXT = "I love your product very much!";\n    // To use a hosted text file, assign the URL variable\n    // static final String TEXT_FILE_URL = "https://samples.clarifai.com/negative_sentence_12.txt";\n    // Or, to use a local text file, assign the location variable\n    // static final String TEXT_FILE_LOCATION = "YOUR_TEXT_FILE_LOCATION_HERE";\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\t\n    \n    public static void main(String[] args) throws IOException {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n                .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        Struct.Builder params = Struct.newBuilder()\n                .putFields("temperature", Value.newBuilder().setNumberValue(0.5).build())\n                .putFields("max_tokens", Value.newBuilder().setNumberValue(2048).build())\n                .putFields("top_k", Value.newBuilder().setNumberValue(0.95).build());\n\n        MultiOutputResponse postModelOutputsResponse = stub.postModelOutputs(\n                PostModelOutputsRequest.newBuilder()\n                        .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n                        .setModelId(MODEL_ID)\n                        .setVersionId(MODEL_VERSION_ID) // This is optional. Defaults to the latest model version.\n                        .addInputs(\n                                Input.newBuilder().setData(\n                                        Data.newBuilder().setText(\n                                                Text.newBuilder().setRaw(RAW_TEXT)\n                                        // Text.newBuilder().setUrl(TEXT_FILE_URL)\n                                        // Text.newBuilder().setRawBytes(ByteString.copyFrom(Files.readAllBytes(\n                                        // new File(TEXT_FILE_LOCATION).toPath()\n                                        // )))\n                                        )\n                                )\n                        )\n                        .setModel(Model.newBuilder()\n                                .setModelVersion(ModelVersion.newBuilder()\n                                        .setOutputInfo(OutputInfo.newBuilder().setParams(params))\n                                )\n                        )\n                        .build()\n        );\n\n        if (postModelOutputsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("Post model outputs failed, status: " + postModelOutputsResponse.getStatus());\n        }\n\n        // Since we have one input, one output will exist here\n        Output output = postModelOutputsResponse.getOutputs(0);\n\n        System.out.println("Completion:");\n        System.out.println(output.getData().getText().getRaw());\n\n    }\n}\n',L='<?php\n\nrequire __DIR__ . "/vendor/autoload.php";\n\n///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, and raw text we want as a prompt.\n// Change these values to run your own example.\n///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\n$PAT = "YOUR_PAT_HERE";\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\n$USER_ID = "meta";\n$APP_ID = "Llama-2";\n// Change these to whatever model and image URL you want to use\n$MODEL_ID = "llama2-7b-chat";\n$MODEL_VERSION_ID = "e52af5d6bc22445aa7a6761f327f7129";\n$RAW_TEXT = "I love your product very much!";\n// To use a hosted text file, assign the URL variable\n// $TEXT_FILE_URL = "https://samples.clarifai.com/negative_sentence_12.txt";\n// Or, to use a local text file, assign the location variable\n// $TEXT_FILE_LOCATION = "YOUR_TEXT_FILE_LOCATION_HERE";\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Text;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\Model;\nuse Clarifai\\Api\\ModelVersion;\nuse Clarifai\\Api\\OutputInfo;\nuse Clarifai\\Api\\PostModelOutputsRequest;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\nuse Google\\Protobuf\\Struct;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ["Authorization" => ["Key " . $PAT]];\n\n$userDataObject = new UserAppIDSet([\n    "user_id" => $USER_ID,\n    "app_id" => $APP_ID,\n]);\n\n// Create Struct instance\n$params = new Struct();\n$params->temperature = 0.5;\n$params->max_tokens = 2048;\n$params->top_k = 0.95;\n\n// $textData = file_get_contents($TEXT_FILE_LOCATION); // Get the text bytes data from the location\n\n// Let\'s make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client->PostModelOutputs(\n    // The request object carries the request along with the request status and other metadata related to the request itself\n    new PostModelOutputsRequest([\n        "user_app_id" => $userDataObject,\n        "model_id" => $MODEL_ID,\n        "version_id" => $MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n        "inputs" => [\n            new Input([\n                // The Input object wraps the Data object in order to meet the API specification\n                "data" => new Data([\n                    // The Data object is constructed around the Text object. It offers a container that has additional text independent\n                    // metadata. In this particular use case, no other metadata is needed to be specified\n                    "text" => new Text([\n                        // In the Clarifai platform, a text is defined by a special Text object\n                        "raw" => $RAW_TEXT,\n                        // "url" => $TEXT_FILE_URL\n                        // "raw" => $textData\n                    ]),\n                ]),\n            ]),\n        ],\n        "model" => new Model([\n            "model_version" => new ModelVersion([\n                "output_info" => new OutputInfo(["params" => $params]),\n            ]),\n        ]),\n    ]),\n    $metadata\n)\n    ->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception("Error: {$status->details}");\n}\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure\n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    throw new Exception(\n        "Failure response: " . $response->getStatus()->getDescription() . " " .\n            $response->getStatus()->getDetails()\n    );\n}\n\necho "Completion: </br>";\n# Since we have one input, one output will exist here\necho $response->getOutputs()[0]->getData()->getText()->getRaw()\n\n?>',R='curl -X POST "https://api.clarifai.com/v2/users/meta/apps/Llama-2/models/llama2-7b-chat/versions/e52af5d6bc22445aa7a6761f327f7129/outputs" \\\n    -H "Authorization: Key YOUR_PAT_HERE" \\\n    -H "Content-Type: application/json" \\\n    -d \'{\n    "inputs": [\n        {\n            "data": {\n                "text": {\n                    "raw": "I love your product very much"\n                }\n            }\n        }\n    ],\n    "model": {\n        "model_version": {\n            "output_info": {\n                "params": {\n                    "temperature": 0.5,\n                    "max_tokens": 2048,\n                    "top_k": 0.95\n                }\n            }\n        }\n    }\n}\'',P="Completion:\n\nHe doesn't have to commute to work. He can telecommute so he can spend more time with his wife and son. Even though he doesn't get a traditional work-life balance, he still has a quality of life. He may choose to work less hours and make less money, which will allow him to have more time for himself and his family. \n\nThere are many advantages and disadvantages of working from home. Some people may find it difficult to focus and be productive. Others may prefer the noise and energy of an office environment. As the world continues to change and technology evolves, working from home may become more common and socially accepted.  \n\nOne of the biggest advantages of working from home is the flexibility and convenience it offers. Instead of having to commute to an office, people can work from the comfort of their own homes. This can save time and money on transportation, and it can also reduce the stress and exhaustion that commuting can cause. Many people find that they are more productive and focused when working from home, where they can avoid office distractions and create a workspace that suits their needs.\n\nOne of the biggest disadvantages of working from home is lack of social interaction. Many people find that",N={description:"Make predictions using LLMs",sidebar_position:4},j="Large Language Models (LLMs)",M={},U=[{value:"Text Completion",id:"text-completion",level:2},{value:"Via URL",id:"via-url",level:3},{value:"Via Local Files",id:"via-local-files",level:3},{value:"Via Raw Text",id:"via-raw-text",level:3},{value:"Use Hyperparameters to Customize LLMs",id:"use-hyperparameters-to-customize-llms",level:2}];function H(e){const t={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,o.R)(),...e.components},{Details:n}=t;return n||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.header,{children:(0,s.jsx)(t.h1,{id:"large-language-models-llms",children:"Large Language Models (LLMs)"})}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.strong,{children:"Make predictions using LLMs"})}),"\n",(0,s.jsx)("hr",{}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.a,{href:"https://clarifai.com/explore/models?page=1&perPage=24&filterData=%5B%7B%22field%22%3A%22use_cases%22%2C%22value%22%3A%5B%22llm%22%5D%7D%5D",children:"Large Language Models (LLMs)"})," are a subset of foundation models that have revolutionized natural language understanding and generation tasks. These models are characterized by their vast size, typically containing hundreds of millions to billions of parameters."]}),"\n",(0,s.jsx)(t.p,{children:"LLMs have learned to perform many kinds of prediction tasks. One of the most notable capabilities of LLMs is text generation. Given a prompt or seed text, they can generate coherent and contextually relevant text that appears as if it were written by a human."}),"\n",(0,s.jsx)(t.admonition,{type:"info",children:(0,s.jsxs)(t.p,{children:["The initialization code used in the following examples is outlined in detail on the ",(0,s.jsx)(t.a,{href:"https://docs.clarifai.com/api-guide/api-overview/api-clients/#client-installation-instructions",children:"client installation page."})]})}),"\n",(0,s.jsx)(t.admonition,{type:"tip",children:(0,s.jsxs)(t.p,{children:["For the third-party models we've wrapped into our platform, like those provided by OpenAI, Anthropic, Cohere, and others, you can also choose to utilize their API keys as an option\u2014in addition to using the default Clarifai keys. You can learn how to add them ",(0,s.jsx)(t.a,{href:"https://docs.clarifai.com/api-guide/predict/text#use-third-party-api-keys",children:"here"}),"."]})}),"\n","\n","\n","\n","\n","\n","\n","\n",(0,s.jsx)(t.h2,{id:"text-completion",children:"Text Completion"}),"\n",(0,s.jsx)(t.h3,{id:"via-url",children:"Via URL"}),"\n",(0,s.jsxs)(t.p,{children:["Below is an example of how you would provide a prompt text via a URL and autocomplete sentences or phrases using the ",(0,s.jsx)(t.a,{href:"https://clarifai.com/meta/Llama-2/models/llama2-7b-chat",children:"llama2-7b-chat"})," large language model."]}),"\n",(0,s.jsxs)(i.A,{children:[(0,s.jsx)(r.A,{value:"python",label:"Python",children:(0,s.jsx)(l.A,{className:"language-python",children:u})}),(0,s.jsx)(r.A,{value:"js_rest",label:"JavaScript (REST)",children:(0,s.jsx)(l.A,{className:"language-javascript",children:c})}),(0,s.jsx)(r.A,{value:"nodejs",label:"NodeJS",children:(0,s.jsx)(l.A,{className:"language-javascript",children:p})}),(0,s.jsx)(r.A,{value:"java",label:"Java",children:(0,s.jsx)(l.A,{className:"language-java",children:d})}),(0,s.jsx)(r.A,{value:"php",label:"PHP",children:(0,s.jsx)(l.A,{className:"language-php",children:h})}),(0,s.jsx)(r.A,{value:"curl",label:"cURL",children:(0,s.jsx)(l.A,{className:"language-bash",children:m})})]}),"\n",(0,s.jsxs)(n,{children:[(0,s.jsx)("summary",{children:"Text Output Example"}),(0,s.jsx)(l.A,{className:"language-text",children:P})]}),"\n",(0,s.jsx)(t.h3,{id:"via-local-files",children:"Via Local Files"}),"\n",(0,s.jsxs)(t.p,{children:["Below is an example of how you would provide a prompt text via a local text file and autocomplete sentences or phrases using the ",(0,s.jsx)(t.a,{href:"https://clarifai.com/meta/Llama-2/models/llama2-7b-chat",children:"llama2-7b-chat"})," large language model."]}),"\n",(0,s.jsxs)(i.A,{children:[(0,s.jsx)(r.A,{value:"python",label:"Python",children:(0,s.jsx)(l.A,{className:"language-python",children:_})}),(0,s.jsx)(r.A,{value:"js_rest",label:"JavaScript (REST)",children:(0,s.jsx)(l.A,{className:"language-javascript",children:f})}),(0,s.jsx)(r.A,{value:"nodejs",label:"NodeJS",children:(0,s.jsx)(l.A,{className:"language-javascript",children:E})}),(0,s.jsx)(r.A,{value:"java",label:"Java",children:(0,s.jsx)(l.A,{className:"language-java",children:g})}),(0,s.jsx)(r.A,{value:"php",label:"PHP",children:(0,s.jsx)(l.A,{className:"language-php",children:I})}),(0,s.jsx)(r.A,{value:"curl",label:"cURL",children:(0,s.jsx)(l.A,{className:"language-bash",children:T})})]}),"\n",(0,s.jsx)(t.h3,{id:"via-raw-text",children:"Via Raw Text"}),"\n",(0,s.jsxs)(t.p,{children:["Below is an example of how you would provide a raw text prompt and autocomplete sentences or phrases using the ",(0,s.jsx)(t.a,{href:"https://clarifai.com/meta/Llama-2/models/llama2-7b-chat",children:"llama2-7b-chat"})," large language model."]}),"\n",(0,s.jsxs)(i.A,{children:[(0,s.jsx)(r.A,{value:"python",label:"Python",children:(0,s.jsx)(l.A,{className:"language-python",children:w})}),(0,s.jsx)(r.A,{value:"js_rest",label:"JavaScript (REST)",children:(0,s.jsx)(l.A,{className:"language-javascript",children:O})}),(0,s.jsx)(r.A,{value:"nodejs",label:"NodeJS",children:(0,s.jsx)(l.A,{className:"language-javascript",children:D})}),(0,s.jsx)(r.A,{value:"java",label:"Java",children:(0,s.jsx)(l.A,{className:"language-java",children:A})}),(0,s.jsx)(r.A,{value:"php",label:"PHP",children:(0,s.jsx)(l.A,{className:"language-php",children:S})}),(0,s.jsx)(r.A,{value:"curl",label:"cURL",children:(0,s.jsx)(l.A,{className:"language-bash",children:b})})]}),"\n",(0,s.jsx)(t.h2,{id:"use-hyperparameters-to-customize-llms",children:"Use Hyperparameters to Customize LLMs"}),"\n",(0,s.jsxs)(t.p,{children:["You can use ",(0,s.jsx)(t.a,{href:"https://docs.clarifai.com/portal-guide/ppredict/generative-ai#inference-parameters",children:"hyperparameters"})," to fine-tune and customize the behavior of LLMs. This allows you to gain precise control over the prediction output of LLMs, shaping their responses to suit your unique needs."]}),"\n",(0,s.jsx)(t.p,{children:"Here are some parameters we support:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Temperature"}),"\u2014It affects the randomness of the model's output. A higher temperature (e.g., 0.8) will make the output more random and creative, while a lower temperature (e.g., 0.2) will make it more deterministic and focused."]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Max Tokens"}),"\u2014It allow you to limit the length of the generated text. You can set a maximum number of tokens to prevent the output from becoming too long or to fit within specific constraints."]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Top K"}),"\u2014It controls the diversity of the output. It limits the vocabulary to the ",(0,s.jsx)(t.code,{children:"top_k"})," most likely tokens at each step. A lower value of K (e.g., 10) will make the output more focused, while a higher value (e.g., 50) will make it more diverse."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(t.admonition,{type:"tip",children:(0,s.jsx)(t.p,{children:"Most of our models now have new versions that support inference hyperparameters like temperature, top_k, etc. This example illustrates how you can configure them."})}),"\n",(0,s.jsxs)(i.A,{children:[(0,s.jsx)(r.A,{value:"python",label:"Python",children:(0,s.jsx)(l.A,{className:"language-python",children:x})}),(0,s.jsx)(r.A,{value:"js_rest",label:"JavaScript (REST)",children:(0,s.jsx)(l.A,{className:"language-javascript",children:v})}),(0,s.jsx)(r.A,{value:"nodejs",label:"NodeJS",children:(0,s.jsx)(l.A,{className:"language-javascript",children:C})}),(0,s.jsx)(r.A,{value:"java",label:"Java",children:(0,s.jsx)(l.A,{className:"language-java",children:y})}),(0,s.jsx)(r.A,{value:"php",label:"PHP",children:(0,s.jsx)(l.A,{className:"language-php",children:L})}),(0,s.jsx)(r.A,{value:"curl",label:"cURL",children:(0,s.jsx)(l.A,{className:"language-bash",children:R})})]})]})}function $(e={}){const{wrapper:t}={...(0,o.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(H,{...e})}):H(e)}},65537:(e,t,n)=>{n.d(t,{A:()=>O});var a=n(96540),s=n(18215),o=n(65627),i=n(56347),r=n(50372),l=n(30604),u=n(11861),c=n(78749);function p(e){return a.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,a.isValidElement)(e)&&function(e){const{props:t}=e;return!!t&&"object"==typeof t&&"value"in t}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function d(e){const{values:t,children:n}=e;return(0,a.useMemo)((()=>{const e=t??function(e){return p(e).map((e=>{let{props:{value:t,label:n,attributes:a,default:s}}=e;return{value:t,label:n,attributes:a,default:s}}))}(n);return function(e){const t=(0,u.XI)(e,((e,t)=>e.value===t.value));if(t.length>0)throw new Error(`Docusaurus error: Duplicate values "${t.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[t,n])}function h(e){let{value:t,tabValues:n}=e;return n.some((e=>e.value===t))}function m(e){let{queryString:t=!1,groupId:n}=e;const s=(0,i.W6)(),o=function(e){let{queryString:t=!1,groupId:n}=e;if("string"==typeof t)return t;if(!1===t)return null;if(!0===t&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:t,groupId:n});return[(0,l.aZ)(o),(0,a.useCallback)((e=>{if(!o)return;const t=new URLSearchParams(s.location.search);t.set(o,e),s.replace({...s.location,search:t.toString()})}),[o,s])]}function _(e){const{defaultValue:t,queryString:n=!1,groupId:s}=e,o=d(e),[i,l]=(0,a.useState)((()=>function(e){let{defaultValue:t,tabValues:n}=e;if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(t){if(!h({value:t,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${t}" but none of its children has the corresponding value. Available values are: ${n.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return t}const a=n.find((e=>e.default))??n[0];if(!a)throw new Error("Unexpected error: 0 tabValues");return a.value}({defaultValue:t,tabValues:o}))),[u,p]=m({queryString:n,groupId:s}),[_,f]=function(e){let{groupId:t}=e;const n=function(e){return e?`docusaurus.tab.${e}`:null}(t),[s,o]=(0,c.Dv)(n);return[s,(0,a.useCallback)((e=>{n&&o.set(e)}),[n,o])]}({groupId:s}),E=(()=>{const e=u??_;return h({value:e,tabValues:o})?e:null})();(0,r.A)((()=>{E&&l(E)}),[E]);return{selectedValue:i,selectValue:(0,a.useCallback)((e=>{if(!h({value:e,tabValues:o}))throw new Error(`Can't select invalid tab value=${e}`);l(e),p(e),f(e)}),[p,f,o]),tabValues:o}}var f=n(9136);const E={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var g=n(74848);function I(e){let{className:t,block:n,selectedValue:a,selectValue:i,tabValues:r}=e;const l=[],{blockElementScrollPositionUntilNextRender:u}=(0,o.a_)(),c=e=>{const t=e.currentTarget,n=l.indexOf(t),s=r[n].value;s!==a&&(u(t),i(s))},p=e=>{let t=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const n=l.indexOf(e.currentTarget)+1;t=l[n]??l[0];break}case"ArrowLeft":{const n=l.indexOf(e.currentTarget)-1;t=l[n]??l[l.length-1];break}}t?.focus()};return(0,g.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,s.A)("tabs",{"tabs--block":n},t),children:r.map((e=>{let{value:t,label:n,attributes:o}=e;return(0,g.jsx)("li",{role:"tab",tabIndex:a===t?0:-1,"aria-selected":a===t,ref:e=>{l.push(e)},onKeyDown:p,onClick:c,...o,className:(0,s.A)("tabs__item",E.tabItem,o?.className,{"tabs__item--active":a===t}),children:n??t},t)}))})}function T(e){let{lazy:t,children:n,selectedValue:o}=e;const i=(Array.isArray(n)?n:[n]).filter(Boolean);if(t){const e=i.find((e=>e.props.value===o));return e?(0,a.cloneElement)(e,{className:(0,s.A)("margin-top--md",e.props.className)}):null}return(0,g.jsx)("div",{className:"margin-top--md",children:i.map(((e,t)=>(0,a.cloneElement)(e,{key:t,hidden:e.props.value!==o})))})}function w(e){const t=_(e);return(0,g.jsxs)("div",{className:(0,s.A)("tabs-container",E.tabList),children:[(0,g.jsx)(I,{...t,...e}),(0,g.jsx)(T,{...t,...e})]})}function O(e){const t=(0,f.A)();return(0,g.jsx)(w,{...e,children:p(e.children)},String(t))}},79329:(e,t,n)=>{n.d(t,{A:()=>i});n(96540);var a=n(18215);const s={tabItem:"tabItem_Ymn6"};var o=n(74848);function i(e){let{children:t,hidden:n,className:i}=e;return(0,o.jsx)("div",{role:"tabpanel",className:(0,a.A)(s.tabItem,i),hidden:n,children:t})}}}]);
"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[6864],{85162:(e,t,a)=>{a.d(t,{Z:()=>l});var n=a(67294),o=a(86010);const i={tabItem:"tabItem_Ymn6"};function l(e){let{children:t,hidden:a,className:l}=e;return n.createElement("div",{role:"tabpanel",className:(0,o.Z)(i.tabItem,l),hidden:a},t)}},74866:(e,t,a)=>{a.d(t,{Z:()=>_});var n=a(87462),o=a(67294),i=a(86010),l=a(12466),r=a(16550),s=a(91980),d=a(67392),u=a(50012);function p(e){return function(e){return o.Children.map(e,(e=>{if(!e||(0,o.isValidElement)(e)&&function(e){const{props:t}=e;return!!t&&"object"==typeof t&&"value"in t}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}(e).map((e=>{let{props:{value:t,label:a,attributes:n,default:o}}=e;return{value:t,label:a,attributes:n,default:o}}))}function m(e){const{values:t,children:a}=e;return(0,o.useMemo)((()=>{const e=t??p(a);return function(e){const t=(0,d.l)(e,((e,t)=>e.value===t.value));if(t.length>0)throw new Error(`Docusaurus error: Duplicate values "${t.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[t,a])}function c(e){let{value:t,tabValues:a}=e;return a.some((e=>e.value===t))}function h(e){let{queryString:t=!1,groupId:a}=e;const n=(0,r.k6)(),i=function(e){let{queryString:t=!1,groupId:a}=e;if("string"==typeof t)return t;if(!1===t)return null;if(!0===t&&!a)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return a??null}({queryString:t,groupId:a});return[(0,s._X)(i),(0,o.useCallback)((e=>{if(!i)return;const t=new URLSearchParams(n.location.search);t.set(i,e),n.replace({...n.location,search:t.toString()})}),[i,n])]}function g(e){const{defaultValue:t,queryString:a=!1,groupId:n}=e,i=m(e),[l,r]=(0,o.useState)((()=>function(e){let{defaultValue:t,tabValues:a}=e;if(0===a.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(t){if(!c({value:t,tabValues:a}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${t}" but none of its children has the corresponding value. Available values are: ${a.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return t}const n=a.find((e=>e.default))??a[0];if(!n)throw new Error("Unexpected error: 0 tabValues");return n.value}({defaultValue:t,tabValues:i}))),[s,d]=h({queryString:a,groupId:n}),[p,g]=function(e){let{groupId:t}=e;const a=function(e){return e?`docusaurus.tab.${e}`:null}(t),[n,i]=(0,u.Nk)(a);return[n,(0,o.useCallback)((e=>{a&&i.set(e)}),[a,i])]}({groupId:n}),y=(()=>{const e=s??p;return c({value:e,tabValues:i})?e:null})();(0,o.useLayoutEffect)((()=>{y&&r(y)}),[y]);return{selectedValue:l,selectValue:(0,o.useCallback)((e=>{if(!c({value:e,tabValues:i}))throw new Error(`Can't select invalid tab value=${e}`);r(e),d(e),g(e)}),[d,g,i]),tabValues:i}}var y=a(72389);const k={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};function v(e){let{className:t,block:a,selectedValue:r,selectValue:s,tabValues:d}=e;const u=[],{blockElementScrollPositionUntilNextRender:p}=(0,l.o5)(),m=e=>{const t=e.currentTarget,a=u.indexOf(t),n=d[a].value;n!==r&&(p(t),s(n))},c=e=>{let t=null;switch(e.key){case"Enter":m(e);break;case"ArrowRight":{const a=u.indexOf(e.currentTarget)+1;t=u[a]??u[0];break}case"ArrowLeft":{const a=u.indexOf(e.currentTarget)-1;t=u[a]??u[u.length-1];break}}t?.focus()};return o.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,i.Z)("tabs",{"tabs--block":a},t)},d.map((e=>{let{value:t,label:a,attributes:l}=e;return o.createElement("li",(0,n.Z)({role:"tab",tabIndex:r===t?0:-1,"aria-selected":r===t,key:t,ref:e=>u.push(e),onKeyDown:c,onClick:m},l,{className:(0,i.Z)("tabs__item",k.tabItem,l?.className,{"tabs__item--active":r===t})}),a??t)})))}function b(e){let{lazy:t,children:a,selectedValue:n}=e;const i=(Array.isArray(a)?a:[a]).filter(Boolean);if(t){const e=i.find((e=>e.props.value===n));return e?(0,o.cloneElement)(e,{className:"margin-top--md"}):null}return o.createElement("div",{className:"margin-top--md"},i.map(((e,t)=>(0,o.cloneElement)(e,{key:t,hidden:e.props.value!==n}))))}function f(e){const t=g(e);return o.createElement("div",{className:(0,i.Z)("tabs-container",k.tabList)},o.createElement(v,(0,n.Z)({},e,t)),o.createElement(b,(0,n.Z)({},e,t)))}function _(e){const t=(0,y.Z)();return o.createElement(f,(0,n.Z)({key:String(t)},e))}},30508:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>M,contentTitle:()=>x,default:()=>N,frontMatter:()=>w,metadata:()=>I,toc:()=>Z});var n=a(87462),o=(a(67294),a(3905)),i=a(74866),l=a(85162),r=a(90814);const s='from clarifai.client.user import User\n#replace your "user_id"\nclient = User(user_id="user_id")\napp = client.create_app(app_id="demo_train", base_workflow="Universal")',d="#importing load_module_dataloader for calling the dataloader object in dataset.py in the local data folder\nfrom clarifai.datasets.upload.utils import load_module_dataloader\n\n\n# Construct the path to the dataset folder\nmodule_path = os.path.join(os.getcwd().split('/models/model_train')[0],'datasets/upload/image_segmentation/coco')\n\n\n# Load the dataloader module using the provided function from your module\ncoco_dataloader = load_module_dataloader(module_path)\n\n# Create a Clarifai dataset with the specified dataset_id (\"image_dataset\")\ndataset = app.create_dataset(dataset_id=\"segmentation_dataset\")\n\n# Upload the dataset using the provided dataloader and get the upload status\ndataset.upload_dataset(dataloader=coco_dataloader)\n",u="print(app.list_trainable_model_types())",p='MODEL_ID = "segmenter"\nMODEL_TYPE_ID = "visual-segmenter"\n\n# Create a model by passing the model name and model type as parameter\nmodel = app.create_model(model_id=MODEL_ID, model_type_id=MODEL_TYPE_ID)\n',m="print(model.list_training_templates())",c="import yaml\nYAML_FILE = 'model_params.yaml'\nmodel_params = model.get_params(template='MMSegmentation_SegFormer',save_to=YAML_FILE)\n# Preview YAML content\nfile = open(YAML_FILE)\ndata = yaml.safe_load(file)\nprint(data)\n",h="# List the concept\nconcepts = [concept.id for concept in app.list_concepts()]\nprint(concepts)",g="#creating dataset version\ndataset_version = dataset.create_version()\ndataset_version_id = dataset_version.version.id\n\n#update params\nmodel.update_params(dataset_id = 'segmentation_dataset', dataset_version_id=dataset_version_id,concepts = concepts, num_epochs = 5)",y='import time\n#Starting the training\nmodel_version_id = model.train()\n\n#Checking the status of training\nwhile True:\n    status = model.training_status(version_id=model_version_id,training_logs=False)\n    if status.code == 21106: #MODEL_TRAINING_FAILED\n        print(status)\n        break\n    elif status.code == 21100: #MODEL_TRAINED\n        print(status)\n        break\n    else:\n        print("Current Status:",status)\n        print("Waiting---")\n        time.sleep(120)\n',k="# Display the predicted masks\nimport cv2\nfrom urllib.request import urlopen\nimport numpy as np\nfrom PIL import Image as PILImage\nfrom io import BytesIO\nimport random\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom google.colab.patches import cv2_imshow\n\nIMAGE_PATH = os.path.join(os.getcwd().split('/models')[0],'datasets/upload/image_segmentation/coco/images/000000424349.jpg')\n\nprediction_response = model.predict_by_filepath(IMAGE_PATH, input_type=\"image\")\n\n# Get the output\nregions = prediction_response.outputs[0].data.regions\n\nimg = cv2.imread(IMAGE_PATH)\nimg=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\nmasks = []\nconcepts = []\nfor region in regions:\n    if region.data.concepts[0].value > 0.05:\n        masks.append(np.array(PILImage.open(BytesIO(region.region_info.mask.image.base64))))\n        concepts.append(region.data.concepts[0].name)\n\n\n# Generate random colors\ncolors = []\nfor i in range(len(masks)):\n    r = random.randint(0,255)\n    g = random.randint(0,255)\n    b = random.randint(0,255)\n    colors.append((b,g,r))\n\n# Map masks to overlays\noverlays = []\nfor i in range(len(masks)):\n    mask = masks[i]\n    color = colors[i]\n\n    overlay = np.zeros_like(img)\n    overlay[mask > 0] = color\n    overlays.append(overlay)\n\n# Overlay masks on original image\noverlayed = np.copy(img)\n\nfor overlay in overlays:\n  # Apply alpha blending\n  cv2.addWeighted(overlay, 0.15, overlayed, 0.85, 0, overlayed)\n\noverlayed = cv2.convertScaleAbs(overlayed, alpha=1.5, beta=50)\n\n\n# Display overlayed image\nimg = overlayed\n# for displaying in google colab or else use cv2.imshow()\ncv2_imshow(img) \n\n# Create legend with colors and concepts\nlegend_items = []\nfor i in range(len(overlays)):\n    color = [c/255 for c in colors[i]]\n    concept = concepts[i]\n    legend_items.append(mpatches.Patch(color=color, label=concept))\n\nplt.legend(handles=legend_items, loc='lower left', bbox_to_anchor=(1.05, 0))\nplt.axis('off')\nplt.show()\n",v="['visual-classifier',\n 'visual-detector',\n 'visual-segmenter',\n 'visual-embedder',\n 'clusterer',\n 'text-classifier',\n 'embedding-classifier',\n 'text-to-text']",b="['MMSegmentation', 'MMSegmentation_SegFormer']",f="{'dataset_id': '',\n 'dataset_version_id': '',\n 'concepts': [],\n 'train_params': {'invalid_data_tolerance_percent': 5.0,\n  'template': 'MMSegmentation_SegFormer',\n  'image_size': [520.0],\n  'batch_size': 2.0,\n  'num_epochs': 1.0,\n  'per_item_lrate': 7.5e-06,\n  'pretrained_weights': 'ade20k'}}\n",_="['id-chair',\n 'id-cup',\n 'id-couch',\n 'id-baseballbat',\n 'id-kite',\n 'id-person',\n 'id-elephant',\n 'id-cellphone',\n 'id-handbag',\n 'id-cat',\n 'id-toilet',\n 'id-laptop',\n 'id-diningtable',\n 'id-keyboard',\n 'id-mouse',\n 'id-oven',\n 'id-pizza',\n 'id-clock']\n",T="{'dataset_id': 'segmentation_dataset',\n 'dataset_version_id': '43cdc090797c41f19bb420ab6e4baf0c',\n 'concepts': ['id-chair',\n  'id-cup',\n  'id-couch',\n  'id-baseballbat',\n  'id-kite',\n  'id-person',\n  'id-elephant',\n  'id-cellphone',\n  'id-handbag',\n  'id-cat',\n  'id-toilet',\n  'id-laptop',\n  'id-diningtable',\n  'id-keyboard',\n  'id-mouse',\n  'id-oven',\n  'id-pizza',\n  'id-clock'],\n 'train_params': {'invalid_data_tolerance_percent': 5.0,\n  'template': 'MMSegmentation_SegFormer',\n  'image_size': [520.0],\n  'batch_size': 2.0,\n  'num_epochs': 5,\n  'per_item_lrate': 7.5e-06,\n  'pretrained_weights': 'ade20k'}}",w={},x="Visual Segmenter",I={unversionedId:"python-sdk/Model Training Tutorial/visual_segmenter",id:"python-sdk/Model Training Tutorial/visual_segmenter",title:"Visual Segmenter",description:"A visual segmenter model is used in computer vision to partition images or videos into distinct regions or objects. Through sophisticated techniques like pixel-based analysis, region grouping, edge detection, or deep learning, a visual segmenter can identify boundaries and patterns within visual data, effectively dividing the image into meaningful segments. You can learn more about Visual Segmenter here.",source:"@site/docs/python-sdk/Model Training Tutorial/visual_segmenter.md",sourceDirName:"python-sdk/Model Training Tutorial",slug:"/python-sdk/Model Training Tutorial/visual_segmenter",permalink:"/python-sdk/Model Training Tutorial/visual_segmenter",draft:!1,editUrl:"https://github.com/Clarifai/docs/blob/main/docs/python-sdk/Model Training Tutorial/visual_segmenter.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Visual Embedder",permalink:"/python-sdk/Model Training Tutorial/visual_embedder"},next:{title:"SDK Notebook Examples",permalink:"/python-sdk/notebook-examples"}},M={},Z=[{value:"App Creation",id:"app-creation",level:2},{value:"Dataset Upload",id:"dataset-upload",level:2},{value:"Choose The Model Type",id:"choose-the-model-type",level:2},{value:"Model Creation",id:"model-creation",level:2},{value:"Template Selection",id:"template-selection",level:2},{value:"Setup Model Parameters",id:"setup-model-parameters",level:2},{value:"Initiate Model Training",id:"initiate-model-training",level:2},{value:"Model Prediction",id:"model-prediction",level:2}],C={toc:Z},E="wrapper";function N(e){let{components:t,...a}=e;return(0,o.kt)(E,(0,n.Z)({},C,a,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"visual-segmenter"},"Visual Segmenter"),(0,o.kt)("p",null,"A visual segmenter model is used in computer vision to partition images or videos into distinct regions or objects. Through sophisticated techniques like pixel-based analysis, region grouping, edge detection, or deep learning, a visual segmenter can identify boundaries and patterns within visual data, effectively dividing the image into meaningful segments. You can learn more about Visual Segmenter ",(0,o.kt)("a",{parentName:"p",href:"https://docs.clarifai.com/portal-guide/model/model-types/visual-segmenter"},"here"),"."),(0,o.kt)("h2",{id:"app-creation"},"App Creation"),(0,o.kt)("p",null,"The first part of model training includes the creation of an app under which the training process takes place. Here we are creating an app with the app id as \u201cdemo_train\u201d and the base workflow is set as \u201cUniversal\u201d. You can change the base workflows to Empty, Universal, Language Understanding, and General according to your use case."),(0,o.kt)(i.Z,{mdxType:"Tabs"},(0,o.kt)(l.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,o.kt)(r.Z,{className:"language-python",mdxType:"CodeBlock"},s))),(0,o.kt)("h2",{id:"dataset-upload"},"Dataset Upload"),(0,o.kt)("p",null,"The next step involves dataset upload. You can upload the dataset to your app so that the model accepts the data directly from the platform. The data used for training in this tutorial is available in the examples repository you have cloned."),(0,o.kt)(i.Z,{mdxType:"Tabs"},(0,o.kt)(l.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,o.kt)(r.Z,{className:"language-python",mdxType:"CodeBlock"},d))),(0,o.kt)("p",null,"If you have followed the steps correctly you should receive an output that looks like this,"),(0,o.kt)("details",null,(0,o.kt)("summary",null,"Output"),(0,o.kt)("img",{src:"/img/python-sdk/vs_du.png",width:"700",height:"700"})),(0,o.kt)("h2",{id:"choose-the-model-type"},"Choose The Model Type"),(0,o.kt)("p",null,"First let's list the all available trainable model types in the platform,"),(0,o.kt)(i.Z,{mdxType:"Tabs"},(0,o.kt)(l.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,o.kt)(r.Z,{className:"language-python",mdxType:"CodeBlock"},u))),(0,o.kt)("details",null,(0,o.kt)("summary",null,"Output"),(0,o.kt)(r.Z,{className:"language-text",mdxType:"CodeBlock"},v)),(0,o.kt)("p",null,"Click ",(0,o.kt)("a",{parentName:"p",href:"https://docs.clarifai.com/portal-guide/model/model-types/"},"here")," to know more about Clarifai Model Types."),(0,o.kt)("h2",{id:"model-creation"},"Model Creation"),(0,o.kt)("p",null,"From the above list of model types we are going to choose visual-segmenter as it is similar to our use case. Now let's create a model with the above model type."),(0,o.kt)(i.Z,{mdxType:"Tabs"},(0,o.kt)(l.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,o.kt)(r.Z,{className:"language-python",mdxType:"CodeBlock"},p))),(0,o.kt)("details",null,(0,o.kt)("summary",null,"Output"),(0,o.kt)("img",{src:"/img/python-sdk/vs_mc.png",width:"700",height:"700"})),(0,o.kt)("h2",{id:"template-selection"},"Template Selection"),(0,o.kt)("p",null,"Inside the Clarifiai platform there is a template feature. Templates give you the control to choose the specific architecture used by your neural network, as well as define a set of hyperparameters you can use to fine-tune the way your model learns. We are going to choose the ",(0,o.kt)("inlineCode",{parentName:"p"},"'MMSegmentation_SegFormer' "),"template for training our model."),(0,o.kt)(i.Z,{mdxType:"Tabs"},(0,o.kt)(l.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,o.kt)(r.Z,{className:"language-python",mdxType:"CodeBlock"},m))),(0,o.kt)("details",null,(0,o.kt)("summary",null,"Output"),(0,o.kt)(r.Z,{className:"language-text",mdxType:"CodeBlock"},b)),(0,o.kt)("h2",{id:"setup-model-parameters"},"Setup Model Parameters"),(0,o.kt)("p",null,"You can save the model parameters into a YAML file so that it can passed on to the model while initiating training."),(0,o.kt)(i.Z,{mdxType:"Tabs"},(0,o.kt)(l.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,o.kt)(r.Z,{className:"language-python",mdxType:"CodeBlock"},c))),(0,o.kt)("details",null,(0,o.kt)("summary",null,"Output"),(0,o.kt)(r.Z,{className:"language-text",mdxType:"CodeBlock"},f)),"You can edit the YAML file according to our needs and then load the files again for model training. Below is an example of the edits made to the YAML file, but first we are going to list the concepts available in app.",(0,o.kt)(i.Z,{mdxType:"Tabs"},(0,o.kt)(l.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,o.kt)(r.Z,{className:"language-python",mdxType:"CodeBlock"},h))),(0,o.kt)("details",null,(0,o.kt)("summary",null,"Output"),(0,o.kt)(r.Z,{className:"language-text",mdxType:"CodeBlock"},_)),"Now we will create a dataset version and then invoke the model.update_params() method to change the model parameters.",(0,o.kt)(i.Z,{mdxType:"Tabs"},(0,o.kt)(l.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,o.kt)(r.Z,{className:"language-python",mdxType:"CodeBlock"},g))),(0,o.kt)("details",null,(0,o.kt)("summary",null,"Output"),(0,o.kt)(r.Z,{className:"language-text",mdxType:"CodeBlock"},T)),(0,o.kt)("h2",{id:"initiate-model-training"},"Initiate Model Training"),(0,o.kt)("p",null,"We can initiate the model training by calling the model.train() method. The Clarifai Python SDK also offers features like showing training status and saving training logs in a local file."),(0,o.kt)("admonition",{type:"note"},(0,o.kt)("p",{parentName:"admonition"},"If the status code is 'MODEL-TRAINED', then the user can know the Model is Trained and ready to use.")),(0,o.kt)(i.Z,{mdxType:"Tabs"},(0,o.kt)(l.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,o.kt)(r.Z,{className:"language-python",mdxType:"CodeBlock"},y))),(0,o.kt)("details",null,(0,o.kt)("summary",null,"Output"),(0,o.kt)("img",{src:"/img/python-sdk/vs_imt.png",width:"700",height:"700"})),(0,o.kt)("h2",{id:"model-prediction"},"Model Prediction"),(0,o.kt)("p",null,"Since the model is trained and ready let\u2019s run some predictions to view the model performance,"),(0,o.kt)(i.Z,{mdxType:"Tabs"},(0,o.kt)(l.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,o.kt)(r.Z,{className:"language-python",mdxType:"CodeBlock"},k))),(0,o.kt)("admonition",{type:"info"},(0,o.kt)("p",{parentName:"admonition"},"The performance of the model can be further improved by training it for more number of epochs")),(0,o.kt)("details",null,(0,o.kt)("summary",null,"Image Output"),(0,o.kt)("img",{src:"/img/python-sdk/vs_mp.png",width:"700",height:"700"})))}N.isMDXComponent=!0}}]);
"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[7978],{15363:(e,t,o)=>{o.r(t),o.d(t,{assets:()=>d,contentTitle:()=>r,default:()=>c,frontMatter:()=>i,metadata:()=>a,toc:()=>l});var n=o(74848),s=o(28453);const i={description:"Use AI to help you build AI. Auto annotation uses your model predictions to label your training data.",sidebar_position:1},r="Auto-Annotation",a={id:"portal-guide/workflows/common-workflows/auto-annotation-walkthrough",title:"Auto-Annotation",description:"Use AI to help you build AI. Auto annotation uses your model predictions to label your training data.",source:"@site/docs/portal-guide/workflows/common-workflows/auto-annotation-walkthrough.md",sourceDirName:"portal-guide/workflows/common-workflows",slug:"/portal-guide/workflows/common-workflows/auto-annotation-walkthrough",permalink:"/portal-guide/workflows/common-workflows/auto-annotation-walkthrough",draft:!1,unlisted:!1,editUrl:"https://github.com/Clarifai/docs/blob/main/docs/portal-guide/workflows/common-workflows/auto-annotation-walkthrough.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{description:"Use AI to help you build AI. Auto annotation uses your model predictions to label your training data.",sidebar_position:1},sidebar:"tutorialSidebar",previous:{title:"Common Workflows",permalink:"/portal-guide/workflows/common-workflows"},next:{title:"Visual Text Recognition",permalink:"/portal-guide/workflows/common-workflows/visual-text-recognition-walkthrough"}},d={},l=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Step 1: Set up Visual Detector",id:"step-1-set-up-visual-detector",level:2},{value:"Step 2: Set up Region Thresholder",id:"step-2-set-up-region-thresholder",level:2},{value:"Step 3: Set up Annotation Writer",id:"step-3-set-up-annotation-writer",level:2},{value:"Step 4: Set up Low Confidence Score Flow",id:"step-4-set-up-low-confidence-score-flow",level:2},{value:"Step 5: Save Workflow",id:"step-5-save-workflow",level:2},{value:"Step 6: Use Workflow",id:"step-6-use-workflow",level:2}];function h(e){const t={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(t.h1,{id:"auto-annotation",children:"Auto-Annotation"}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.strong,{children:"Use AI to help you build AI. Auto annotation uses your model predictions to label your training data"})}),"\n",(0,n.jsx)("hr",{}),"\n",(0,n.jsx)(t.p,{children:"This tutorial demonstrates how auto-annotation workflows can be configured within the Clarifai platform. With auto-annotation, you can use model predictions to label your inputs. Auto-annotation can help you to prepare training data, or assign other useful labels and metadata to your inputs."}),"\n",(0,n.jsx)(t.p,{children:"When a concept is predicted by a model, it is predicted with a confidence score between 0 and 1. For example, when your model predictions are confident (close to 1), you can have your data automatically labeled with that concept. When your predictions are less-than-confident, you can have your input sent to a human being for review."}),"\n",(0,n.jsx)(t.p,{children:"This enables you to speed-up and scale-up your annotation process while ensuring quality standards."}),"\n",(0,n.jsx)(t.admonition,{title:"Workflows",type:"tip",children:(0,n.jsx)(t.p,{children:"In the Clarifai platform, the outputs from one model can be used as inputs to another model. This forms a workflow. Different models accept and produce different types of inputs and outputs."})}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{src:o(87424).A+"",width:"519",height:"331"})}),"\n",(0,n.jsx)(t.admonition,{title:"objective",type:"warning",children:(0,n.jsx)(t.p,{children:"In this tutorial, we'll create a workflow that detects bounding box regions in images of cats and dogs. Once a certain threshold is met, the workflow will automatically generate annotations for these detected regions. If the threshold is not met, the annotation will be marked as pending review."})}),"\n",(0,n.jsx)(t.p,{children:"Here's what our final workflow will look like:"}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{src:o(91100).A+"",width:"1114",height:"698"})}),"\n",(0,n.jsx)(t.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsxs)(t.li,{children:["Create an ",(0,n.jsx)(t.a,{href:"https://docs.clarifai.com/clarifai-basics/applications/create-an-application",children:"application"}),", add images of cats and dogs to a ",(0,n.jsx)(t.a,{href:"https://docs.clarifai.com/portal-guide/datasets/create-get-update-delete",children:"dataset"})," in the app, and add ",(0,n.jsx)(t.a,{href:"https://docs.clarifai.com/portal-guide/annotate/label-types#detection",children:"bounding box labels"})," of ",(0,n.jsx)(t.code,{children:"cat"})," and ",(0,n.jsx)(t.code,{children:"dog"})," to the images, respectively.\nYou could use the following images:"]}),"\n"]}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-text",children:"https://samples.clarifai.com/dog1.jpeg\nhttps://samples.clarifai.com/dog2.jpeg\nhttps://samples.clarifai.com/dog3.jpeg\nhttps://samples.clarifai.com/cat1.jpeg\nhttps://samples.clarifai.com/cat2.jpeg\nhttps://samples.clarifai.com/cat3.jpeg\n"})}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsxs)(t.li,{children:["Create a ",(0,n.jsx)(t.a,{href:"https://docs.clarifai.com/portal-guide/model/model-types/visual-detector",children:"visual detector"})," model and train it with the ",(0,n.jsx)(t.code,{children:"cat"})," and ",(0,n.jsx)(t.code,{children:"dog"})," concepts."]}),"\n",(0,n.jsxs)(t.li,{children:["Create a ",(0,n.jsx)(t.a,{href:"https://docs.clarifai.com/portal-guide/annotate/create-a-task/",children:"labeling task"}),". Remember to choose ",(0,n.jsx)(t.code,{children:"Detection"})," as the modeling objective. Then, go to the ",(0,n.jsx)(t.strong,{children:"Tasks"})," listing page and copy the ID of the task."]}),"\n"]}),"\n",(0,n.jsx)(t.h2,{id:"step-1-set-up-visual-detector",children:"Step 1: Set up Visual Detector"}),"\n",(0,n.jsxs)(t.admonition,{title:"visual detector",type:"caution",children:[(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Input"}),": ",(0,n.jsx)(t.code,{children:"image"})]}),(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Output"}),":  ",(0,n.jsx)(t.code,{children:"regions[\u2026].data.concepts"}),", ",(0,n.jsx)(t.code,{children:"regions[\u2026].region_info.bounding_box"})]}),(0,n.jsx)(t.p,{children:"The visual detector model will detect bounding box regions in images and then classify objects within the boxes."})]}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"a."})," Go to the ",(0,n.jsx)(t.a,{href:"https://docs.clarifai.com/portal-guide/workflows/working_with_workflows/",children:"workflow builder"})," page."]}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"b."})," Search for the ",(0,n.jsx)(t.code,{children:"visual-detector"})," node in the left sidebar of the page."]}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"c."})," Drag it onto the empty workspace and connect it to the ",(0,n.jsx)(t.code,{children:"IN"})," element."]}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"d."})," Use the pop-up that appears in the right sidebar to search for the detection model you created and add it to the node. After selecting the model, we'll use the default settings for the other output configuration options for the purpose of this illustration."]}),"\n",(0,n.jsx)(t.admonition,{type:"note",children:(0,n.jsx)(t.p,{children:"You can use the tools on the left side of the workspace pane to manage the workflow creation process. These tools allow you to zoom in and out, fit the view, arrange the workflow, reset the workspace, and more."})}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{src:o(50676).A+"",width:"1909",height:"834"})}),"\n",(0,n.jsx)(t.h2,{id:"step-2-set-up-region-thresholder",children:"Step 2: Set up Region Thresholder"}),"\n",(0,n.jsxs)(t.admonition,{title:"Region Thresholder",type:"caution",children:[(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Input"}),": ",(0,n.jsx)(t.code,{children:"regions[\u2026].data.concepts"})]}),(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Output"}),":  ",(0,n.jsx)(t.code,{children:"regions[\u2026].data.concepts"})]}),(0,n.jsxs)(t.p,{children:['This operator allows you to filter regions based on the concepts they contain using specific thresholds for each concept and an overall operator (>, >=, =, <=, or <). In this example, we use the " > " (',(0,n.jsx)(t.code,{children:"GREATER_THAN"}),") threshold type. Thus, only the ",(0,n.jsx)(t.code,{children:"cat"})," and ",(0,n.jsx)(t.code,{children:"dog"})," concepts outputted by the visual detector model above a certain threshold will be processed further downstream."]})]}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"a."})," Search for the ",(0,n.jsx)(t.code,{children:"region-thresholder"})," node in the left sidebar and drag it onto the workspace. Connect it to the ",(0,n.jsx)(t.code,{children:"visual-detector"})," node."]}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{src:o(25167).A+"",width:"1889",height:"826"})}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"b."})," Click the ",(0,n.jsx)(t.strong,{children:"SELECT CONCEPTS"})," button in the right sidebar."]}),"\n",(0,n.jsxs)(t.p,{children:["In the window that appears, select the relevant concepts already existing in your application. For this example, let's select the ",(0,n.jsx)(t.code,{children:"cat"})," and ",(0,n.jsx)(t.code,{children:"dog"})," concepts, and use the slider to set their threshold values to 0.95 each. This threshold will determine which annotations are accepted and which are set aside for follow-up review."]}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{src:o(86150).A+"",width:"1903",height:"890"})}),"\n",(0,n.jsxs)(t.p,{children:["Click the ",(0,n.jsx)(t.strong,{children:"OK"})," button to save the changes. You'll see the selected concepts highlighted in the right sidebar, along with their threshold values."]}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{src:o(35545).A+"",width:"1441",height:"819"})}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"c."})," Select the ",(0,n.jsx)(t.code,{children:"concept_threshold_type"})," as ",(0,n.jsx)(t.code,{children:"GREATER_THAN"}),". Keep the other configuration options set to their default values."]}),"\n",(0,n.jsx)(t.h2,{id:"step-3-set-up-annotation-writer",children:"Step 3: Set up Annotation Writer"}),"\n",(0,n.jsxs)(t.admonition,{title:"Annotation Writer",type:"caution",children:[(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Input"}),": ",(0,n.jsx)(t.code,{children:"Any"})]}),(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Output"}),":  ",(0,n.jsx)(t.code,{children:"Any"})]}),(0,n.jsx)(t.p,{children:"Annotation writer allows you to write the input data to the database in the form of an annotation with a specified status as if a specific user created the annotation."})]}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"a."})," Search for the ",(0,n.jsx)(t.code,{children:"annotation-writer"})," node in the left sidebar and drag it onto the workspace. Connect it to the ",(0,n.jsx)(t.code,{children:"region-thresholder"})," node."]}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"b."})," In the right sidebar, set up the following output configurations:"]}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsxs)(t.li,{children:["Set the ",(0,n.jsx)(t.code,{children:"annotation_status"})," as ",(0,n.jsx)(t.code,{children:"ANNOTATION_SUCCESS"}),". This will write the annotations with the ",(0,n.jsx)(t.a,{href:"https://docs.clarifai.com/api-guide/advanced-topics/status-codes#annotation-related-codes-24xxx",children:(0,n.jsx)(t.code,{children:"ANNOTATION_SUCCESS"})})," status."]}),"\n",(0,n.jsxs)(t.li,{children:["Set the ",(0,n.jsx)(t.code,{children:"annotation_user_id"}),". This is the ",(0,n.jsx)(t.code,{children:"user_id"}),"for which to write the annotations on their behalf as if they manually did the work themselves. You could also choose your own ",(0,n.jsx)(t.code,{children:"user_id"}),"."]}),"\n",(0,n.jsxs)(t.li,{children:["In this example, let's leave the ",(0,n.jsx)(t.code,{children:"annotation_info"})," field empty."]}),"\n",(0,n.jsxs)(t.li,{children:["Set the ",(0,n.jsx)(t.code,{children:"task_id"}),". This is the ID the task annotation work belongs to. You can retrieve it from the ",(0,n.jsx)(t.strong,{children:"Tasks"})," listing page \u2014 as earlier mentioned."]}),"\n"]}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{src:o(29296).A+"",width:"1887",height:"834"})}),"\n",(0,n.jsx)(t.h2,{id:"step-4-set-up-low-confidence-score-flow",children:"Step 4: Set up Low Confidence Score Flow"}),"\n",(0,n.jsxs)(t.p,{children:["In the flow described above, the visual detector model identifies bounding box regions for the provided inputs. These regions are then sent to the region thresholder node. The ",(0,n.jsx)(t.code,{children:"GREATER_THAN"})," operator filters out regions that do not meet the threshold and forwards the remaining regions to the annotation writer model. Bounding boxes that pass this filter are written to the input with an ",(0,n.jsx)(t.code,{children:"ANNOTATION_SUCCESS"})," status."]}),"\n",(0,n.jsxs)(t.p,{children:["To capture predictions with low prediction scores, we'll set up a separate region thresholder node that uses the ",(0,n.jsx)(t.code,{children:"LESS_THAN"})," operator. This operator will filter out regions with prediction scores below the threshold and write these regions to the input with an ",(0,n.jsx)(t.code,{children:"ANNOTATION_AWAITING_REVIEW"})," status."]}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"a."})," Search for the ",(0,n.jsx)(t.code,{children:"region-thresholder"})," node in the left sidebar and drag it onto the workspace. Connect it to the ",(0,n.jsx)(t.code,{children:"visual-detector"})," node. Also, click the ",(0,n.jsx)(t.strong,{children:"SELECT CONCEPTS"})," button in the right sidebar, and set the threshold values for the ",(0,n.jsx)(t.code,{children:"cat"})," and ",(0,n.jsx)(t.code,{children:"dog"}),"concepts to 0.95 each. Lastly, set the ",(0,n.jsx)(t.code,{children:"concept_threshold_type"})," as ",(0,n.jsx)(t.code,{children:"LESS_THAN_OR_EQUAL"}),"."]}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{src:o(587).A+"",width:"1874",height:"812"})}),"\n",(0,n.jsx)(t.admonition,{title:"LESS_THAN_OR_EQUAL",type:"caution",children:(0,n.jsxs)(t.p,{children:["The ",(0,n.jsx)(t.code,{children:"LESS_THAN_OR_EQUAL"})," (<=) operator ensures that all values that are either less than or exactly equal to the specified threshold are included in the result. In this case, if we set a threshold of 0.95 using the operator, any value that is 0.95 or lower will meet the condition."]})}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"b."})," Search for another ",(0,n.jsx)(t.code,{children:"region-thresholder"})," node in the left sidebar and drag it onto the workspace. Connect it to the previous ",(0,n.jsx)(t.code,{children:"region-thresholder"})," node."]}),"\n",(0,n.jsxs)(t.p,{children:["Also, click the ",(0,n.jsx)(t.strong,{children:"SELECT CONCEPTS"})," button in the right sidebar, and set the threshold values for the ",(0,n.jsx)(t.code,{children:"cat"})," and ",(0,n.jsx)(t.code,{children:"dog"}),"concepts to 0.50 each. Lastly, set the ",(0,n.jsx)(t.code,{children:"concept_threshold_type"})," as ",(0,n.jsx)(t.code,{children:"GREATER_THAN"}),". This ensures that only values that are higher than the 0.50 threshold are included in the result."]}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{src:o(79458).A+"",width:"1880",height:"805"})}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"c."})," Search for an ",(0,n.jsx)(t.code,{children:"annotation-writer"})," node in the left sidebar and drag it onto the workspace. Connect it to the previous ",(0,n.jsx)(t.code,{children:"region-thresholder"})," node."]}),"\n",(0,n.jsxs)(t.p,{children:["In the right sidebar, set up ",(0,n.jsx)(t.code,{children:"annotation_status"})," as ",(0,n.jsx)(t.code,{children:"ANNOTATION_AWAITING_REVIEW"}),". This will write the annotations with a pending review status. Also, set the values for ",(0,n.jsx)(t.code,{children:"annotation_user_id"})," and ",(0,n.jsx)(t.code,{children:"task_id"})," \u2014 as earlier described."]}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{src:o(73989).A+"",width:"1844",height:"829"})}),"\n",(0,n.jsx)(t.h2,{id:"step-5-save-workflow",children:"Step 5: Save Workflow"}),"\n",(0,n.jsxs)(t.p,{children:["Lastly, click the ",(0,n.jsx)(t.strong,{children:"Save Workflow"})," button at the upper-right corner of the page."]}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{src:o(91100).A+"",width:"1114",height:"698"})}),"\n",(0,n.jsx)(t.p,{children:"Your workflow is now ready for use!"}),"\n",(0,n.jsx)(t.h2,{id:"step-6-use-workflow",children:"Step 6: Use Workflow"}),"\n",(0,n.jsxs)(t.p,{children:["Go to your app's settings page and ",(0,n.jsx)(t.a,{href:"https://docs.clarifai.com/portal-guide/workflows/base-workflows/#how-to-change-a-base-workflow",children:"change the Base Workflow"})," to the workflow you just created. This ensures that the workflow runs every time you add an input to your app."]}),"\n",(0,n.jsxs)(t.p,{children:["For example, adding a new image of a dog will trigger the auto-annotation process Then, a bounding box label will be added to the image and it will be assigned with the ",(0,n.jsx)(t.code,{children:"dog"})," concept."]}),"\n",(0,n.jsx)(t.p,{children:"You can view the annotation in the Input-Viewer page:"}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{src:o(13518).A+"",width:"1905",height:"907"})}),"\n",(0,n.jsx)(t.p,{children:"That's it!"})]})}function c(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(h,{...e})}):h(e)}},50676:(e,t,o)=>{o.d(t,{A:()=>n});const n=o.p+"assets/images/auto-annotation-1-dfa1b8458b91f573ad397f2b199ad18b.png"},13518:(e,t,o)=>{o.d(t,{A:()=>n});const n=o.p+"assets/images/auto-annotation-10-22b9526105f345bd2ca40be98f2f278d.png"},25167:(e,t,o)=>{o.d(t,{A:()=>n});const n=o.p+"assets/images/auto-annotation-2-f04c071a3cb11704f394f7ef292cb56e.png"},86150:(e,t,o)=>{o.d(t,{A:()=>n});const n=o.p+"assets/images/auto-annotation-3-bfd52fe757b67472375f2a7cc0cfa92b.png"},35545:(e,t,o)=>{o.d(t,{A:()=>n});const n=o.p+"assets/images/auto-annotation-4-98d8aa1a0f7d44a0eacfb80ab57ceaa1.png"},29296:(e,t,o)=>{o.d(t,{A:()=>n});const n=o.p+"assets/images/auto-annotation-5-61236006c4427cb150d0cadca32c02cb.png"},587:(e,t,o)=>{o.d(t,{A:()=>n});const n=o.p+"assets/images/auto-annotation-6-42856e57397090855867eaab6e959268.png"},79458:(e,t,o)=>{o.d(t,{A:()=>n});const n=o.p+"assets/images/auto-annotation-7-328be1e18e0cb69997fb1fd027d5101b.png"},73989:(e,t,o)=>{o.d(t,{A:()=>n});const n=o.p+"assets/images/auto-annotation-8-fb6a79f1907313ced686ad46b00fdbc0.png"},91100:(e,t,o)=>{o.d(t,{A:()=>n});const n=o.p+"assets/images/auto-annotation-9-fb1310ea1f7acae92d3916ba3a8c69a1.png"},87424:(e,t,o)=>{o.d(t,{A:()=>n});const n=o.p+"assets/images/auto_annotation-2ab768fe40e5dee612271657f5764875.jpg"},28453:(e,t,o)=>{o.d(t,{R:()=>r,x:()=>a});var n=o(96540);const s={},i=n.createContext(s);function r(e){const t=n.useContext(i);return n.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function a(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),n.createElement(i.Provider,{value:t},e.children)}}}]);
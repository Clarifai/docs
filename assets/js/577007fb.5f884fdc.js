"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[3630],{3905:(e,t,a)=>{a.d(t,{Zo:()=>p,kt:()=>h});var n=a(67294);function i(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function r(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){i(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function l(e,t){if(null==e)return{};var a,n,i=function(e,t){if(null==e)return{};var a,n,i={},o=Object.keys(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||(i[a]=e[a]);return i}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(i[a]=e[a])}return i}var s=n.createContext({}),d=function(e){var t=n.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):r(r({},t),e)),a},p=function(e){var t=d(e.components);return n.createElement(s.Provider,{value:t},e.children)},u="mdxType",c={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},m=n.forwardRef((function(e,t){var a=e.components,i=e.mdxType,o=e.originalType,s=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),u=d(a),m=i,h=u["".concat(s,".").concat(m)]||u[m]||c[m]||o;return a?n.createElement(h,r(r({ref:t},p),{},{components:a})):n.createElement(h,r({ref:t},p))}));function h(e,t){var a=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var o=a.length,r=new Array(o);r[0]=m;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[u]="string"==typeof e?e:i,r[1]=l;for(var d=2;d<o;d++)r[d]=a[d];return n.createElement.apply(null,r)}return n.createElement.apply(null,a)}m.displayName="MDXCreateElement"},23824:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>s,contentTitle:()=>r,default:()=>c,frontMatter:()=>o,metadata:()=>l,toc:()=>d});var n=a(87462),i=(a(67294),a(3905));const o={description:"Frequently asked questions on model training",sidebar_position:4},r="Model Training FAQs",l={unversionedId:"portal-guide/model/training-faqs",id:"portal-guide/model/training-faqs",title:"Model Training FAQs",description:"Frequently asked questions on model training",source:"@site/docs/portal-guide/model/training-faqs.md",sourceDirName:"portal-guide/model",slug:"/portal-guide/model/training-faqs",permalink:"/portal-guide/model/training-faqs",draft:!1,tags:[],version:"current",sidebarPosition:4,frontMatter:{description:"Frequently asked questions on model training",sidebar_position:4},sidebar:"tutorialSidebar",previous:{title:"Training Basics",permalink:"/portal-guide/model/training-basics"},next:{title:"Model Types",permalink:"/portal-guide/model/model-types"}},s={},d=[{value:"Framing the right problem: what should I predict?",id:"framing-the-right-problem-what-should-i-predict",level:2},{value:"What are the key steps in the model building process?",id:"what-are-the-key-steps-in-the-model-building-process",level:2},{value:"What are the differences between the various model types?",id:"what-are-the-differences-between-the-various-model-types",level:2},{value:"What is a \u201cgood\u201d training dataset, and how do data quality and quantity affect model performance?",id:"what-is-a-good-training-dataset-and-how-do-data-quality-and-quantity-affect-model-performance",level:2},{value:"What is the difference between bias and variance in model training?",id:"what-is-the-difference-between-bias-and-variance-in-model-training",level:2},{value:"Is there a checklist I can use for training custom models?",id:"is-there-a-checklist-i-can-use-for-training-custom-models",level:2},{value:"What is \u201cground truth\u201d and how do we use it to dictate model building?",id:"what-is-ground-truth-and-how-do-we-use-it-to-dictate-model-building",level:2},{value:"How do we train for closed-set and open-set recognition?",id:"how-do-we-train-for-closed-set-and-open-set-recognition",level:2}],p={toc:d},u="wrapper";function c(e){let{components:t,...o}=e;return(0,i.kt)(u,(0,n.Z)({},p,o,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"model-training-faqs"},"Model Training FAQs"),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Frequently asked questions on model training")),(0,i.kt)("hr",null),(0,i.kt)("h2",{id:"framing-the-right-problem-what-should-i-predict"},"Framing the right problem: what should I predict?"),(0,i.kt)("p",null,"It\u2019s critical to create visually distinctive concepts with a curated taxonomy (aka oncology) based on the business problem, data collected, and success measures derived from model performance metrics.  "),(0,i.kt)("table",null,(0,i.kt)("thead",{parentName:"table"},(0,i.kt)("tr",{parentName:"thead"},(0,i.kt)("th",{parentName:"tr",align:null},(0,i.kt)("img",{alt:"framing the right problem",src:a(59116).Z,width:"338",height:"334"})),(0,i.kt)("th",{parentName:"tr",align:null},(0,i.kt)("img",{alt:"framing the right problem concepts",src:a(34990).Z,width:"348",height:"532"}))))),(0,i.kt)("p",null,"For example, if you take a look at the above image marked with client metadata, you can see that the data was labeled for the presence of something, rather than the subject of the entire image. "),(0,i.kt)("p",null,"This is perfect for humans, but definitely not a scenario in which a model would easily discern concepts."),(0,i.kt)("h2",{id:"what-are-the-key-steps-in-the-model-building-process"},"What are the key steps in the model building process?"),(0,i.kt)("p",null,"These are the six key steps in the process for building new models:"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("strong",{parentName:"li"},"Gathering data"),"\u2014Involves collecting data either from Clarifai\u2019s data reserves or collecting your own unique datasets."),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("strong",{parentName:"li"},"Building a visual dictionary"),"\u2014Involves defining the process and the success criteria."),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("strong",{parentName:"li"},"Preparing or \u2018cleaning\u2019 the data"),"\u2014This step prepares the data for use in training the model. You can recognize important variables or visual features, and check for data imbalances that could impact the predictions of your model."),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("strong",{parentName:"li"},"Training the model"),"\u2014Involves using a process called training to \u201cteach\u201d the model what it will eventually predict on, based on the data you prepared. This happens over many interactions, improving accuracy."),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("strong",{parentName:"li"},"Evaluating or testing the model"),"\u2014Similar to how someone learns any skill, practice makes perfect, and this step involves using evaluation data that the model has never seen before to test its effectiveness."),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("strong",{parentName:"li"},"Tuning parameters"),"\u2014Often considered more of an art than a hard-and-fast scientific or mathematical ruling, this step involves choosing the best prediction thresholds, or parameters, to determine what is a \u201ccorrect\u201d or \u201cincorrect\u201d prediction.")),(0,i.kt)("h2",{id:"what-are-the-differences-between-the-various-model-types"},"What are the differences between the various model types?"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"Visual classification model"),"\u2014Predicts and then assigns classification of one or more objects | ",(0,i.kt)("em",{parentName:"p"},"\u201cThis is an image of an eagle and an octopus.\u201d"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"Visual detection model"),"\u2014Given an image, allows a user to classify and find the location of an object | ",(0,i.kt)("em",{parentName:"p"},"\u201cThe octopus is located at this bounding box in the image.\u201d"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"Visual search model"),"\u2014Allows a user to detect an object or subject in different scenarios and settings, and then define the object | ",(0,i.kt)("em",{parentName:"p"},"\u201cI will use this image of an octopus to find other octopuses.\u201d"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"Visual segmentation model"),"\u2014Given an image, allows a user to find all pixels (instead of a bounding box in the case of detection model) | ",(0,i.kt)("em",{parentName:"p"},"\u201cThe octopus\u2019s precise shape (aka mask) is defined by all pixels in the image.\u201d"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"Visual anomaly model"),"\u2014Allow users to find anomalous examples with only normal examples in training | ",(0,i.kt)("em",{parentName:"p"},"\u201cI will find an octopus holding spaghetti and meatballs among all images of octopuses (with very similar ocean background).\u201d"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"Text classification model"),"\u2014Given a piece of a text passage, allows a user to classify one or more categories | ",(0,i.kt)("em",{parentName:"p"},"\u201cOctopus is very intelligent\u201d = good; \u201cOctopus is very stupid\u201d = bad")))),(0,i.kt)("h2",{id:"what-is-a-good-training-dataset-and-how-do-data-quality-and-quantity-affect-model-performance"},"What is a \u201cgood\u201d training dataset, and how do data quality and quantity affect model performance?"),(0,i.kt)("p",null,"A training dataset refers to the data that is used as positive inputs for concepts in a model. A \u201cgood\u201d training dataset will set the model to make predictions as close to the world as a user would see it.  "),(0,i.kt)("p",null,"Data ",(0,i.kt)("strong",{parentName:"p"},"quality"),", in the context of machine learning, is data that:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"visually adheres to concept descriptions laid out in a taxonomy, and;"),(0,i.kt)("li",{parentName:"ul"},"reflects the expected user\u2019s data for the model\u2019s intended use case.")),(0,i.kt)("p",null,"Data ",(0,i.kt)("strong",{parentName:"p"},"quantity")," casts a wider net for model understanding. With more data, we can be more specific and more granular with our concepts."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Example:")),(0,i.kt)("p",null,"For a Guinness beer\u2019s \u201cPerfect Pint\u201d model, the client had a few thousand images of professionally photographed pints of beer in different stages of pour appearance on bar tops taken in daylight. The model was meant to see a photo of a pint of beer, and return a judgment on how the pint of beer looks compared to the \u201cperfect\u201d looking pint of beer."),(0,i.kt)("p",null,"Unfortunately, because the images used to initially train the model did not look ",(0,i.kt)("em",{parentName:"p"},(0,i.kt)("strong",{parentName:"em"},"qualitatively"))," similar to the user-generated content (UGC), ",(0,i.kt)("strong",{parentName:"p"},"and")," the user data was photographed in ",(0,i.kt)("em",{parentName:"p"},(0,i.kt)("strong",{parentName:"em"},"more"))," scenarios than bars, the model needed improvement in the form of:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"1) more data to capture the breadth of scenarios, and;"),(0,i.kt)("li",{parentName:"ul"},"2) data that reflected the quality of smartphone cameras. ")),(0,i.kt)("p",null,"After applying these changes, the model performance improved."),(0,i.kt)("h2",{id:"what-is-the-difference-between-bias-and-variance-in-model-training"},"What is the difference between bias and variance in model training?"),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Bias")," occurs when the scope of your training data is ",(0,i.kt)("em",{parentName:"p"},"too narrow.")," If you only see green apples, you\u2019ll assume that all apples are green and think red apples are another kind of fruit. If the training data contains only a small number of examples, it\u2019ll react accordingly, taking it as truth. Small datasets make for a smaller worldview."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Variance"),", on the other hand, occurs when your training set is ",(0,i.kt)("em",{parentName:"p"},"cast too wide.")," If you train a concept of too many different kinds of images, and they are all visually different, the training set will become noisy. This will make it difficult for the model to find visually distinct qualities to learn from. "),(0,i.kt)("p",null,"The key to avoiding both of these pitfalls is building a dataset with the quality ",(0,i.kt)("em",{parentName:"p"},"and")," quantity of content kept in mind. "),(0,i.kt)("h2",{id:"is-there-a-checklist-i-can-use-for-training-custom-models"},"Is there a checklist I can use for training custom models?"),(0,i.kt)("p",null,"Here is a checklist you can use to help you make the most out of training your own models. "),(0,i.kt)("table",null,(0,i.kt)("thead",{parentName:"table"},(0,i.kt)("tr",{parentName:"thead"},(0,i.kt)("th",{parentName:"tr",align:null}),(0,i.kt)("th",{parentName:"tr",align:null}))),(0,i.kt)("tbody",{parentName:"table"},(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("ul",null,(0,i.kt)("li",null,"Are the images visually distinct?"))),(0,i.kt)("td",{parentName:"tr",align:null},"Ensure the images have visually noticeable qualities. They should not be too subtle for humans to pick up on, and they should also be able to be picked up through the noise of a photo. Also, look for whether or not those noticeable qualities are distinct enough to be repeatable across inputs.")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("ul",null,(0,i.kt)("li",null,"Are the ideal/not ideal inputs defined?"))),(0,i.kt)("td",{parentName:"tr",align:null},"Create a visual dictionary of what each concept\u2019s training data will and will not be trained for. Determine the best resolution and image size for optimal data throughput. Remember to set aside evaluation data at the start of each project. A split of 80% training data to 20% evaluation data should be good to start.")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("ul",null,(0,i.kt)("li",null,"Are the labels semantically clear?"))),(0,i.kt)("td",{parentName:"tr",align:null},"Make sure the labels for concepts reflect what the photo is of, not just what is in the photo.")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("ul",null,(0,i.kt)("li",null,"Is the training data UGC optimized?"))),(0,i.kt)("td",{parentName:"tr",align:null},"You need to ensure the training and test data match the reality of the use case appertaining to user-generated content (UGC). Your evaluation test set should reflect this.")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("ul",null,(0,i.kt)("li",null,"Do you have the assets ready?"))),(0,i.kt)("td",{parentName:"tr",align:null},"Have model versions and API keys on-hand for testing and for running scripts, if applicable.")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("ul",null,(0,i.kt)("li",null,"Do you track labels?"))),(0,i.kt)("td",{parentName:"tr",align:null},"Keep track of the number of images labeled, either via JIRA or any docs platform.")))),(0,i.kt)("h2",{id:"what-is-ground-truth-and-how-do-we-use-it-to-dictate-model-building"},"What is \u201cground truth\u201d and how do we use it to dictate model building?"),(0,i.kt)("p",null,"Ground truth is the mutual understanding of a concept based solely on visual criteria. We work with clients to define ground truth early in the model building process in order to be sure that we are speaking the same language when it comes to choosing training data for a concept. "),(0,i.kt)("p",null,"We are asking not what the meaning of a concept is, but how a concept appears in the worldview of our models."),(0,i.kt)("p",null,"We will not be able to determine the accuracy of a model if we do not have ground truth. Once we know what a model ",(0,i.kt)("em",{parentName:"p"},"should")," be predicting, then we can move forward in determining the model\u2019s accuracy."),(0,i.kt)("h2",{id:"how-do-we-train-for-closed-set-and-open-set-recognition"},"How do we train for closed-set and open-set recognition?"),(0,i.kt)("p",null,"When we have a multi-class recognition model, there is no hard and fast \u201crule\u201d as to how one should build its training set, but how one should approach training depends on the use-case of your model."),(0,i.kt)("p",null,"In closed-set recognition, all concepts are known at training time. In open-set recognition, an incomplete knowledge of the world is present at the time of training your model, and unknown concepts can be submitted to the model during testing."),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"closed-set and open-set recognition",src:a(13746).Z,width:"1451",height:"1048"}),"\n",(0,i.kt)("em",{parentName:"p"},"From Statistical Methods for Open Set Recognition ",(0,i.kt)("a",{parentName:"em",href:"https://www.wjscheirer.com/misc/openset/cvpr2016-open-set-part1.pdf"},"[1]"))),(0,i.kt)("p",null,"More often than not, open-set problems are found in nearly every case where visual recognition algorithms are used."))}c.isMDXComponent=!0},59116:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/framing_right_problem-1d609e37baa3209736bbb249f76def9d.png"},34990:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/framing_right_problem_concepts-df3760de68b38de5ea018833eb247c5b.png"},13746:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/negatives_and_positives-4eb0c6a1203142b6e14eeb3e4cb148a3.png"}}]);
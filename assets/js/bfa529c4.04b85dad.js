"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[7457],{85162:(e,n,t)=>{t.d(n,{Z:()=>r});var a=t(67294),i=t(86010);const o={tabItem:"tabItem_Ymn6"};function r(e){let{children:n,hidden:t,className:r}=e;return a.createElement("div",{role:"tabpanel",className:(0,i.Z)(o.tabItem,r),hidden:t},n)}},74866:(e,n,t)=>{t.d(n,{Z:()=>D});var a=t(87462),i=t(67294),o=t(86010),r=t(12466),s=t(16550),l=t(91980),p=t(67392),d=t(50012);function _(e){return function(e){return i.Children.map(e,(e=>{if(!e||(0,i.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}(e).map((e=>{let{props:{value:n,label:t,attributes:a,default:i}}=e;return{value:n,label:t,attributes:a,default:i}}))}function u(e){const{values:n,children:t}=e;return(0,i.useMemo)((()=>{const e=n??_(t);return function(e){const n=(0,p.l)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,t])}function c(e){let{value:n,tabValues:t}=e;return t.some((e=>e.value===n))}function m(e){let{queryString:n=!1,groupId:t}=e;const a=(0,s.k6)(),o=function(e){let{queryString:n=!1,groupId:t}=e;if("string"==typeof n)return n;if(!1===n)return null;if(!0===n&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:n,groupId:t});return[(0,l._X)(o),(0,i.useCallback)((e=>{if(!o)return;const n=new URLSearchParams(a.location.search);n.set(o,e),a.replace({...a.location,search:n.toString()})}),[o,a])]}function f(e){const{defaultValue:n,queryString:t=!1,groupId:a}=e,o=u(e),[r,s]=(0,i.useState)((()=>function(e){let{defaultValue:n,tabValues:t}=e;if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(n){if(!c({value:n,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${n}" but none of its children has the corresponding value. Available values are: ${t.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return n}const a=t.find((e=>e.default))??t[0];if(!a)throw new Error("Unexpected error: 0 tabValues");return a.value}({defaultValue:n,tabValues:o}))),[l,p]=m({queryString:t,groupId:a}),[_,f]=function(e){let{groupId:n}=e;const t=function(e){return e?`docusaurus.tab.${e}`:null}(n),[a,o]=(0,d.Nk)(t);return[a,(0,i.useCallback)((e=>{t&&o.set(e)}),[t,o])]}({groupId:a}),h=(()=>{const e=l??_;return c({value:e,tabValues:o})?e:null})();(0,i.useLayoutEffect)((()=>{h&&s(h)}),[h]);return{selectedValue:r,selectValue:(0,i.useCallback)((e=>{if(!c({value:e,tabValues:o}))throw new Error(`Can't select invalid tab value=${e}`);s(e),p(e),f(e)}),[p,f,o]),tabValues:o}}var h=t(72389);const E={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};function g(e){let{className:n,block:t,selectedValue:s,selectValue:l,tabValues:p}=e;const d=[],{blockElementScrollPositionUntilNextRender:_}=(0,r.o5)(),u=e=>{const n=e.currentTarget,t=d.indexOf(n),a=p[t].value;a!==s&&(_(n),l(a))},c=e=>{let n=null;switch(e.key){case"Enter":u(e);break;case"ArrowRight":{const t=d.indexOf(e.currentTarget)+1;n=d[t]??d[0];break}case"ArrowLeft":{const t=d.indexOf(e.currentTarget)-1;n=d[t]??d[d.length-1];break}}n?.focus()};return i.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,o.Z)("tabs",{"tabs--block":t},n)},p.map((e=>{let{value:n,label:t,attributes:r}=e;return i.createElement("li",(0,a.Z)({role:"tab",tabIndex:s===n?0:-1,"aria-selected":s===n,key:n,ref:e=>d.push(e),onKeyDown:c,onClick:u},r,{className:(0,o.Z)("tabs__item",E.tabItem,r?.className,{"tabs__item--active":s===n})}),t??n)})))}function y(e){let{lazy:n,children:t,selectedValue:a}=e;const o=(Array.isArray(t)?t:[t]).filter(Boolean);if(n){const e=o.find((e=>e.props.value===a));return e?(0,i.cloneElement)(e,{className:"margin-top--md"}):null}return i.createElement("div",{className:"margin-top--md"},o.map(((e,n)=>(0,i.cloneElement)(e,{key:n,hidden:e.props.value!==a}))))}function I(e){const n=f(e);return i.createElement("div",{className:(0,o.Z)("tabs-container",E.tabList)},i.createElement(g,(0,a.Z)({},e,n)),i.createElement(y,(0,a.Z)({},e,n)))}function D(e){const n=(0,h.Z)();return i.createElement(I,(0,a.Z)({key:String(n)},e))}},49662:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>W,contentTitle:()=>z,default:()=>q,frontMatter:()=>G,metadata:()=>V,toc:()=>$});var a=t(87462),i=(t(67294),t(3905)),o=t(74866),r=t(85162),s=t(90814);const l="##########################################################################################\n# In this section, we set the user authentication, app ID, model ID, and model type ID.\n# Change these strings to run your own example.\n#########################################################################################\n\nUSER_ID = 'YOUR_USER_ID_HERE'\n# Your PAT (Personal Access Token) can be found in the portal under Authentification\nPAT = 'YOUR_PAT_HERE'\nAPP_ID = 'YOUR_APP_ID_HERE'\n# Change these to create your own model\nMODEL_ID = 'petsID'\nMODEL_TYPE_ID = 'visual-classifier'\n\n##########################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n##########################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (('authorization', 'Key ' + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\npost_models_response = stub.PostModels(\n    service_pb2.PostModelsRequest(\n        user_app_id=userDataObject,\n        models=[\n            resources_pb2.Model(\n                id=MODEL_ID,\n                model_type_id=MODEL_TYPE_ID             \n            )\n        ]\n    ),\n    metadata=metadata\n)\n\nif post_models_response.status.code != status_code_pb2.SUCCESS:\n    print(post_models_response.status)\n    raise Exception(\"Post models failed, status: \" + post_models_response.status.description)\n",p='########################################################################################\n# In this section, we set the user authentication, app ID, model ID, and concept IDs.\n# Change these strings to run your own example.\n########################################################################################\n\nUSER_ID = "YOUR_USER_ID_HERE"\n# Your PAT (Personal Access Token) can be found in the portal under Authentification\nPAT = "YOUR_PAT_HERE"\nAPP_ID = "YOUR_APP_ID_HERE"\n# Change these to train your own model\nMODEL_ID = "petsID"\nCONCEPT_ID_1 = "ferrari23"\nCONCEPT_ID_2 = "outdoors23"\n\n##########################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n##########################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\nfrom google.protobuf.struct_pb2 import Struct\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nparams = Struct()\nparams.update(\n    {\n        "template": "MMClassification_ResNet_50_RSB_A1", \n        "num_epochs": 2\n    }\n    )\n\nmetadata = (("authorization", "Key " + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\npost_model_versions = stub.PostModelVersions(\n    service_pb2.PostModelVersionsRequest(\n        user_app_id=userDataObject,\n        model_id=MODEL_ID,\n        model_versions=[\n            resources_pb2.ModelVersion(\n                train_info=resources_pb2.TrainInfo(\n                    params=params,\n                ),\n                output_info=resources_pb2.OutputInfo(\n                    data=resources_pb2.Data(\n                        concepts=[\n                            resources_pb2.Concept(id=CONCEPT_ID_1),\n                            resources_pb2.Concept(id=CONCEPT_ID_2)\n                            ]\n                    ),\n                ),\n            )\n        ],\n    ),\n    metadata=metadata,\n)\n\nif post_model_versions.status.code != status_code_pb2.SUCCESS:\n    print(post_model_versions.status)\n    raise Exception("Post models versions failed, status: " + post_model_versions.status.description)\n',d="#####################################################################################\n# In this section, we set the user authentication, app ID, and model type ID. \n# Change these strings to run your own example.\n####################################################################################\n\nUSER_ID = 'YOUR_USER_ID_HERE'\n# Your PAT (Personal Access Token) can be found in the portal under Authentification\nPAT = 'YOUR_PAT_HERE'\nAPP_ID = 'YOUR_APP_ID_HERE'\n# Change this to list the template types of your preferred model \nMODEL_TYPE = 'visual-classifier'\n\n##########################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n##########################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (('authorization', 'Key ' + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\nresponse = stub.ListModelTypes(\n    service_pb2.ListModelTypesRequest(\n        user_app_id=userDataObject\n    ),\n    metadata=metadata\n    )\n\nif response.status.code != status_code_pb2.SUCCESS:\n    print(response.status)\n    raise Exception(\"List models failed, status: \" + response.status.description)\n\nfor model_type in response.model_types:\n   if model_type.id == MODEL_TYPE:\n      for modeltypefield in model_type.model_type_fields:\n        if modeltypefield.path.split('.')[-1] == \"template\":\n          for template in modeltypefield.model_type_enum_options:\n            print(template)",_="###################################################################################\n# In this section, we set the user authentication, app ID, and the details we want\n# to use to create a workflow. Change these strings to run your own example.\n###################################################################################\n\nUSER_ID = 'YOUR_USER_ID_HERE'\n# Your PAT (Personal Access Token) can be found in the portal under Authentification\nPAT = 'YOUR_PAT_HERE'\nAPP_ID = 'YOUR_APP_ID_HERE'\n# Change these to create your own workflow\nWORKFLOW_ID = 'my-new-workflow-id' \nEMBED_MODEL_ID = 'YOUR_EMBED_MODEL_ID'\nEMBED_MODEL_VERSION_ID = 'YOUR_EMBED_MODEL_VERSION_ID'\nWORKFLOWNODE_ID = 'my-custom-model' \nCUSTOM_MODEL_ID = 'YOUR_CUSTOM_MODEL_ID'\nCUSTOM_MODEL_VERSION_ID = 'YOUR_CUSTOM_MODEL_VERSION_ID'\n\n##########################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n##########################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (('authorization', 'Key ' + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\npost_workflows_response = stub.PostWorkflows(\n    service_pb2.PostWorkflowsRequest(\n        user_app_id=userDataObject,\n        workflows=[\n            resources_pb2.Workflow(\n                id=WORKFLOW_ID,\n                nodes=[\n                    resources_pb2.WorkflowNode(\n                        id=\"embed\",\n                        model=resources_pb2.Model(\n                            id=EMBED_MODEL_ID,\n                            model_version=resources_pb2.ModelVersion(\n                                id=EMBED_MODEL_VERSION_ID\n                            )\n                        )\n                    ),\n                    resources_pb2.WorkflowNode(\n                        id=WORKFLOWNODE_ID,\n                        model=resources_pb2.Model(\n                            id=CUSTOM_MODEL_ID,\n                            model_version=resources_pb2.ModelVersion(\n                                id=CUSTOM_MODEL_VERSION_ID\n                            )\n                        ),\n                        node_inputs=[\n                            resources_pb2.NodeInput(node_id=\"embed\")\n                        ]\n                    ),\n                ]\n            )\n        ]\n    ),\n    metadata=metadata\n)\n\nif  post_workflows_response.status.code != status_code_pb2.SUCCESS:\n    print(post_workflows_response.status)\n    raise Exception(\"Post workflows failed, status: \" +  post_workflows_response.status.description)",u="########################################################################\n# In this section, we set the user authentication, app ID, and default\n# workflow ID. Change these strings to run your own example.\n########################################################################\n\nUSER_ID = 'YOUR_USER_ID_HERE'\n# Your PAT (Personal Access Token) can be found in the portal under Authentification\nPAT = 'YOUR_PAT_HERE'\nAPP_ID = 'YOUR_APP_ID_HERE'\n# Change this to update your default workflow\nDEFAULT_WORKFlOW_ID = 'auto-annotation-workflow-id'\n\n##########################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n##########################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (('authorization', 'Key ' + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\npatch_apps_response = stub.PatchApps(\n    service_pb2.PatchAppsRequest(\n        user_app_id=userDataObject,\n        action=\"overwrite\",\n        apps=[\n            resources_pb2.App(\n                id=APP_ID,\n                default_workflow_id=DEFAULT_WORKFlOW_ID\n            )\n        ]\n    ),\n    metadata=metadata\n)\n\nif patch_apps_response.status.code != status_code_pb2.SUCCESS:\n    print(patch_apps_response.status)\n    raise Exception(\"Patch apps failed, status: \" + patch_apps_response.status.description) \n",c="_base_ = '/mmdetection/configs/yolof/yolof_r50_c5_8x8_1x_coco.py'\nmodel=dict(\n  bbox_head=dict(num_classes=0))\ndata=dict(\n  train=dict(\n    ann_file='',\n    img_prefix='',\n    classes=''\n    ),\n  val=dict(\n    ann_file='',\n    img_prefix='',\n    classes=''))\noptimizer=dict(\n  _delete_=True,\n  type='Adam',\n  lr=0.0001,\n  weight_decay=0.0001)\nlr_config = dict(\n  _delete_=True,\n  policy='CosineAnnealing',\n  warmup='linear',\n  warmup_iters=1000,\n  warmup_ratio=0.1,\n  min_lr_ratio=1e-5)\nrunner = dict(\n  _delete_=True,\n  type='EpochBasedRunner',\n  max_epochs=10)\n\n",m='########################################################################################\n# In this section, we set the user authentication, app ID, model ID, and concept ID.\n# Change these strings to run your own example.\n########################################################################################\n\nUSER_ID = "YOUR_USER_ID_HERE"\n# Your PAT (Personal Access Token) can be found in the portal under Authentification\nPAT = "YOUR_PAT_HERE"\nAPP_ID = "YOUR_APP_ID_HERE"\n# Change this to train your own model\nMODEL_ID = "test_config"\nCONCEPT_ID_1 = "house"\n\n##########################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n##########################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\nfrom google.protobuf.struct_pb2 import Struct\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nparams = Struct()\nparams.update({"template": "MMDetection"})\n\nCONFIG_FILE = \'training_config.py\'\nparams.update({"custom_config": open(CONFIG_FILE, "r").read()})\n\nmetadata = (("authorization", "Key " + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\npost_model_versions = stub.PostModelVersions(\n    service_pb2.PostModelVersionsRequest(\n        user_app_id=userDataObject,\n        model_id=MODEL_ID,\n        model_versions=[\n            resources_pb2.ModelVersion(\n                train_info=resources_pb2.TrainInfo(\n                    params=params,\n                ),\n                output_info=resources_pb2.OutputInfo(\n                    data=resources_pb2.Data(\n                        concepts=[\n                            resources_pb2.Concept(id=CONCEPT_ID_1, value=1)\n                        ]\n                    ),\n                )\n            )\n        ],\n    ),\n    metadata=metadata,\n)\n\nif post_model_versions.status.code != status_code_pb2.SUCCESS:\n    print(post_model_versions.status)\n    raise Exception(\n        "Post models versions failed, status: " + post_model_versions.status.description\n    )\n\nprint(post_model_versions)\n',f='###################################################################################################\n# In this section, we set the user authentication, app ID, model ID, and estimated input count.\n# Change these strings to run your own example.\n##################################################################################################\n\nUSER_ID = "YOUR_USER_ID_HERE"\n# Your PAT (Personal Access Token) can be found in the portal under Authentification\nPAT = "YOUR_PAT_HERE"\nAPP_ID = "YOUR_APP_ID_HERE"\n# Change these to get your training time estimate\nMODEL_ID = "YOUR_CUSTOM_MODEL_ID_HERE"\nESTIMATED_INPUT_COUNT = 100\n\n##########################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n##########################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\nfrom google.protobuf.struct_pb2 import Struct\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nparams = Struct()\nparams.update({\n        "template": "MMDetection_FasterRCNN"\n    })\n\nmetadata = (("authorization", "Key " + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\ntraining_time_estimate_response = stub.PostModelVersionsTrainingTimeEstimate(\n    service_pb2.PostModelVersionsTrainingTimeEstimateRequest(\n        user_app_id=userDataObject,\n        model_id=MODEL_ID,\n        model_versions=[\n            resources_pb2.ModelVersion(\n                train_info=resources_pb2.TrainInfo(params=params)\n            ),\n        ],\n        estimated_input_count=ESTIMATED_INPUT_COUNT\n    ),\n    metadata=metadata,\n)\n\nif training_time_estimate_response.status.code != status_code_pb2.SUCCESS:\n    print(training_time_estimate_response.status)\n    raise Exception("Post model outputs failed, status: " + training_time_estimate_response.status.description)\n\nprint(training_time_estimate_response)\n',h="\x3c!--index.html file--\x3e\n\n<script>\n    ///////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, app ID, model ID, and model type ID.\n    // Change these strings to run your own example.\n    //////////////////////////////////////////////////////////////////////////////////////////\n\n    const USER_ID = 'YOUR_USER_ID_HERE';\n    // Your PAT (Personal Access Token) can be found in the portal under Authentification\n    const PAT = 'YOUR_PAT_HERE';\n    const APP_ID = 'YOUR_APP_ID_HERE';\n    // Change these to create your own model\n    const MODEL_ID = 'petsID';\n    const MODEL_TYPE_ID = 'visual-classifier';\n    \n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    const raw = JSON.stringify({\n        \"user_app_id\": {\n            \"user_id\": USER_ID,\n            \"app_id\": APP_ID\n        },\n        \"model\": {\n            \"id\": MODEL_ID,\n            \"model_type_id\": MODEL_TYPE_ID         \n        }\n    });\n\n    const requestOptions = {\n        method: 'POST',\n        headers: {\n            'Accept': 'application/json',\n            'Authorization': 'Key ' + PAT\n        },\n        body: raw\n    };\n\n    fetch(\"https://api.clarifai.com/v2/models\", requestOptions)\n        .then(response => response.text())\n        .then(result => console.log(result))\n        .catch(error => console.log('error', error));\n\n<\/script>",E='\x3c!--index.html file--\x3e\n\n<script>\n    //////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, app ID, model ID, and concept IDs.\n    // Change these strings to run your own example.\n    /////////////////////////////////////////////////////////////////////////////////////////\n\n    const USER_ID = "YOUR_USER_ID_HERE";\n    // Your PAT (Personal Access Token) can be found in the portal under Authentification\n    const PAT = "YOUR_PAT_HERE";\n    const APP_ID = "YOUR_APP_ID_HERE";\n    // Change these to train your own model\n    const MODEL_ID = "petsID";\n    const CONCEPT_ID_1 = "ferrari23";\n    const CONCEPT_ID_2 = "outdoors23";\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    const raw = JSON.stringify({\n        "user_app_id": {\n            "user_id": USER_ID,\n            "app_id": APP_ID\n        },\n        "model_versions": [{\n            "train_info": {\n                "params": {\n                    "template": "MMClassification_ResNet_50_RSB_A1",\n                    "num_epochs": 2\n                }\n            },\n            "output_info": {\n                "data": {\n                    "concepts": [\n                        {\n                            "id": CONCEPT_ID_1\n                        },\n                        {\n                            "id": CONCEPT_ID_2\n                        }\n                    ]\n                }\n            }\n        }]\n\n    });\n\n    const requestOptions = {\n        method: "POST",\n        headers: {\n            "Content-Type": "application/json",\n            "Authorization": "Key " + PAT\n        },\n        body: raw\n    };\n\n    fetch(`https://api.clarifai.com/v2/models/${MODEL_ID}/versions`, requestOptions)\n        .then(response => response.text())\n        .then(result => console.log(result))\n        .catch(error => console.log("error", error));\n\n<\/script>',g="\x3c!--index.html file--\x3e\n\n<script>\n    ////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, app ID, and model type ID. \n    // Change these strings to run your own example.\n    ////////////////////////////////////////////////////////////////////////////////////////  \n\n    const USER_ID = 'YOUR_USER_ID_HERE';\n    // Your PAT (Personal Access Token) can be found in the portal under Authentification\n    const PAT = 'YOUR_PAT_HERE';\n    const APP_ID = 'YOUR_APP_ID_HERE';\n    // Change this to list the template types of your preferred model \n    const MODEL_TYPE = 'visual-classifier';\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    const requestOptions = {\n        method: 'GET',\n        headers: {\n            'Accept': 'application/json',\n            'Authorization': 'Key ' + PAT\n        }\n    };\n\n    fetch(`https://api.clarifai.com/v2/users/${USER_ID}/apps/${APP_ID}/models/types?per_page=20&page=1`, requestOptions)\n        .then(response => response.json())\n        .then(result => {\n            console.log('Clarifai API Response:', result);\n\n            result.model_types.forEach(modelType => {\n                if (modelType.id === MODEL_TYPE) {\n                    modelType.model_type_fields.forEach(modelTypeField => {\n                        if (modelTypeField.path.split('.').slice(-1)[0] === 'template') {\n                            modelTypeField.model_type_enum_options.forEach(template => {\n                                console.log('Template:', template);\n                            });\n                        }\n                    });\n                }\n            });\n        })\n        .catch(error => console.error('Clarifai API Error:', error));\n\n\n<\/script>",y='\x3c!--index.html file--\x3e\n\n<script>\n    //////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, app ID, and the details we want\n    // to use to create a workflow. Change these strings to run your own example.\n    /////////////////////////////////////////////////////////////////////////////////////////\n\n    const USER_ID = "YOUR_USER_ID_HERE";\n    // Your PAT (Personal Access Token) can be found in the portal under Authentification\n    const PAT = "YOUR_PAT_HERE";\n    const APP_ID = "YOUR_APP_ID_HERE";\n    // Change these to create your own custom workflow\n    const WORKFLOW_ID = "my-new-workflow-id";\n    const EMBED_MODEL_ID = "YOUR_EMBED_MODEL_ID";\n    const EMBED_MODEL_VERSION_ID = "YOUR_EMBED_MODEL_VERSION_ID";\n    const WORKFLOWNODE_ID = "my-custom-model";\n    const CUSTOM_MODEL_ID = "YOUR_CUSTOM_MODEL_ID";\n    const CUSTOM_MODEL_VERSION_ID = "YOUR_CUSTOM_MODEL_VERSION_ID"; \n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////   \n\n    const raw = JSON.stringify({\n        "user_app_id": {\n            "user_id": USER_ID,\n            "app_id": APP_ID\n        },\n        "workflows": [{\n            "id": WORKFLOW_ID,\n            "nodes": [\n                {\n                    "id": "embed",\n                    "model": {\n                        "id": EMBED_MODEL_ID,\n                        "model_version": {\n                            "id": EMBED_MODEL_VERSION_ID\n                        }\n                    }\n                },\n                {\n                    "id": WORKFLOWNODE_ID,\n                    "model": {\n                        "id": CUSTOM_MODEL_ID,\n                        "model_version": {\n                            "id": CUSTOM_MODEL_VERSION_ID\n                        }\n                    },\n                        "node_inputs": [\n                            {\n                                "node_id": "embed"\n                            }\n                        ]\n                }\n            ]\n        }]\n    });\n\n    const requestOptions = {\n        method: \'POST\',\n        headers: {\n            \'Accept\': \'application/json\',\n            \'Authorization\': \'Key \' + PAT\n        },\n        body: raw\n    };\n\n    fetch(`https://api.clarifai.com/v2/workflows`, requestOptions)\n        .then(response => response.text())\n        .then(result => console.log(result))\n        .catch(error => console.log(\'error\', error));\n\n<\/script>',I='\x3c!--index.html file--\x3e\n\n<script>\n    ///////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, app ID, model ID, and estimated input count.\n    // Change these strings to run your own example.\n    //////////////////////////////////////////////////////////////////////////////////////////////////\n\n    const USER_ID = "YOUR_USER_ID_HERE";\n    // Your PAT (Personal Access Token) can be found in the portal under Authentification\n    const PAT = "YOUR_PAT_HERE";\n    const APP_ID = "YOUR_APP_ID_HERE";\n    // Change these to get your training time estimate\n    const MODEL_ID = "YOUR_CUSTOM_MODEL_ID_HERE";\n    const ESTIMATED_INPUT_COUNT = 100;\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    const raw = JSON.stringify({\n        "user_app_id": {\n            "user_id": USER_ID,\n            "app_id": APP_ID\n        },\n        "model_versions": [{\n            "train_info": {\n                "params": {\n                    "template": "MMDetection_FasterRCNN"\n                }\n            },\n\n        }],\n        "estimated_input_count": ESTIMATED_INPUT_COUNT\n\n    });\n\n    const requestOptions = {\n        method: "POST",\n        headers: {\n            "Content-Type": "application/json",\n            "Authorization": "Key " + PAT\n        },\n        body: raw\n    };\n\n    fetch(`https://api.clarifai.com/v2/users/${USER_ID}/apps/${APP_ID}/models/${MODEL_ID}/versions/time_estimate/`, requestOptions)\n        .then(response => response.text())\n        .then(result => console.log(result))\n        .catch(error => console.log("error", error));\n\n<\/script>',D='//index.js file\n\n////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, app ID, model ID, and model type ID.\n// Change these strings to run your own example.\n///////////////////////////////////////////////////////////////////////////////////////////\n\nconst USER_ID = \'YOUR_USER_ID_HERE\';\n// Your PAT (Personal Access Token) can be found in the portal under Authentification\nconst PAT = \'YOUR_PAT_HERE\';\nconst APP_ID = \'YOUR_APP_ID_HERE\';\n// Change these to create your own model\nconst MODEL_ID = \'petsID\';\nconst MODEL_TYPE_ID = \'visual-classifier\';\n\n/////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n/////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\nstub.PostModels(\n    {\n        user_app_id: {\n            "user_id": USER_ID,\n            "app_id": APP_ID\n        },\n        models: [\n            {\n                id: MODEL_ID,\n                model_type_id: MODEL_TYPE_ID               \n            }\n        ]\n    },\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            throw new Error("Post models failed, status: " + response.status.description);\n        }\n    }\n);',T='//index.js file\n\n///////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, app ID, and model type ID. \n// Change these strings to run your own example.\n//////////////////////////////////////////////////////////////////////////////////////\n\nconst USER_ID = "YOUR_USER_ID_HERE";\n// Your PAT (Personal Access Token) can be found in the portal under Authentification\nconst PAT = "YOUR_PAT_HERE";\nconst APP_ID = "YOUR_APP_ID_HERE";\n// Change this to list the template types of your preferred model \nconst MODEL_TYPE = "visual-classifier";\n\n/////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n/////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\nstub.ListModelTypes(\n    {\n        user_app_id: {\n            "user_id": USER_ID,\n            "app_id": APP_ID\n        },\n        page: 1,\n        per_page: 500\n    },\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            throw new Error("Received status: " + response.status.description + "\\n" + response.status.details);\n        }\n\n        response.model_types.forEach((modelType) => {\n            if (modelType.id === MODEL_TYPE) {\n                modelType.model_type_fields.forEach((modelTypeField) => {\n                    if (modelTypeField.path.split(\'.\').pop() === \'template\') {\n                        modelTypeField.model_type_enum_options.forEach((template) => {\n                            console.log(template);\n                        });\n                    }\n                });\n            }\n        });\n\n    }\n);',b="//index.js file\n\n//////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, app ID, and the details we want\n// to use to create a workflow. Change these strings to run your own example.\n/////////////////////////////////////////////////////////////////////////////////////\n\nconst USER_ID = 'YOUR_USER_ID_HERE';\n// Your PAT (Personal Access Token) can be found in the portal under Authentification\nconst PAT = 'YOUR_PAT_HERE';\nconst APP_ID = 'YOUR_APP_ID_HERE';\n// Change these to create your own workflow\nconst WORKFLOW_ID = 'my-new-workflow-id';\nconst EMBED_MODEL_ID = 'YOUR_EMBED_MODEL_ID';\nconst EMBED_MODEL_VERSION_ID = 'YOUR_EMBED_MODEL_VERSION_ID';\nconst WORKFLOWNODE_ID = 'my-custom-model';\nconst CUSTOM_MODEL_ID = 'YOUR_CUSTOM_MODEL_ID';\nconst CUSTOM_MODEL_VERSION_ID = 'YOUR_CUSTOM_MODEL_VERSION_ID';\n\n/////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n/////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require(\"clarifai-nodejs-grpc\");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set(\"authorization\", \"Key \" + PAT);\n\nstub.PostWorkflows(\n    {\n        user_app_id: {\n            \"user_id\": USER_ID,\n            \"app_id\": APP_ID\n        },\n        workflows: [\n            {\n                id: WORKFLOW_ID,\n                nodes: [\n                    {\n                        id: \"embed\",\n                        model: {\n                            id: EMBED_MODEL_ID,\n                            model_version: {\n                                id: EMBED_MODEL_VERSION_ID\n                            }\n                        }\n                    },\n                    {\n                        id: WORKFLOWNODE_ID,\n                        model: {\n                            id: CUSTOM_MODEL_ID,\n                            model_version: {\n                                id: CUSTOM_MODEL_VERSION_ID\n                            }\n                        },\n                        node_inputs: [\n                            { node_id: \"embed\" }\n                        ]\n                    }\n                ]\n            }\n        ]\n    },\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            console.log(response.status);\n            throw new Error(\"Post workflows failed, status: \" + response.status.description);\n        }\n    }\n);",O='//index.js file\n\n/////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, app ID, and default\n// workflow ID. Change these strings to run your own example.\n/////////////////////////////////////////////////////////////////////////////\n\nconst USER_ID = \'YOUR_USER_ID_HERE\';\n// Your PAT (Personal Access Token) can be found in the portal under Authentification\nconst PAT = \'YOUR_PAT_HERE\';\nconst APP_ID = \'YOUR_APP_ID_HERE\';\n// Change this to update your default workflow\nconst DEFAULT_WORKFlOW_ID = \'auto-annotation-workflow-id\';\n\n/////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n/////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\nstub.PatchApps(\n    {\n        user_app_id: {\n            "user_id": USER_ID,\n            "app_id": APP_ID\n        },\n        action: "overwrite",\n        apps: [\n            {\n                id: APP_ID,\n                default_workflow_id: DEFAULT_WORKFlOW_ID\n            }\n        ]\n    },\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            console.log(response.status);\n            throw new Error("Patch apps failed, status: " + response.status.description);\n        }\n    }\n);',R='package com.clarifai.example;\n\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.status.StatusCode;\n\npublic class ClarifaiExample {\n\n    ////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, app ID, model ID, and model type ID.\n    // Change these strings to run your own example.\n    ///////////////////////////////////////////////////////////////////////////////////////////\n\n    static final String USER_ID = "YOUR_USER_ID_HERE";\n    //Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    static final String APP_ID = "YOUR_APP_ID_HERE";\n    // Change these to create your own model    \n    static final String MODEL_ID = "petsID";\n    static final String MODEL_TYPE_ID = "visual-classifier";\n    \n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\t\n\n    public static void main(String[] args) {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n            .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        SingleModelResponse postModelsResponse = stub.postModels(\n            PostModelsRequest.newBuilder()\n            .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n            .addModels(\n                Model.newBuilder()\n                .setId(MODEL_ID)\n                .setModelTypeId(MODEL_TYPE_ID)              \n            ).build()\n        );\n\n        if (postModelsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("Post models failed, status: " + postModelsResponse.getStatus());\n        }\n\n    }\n\n}',v='package com.clarifai.example;\n\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.status.StatusCode;\nimport com.google.protobuf.Struct;\nimport com.google.protobuf.Value;\n\npublic class ClarifaiExample {\n\n    //////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, app ID, model ID, and concept IDs.\n    // Change these strings to run your own example.\n    //////////////////////////////////////////////////////////////////////////////////////////\n\n    static final String USER_ID = "YOUR_USER_ID_HERE";\n    // Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    static final String APP_ID = "YOUR_APP_ID_HERE";\n    // Change these to train your own model\n    static final String MODEL_ID = "petsID";\n    static final String CONCEPT_ID_1 = "ferrari23";\n    static final String CONCEPT_ID_2 = "outdoors23";\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\t\n\n    public static void main(String[] args) {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n                .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        Struct.Builder params = Struct.newBuilder()\n                .putFields("template", Value.newBuilder().setStringValue("MMClassification_ResNet_50_RSB_A1").build())\n                .putFields("num_epochs", Value.newBuilder().setNumberValue(2).build());\n\n        SingleModelResponse postModelVersionsResponse = stub.postModelVersions(\n                PostModelVersionsRequest.newBuilder()\n                        .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n                        .setModelId(MODEL_ID)\n                        .addModelVersions(ModelVersion.newBuilder()\n                                .setTrainInfo(TrainInfo.newBuilder()\n                                        .setParams(params)\n                                )\n                                .setOutputInfo(OutputInfo.newBuilder()\n                                        .setData(Data.newBuilder()\n                                                .addConcepts(Concept.newBuilder()\n                                                        .setId(CONCEPT_ID_1)\n                                                )\n                                                .addConcepts(Concept.newBuilder()\n                                                        .setId(CONCEPT_ID_2)\n                                                )\n                                        )\n                                )\n                        )\n                        .build()\n        );\n\n        if (postModelVersionsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("Post model outputs failed, status: " + postModelVersionsResponse.getStatus());\n        }\n\n    }\n}\n',w='package com.clarifai.example;\n\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.grpc.api.status.StatusCode;\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\n\npublic class ClarifaiExample {\n\n    //////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, app ID, and model type ID. \n    // Change these strings to run your own example.\n    //////////////////////////////////////////////////////////////////////////////////\n\n    static final String USER_ID = "YOUR_USER_ID_HERE";\n    //Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    static final String APP_ID = "YOUR_APP_ID_HERE";\n    // Change this to list the template types of your preferred model \n    static final String MODEL_TYPE = "visual-classifier";\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    public static void main(String[] args) {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n                .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        MultiModelTypeResponse listModelTypesResponse = stub.listModelTypes(\n                ListModelTypesRequest.newBuilder()\n                        .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n                        .build()\n        );\n\n        if (listModelTypesResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("List models failed, status: " + listModelTypesResponse.getStatus());\n        }\n\n        for (ModelType modelType : listModelTypesResponse.getModelTypesList()) {\n            if (modelType.getId().equals(MODEL_TYPE)) {\n                for (ModelTypeField modelTypeField : modelType.getModelTypeFieldsList()) {\n                    if (modelTypeField.getPath().split("\\\\.")[modelTypeField.getPath().split("\\\\.").length - 1]\n                            .equals("template")) {\n                        for (ModelTypeEnumOption template : modelTypeField.getModelTypeEnumOptionsList()) {\n                            System.out.println(template);\n                        }\n                    }\n                }\n            }\n        }\n\n    }\n\n}\n',N='package com.clarifai.example;\n\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.grpc.api.status.StatusCode;\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\n\npublic class ClarifaiExample {\n\n    //////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, app ID, and the details we want\n    // to use to create a workflow. Change these strings to run your own example.\n    /////////////////////////////////////////////////////////////////////////////////////\n\n    static final String USER_ID = "YOUR_USER_ID_HERE";\n    //Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    static final String APP_ID = "YOUR_APP_ID_HERE";\n    // Change these to create your own workflow\n    static final String WORKFLOW_ID = "my-new-workflow-id";\n    static final String EMBED_MODEL_ID = "YOUR_EMBED_MODEL_ID";\n    static final String EMBED_MODEL_VERSION_ID = "YOUR_EMBED_MODEL_VERSION_ID";\n    static final String WORKFLOWNODE_ID = "my-custom-model";\n    static final String CUSTOM_MODEL_ID = "YOUR_CUSTOM_MODEL_ID";\n    static final String CUSTOM_MODEL_VERSION_ID = "YOUR_CUSTOM_MODEL_VERSION_ID";\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    public static void main(String[] args) {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n            .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        MultiWorkflowResponse postWorkflowsResponse = stub.postWorkflows(\n            PostWorkflowsRequest.newBuilder()\n            .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n            .addWorkflows(\n                Workflow.newBuilder()\n                .setId(WORKFLOW_ID)\n                .addNodes(\n                    WorkflowNode.newBuilder()\n                    .setId("embed")\n                    .setModel(\n                        Model.newBuilder()\n                        .setId(EMBED_MODEL_ID)\n                        .setModelVersion(\n                            ModelVersion.newBuilder()\n                            .setId(EMBED_MODEL_VERSION_ID)\n                        )\n                    )\n                )\n                .addNodes(\n                    WorkflowNode.newBuilder()\n                    .setId(WORKFLOWNODE_ID)\n                    .setModel(\n                        Model.newBuilder()\n                        .setId(CUSTOM_MODEL_ID)\n                        .setModelVersion(\n                            ModelVersion.newBuilder()\n                            .setId(CUSTOM_MODEL_VERSION_ID)\n                        )\n                    )\n                    .addNodeInputs(NodeInput.newBuilder().setNodeId("embed"))\n                )\n            )\n            .build()\n        );\n\n        if (postWorkflowsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("Post workflows failed, status: " + postWorkflowsResponse.getStatus());\n        }\n\n    }\n\n}',A='package com.clarifai.example;\n\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.grpc.api.status.StatusCode;\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\n\npublic class ClarifaiExample {\n\n    /////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, app ID, and default\n    // workflow ID. Change these strings to run your own example.\n    /////////////////////////////////////////////////////////////////////////////\n\n    static final String USER_ID = "YOUR_USER_ID_HERE";\n    //Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    static final String APP_ID = "YOUR_APP_ID_HERE";\n    // Change this to update your default workflow\n    static final String DEFAULT_WORKFlOW_ID = "auto-annotation-workflow-id";\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    public static void main(String[] args) {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n            .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        MultiAppResponse patchAppsResponse = stub.patchApps(\n            PatchAppsRequest.newBuilder()\n            .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n            .setAction("overwrite")\n            .addApps(\n                App.newBuilder()\n                .setId(APP_ID)\n                .setDefaultWorkflowId(DEFAULT_WORKFlOW_ID)\n            ).build()\n        );\n\n        if (patchAppsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("Patch apps failed, status: " + patchAppsResponse.getStatus());\n        }\n\n    }\n\n}',C='package com.clarifai.example;\n\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.status.StatusCode;\nimport com.google.protobuf.Struct;\nimport com.google.protobuf.Value;\n\npublic class ClarifaiExample {\n\n    //////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, app ID, model ID, and estimated input count.\n    // Change these strings to run your own example.\n    /////////////////////////////////////////////////////////////////////////////////////////////////////\n\n    static final String USER_ID = "YOUR_USER_ID_HERE";\n    // Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    static final String APP_ID = "YOUR_APP_ID_HERE";\n    // Change these to get your training time estimate\n    static final String MODEL_ID = "YOUR_CUSTOM_MODEL_ID_HERE";\n    static final int ESTIMATED_INPUT_COUNT = 100;\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\t\n\n    public static void main(String[] args) {\n        \n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n                .withCallCredentials(new ClarifaiCallCredentials(PAT));\n        \n        Struct.Builder params = Struct.newBuilder()\n                .putFields("template", Value.newBuilder().setStringValue("MMDetection_FasterRCNN").build());\n        \n        MultiTrainingTimeEstimateResponse trainingTimeEstimateResponse = stub.postModelVersionsTrainingTimeEstimate(\n                PostModelVersionsTrainingTimeEstimateRequest.newBuilder()\n                        .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n                        .setModelId(MODEL_ID)\n                        .addModelVersions(ModelVersion.newBuilder()\n                                .setTrainInfo(TrainInfo.newBuilder()\n                                        .setParams(params)\n                                )\n                        )\n                        .setEstimatedInputCount(ESTIMATED_INPUT_COUNT)\n                        .build()\n        );\n        \n        if (trainingTimeEstimateResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("Post model outputs failed, status: " + trainingTimeEstimateResponse.getStatus());\n        }\n        \n        System.out.print(trainingTimeEstimateResponse);\n        \n    }\n}\n',S='<?php\n\nrequire __DIR__ . "/vendor/autoload.php";\n\n/////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, app ID, model ID, and model type ID.\n// Change these strings to run your own example.\n/////////////////////////////////////////////////////////////////////////////////////////////////\n\n$USER_ID = "YOUR_USER_ID_HERE";\n// Your PAT (Personal Access Token) can be found in the portal under Authentification\n$PAT = "YOUR_PAT_HERE";\n$APP_ID = "YOUR_APP_ID_HERE";\n// Change these to create your own model\n$MODEL_ID = "petsID";\n$MODEL_TYPE_ID = "visual-classifier";\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\Model;\nuse Clarifai\\Api\\PostModelsRequest;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ["Authorization" => ["Key " . $PAT]];\n\n$userDataObject = new UserAppIDSet([\n    "user_id" => $USER_ID,\n    "app_id" => $APP_ID,\n]);\n\n// Let\'s make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client->PostModels(\n        // The request object carries the request along with the request status and other metadata related to the request itself\n        new PostModelsRequest([\n            "user_app_id" => $userDataObject,\n            "models" => [\n                new Model([                    \n                    "id" => $MODEL_ID,\n                    "model_type_id" => $MODEL_TYPE_ID,\n                ]),\n            ],\n        ]),\n        $metadata\n    )->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception("Error: {$status->details}");\n}\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure\n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    throw new Exception("Failure response: " . $response->getStatus()->getDescription() . " " . $response->getStatus()->getDetails());\n}\n\n?>',P='<?php\n\nrequire __DIR__ . "/vendor/autoload.php";\n\n///////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, app ID, and model type ID. \n// Change these strings to run your own example.\n/////////////////////////////////////////////////////////////////////////////////////\n\n$USER_ID = "YOUR_USER_ID_HERE";\n// Your PAT (Personal Access Token) can be found in the portal under Authentification\n$PAT = "YOUR_PAT_HERE";\n$APP_ID = "YOUR_APP_ID_HERE";\n// Change this to list the template types of your preferred model \n$MODEL_TYPE = "visual-classifier";\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\ListModelTypesRequest;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ["Authorization" => ["Key " . $PAT]];\n\n$userDataObject = new UserAppIDSet([\n    "user_id" => $USER_ID,\n    "app_id" => $APP_ID,\n]);\n\n// Let\'s make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client->ListModelTypes(\n    // The request object carries the request along with the request status and other metadata related to the request itself\n    new ListModelTypesRequest([\n        "user_app_id" => $userDataObject\n    ]),\n    $metadata\n)->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception("Error: {$status->details}");\n}\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure\n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    throw new Exception("Failure response: " . $response->getStatus()->getDescription() . " " . $response->getStatus()->getDetails());\n}\n\nforeach ($response->getModelTypes() as $modelType) {\n    if ($modelType->getId() === $MODEL_TYPE) {\n        foreach ($modelType->getModelTypeFields() as $modelTypeField) {\n            $pathComponents = explode(\'.\', $modelTypeField->getPath());\n            if (end($pathComponents) === \'template\') {\n                foreach ($modelTypeField->getModelTypeEnumOptions() as $template) {\n                    echo $template->serializeToJsonString() . "\\n";\n                }\n            }\n        }\n    }\n}\n',U='<?php\n\nrequire __DIR__ . "/vendor/autoload.php";\n\n//////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, app ID, and the details we want\n// to use to create a workflow. Change these strings to run your own example.\n/////////////////////////////////////////////////////////////////////////////////////////\n\n$USER_ID = "YOUR_USER_ID_HERE";\n// Your PAT (Personal Access Token) can be found in the portal under Authentification\n$PAT = "YOUR_PAT_HERE";\n$APP_ID = "YOUR_APP_ID_HERE";\n// Change these to create your own custom workflow\n$WORKFLOW_ID = "my-new-workflow-id";\n$EMBED_MODEL_ID = "YOUR_EMBED_MODEL_ID";\n$EMBED_MODEL_VERSION_ID = "YOUR_EMBED_MODEL_VERSION_ID";\n$WORKFLOWNODE_ID = "my-custom-model";\n$CUSTOM_MODEL_ID = "YOUR_CUSTOM_MODEL_ID";\n$CUSTOM_MODEL_VERSION_ID = "YOUR_CUSTOM_MODEL_VERSION_ID";\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\PostWorkflowsRequest;\nuse Clarifai\\Api\\Workflow;\nuse Clarifai\\Api\\WorkflowNode;\nuse Clarifai\\Api\\NodeInput;\nuse Clarifai\\Api\\Model;\nuse Clarifai\\Api\\ModelVersion;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ["Authorization" => ["Key " . $PAT]]; \n\n$userDataObject = new UserAppIDSet([\n    "user_id" => $USER_ID,\n    "app_id" => $APP_ID\n]);\n\n// Let\'s make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client->PostWorkflows(\n        // The request object carries the request along with the request status and other metadata related to the request itself\n        new PostWorkflowsRequest([\n            "user_app_id" => $userDataObject,\n            "workflows" => [\n                new Workflow([\n                    "id"=> $WORKFLOW_ID,\n                    "nodes" => [\n                        new WorkflowNode([\n                            "id" => "embed",\n                            "model" => new Model([\n                                "id" => $EMBED_MODEL_ID,\n                                "model_version" => new ModelVersion([\n                                    "id" => $EMBED_MODEL_VERSION_ID\n                                ])\n                            ])\n\n                        ]),\n                        new WorkflowNode([\n                            "id" => $WORKFLOWNODE_ID,\n                            "model"=> new Model([\n                                "id" => $CUSTOM_MODEL_ID,\n                                "model_version" => new ModelVersion([\n                                    "id" => $CUSTOM_MODEL_VERSION_ID\n                                ])\n                            ]),\n                            "node_inputs" => [\n                                new NodeInput([\n                                    "node_id"=> "embed"\n                                ])\n                            ]\n                        ])                       \n                    ]\n                ])\n            ]\n        ]),\n        $metadata\n    )->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception("Error: {$status->details}");\n}\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure \n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    print $response->getStatus()->getDetails();\n    throw new Exception("Failure response: " . $response->getStatus()->getDescription());\n}\n\n?>',M='curl -X POST "https://api.clarifai.com/v2/users/YOUR_USER_ID_HERE/apps/YOUR_APP_ID_HERE/models" \\\n  -H "Authorization: Key YOUR_PAT_HERE" \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "model": {\n      "id": "petsID",\n      "model_type_id": "visual-classifier"\n    }\n  }\'',k='curl -X POST "https://api.clarifai.com/v2/users/YOUR_USER_ID_HERE/apps/YOUR_APP_ID_HERE/models/YOUR_MODEL_ID_HERE/versions" \\\n  -H "Authorization: Key YOUR_PAT_HERE" \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n        "model_versions": [{\n            "train_info": {\n                "params": {\n                    "template": "MMClassification_ResNet_50_RSB_A1",\n                    "num_epochs": 2\n                }\n            },\n            "output_info": {\n                "data": {\n                    "concepts": [\n                        {\n                            "id": "ferrari23"\n                        },\n                        {\n                            "id": "outdoors23"\n                        }\n                    ]\n                }\n            }\n        }] \n  }\'',x='curl -X POST "https://api.clarifai.com/v2/users/YOUR_USER_ID_HERE/apps/YOUR_APP_ID_HERE/workflows" \\\n    -H "Authorization: Key YOUR_PAT_HERE" \\\n    -H "Content-Type: application/json" \\\n    --data-raw \'{\n        "workflows": [\n            {\n                "id": "my-new-workflow-id",\n                "nodes": [\n                    {\n                        "id": "embed",\n                        "model": {\n                            "id": "YOUR_EMBED_MODEL_ID_HERE",\n                            "model_version": {\n                                "id": "YOUR_EMBED_MODEL_VERSION_ID_HERE"\n                            }\n                        }\n                    },\n                    {\n                        "id": "my-custom-model",\n                        "model": {\n                            "id": "YOUR_CUSTOM_MODEL_ID_HERE",\n                            "model_version": {\n                                "id": "YOUR_CUSTOM_MODEL_VERSION_ID_HERE"\n                            }\n                        },\n                        "node_inputs": [\n                            {\n                                "node_id": "embed"\n                            }\n                        ]\n                    }\n                ]\n            }\n        ]\n    }\'',L='curl -X PATCH "https://api.clarifai.com/v2/users/YOUR_USER_ID_HERE/apps/" \\\n    -H "Authorization: Key YOUR_PAT_HERE" \\\n    -H "Content-Type: application/json" \\\n    --data-raw \'{\n        "action": "overwrite",\n        "apps": [\n            {\n                "id": "YOUR_APP_ID_HERE",\n                "default_workflow_id": "auto-annotation-workflow-ID"\n            }\n        ]\n    }\'',Y='curl -X POST "https://api.clarifai.com/v2/users/YOUR_USER_ID_HERE/apps/YOUR_APP_ID_HERE/models/YOUR_MODEL_ID_HERE/versions/time_estimate/" \\\n  -H "Authorization: Key YOUR_PAT_HERE" \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n        "model_versions": [{\n            "train_info": {\n                "params": {\n                    "template": "MMDetection_FasterRCNN"                    \n                }\n            }\n        }],\n        "estimated_input_count": 100\n  }\'',H='model_type_fields {\n  path: "train_info.params.num_gpus"\n  field_type: RANGE\n  default_value {\n    number_value: 1.0\n  }\n  description: "[internal_only] the number of gpus to train with."\n  placeholder: "num_gpus"\n  internal_only: true\n  model_type_range_info {\n    max: 1.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.logreg"\n  field_type: RANGE\n  default_value {\n    number_value: 1.0\n  }\n  description: "Whether to use sigmoid units (logreg=1) or softmax (logreg=0)."\n  placeholder: "logreg"\n  internal_only: true\n  model_type_range_info {\n    max: 1.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.image_size"\n  field_type: RANGE\n  default_value {\n    number_value: 256.0\n  }\n  description: "Input image size (minimum side dimension)."\n  placeholder: "image_size"\n  internal_only: true\n  model_type_range_info {\n    min: 32.0\n    max: 1024.0\n    step: 16.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.batch_size"\n  field_type: RANGE\n  default_value {\n    number_value: 128.0\n  }\n  description: "the batch size to use during training."\n  placeholder: "batch_size"\n  internal_only: true\n  model_type_range_info {\n    min: 1.0\n    max: 128.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.lrate"\n  field_type: NUMBER\n  default_value {\n    number_value: 0.1\n  }\n  description: "the learning rate (per minibatch)"\n  placeholder: "lrate"\n  internal_only: true\n}\nmodel_type_fields {\n  path: "train_info.params.base_gradient_multiplier"\n  field_type: NUMBER\n  default_value {\n    number_value: 0.001\n  }\n  description: "learning rate multipler applied to the pre-initialized backbone model weights"\n  placeholder: "base_gradient_multiplier"\n  internal_only: true\n}\nmodel_type_fields {\n  path: "train_info.params.num_epochs"\n  field_type: RANGE\n  default_value {\n    number_value: 20.0\n  }\n  description: "the total number of epochs to train for."\n  placeholder: "num_epochs"\n  internal_only: true\n  model_type_range_info {\n    min: 1.0\n    max: 200.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.num_items_per_epoch"\n  field_type: NUMBER\n  default_value {\n    number_value: 0.0\n  }\n  description: "number of input images that constitute an \\"epoch\\".  Default is the number of images in the dataset."\n  placeholder: "num_items_per_epoch"\n  internal_only: true\n}\nmodel_type_fields {\n  path: "train_info.params.embeddings_layer"\n  field_type: STRING\n  default_value {\n    string_value: "mod5B.concat"\n  }\n  description: "the embedding layer to use as output from this model."\n  placeholder: "embeddings_layer"\n  internal_only: true\n}\nmodel_type_fields {\n  path: "train_info.params.average_horizontal_flips"\n  field_type: BOOLEAN\n  default_value {\n    bool_value: true\n  }\n  description: "if true then average the embeddings from the image and a horizontal flip of the image to get the final embedding vectors to output."\n  placeholder: "average_horizontal_flips"\n  internal_only: true\n}\ninternal_only: true\n\nid: "classification_basemodel_v1"\ndescription: "A training template that uses Clarifais training implementation. "\nmodel_type_fields {\n  path: "train_info.params.num_gpus"\n  field_type: RANGE\n  default_value {\n    number_value: 1.0\n  }\n  description: "[internal_only] the number of gpus to train with."\n  placeholder: "num_gpus"\n  internal_only: true\n  model_type_range_info {\n    max: 1.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.model_cfg"\n  field_type: STRING\n  default_value {\n    string_value: "resnext"\n  }\n  description: "the underlying model configuration to use."\n  placeholder: "model_cfg"\n  internal_only: true\n}\nmodel_type_fields {\n  path: "train_info.params.preinit"\n  field_type: STRING\n  default_value {\n    string_value: "general-v1.5"\n  }\n  description: "specifies pre-initialized net to use."\n  placeholder: "preinit"\n  internal_only: true\n}\nmodel_type_fields {\n  path: "train_info.params.logreg"\n  field_type: NUMBER\n  default_value {\n    number_value: 1.0\n  }\n  description: "Whether to use sigmoid units (logreg=1) or softmax (logreg=0)."\n  placeholder: "logreg"\n  internal_only: true\n}\nmodel_type_fields {\n  path: "train_info.params.image_size"\n  field_type: RANGE\n  default_value {\n    number_value: 256.0\n  }\n  description: "Input image size (minimum side dimension)."\n  placeholder: "image_size"\n  internal_only: true\n  model_type_range_info {\n    min: 32.0\n    max: 1024.0\n    step: 16.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.batch_size"\n  field_type: RANGE\n  default_value {\n    number_value: 64.0\n  }\n  description: "the batch size to use during training."\n  placeholder: "batch_size"\n  internal_only: true\n  model_type_range_info {\n    min: 1.0\n    max: 128.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.init_epochs"\n  field_type: RANGE\n  default_value {\n    number_value: 25.0\n  }\n  description: "number of epochs to run at the initial learning rate."\n  placeholder: "init_epochs"\n  internal_only: true\n  model_type_range_info {\n    min: 1.0\n    max: 200.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.step_epochs"\n  field_type: RANGE\n  default_value {\n    number_value: 7.0\n  }\n  description: "the number of epochs between learning rate decreases."\n  placeholder: "step_epochs"\n  internal_only: true\n  model_type_range_info {\n    min: 1.0\n    max: 200.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.num_epochs"\n  field_type: RANGE\n  default_value {\n    number_value: 65.0\n  }\n  description: "the total number of epochs to train for."\n  placeholder: "num_epochs"\n  internal_only: true\n  model_type_range_info {\n    min: 1.0\n    max: 200.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.per_item_lrate"\n  field_type: NUMBER\n  default_value {\n    number_value: 7.8125e-05\n  }\n  description: "the initial learning rate per item. The overall learning rate (per step) is set to lrate = batch_size * per_item_lrate"\n  placeholder: "per_item_lrate"\n  internal_only: true\n}\nmodel_type_fields {\n  path: "train_info.params.num_items_per_epoch"\n  field_type: NUMBER\n  default_value {\n    number_value: 0.0\n  }\n  description: "number of input images that constitute an \\"epoch\\".  Default is the number of images in the dataset."\n  placeholder: "num_items_per_epoch"\n  internal_only: true\n}\nmodel_type_fields {\n  path: "train_info.params.inference_crop_type"\n  field_type: STRING\n  default_value {\n    string_value: "sorta2"\n  }\n  description: "the crop type to use for inference (used when evaluating the model)."\n  placeholder: "inference_crop_type"\n  internal_only: true\n}\ninternal_only: true\n\nid: "classification_cifar10_v1"\ndescription: "A runner optimized for cifar10 training. Not to be used in real use cases. "\nmodel_type_fields {\n  path: "train_info.params.num_gpus"\n  field_type: RANGE\n  default_value {\n    number_value: 1.0\n  }\n  description: "the number of gpus to train with."\n  placeholder: "num_gpus"\n  internal_only: true\n  model_type_range_info {\n    max: 1.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.image_size"\n  field_type: RANGE\n  default_value {\n    number_value: 32.0\n  }\n  description: "the image size to train on. This is for the minimum dimension."\n  placeholder: "image_size"\n  internal_only: true\n  model_type_range_info {\n    min: 32.0\n    max: 1024.0\n    step: 16.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.batch_size"\n  field_type: RANGE\n  default_value {\n    number_value: 128.0\n  }\n  description: "the batch size to use during training."\n  placeholder: "batch_size"\n  internal_only: true\n  model_type_range_info {\n    min: 1.0\n    max: 128.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.num_epochs"\n  field_type: RANGE\n  default_value {\n    number_value: 65.0\n  }\n  description: "the total number of epochs to train for."\n  placeholder: "num_epochs"\n  internal_only: true\n  model_type_range_info {\n    min: 1.0\n    max: 200.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.inference_crop_type"\n  field_type: STRING\n  default_value {\n    string_value: "sorta2"\n  }\n  description: "the crop type to use for inference (used when evaluating the model)."\n  placeholder: "inference_crop_type"\n  internal_only: true\n}\ninternal_only: true\n\nid: "Clarifai_InceptionTransferEmbedNorm"\ndescription: "A custom visual classifier template inspired by Inception networks and tuned for speed with\\nother optimizations for transfer learning. "\nmodel_type_fields {\n  path: "train_info.params.logreg"\n  field_type: RANGE\n  default_value {\n    number_value: 1.0\n  }\n  description: "Whether to use sigmoid units (logreg=1) or softmax (logreg=0)."\n  placeholder: "logreg"\n  model_type_range_info {\n    max: 1.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.image_size"\n  field_type: RANGE\n  default_value {\n    number_value: 256.0\n  }\n  description: "Input image size (minimum side dimension)."\n  placeholder: "image_size"\n  model_type_range_info {\n    min: 32.0\n    max: 1024.0\n    step: 16.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.batch_size"\n  field_type: RANGE\n  default_value {\n    number_value: 128.0\n  }\n  description: "the batch size to use during training."\n  placeholder: "batch_size"\n  model_type_range_info {\n    min: 1.0\n    max: 128.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.lrate"\n  field_type: NUMBER\n  default_value {\n    number_value: 0.1\n  }\n  description: "the learning rate (per minibatch)"\n  placeholder: "lrate"\n}\nmodel_type_fields {\n  path: "train_info.params.base_gradient_multiplier"\n  field_type: NUMBER\n  default_value {\n    number_value: 0.001\n  }\n  description: "learning rate multipler applied to the pre-initialized backbone model weights"\n  placeholder: "base_gradient_multiplier"\n}\nmodel_type_fields {\n  path: "train_info.params.num_epochs"\n  field_type: RANGE\n  default_value {\n    number_value: 20.0\n  }\n  description: "the total number of epochs to train for."\n  placeholder: "num_epochs"\n  model_type_range_info {\n    min: 1.0\n    max: 200.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.num_items_per_epoch"\n  field_type: NUMBER\n  default_value {\n    number_value: 0.0\n  }\n  description: "number of input images that constitute an \\"epoch\\".  Default is the number of images in the dataset."\n  placeholder: "num_items_per_epoch"\n}\nmodel_type_fields {\n  path: "train_info.params.average_horizontal_flips"\n  field_type: BOOLEAN\n  default_value {\n    bool_value: true\n  }\n  description: "if true then average the embeddings from the image and a horizontal flip of the image to get the final embedding vectors to output."\n  placeholder: "average_horizontal_flips"\n}\n\nid: "Clarifai_ResNext"\ndescription: "A custom visual classifier template inspired by ResNext networks. "\nmodel_type_fields {\n  path: "train_info.params.logreg"\n  field_type: RANGE\n  default_value {\n    number_value: 1.0\n  }\n  description: "Whether to use sigmoid units (logreg=1) or softmax (logreg=0)."\n  placeholder: "logreg"\n  model_type_range_info {\n    max: 1.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.image_size"\n  field_type: RANGE\n  default_value {\n    number_value: 256.0\n  }\n  description: "Input image size (minimum side dimension)."\n  placeholder: "image_size"\n  model_type_range_info {\n    min: 32.0\n    max: 1024.0\n    step: 16.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.batch_size"\n  field_type: RANGE\n  default_value {\n    number_value: 64.0\n  }\n  description: "the batch size to use during training."\n  placeholder: "batch_size"\n  model_type_range_info {\n    min: 1.0\n    max: 128.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.init_epochs"\n  field_type: RANGE\n  default_value {\n    number_value: 25.0\n  }\n  description: "number of epochs to run at the initial learning rate."\n  placeholder: "init_epochs"\n  model_type_range_info {\n    min: 1.0\n    max: 200.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.step_epochs"\n  field_type: RANGE\n  default_value {\n    number_value: 7.0\n  }\n  description: "the number of epochs between learning rate decreases."\n  placeholder: "step_epochs"\n  model_type_range_info {\n    min: 1.0\n    max: 200.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.num_epochs"\n  field_type: RANGE\n  default_value {\n    number_value: 65.0\n  }\n  description: "the total number of epochs to train for."\n  placeholder: "num_epochs"\n  model_type_range_info {\n    min: 1.0\n    max: 200.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.per_item_lrate"\n  field_type: NUMBER\n  default_value {\n    number_value: 7.8125e-05\n  }\n  description: "the initial learning rate per item. The overall learning rate (per step) is set to lrate = batch_size * per_item_lrate"\n  placeholder: "per_item_lrate"\n}\nmodel_type_fields {\n  path: "train_info.params.num_items_per_epoch"\n  field_type: NUMBER\n  default_value {\n    number_value: 0.0\n  }\n  description: "number of input images that constitute an \\"epoch\\".  Default is the number of images in the dataset."\n  placeholder: "num_items_per_epoch"\n}\n\nid: "Clarifai_InceptionV2"\ndescription: "A custom visual classifier template inspired by Inception-V2 networks. "\nmodel_type_fields {\n  path: "train_info.params.logreg"\n  field_type: RANGE\n  default_value {\n    number_value: 1.0\n  }\n  description: "Whether to use sigmoid units (logreg=1) or softmax (logreg=0)."\n  placeholder: "logreg"\n  model_type_range_info {\n    max: 1.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.image_size"\n  field_type: RANGE\n  default_value {\n    number_value: 256.0\n  }\n  description: "Input image size (minimum side dimension)."\n  placeholder: "image_size"\n  model_type_range_info {\n    min: 32.0\n    max: 1024.0\n    step: 16.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.batch_size"\n  field_type: RANGE\n  default_value {\n    number_value: 64.0\n  }\n  description: "the batch size to use during training."\n  placeholder: "batch_size"\n  model_type_range_info {\n    min: 1.0\n    max: 128.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.init_epochs"\n  field_type: RANGE\n  default_value {\n    number_value: 25.0\n  }\n  description: "number of epochs to run at the initial learning rate."\n  placeholder: "init_epochs"\n  model_type_range_info {\n    min: 1.0\n    max: 200.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.step_epochs"\n  field_type: RANGE\n  default_value {\n    number_value: 7.0\n  }\n  description: "the number of epochs between learning rate decreases."\n  placeholder: "step_epochs"\n  model_type_range_info {\n    min: 1.0\n    max: 200.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.num_epochs"\n  field_type: RANGE\n  default_value {\n    number_value: 65.0\n  }\n  description: "the total number of epochs to train for."\n  placeholder: "num_epochs"\n  model_type_range_info {\n    min: 1.0\n    max: 200.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.per_item_lrate"\n  field_type: NUMBER\n  default_value {\n    number_value: 7.8125e-05\n  }\n  description: "the initial learning rate per item. The overall learning rate (per step) is set to lrate = batch_size * per_item_lrate"\n  placeholder: "per_item_lrate"\n}\nmodel_type_fields {\n  path: "train_info.params.num_items_per_epoch"\n  field_type: NUMBER\n  default_value {\n    number_value: 0.0\n  }\n  description: "number of input images that constitute an \\"epoch\\".  Default is the number of images in the dataset."\n  placeholder: "num_items_per_epoch"\n}\n\nid: "Clarifai_InceptionBatchNorm"\ndescription: "A custom visual classifier template inspired by Inception networks tuned for speed. "\nmodel_type_fields {\n  path: "train_info.params.logreg"\n  field_type: RANGE\n  default_value {\n    number_value: 1.0\n  }\n  description: "Whether to use sigmoid units (logreg=1) or softmax (logreg=0)."\n  placeholder: "logreg"\n  model_type_range_info {\n    max: 1.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.image_size"\n  field_type: RANGE\n  default_value {\n    number_value: 256.0\n  }\n  description: "Input image size (minimum side dimension)."\n  placeholder: "image_size"\n  model_type_range_info {\n    min: 32.0\n    max: 1024.0\n    step: 16.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.batch_size"\n  field_type: RANGE\n  default_value {\n    number_value: 64.0\n  }\n  description: "the batch size to use during training."\n  placeholder: "batch_size"\n  model_type_range_info {\n    min: 1.0\n    max: 128.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.init_epochs"\n  field_type: RANGE\n  default_value {\n    number_value: 25.0\n  }\n  description: "number of epochs to run at the initial learning rate."\n  placeholder: "init_epochs"\n  model_type_range_info {\n    min: 1.0\n    max: 200.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.step_epochs"\n  field_type: RANGE\n  default_value {\n    number_value: 7.0\n  }\n  description: "the number of epochs between learning rate decreases."\n  placeholder: "step_epochs"\n  model_type_range_info {\n    min: 1.0\n    max: 200.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.num_epochs"\n  field_type: RANGE\n  default_value {\n    number_value: 65.0\n  }\n  description: "the total number of epochs to train for."\n  placeholder: "num_epochs"\n  model_type_range_info {\n    min: 1.0\n    max: 200.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.per_item_lrate"\n  field_type: NUMBER\n  default_value {\n    number_value: 7.8125e-05\n  }\n  description: "the initial learning rate per item. The overall learning rate (per step) is set to lrate = batch_size * per_item_lrate"\n  placeholder: "per_item_lrate"\n}\nmodel_type_fields {\n  path: "train_info.params.num_items_per_epoch"\n  field_type: NUMBER\n  default_value {\n    number_value: 0.0\n  }\n  description: "number of input images that constitute an \\"epoch\\".  Default is the number of images in the dataset."\n  placeholder: "num_items_per_epoch"\n}\n\nid: "MMClassification"\ndescription: "A training template that uses the MMClassification toolkit and a custom configuration "\nmodel_type_fields {\n  path: "train_info.params.seed"\n  field_type: NUMBER\n  default_value {\n    number_value: -1.0\n  }\n  description: "[internal_only] the random seed to init training. If seed < 0, it is not set"\n  placeholder: "seed"\n  internal_only: true\n}\nmodel_type_fields {\n  path: "train_info.params.custom_config"\n  field_type: PYTHON_CODE\n  default_value {\n    string_value: "\\n_base_ = \\\'/mmclassification/configs/resnext/resnext101_32x4d_b32x8_imagenet.py\\\'\\nrunner = dict(type=\\\'EpochBasedRunner\\\', max_epochs=60)\\ndata = dict(\\n    train=dict(\\n        data_prefix=\\\'\\\',\\n        ann_file=\\\'\\\',\\n        classes=\\\'\\\'),\\n    val=dict(\\n        data_prefix=\\\'\\\',\\n        ann_file=\\\'\\\',\\n        classes=\\\'\\\'))\\n"\n  }\n  description: "custom mmclassification config, in python config file format. Note that the \\\'_base_\\\' field, if used, should be a config file relative to the parent directory \\\'/mmclassification/\\\', e.g. \\"_base_ = \\\'/mmclassification/configs/efficientnet/efficientnet-b8_8xb32-01norm_in1k.py\\\'\\". The \\\'num_classes\\\' field must be included somewhere in the config. The \\\'data\\\' section should include \\\'train\\\' and \\\'val\\\' sections, each with \\\'ann_file\\\', \\\'data_prefix\\\', and \\\'classes\\\' fields with empty strings as values. These values will be overwritten to be compatible with Clarifai\\\'s system, but must be included in the imported config."\n  placeholder: "custom_config"\n}\nmodel_type_fields {\n  path: "train_info.params.concepts_mutually_exclusive"\n  field_type: BOOLEAN\n  default_value {\n    bool_value: false\n  }\n  description: "whether the concepts are mutually exclusive. If true then each input is expected to only be tagged with a single concept."\n  placeholder: "concepts_mutually_exclusive"\n}\nmodel_type_fields {\n  path: "train_info.params.num_gpus"\n  field_type: RANGE\n  default_value {\n    number_value: 1.0\n  }\n  description: "[internal_only] the number of gpus to train with."\n  placeholder: "num_gpus"\n  internal_only: true\n  model_type_range_info {\n    max: 1.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.image_size"\n  field_type: ARRAY_OF_NUMBERS\n  default_value {\n    list_value {\n      values {\n        number_value: 320.0\n      }\n    }\n  }\n  description: "the image size for inference (the training image size is defined in the mmcv config). If a single value, specifies the size of the min side."\n  placeholder: "image_size"\n}\n\nid: "MMClassification_EfficientNet"\ndescription: "A training template that uses the MMClassification toolkit and EfficientNet-B8 configuration "\nmodel_type_fields {\n  path: "train_info.params.seed"\n  field_type: NUMBER\n  default_value {\n    number_value: -1.0\n  }\n  description: "[internal_only] the random seed to init training. If seed < 0, we will not set it."\n  placeholder: "seed"\n  internal_only: true\n}\nmodel_type_fields {\n  path: "train_info.params.num_gpus"\n  field_type: RANGE\n  default_value {\n    number_value: 1.0\n  }\n  description: "[internal_only] the number of gpus to train with."\n  placeholder: "num_gpus"\n  internal_only: true\n  model_type_range_info {\n    max: 1.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.image_size"\n  field_type: NUMBER\n  default_value {\n    number_value: 336.0\n  }\n  description: "the image size for training and inference. EfficientNet works on square images."\n  placeholder: "image_size"\n  internal_only: true\n}\nmodel_type_fields {\n  path: "train_info.params.batch_size"\n  field_type: RANGE\n  default_value {\n    number_value: 4.0\n  }\n  description: "the batch size to use during training."\n  placeholder: "batch_size"\n  internal_only: true\n  model_type_range_info {\n    min: 1.0\n    max: 256.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.num_epochs"\n  field_type: RANGE\n  default_value {\n    number_value: 30.0\n  }\n  description: "the total number of epochs to train for."\n  placeholder: "num_epochs"\n  internal_only: true\n  model_type_range_info {\n    min: 1.0\n    max: 200.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.per_item_lrate"\n  field_type: NUMBER\n  default_value {\n    number_value: 0.000390625\n  }\n  description: "the initial learning rate per item. The overall learning rate (per step) is set to lrate = batch_size * per_item_lrate"\n  placeholder: "per_item_lrate"\n  internal_only: true\n}\nmodel_type_fields {\n  path: "train_info.params.weight_decay"\n  field_type: RANGE\n  default_value {\n    number_value: 0.0001\n  }\n  description: "the weight decay value"\n  placeholder: "weight_decay"\n  internal_only: true\n  model_type_range_info {\n    max: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.momentum"\n  field_type: RANGE\n  default_value {\n    number_value: 0.9\n  }\n  description: "the momentum value for the SGD optimizer"\n  placeholder: "momentum"\n  internal_only: true\n  model_type_range_info {\n    max: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.pretrained_weights"\n  field_type: ENUM\n  default_value {\n    string_value: "ImageNet-1k"\n  }\n  description: "whether to use pretrained weights."\n  placeholder: "pretrained_weights"\n  model_type_enum_options {\n    id: "None"\n  }\n  model_type_enum_options {\n    id: "ImageNet-1k"\n  }\n  internal_only: true\n}\nmodel_type_fields {\n  path: "train_info.params.flip_probability"\n  field_type: RANGE\n  default_value {\n    number_value: 0.5\n  }\n  description: "the probability an image will be flipped during training"\n  placeholder: "flip_probability"\n  internal_only: true\n  model_type_range_info {\n    max: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.flip_direction"\n  field_type: ENUM\n  default_value {\n    string_value: "horizontal"\n  }\n  description: "the direction to randomly flip during training."\n  placeholder: "flip_direction"\n  model_type_enum_options {\n    id: "horizontal"\n  }\n  model_type_enum_options {\n    id: "vertical"\n  }\n  internal_only: true\n}\nmodel_type_fields {\n  path: "train_info.params.concepts_mutually_exclusive"\n  field_type: BOOLEAN\n  default_value {\n    bool_value: false\n  }\n  description: "whether the concepts are mutually exclusive. If true then each input is expected to only be tagged with a single concept."\n  placeholder: "concepts_mutually_exclusive"\n  internal_only: true\n}\ninternal_only: true\n\nid: "MMClassification_ResNet_50_RSB_A1"\ndescription: "A training template that uses the MMClassification toolkit and ResNet-50 (rsb-a1) configuration "\nmodel_type_fields {\n  path: "train_info.params.seed"\n  field_type: NUMBER\n  default_value {\n    number_value: -1.0\n  }\n  description: "[internal_only] the random seed to init training. If seed < 0, we will not set it."\n  placeholder: "seed"\n  internal_only: true\n}\nmodel_type_fields {\n  path: "train_info.params.num_gpus"\n  field_type: RANGE\n  default_value {\n    number_value: 1.0\n  }\n  description: "[internal_only] the number of gpus to train with."\n  placeholder: "num_gpus"\n  internal_only: true\n  model_type_range_info {\n    max: 1.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.image_size"\n  field_type: NUMBER\n  default_value {\n    number_value: 224.0\n  }\n  description: "the image size for training and inference. ResNet uses square images."\n  placeholder: "image_size"\n}\nmodel_type_fields {\n  path: "train_info.params.batch_size"\n  field_type: RANGE\n  default_value {\n    number_value: 64.0\n  }\n  description: "the batch size to use during training."\n  placeholder: "batch_size"\n  model_type_range_info {\n    min: 1.0\n    max: 256.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.num_epochs"\n  field_type: RANGE\n  default_value {\n    number_value: 60.0\n  }\n  description: "the total number of epochs to train for."\n  placeholder: "num_epochs"\n  model_type_range_info {\n    min: 1.0\n    max: 600.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.per_item_lrate"\n  field_type: NUMBER\n  default_value {\n    number_value: 1.953125e-05\n  }\n  description: "the initial learning rate per item. The overall learning rate (per step) is set to lrate = batch_size * per_item_lrate"\n  placeholder: "per_item_lrate"\n}\nmodel_type_fields {\n  path: "train_info.params.weight_decay"\n  field_type: RANGE\n  default_value {\n    number_value: 0.01\n  }\n  description: "the weight decay value"\n  placeholder: "weight_decay"\n  model_type_range_info {\n    max: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.per_item_min_lrate"\n  field_type: NUMBER\n  default_value {\n    number_value: 1.5625e-08\n  }\n  description: "The minimum learning (per item) at end of training using cosine schedule."\n  placeholder: "per_item_min_lrate"\n}\nmodel_type_fields {\n  path: "train_info.params.warmup_iters"\n  field_type: NUMBER\n  default_value {\n    number_value: 100.0\n  }\n  description: "The number of steps in the warmup phase"\n  placeholder: "warmup_iters"\n}\nmodel_type_fields {\n  path: "train_info.params.warmup_ratio"\n  field_type: NUMBER\n  default_value {\n    number_value: 0.0001\n  }\n  description: " Warmup phase learning rate multiplier"\n  placeholder: "warmup_ratio"\n}\nmodel_type_fields {\n  path: "train_info.params.pretrained_weights"\n  field_type: ENUM\n  default_value {\n    string_value: "ImageNet-1k"\n  }\n  description: "whether to use pretrained weights."\n  placeholder: "pretrained_weights"\n  model_type_enum_options {\n    id: "None"\n  }\n  model_type_enum_options {\n    id: "ImageNet-1k"\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.flip_probability"\n  field_type: RANGE\n  default_value {\n    number_value: 0.5\n  }\n  description: "the probability an image will be flipped during training"\n  placeholder: "flip_probability"\n  model_type_range_info {\n    max: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.flip_direction"\n  field_type: ENUM\n  default_value {\n    string_value: "horizontal"\n  }\n  description: "the direction to randomly flip during training."\n  placeholder: "flip_direction"\n  model_type_enum_options {\n    id: "horizontal"\n  }\n  model_type_enum_options {\n    id: "vertical"\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.concepts_mutually_exclusive"\n  field_type: BOOLEAN\n  default_value {\n    bool_value: false\n  }\n  description: "whether the concepts are mutually exclusive. If true then each input is expected to only be tagged with a single concept."\n  placeholder: "concepts_mutually_exclusive"\n}\nrecommended: true\n\nid: "MMClassification_ResNet_50"\ndescription: "A training template that uses the MMClassification toolkit and ResNet-50 configuration "\nmodel_type_fields {\n  path: "train_info.params.seed"\n  field_type: NUMBER\n  default_value {\n    number_value: -1.0\n  }\n  description: "[internal_only] the random seed to init training. If seed < 0, we will not set it."\n  placeholder: "seed"\n  internal_only: true\n}\nmodel_type_fields {\n  path: "train_info.params.num_gpus"\n  field_type: RANGE\n  default_value {\n    number_value: 1.0\n  }\n  description: "[internal_only] the number of gpus to train with."\n  placeholder: "num_gpus"\n  internal_only: true\n  model_type_range_info {\n    max: 1.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.image_size"\n  field_type: NUMBER\n  default_value {\n    number_value: 224.0\n  }\n  description: "the image size for training and inference. ResNet works on square images."\n  placeholder: "image_size"\n  internal_only: true\n}\nmodel_type_fields {\n  path: "train_info.params.batch_size"\n  field_type: RANGE\n  default_value {\n    number_value: 64.0\n  }\n  description: "the batch size to use per gpu during training."\n  placeholder: "batch_size"\n  internal_only: true\n  model_type_range_info {\n    min: 1.0\n    max: 256.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.num_epochs"\n  field_type: RANGE\n  default_value {\n    number_value: 60.0\n  }\n  description: "the total number of epochs to train for."\n  placeholder: "num_epochs"\n  internal_only: true\n  model_type_range_info {\n    min: 1.0\n    max: 600.0\n    step: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.per_item_lrate"\n  field_type: NUMBER\n  default_value {\n    number_value: 0.000390625\n  }\n  description: "the initial learning rate per item. The overall learning rate (per step) is set to lrate = batch_size * per_item_lrate"\n  placeholder: "per_item_lrate"\n  internal_only: true\n}\nmodel_type_fields {\n  path: "train_info.params.learning_rate_steps"\n  field_type: ARRAY_OF_NUMBERS\n  default_value {\n    list_value {\n      values {\n        number_value: 30.0\n      }\n      values {\n        number_value: 40.0\n      }\n      values {\n        number_value: 50.0\n      }\n    }\n  }\n  description: "epoch schedule for stepping down learning rate"\n  placeholder: "learning_rate_steps"\n  internal_only: true\n}\nmodel_type_fields {\n  path: "train_info.params.weight_decay"\n  field_type: RANGE\n  default_value {\n    number_value: 0.0001\n  }\n  description: "the weight decay value"\n  placeholder: "weight_decay"\n  internal_only: true\n  model_type_range_info {\n    max: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.momentum"\n  field_type: RANGE\n  default_value {\n    number_value: 0.9\n  }\n  description: "the momentum value for the SGD optimizer"\n  placeholder: "momentum"\n  internal_only: true\n  model_type_range_info {\n    max: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.pretrained_weights"\n  field_type: ENUM\n  default_value {\n    string_value: "ImageNet-1k"\n  }\n  description: "whether to use pretrained weights."\n  placeholder: "pretrained_weights"\n  model_type_enum_options {\n    id: "None"\n  }\n  model_type_enum_options {\n    id: "ImageNet-1k"\n  }\n  internal_only: true\n}\nmodel_type_fields {\n  path: "train_info.params.flip_probability"\n  field_type: RANGE\n  default_value {\n    number_value: 0.5\n  }\n  description: "the probability an image will be flipped during training"\n  placeholder: "flip_probability"\n  internal_only: true\n  model_type_range_info {\n    max: 1.0\n  }\n}\nmodel_type_fields {\n  path: "train_info.params.flip_direction"\n  field_type: ENUM\n  default_value {\n    string_value: "horizontal"\n  }\n  description: "the direction to randomly flip during training."\n  placeholder: "flip_direction"\n  model_type_enum_options {\n    id: "horizontal"\n  }\n  model_type_enum_options {\n    id: "vertical"\n  }\n  internal_only: true\n}\nmodel_type_fields {\n  path: "train_info.params.concepts_mutually_exclusive"\n  field_type: BOOLEAN\n  default_value {\n    bool_value: false\n  }\n  description: "whether the concepts are mutually exclusive. If true then each input is expected to only be tagged with a single concept."\n  placeholder: "concepts_mutually_exclusive"\n  internal_only: true\n}\ninternal_only: true\n',B='status {\n    code: SUCCESS\n    description: "Ok"\n    req_id: "f45dfcf36746a567f690744f0b3805a7"\n  }\n  training_time_estimates {\n    seconds: 308\n  }\n  ',G={description:"Train the complete graph of your model.",sidebar_position:6},z="Deep Fine-Tuning",V={unversionedId:"api-guide/model/deep-training",id:"api-guide/model/deep-training",title:"Deep Fine-Tuning",description:"Train the complete graph of your model.",source:"@site/docs/api-guide/model/deep-training.md",sourceDirName:"api-guide/model",slug:"/api-guide/model/deep-training",permalink:"/api-guide/model/deep-training",draft:!1,editUrl:"https://github.com/Clarifai/docs/blob/main/docs/api-guide/model/deep-training.md",tags:[],version:"current",sidebarPosition:6,frontMatter:{description:"Train the complete graph of your model.",sidebar_position:6},sidebar:"tutorialSidebar",previous:{title:"Create, Train, Get, Update, Delete",permalink:"/api-guide/model/create-get-update-and-delete"},next:{title:"Evaluating Models",permalink:"/api-guide/evaluate/"}},W={},$=[{value:"Create Models",id:"create-models",level:2},{value:"Template Types",id:"template-types",level:2},{value:"Training Time Estimator",id:"training-time-estimator",level:2},{value:"Train a Model",id:"train-a-model",level:2},{value:"Use Your Own Template",id:"use-your-own-template",level:3},{value:"Create a Workflow",id:"create-a-workflow",level:2},{value:"Update Your Default Workflow",id:"update-your-default-workflow",level:3}],Z={toc:$},j="wrapper";function q(e){let{components:n,...t}=e;return(0,i.kt)(j,(0,a.Z)({},Z,t,{components:n,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"deep-fine-tuning"},"Deep Fine-Tuning"),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Train the complete graph of your model")),(0,i.kt)("hr",null),(0,i.kt)("p",null,'Clarifai offers a variety of pre-built models that are designed to help you create AI solutions quickly and efficiently. Clarifai models are the recommended starting points for many users because they offer incredibly fast training times when you customize them using the "embedding-classifier" (',(0,i.kt)("a",{parentName:"p",href:"https://docs.clarifai.com/portal-guide/model/model-types/transfer-learning"},"transfer learning classifier"),") model type."),(0,i.kt)("p",null,'But there are many cases where accuracy and the ability to carefully target solutions take priority over speed and ease of use. Additionally, you may need a model to learn new features, not recognized by existing Clarifai models. For these cases, it is possible to "deep fine-tune" your custom models and integrate them directly within your workflows.'),(0,i.kt)("p",null,"In general, deep trained models need more data than those trained on top of Clarifai models. For most applications, you\u2019ll need at least 1000 training inputs, but it could be much more than this, depending on your specific use case."),(0,i.kt)("p",null,"You might consider deep training if you have:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"A custom tailored dataset"),(0,i.kt)("li",{parentName:"ul"},"Accurate labels"),(0,i.kt)("li",{parentName:"ul"},"Expertise and time to fine-tune models")),(0,i.kt)("admonition",{type:"info"},(0,i.kt)("p",{parentName:"admonition"},"The initialization code used in the following examples is outlined in detail on the ",(0,i.kt)("a",{parentName:"p",href:"https://docs.clarifai.com/api-guide/api-overview/api-clients/#client-installation-instructions"},"client installation page."))),(0,i.kt)("h2",{id:"create-models"},"Create Models"),(0,i.kt)("p",null,"To create a deep fine-tuned model, you need to specify the ",(0,i.kt)("a",{parentName:"p",href:"https://docs.clarifai.com/portal-guide/model/model-types/"},"type of model")," using the ",(0,i.kt)("inlineCode",{parentName:"p"},"model_type_id")," parameter\u2060. "),(0,i.kt)("admonition",{type:"tip"},(0,i.kt)("p",{parentName:"admonition"},"You can use the ",(0,i.kt)("a",{parentName:"p",href:"https://docs.clarifai.com/api-guide/model/create-get-update-and-delete/#list-model-types"},(0,i.kt)("inlineCode",{parentName:"a"},"ListModelTypes"))," method to learn more about the available model types and their hyperparameters.")),(0,i.kt)("p",null,"Here some types of deep fine-tuned models you can create:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Visual classifier")," (",(0,i.kt)("inlineCode",{parentName:"li"},"visual-classifier"),")\u2014Create this model to classify images and video frames into a set of concepts. "),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Visual detector")," (",(0,i.kt)("inlineCode",{parentName:"li"},"visual-detector"),")\u2014Create this model to detect bounding box regions in images or video frames and then classify the detected images. You can also send the image regions to an image cropper model to create a new cropped image."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Visual embedder")," (",(0,i.kt)("inlineCode",{parentName:"li"},"visual-embedder"),')\u2014Create this model to transform images and video frames into "high level" vector representation understood by our AI models. These embeddings enable visual search and can be used as base models to train other models.'),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Visual segmenter")," (",(0,i.kt)("inlineCode",{parentName:"li"},"visual-segmenter"),")\u2014Create this model to segment a per-pixel mask in images where things are and then classify objects, descriptive words, or topics within the masks."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Visual anomaly heatmap")," (",(0,i.kt)("inlineCode",{parentName:"li"},"visual-anomaly-heatmap"),")\u2014Create this model to perform visual anomaly detection with image-level score and anomaly heatmap."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Text classifier")," (",(0,i.kt)("inlineCode",{parentName:"li"},"text-classifier"),")\u2014Create this model to classify text into a set of concepts."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Text generator")," (",(0,i.kt)("inlineCode",{parentName:"li"},"text-to-text"),")\u2014Create this model to generate or convert text based on the provided text input. For example, you can create it for prompt completion, translation, or summarization tasks.")),(0,i.kt)("p",null,"Below is an example of how you would create a visual classifier model."),(0,i.kt)(o.Z,{mdxType:"Tabs"},(0,i.kt)(r.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,i.kt)(s.Z,{className:"language-python",mdxType:"CodeBlock"},l)),(0,i.kt)(r.Z,{value:"js_rest",label:"JavaScript (REST)",mdxType:"TabItem"},(0,i.kt)(s.Z,{className:"language-javascript",mdxType:"CodeBlock"},h)),(0,i.kt)(r.Z,{value:"nodejs",label:"NodeJS",mdxType:"TabItem"},(0,i.kt)(s.Z,{className:"language-javascript",mdxType:"CodeBlock"},D)),(0,i.kt)(r.Z,{value:"java",label:"Java",mdxType:"TabItem"},(0,i.kt)(s.Z,{className:"language-java",mdxType:"CodeBlock"},R)),(0,i.kt)(r.Z,{value:"php",label:"PHP",mdxType:"TabItem"},(0,i.kt)(s.Z,{className:"language-php",mdxType:"CodeBlock"},S)),(0,i.kt)(r.Z,{value:"curl",label:"cURL",mdxType:"TabItem"},(0,i.kt)(s.Z,{className:"language-bash",mdxType:"CodeBlock"},M))),(0,i.kt)("h2",{id:"template-types"},"Template Types"),(0,i.kt)("p",null,"You can take advantage of a variety of our pre-configured templates when developing your deep fine-tuned models. Templates give you the control to choose the specific architecture used by your neural network, and also define a set of hyperparameters that you can use to fine-tune the way your model learns."),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://docs.clarifai.com/portal-guide/model/deep-training/#template-types"},"Click here")," to learn more about the template types we offer\u2014alongside their hyperparameters."),(0,i.kt)("p",null,"Below is an example of how you would use the ",(0,i.kt)("inlineCode",{parentName:"p"},"ListModelTypes")," endpoint to list the templates and hyperparameters available in a specific model type. "),(0,i.kt)(o.Z,{mdxType:"Tabs"},(0,i.kt)(r.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,i.kt)(s.Z,{className:"language-python",mdxType:"CodeBlock"},d)),(0,i.kt)(r.Z,{value:"js_rest",label:"JavaScript (REST)",mdxType:"TabItem"},(0,i.kt)(s.Z,{className:"language-javascript",mdxType:"CodeBlock"},g)),(0,i.kt)(r.Z,{value:"nodejs",label:"NodeJS",mdxType:"TabItem"},(0,i.kt)(s.Z,{className:"language-javascript",mdxType:"CodeBlock"},T)),(0,i.kt)(r.Z,{value:"java",label:"Java",mdxType:"TabItem"},(0,i.kt)(s.Z,{className:"language-java",mdxType:"CodeBlock"},w)),(0,i.kt)(r.Z,{value:"php",label:"PHP",mdxType:"TabItem"},(0,i.kt)(s.Z,{className:"language-php",mdxType:"CodeBlock"},P))),(0,i.kt)("details",null,(0,i.kt)("summary",null,"Output Example"),(0,i.kt)(s.Z,{className:"language-text",mdxType:"CodeBlock"},H)),(0,i.kt)("h2",{id:"training-time-estimator"},"Training Time Estimator"),(0,i.kt)("p",null,"Before you train a deep fine-tuned model, you can use the ",(0,i.kt)("a",{parentName:"p",href:"https://docs.clarifai.com/portal-guide/model/training-basics#training-time-estimator"},"Training Time Estimator")," feature to approximate the amount of time the training process could take. This offers transparency in expected training costs."),(0,i.kt)("admonition",{type:"tip"},(0,i.kt)("p",{parentName:"admonition"},"Instead of providing an estimated input count, an alternative approach is to specify a dataset version ID in the ",(0,i.kt)("inlineCode",{parentName:"p"},"train_info.params")," of the request. Here is an example: ",(0,i.kt)("inlineCode",{parentName:"p"},'params.update({"template":"MMDetection_FasterRCNN", "dataset_version_id":"dataset-version-1681974758238s"})'),".")),(0,i.kt)(o.Z,{mdxType:"Tabs"},(0,i.kt)(r.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,i.kt)(s.Z,{className:"language-python",mdxType:"CodeBlock"},f)),(0,i.kt)(r.Z,{value:"js_rest",label:"JavaScript (REST)",mdxType:"TabItem"},(0,i.kt)(s.Z,{className:"language-javascript",mdxType:"CodeBlock"},I)),(0,i.kt)(r.Z,{value:"java",label:"Java",mdxType:"TabItem"},(0,i.kt)(s.Z,{className:"language-java",mdxType:"CodeBlock"},C)),(0,i.kt)(r.Z,{value:"curl",label:"cURL",mdxType:"TabItem"},(0,i.kt)(s.Z,{className:"language-bash",mdxType:"CodeBlock"},Y))),(0,i.kt)("details",null,(0,i.kt)("summary",null,"Output Example"),(0,i.kt)(s.Z,{className:"language-text",mdxType:"CodeBlock"},B)),(0,i.kt)("h2",{id:"train-a-model"},"Train a Model"),(0,i.kt)("p",null,"After creating a model, you can now train it. It is an asynchronous operation."),(0,i.kt)("p",null,"Training enables the deep fine-tuned model to learn patterns, relationships, and representations from the input data. It allows the model to adjust its parameters based on the provided input data so that it can make accurate predictions."),(0,i.kt)("p",null,"You can repeat the training operation as often as you like. By adding more input data with concepts and training, you can get the model to predict exactly how you want it to. "),(0,i.kt)("p",null,"The ",(0,i.kt)("inlineCode",{parentName:"p"},"PostModelVersions")," endpoint kicks off training and creates a new model version. You can also add concepts to a model when creating the model version\u2014and only if the model type supports it as defined in the model type parameters."),(0,i.kt)("p",null,"Below is an example of how you would train a visual classifier model. "),(0,i.kt)("admonition",{type:"note"},(0,i.kt)("p",{parentName:"admonition"},"We use the ",(0,i.kt)("inlineCode",{parentName:"p"},"params.update()")," method to set the template and hyperparameters for the visual classifier model. If training another model type, you'll need to state the specific template and hyperparameters associated with that particular model. ")),(0,i.kt)(o.Z,{mdxType:"Tabs"},(0,i.kt)(r.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,i.kt)(s.Z,{className:"language-python",mdxType:"CodeBlock"},p)),(0,i.kt)(r.Z,{value:"js_rest",label:"JavaScript (REST)",mdxType:"TabItem"},(0,i.kt)(s.Z,{className:"language-javascript",mdxType:"CodeBlock"},E)),(0,i.kt)(r.Z,{value:"java",label:"Java",mdxType:"TabItem"},(0,i.kt)(s.Z,{className:"language-java",mdxType:"CodeBlock"},v)),(0,i.kt)(r.Z,{value:"curl",label:"cURL",mdxType:"TabItem"},(0,i.kt)(s.Z,{className:"language-bash",mdxType:"CodeBlock"},k))),(0,i.kt)("h3",{id:"use-your-own-template"},"Use Your Own Template"),(0,i.kt)("p",null,"You can ",(0,i.kt)("a",{parentName:"p",href:"https://docs.clarifai.com/portal-guide/model/deep-training/custom-templates/"},"create your own deep fine-tuned template")," and use it to train a model. "),(0,i.kt)("p",null,"You need to create a Python configuration file and pass it as a training parameter to the ",(0,i.kt)("inlineCode",{parentName:"p"},"PostModelVersions")," endpoint. Here is an example of a ",(0,i.kt)("inlineCode",{parentName:"p"},"training_config.py")," file for creating a custom deep fine-tuned template using the MMDetection open source toolbox for visual detection tasks."),(0,i.kt)(o.Z,{mdxType:"Tabs"},(0,i.kt)(r.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,i.kt)(s.Z,{className:"language-python",mdxType:"CodeBlock"},c))),(0,i.kt)("p",null,"Here is how you could use the custom template to train a deep fine-tuned model."),(0,i.kt)(o.Z,{mdxType:"Tabs"},(0,i.kt)(r.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,i.kt)(s.Z,{className:"language-python",mdxType:"CodeBlock"},m))),(0,i.kt)("h2",{id:"create-a-workflow"},"Create a Workflow"),(0,i.kt)("p",null,"Put your new deep trained model to work by adding it to a workflow. Below is an example of how to create a workflow with a deep trained model."),(0,i.kt)(o.Z,{mdxType:"Tabs"},(0,i.kt)(r.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,i.kt)(s.Z,{className:"language-python",mdxType:"CodeBlock"},_)),(0,i.kt)(r.Z,{value:"js_rest",label:"JavaScript (REST)",mdxType:"TabItem"},(0,i.kt)(s.Z,{className:"language-javascript",mdxType:"CodeBlock"},y)),(0,i.kt)(r.Z,{value:"nodejs",label:"NodeJS",mdxType:"TabItem"},(0,i.kt)(s.Z,{className:"language-javascript",mdxType:"CodeBlock"},b)),(0,i.kt)(r.Z,{value:"java",label:"Java",mdxType:"TabItem"},(0,i.kt)(s.Z,{className:"language-java",mdxType:"CodeBlock"},N)),(0,i.kt)(r.Z,{value:"php",label:"PHP",mdxType:"TabItem"},(0,i.kt)(s.Z,{className:"language-php",mdxType:"CodeBlock"},U)),(0,i.kt)(r.Z,{value:"curl",label:"cURL",mdxType:"TabItem"},(0,i.kt)(s.Z,{className:"language-bash",mdxType:"CodeBlock"},x))),(0,i.kt)("h3",{id:"update-your-default-workflow"},"Update Your Default Workflow"),(0,i.kt)("p",null,"You can index your inputs with a deep fine-tuned model by updating your default workflow. You can also use your deep fine-tuned embeddings as the basis for clustering and search."),(0,i.kt)("p",null,"Below is an example of how to update your default workflow with a deep fine-tuned model."),(0,i.kt)(o.Z,{mdxType:"Tabs"},(0,i.kt)(r.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,i.kt)(s.Z,{className:"language-python",mdxType:"CodeBlock"},u)),(0,i.kt)(r.Z,{value:"nodejs",label:"NodeJS",mdxType:"TabItem"},(0,i.kt)(s.Z,{className:"language-javascript",mdxType:"CodeBlock"},O)),(0,i.kt)(r.Z,{value:"java",label:"Java",mdxType:"TabItem"},(0,i.kt)(s.Z,{className:"language-java",mdxType:"CodeBlock"},A)),(0,i.kt)(r.Z,{value:"curl",label:"cURL",mdxType:"TabItem"},(0,i.kt)(s.Z,{className:"language-bash",mdxType:"CodeBlock"},L))))}q.isMDXComponent=!0}}]);
"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[7638],{15680:(e,t,r)=>{r.d(t,{xA:()=>m,yg:()=>g});var a=r(96540);function n(e,t,r){return t in e?Object.defineProperty(e,t,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[t]=r,e}function o(e,t){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),r.push.apply(r,a)}return r}function i(e){for(var t=1;t<arguments.length;t++){var r=null!=arguments[t]?arguments[t]:{};t%2?o(Object(r),!0).forEach((function(t){n(e,t,r[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):o(Object(r)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(r,t))}))}return e}function d(e,t){if(null==e)return{};var r,a,n=function(e,t){if(null==e)return{};var r,a,n={},o=Object.keys(e);for(a=0;a<o.length;a++)r=o[a],t.indexOf(r)>=0||(n[r]=e[r]);return n}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)r=o[a],t.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(n[r]=e[r])}return n}var l=a.createContext({}),s=function(e){var t=a.useContext(l),r=t;return e&&(r="function"==typeof e?e(t):i(i({},t),e)),r},m=function(e){var t=s(e.components);return a.createElement(l.Provider,{value:t},e.children)},u="mdxType",p={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},c=a.forwardRef((function(e,t){var r=e.components,n=e.mdxType,o=e.originalType,l=e.parentName,m=d(e,["components","mdxType","originalType","parentName"]),u=s(r),c=n,g=u["".concat(l,".").concat(c)]||u[c]||p[c]||o;return r?a.createElement(g,i(i({ref:t},m),{},{components:r})):a.createElement(g,i({ref:t},m))}));function g(e,t){var r=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var o=r.length,i=new Array(o);i[0]=c;var d={};for(var l in t)hasOwnProperty.call(t,l)&&(d[l]=t[l]);d.originalType=e,d[u]="string"==typeof e?e:n,i[1]=d;for(var s=2;s<o;s++)i[s]=r[s];return a.createElement.apply(null,i)}return a.createElement.apply(null,r)}c.displayName="MDXCreateElement"},72419:(e,t,r)=>{r.r(t),r.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>p,frontMatter:()=>o,metadata:()=>d,toc:()=>s});var a=r(58168),n=(r(96540),r(15680));const o={description:"Learn about our visual embedder model type",sidebar_position:6,keywords:["visual embedder","image embedding","visual embedding models","AI image embedding","image feature extraction","machine learning image embedder","computer vision embedding","visual embedding AI","deep learning embedding models"]},i="Visual Embedder",d={unversionedId:"portal-guide/model/model-types/visual-embedder",id:"portal-guide/model/model-types/visual-embedder",title:"Visual Embedder",description:"Learn about our visual embedder model type",source:"@site/docs/portal-guide/model/model-types/visual-embedder.md",sourceDirName:"portal-guide/model/model-types",slug:"/portal-guide/model/model-types/visual-embedder",permalink:"/portal-guide/model/model-types/visual-embedder",draft:!1,editUrl:"https://github.com/Clarifai/docs/blob/main/docs/portal-guide/model/model-types/visual-embedder.md",tags:[],version:"current",sidebarPosition:6,frontMatter:{description:"Learn about our visual embedder model type",sidebar_position:6,keywords:["visual embedder","image embedding","visual embedding models","AI image embedding","image feature extraction","machine learning image embedder","computer vision embedding","visual embedding AI","deep learning embedding models"]},sidebar:"tutorialSidebar",previous:{title:"Visual Anomaly",permalink:"/portal-guide/model/model-types/visual-anomaly"},next:{title:"Clusterer",permalink:"/portal-guide/model/model-types/clusterer"}},l={},s=[],m={toc:s},u="wrapper";function p(e){let{components:t,...r}=e;return(0,n.yg)(u,(0,a.A)({},m,r,{components:t,mdxType:"MDXLayout"}),(0,n.yg)("h1",{id:"visual-embedder"},"Visual Embedder"),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Learn about our visual embedder model type")),(0,n.yg)("hr",null),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Input"),": Images and videos"),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Output"),": Embeddings"),(0,n.yg)("p",null,"Visual embedder, also known as visual embedding, is a type of deep fine-tuned model specifically designed to generate meaningful numerical representations (embeddings) from images and video frames."),(0,n.yg)("p",null,"The primary goal of a visual embedder model is to transform the raw pixel values of images or video frames into a compact and high-dimensional vector. These vectors capture essential features and patterns in the visual content, enabling the model to understand and process the data in a more structured and interpretable way."),(0,n.yg)("p",null,"These vectors can then be used for a variety of tasks, such as:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Visual search"),": This is the task of finding images or videos that are similar to a given query image or video. The visual embedder model can be used to create a similarity metric between images or videos, which can then be used to search for similar visual content in a vector database."),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Training on top of them"),": The visual embedder model can also be used as a starting point for training other machine learning models. For example, a model that can classify images or videos can be trained on top of the visual embedder model.")),(0,n.yg)("admonition",{type:"info"},(0,n.yg)("p",{parentName:"admonition"},"The visual embedder model type also comes with various ",(0,n.yg)("a",{parentName:"p",href:"https://docs.clarifai.com/portal-guide/model/deep-training/visual-embedding-templates"},"templates")," that give you the control to choose the specific architecture used by your neural network, as well as define a set of hyperparameters you can use to fine-tune the way your model learns.")),(0,n.yg)("p",null,"You may choose a visual embedder model type in cases where:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"You need a model that can accurately represent images and video frames as vectors. Once the model is trained, you can use it to embed new images or videos into vectors."),(0,n.yg)("li",{parentName:"ul"},'You need an embedding model to learn new features not recognized by the existing Clarifai models. In that case, you may need to "deep fine-tune" your custom model and integrate it directly within your ',(0,n.yg)("a",{parentName:"li",href:"https://docs.clarifai.com/portal-guide/workflows/"},"workflows"),"."),(0,n.yg)("li",{parentName:"ul"},"You have a custom-tailored dataset, accurate labels, and the expertise and time to fine-tune models.")))}p.isMDXComponent=!0}}]);
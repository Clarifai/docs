"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[4255],{11470:(e,n,t)=>{t.d(n,{A:()=>v});var r=t(96540),o=t(18215),a=t(17559),i=t(23104),s=t(56347),l=t(205),c=t(57485),d=t(31682),u=t(70679);function h(e){return r.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,r.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function p(e){const{values:n,children:t}=e;return(0,r.useMemo)(()=>{const e=n??function(e){return h(e).map(({props:{value:e,label:n,attributes:t,default:r}})=>({value:e,label:n,attributes:t,default:r}))}(t);return function(e){const n=(0,d.XI)(e,(e,n)=>e.value===n.value);if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[n,t])}function m({value:e,tabValues:n}){return n.some(n=>n.value===e)}function f({queryString:e=!1,groupId:n}){const t=(0,s.W6)(),o=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,c.aZ)(o),(0,r.useCallback)(e=>{if(!o)return;const n=new URLSearchParams(t.location.search);n.set(o,e),t.replace({...t.location,search:n.toString()})},[o,t])]}function g(e){const{defaultValue:n,queryString:t=!1,groupId:o}=e,a=p(e),[i,s]=(0,r.useState)(()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!m({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const t=n.find(e=>e.default)??n[0];if(!t)throw new Error("Unexpected error: 0 tabValues");return t.value}({defaultValue:n,tabValues:a})),[c,d]=f({queryString:t,groupId:o}),[h,g]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[t,o]=(0,u.Dv)(n);return[t,(0,r.useCallback)(e=>{n&&o.set(e)},[n,o])]}({groupId:o}),x=(()=>{const e=c??h;return m({value:e,tabValues:a})?e:null})();(0,l.A)(()=>{x&&s(x)},[x]);return{selectedValue:i,selectValue:(0,r.useCallback)(e=>{if(!m({value:e,tabValues:a}))throw new Error(`Can't select invalid tab value=${e}`);s(e),d(e),g(e)},[d,g,a]),tabValues:a}}var x=t(92303);const y={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var b=t(74848);function j({className:e,block:n,selectedValue:t,selectValue:r,tabValues:a}){const s=[],{blockElementScrollPositionUntilNextRender:l}=(0,i.a_)(),c=e=>{const n=e.currentTarget,o=s.indexOf(n),i=a[o].value;i!==t&&(l(n),r(i))},d=e=>{let n=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const t=s.indexOf(e.currentTarget)+1;n=s[t]??s[0];break}case"ArrowLeft":{const t=s.indexOf(e.currentTarget)-1;n=s[t]??s[s.length-1];break}}n?.focus()};return(0,b.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,o.A)("tabs",{"tabs--block":n},e),children:a.map(({value:e,label:n,attributes:r})=>(0,b.jsx)("li",{role:"tab",tabIndex:t===e?0:-1,"aria-selected":t===e,ref:e=>{s.push(e)},onKeyDown:d,onClick:c,...r,className:(0,o.A)("tabs__item",y.tabItem,r?.className,{"tabs__item--active":t===e}),children:n??e},e))})}function _({lazy:e,children:n,selectedValue:t}){const a=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=a.find(e=>e.props.value===t);return e?(0,r.cloneElement)(e,{className:(0,o.A)("margin-top--md",e.props.className)}):null}return(0,b.jsx)("div",{className:"margin-top--md",children:a.map((e,n)=>(0,r.cloneElement)(e,{key:n,hidden:e.props.value!==t}))})}function k(e){const n=g(e);return(0,b.jsxs)("div",{className:(0,o.A)(a.G.tabs.container,"tabs-container",y.tabList),children:[(0,b.jsx)(j,{...n,...e}),(0,b.jsx)(_,{...n,...e})]})}function v(e){const n=(0,x.A)();return(0,b.jsx)(k,{...e,children:h(e.children)},String(n))}},19365:(e,n,t)=>{t.d(n,{A:()=>i});t(96540);var r=t(18215);const o={tabItem:"tabItem_Ymn6"};var a=t(74848);function i({children:e,hidden:n,className:t}){return(0,a.jsx)("div",{role:"tabpanel",className:(0,r.A)(o.tabItem,t),hidden:n,children:e})}},27593:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>b,contentTitle:()=>y,default:()=>k,frontMatter:()=>x,metadata:()=>r,toc:()=>j});const r=JSON.parse('{"id":"compute/toolkits/hf","title":"Hugging Face","description":"Download and run Hugging Face models locally and make them available via a public API","source":"@site/docs/compute/toolkits/hf.md","sourceDirName":"compute/toolkits","slug":"/compute/toolkits/hf","permalink":"/compute/toolkits/hf","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"description":"Download and run Hugging Face models locally and make them available via a public API","sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Ollama","permalink":"/compute/toolkits/ollama"},"next":{"title":"LM Studio","permalink":"/compute/toolkits/lmstudio"}}');var o=t(74848),a=t(28453),i=t(11470),s=t(19365),l=t(88149);const c='from typing import List, Iterator\nfrom threading import Thread\nimport os\nimport torch\n\nfrom clarifai.runners.models.model_class import ModelClass\nfrom clarifai.utils.logging import logger\nfrom clarifai.runners.models.model_builder import ModelBuilder\nfrom clarifai.runners.utils.openai_convertor import openai_response\nfrom clarifai.runners.utils.data_utils import Param\nfrom transformers import (AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer)\n\n\nclass MyModel(ModelClass):\n  """A custom runner for llama-3.2-1b-instruct llm that integrates with the Clarifai platform"""\n\n  def load_model(self):\n    """Load the model here."""\n    if torch.backends.mps.is_available():\n        self.device = \'mps\'\n    elif torch.cuda.is_available():\n        self.device = \'cuda\'\n    else:\n        self.device = \'cpu\'\n\n    logger.info(f"Running on device: {self.device}")\n\n    # Load checkpoints\n    model_path = os.path.dirname(os.path.dirname(__file__))\n    builder = ModelBuilder(model_path, download_validation_only=True)\n    self.checkpoints = builder.config[\'checkpoints\'][\'repo_id\']\n    logger.info(f"Loading model from: {self.checkpoints}")\n    # Load model and tokenizer\n    self.tokenizer = AutoTokenizer.from_pretrained(self.checkpoints,)\n    self.tokenizer.pad_token = self.tokenizer.eos_token  # Set pad token to eos token\n    self.model = AutoModelForCausalLM.from_pretrained(\n        self.checkpoints,\n        low_cpu_mem_usage=True,\n        device_map=self.device,\n        torch_dtype=torch.bfloat16,\n    )\n    self.streamer = TextIteratorStreamer(tokenizer=self.tokenizer, skip_prompt=True, skip_special_tokens=True)\n    self.chat_template = None\n    logger.info("Done loading!")\n\n  @ModelClass.method\n  def predict(self,\n              prompt: str ="",\n              chat_history: List[dict] = None,\n              max_tokens: int = Param(default=512, description="The maximum number of tokens to generate. Shorter token lengths will provide faster performance.", ),\n              temperature: float = Param(default=0.7, description="A decimal number that determines the degree of randomness in the response", ),\n              top_p: float = Param(default=0.8, description="An alternative to sampling with temperature, where the model considers the results of the tokens with top_p probability mass.", )) -> str:\n    """\n    Predict the response for the given prompt and chat history using the model.\n    """\n    # Construct chat-style messages\n    messages = chat_history if chat_history else []\n    if prompt:\n        messages.append({\n            "role": "user",\n            "content":  prompt\n        })\n    \n    inputs = self.tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors="pt", return_dict=True).to(self.model.device)\n\n    generation_kwargs = {\n        "do_sample": True,\n        "max_new_tokens": max_tokens,\n        "temperature": temperature,\n        "top_p": top_p,\n        "eos_token_id": self.tokenizer.eos_token_id,\n    }\n\n    output = self.model.generate(**inputs, **generation_kwargs)\n    generated_tokens = output[0][inputs["input_ids"].shape[-1]:]\n    return self.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n\n  @ModelClass.method\n  def generate(self,\n              prompt: str="",\n              chat_history: List[dict] = None,\n              max_tokens: int = Param(default=512, description="The maximum number of tokens to generate. Shorter token lengths will provide faster performance.", ),\n              temperature: float = Param(default=0.7, description="A decimal number that determines the degree of randomness in the response", ),\n              top_p: float = Param(default=0.8, description="An alternative to sampling with temperature, where the model considers the results of the tokens with top_p probability mass.", )) -> Iterator[str]:\n      """Stream generated text tokens from a prompt + optional chat history."""\n\n\n      # Construct chat-style messages\n      messages = chat_history if chat_history else []\n      if prompt:\n          messages.append({\n            "role": "user",\n            "content":  prompt\n        })\n      logger.info(f"Generating response for messages: {messages}")\n      response = self.chat(\n          messages=messages,\n          max_tokens=max_tokens,\n          temperature=temperature,\n          top_p=top_p\n      )\n      \n      for each in response:\n          if \'choices\' in each and \'delta\' in each[\'choices\'][0] and \'content\' in each[\'choices\'][0][\'delta\']:\n                  yield each[\'choices\'][0][\'delta\'][\'content\']\n                  \n  @ModelClass.method\n  def chat(self,\n          messages: List[dict],\n          max_tokens: int = Param(default=512, description="The maximum number of tokens to generate. Shorter token lengths will provide faster performance.", ),\n          temperature: float = Param(default=0.7, description="A decimal number that determines the degree of randomness in the response", ),\n          top_p: float = Param(default=0.8, description="An alternative to sampling with temperature, where the model considers the results of the tokens with top_p probability mass.", )\n          ) -> Iterator[dict]:\n      """\n      Stream back JSON dicts for assistant messages.\n      Example return format:\n      {"role": "assistant", "content": [{"type": "text", "text": "response here"}]}\n      """\n      # Tokenize using chat template\n      inputs = self.tokenizer.apply_chat_template(\n          messages,\n          tokenize=True,\n          add_generation_prompt=True,\n          return_tensors="pt"\n      ).to(self.model.device)\n\n      generation_kwargs = {\n          "input_ids": inputs,\n          "do_sample": True,\n          "max_new_tokens": max_tokens,\n          "temperature": temperature,\n          "top_p": top_p,\n          "eos_token_id": self.tokenizer.eos_token_id,\n          "streamer": self.streamer\n      }\n\n      thread = Thread(target=self.model.generate, kwargs=generation_kwargs)\n      thread.start()\n\n      # Accumulate response text\n      for chunk in openai_response(self.streamer):\n          yield chunk\n\n      thread.join()\n\n  def test(self):\n    """Test the model here."""\n    try:\n      print("Testing predict...")\n      # Test predict\n      print(self.predict(prompt="What is the capital of India?",))\n    except Exception as e:\n      print("Error in predict", e)\n\n    try:\n      print("Testing generate...")\n      # Test generate\n      for each in self.generate(prompt="What is the capital of India?",):\n        print(each, end="")\n      print()\n    except Exception as e:\n      print("Error in generate", e)\n\n    try:\n      print("Testing chat...")\n      messages = [\n        {"role": "system", "content": "You are an helpful assistant."},\n        {"role": "user", "content": "What is the capital of India?"},\n      ]\n      for each in self.chat(messages=messages,):\n        print(each, end="")\n      print()\n    except Exception as e:\n      print("Error in generate", e)',d="build_info:\n  python_version: '3.11'\ncheckpoints:\n  hf_token: hf_token\n  repo_id: unsloth/Llama-3.2-1B-Instruct\n  type: huggingface\n  when: runtime\ninference_compute_info:\n  accelerator_memory: 44Gi\n  accelerator_type:\n  - NVIDIA-*\n  cpu_limit: '1'\n  cpu_memory: 13Gi\n  num_accelerators: 1\nmodel:\n  app_id: your-app-id-here\n  id: hf-local-runner-model\n  model_type_id: text-to-text\n  user_id: your-user-id-here",u="- unsloth/Llama-3.2-1B-Instruct\n- Qwen/Qwen2-0.5B\n- Qwen/Qwen3-1.7B\n- Qwen/Qwen3-0.6B\n- Qwen/Qwen3-4B-Thinking-2507\n- Qwen/Qwen3-4B-Instruct-2507\n- HuggingFaceTB/SmolLM-1.7B-Instruct\n- stabilityai/stablelm-zephyr-3b\n- microsoft/Phi-3-mini-4k-instruct\n- google/gemma-3n-E2B-it",h="clarifai login                                      \nEnter your Clarifai user ID: alfrick\n> To authenticate, you'll need a Personal Access Token (PAT).\n> You can create one from your account settings: https://clarifai.com/alfrick/settings/security\n\nEnter your Personal Access Token (PAT) value (or type \"ENVVAR\" to use an environment variable): ENVVAR\n\n> Verifying token...\n[INFO] 13:39:42.773825 Validating the Context Credentials... |  thread=8800297152 \n[INFO] 13:39:46.740886 \u2705 Context is valid |  thread=8800297152 \n\n> Let's save these credentials to a new context.\n> You can have multiple contexts to easily switch between accounts or projects.\n\nEnter a name for this context [default]: default\n\u2705 Success! You are now logged in.\nCredentials saved to the 'default' context.\n\n\ud83d\udca1 To switch contexts later, use `clarifai config use-context <name>`.\n[INFO] 13:41:01.395603 Login successful for user 'alfrick' in context 'default' |  thread=8800297152 ",p="torch==2.5.1\ntokenizers>=0.21.0\ntransformers>=4.47.0\naccelerate>=1.2.0\nscipy>=1.10.0\noptimum>=1.23.3\nprotobuf==5.27.3\neinops>=0.8.0\nrequests==2.32.3\nclarifai>=11.4.1\ntimm",m="clarifai model init --toolkit huggingface\n[INFO] 14:15:28.372128 Parsed GitHub repository: owner=Clarifai, repo=runners-examples, branch=huggingface, folder_path= |  thread=8800297152 \n[INFO] 14:15:29.583471 Files to be downloaded are:\n1. 1/model.py\n2. config.yaml\n3. requirements.txt |  thread=8800297152 \nPress Enter to continue...\n[INFO] 14:15:31.611534 Initializing model from GitHub repository: https://github.com/Clarifai/runners-examples |  thread=8800297152 \n[INFO] 14:15:34.840210 Successfully cloned repository from https://github.com/Clarifai/runners-examples (branch: huggingface) |  thread=8800297152 \n[INFO] 14:15:34.845345 Model initialization complete with GitHub repository |  thread=8800297152 \n[INFO] 14:15:34.845394 Next steps: |  thread=8800297152 \n[INFO] 14:15:34.845423 1. Review the model configuration |  thread=8800297152 \n[INFO] 14:15:34.845444 2. Install any required dependencies manually |  thread=8800297152 \n[INFO] 14:15:34.845466 3. Test the model locally using 'clarifai model local-test' |  thread=8800297152 ",f='clarifai model local-runner\n[INFO] 15:04:30.110675 Checking setup for local runner... |  thread=8800297152 \n[INFO] 15:04:30.110764 Current context: default |  thread=8800297152 \n[INFO] 15:04:30.110803 Current user_id: alfrick |  thread=8800297152 \n[INFO] 15:04:30.133269 Current compute_cluster_id: local-runner-compute-cluster |  thread=8800297152 \n[INFO] 15:04:32.213980 Failed to get compute cluster with ID local-runner-compute-cluster: code: CONN_DOES_NOT_EXIST\ndescription: "Resource does not exist"\ndetails: "ComputeCluster with ID \\\'local-runner-compute-cluster\\\' not found. Check your request fields."\nreq_id: "sdk-python-11.6.4-adac6224603147b4a6422e7ab3d8999f"\n |  thread=8800297152 \nCompute cluster not found. Do you want to create a new compute cluster alfrick/local-runner-compute-cluster? (y/n): y\n[INFO] 15:04:39.978695 \nCompute Cluster created\ncode: SUCCESS\ndescription: "Ok"\nreq_id: "sdk-python-11.6.4-02d952ca15d4431ebf1d998247b5559f"\n |  thread=8800297152 \n[INFO] 15:04:39.986611 Current nodepool_id: local-runner-nodepool |  thread=8800297152 \n[INFO] 15:04:41.235547 Failed to get nodepool with ID local-runner-nodepool: code: CONN_DOES_NOT_EXIST\ndescription: "Resource does not exist"\ndetails: "Nodepool not found. Check your request fields."\nreq_id: "sdk-python-11.6.4-f1af74b390d54ee68b1c0d7025c412a8"\n |  thread=8800297152 \nNodepool not found. Do you want to create a new nodepool alfrick/local-runner-compute-cluster/local-runner-nodepool? (y/n): y\n[INFO] 15:04:43.256490 \nNodepool created\ncode: SUCCESS\ndescription: "Ok"\nreq_id: "sdk-python-11.6.4-3c0ebab572cf495996ce5337da2cc24e"\n |  thread=8800297152 \n[INFO] 15:04:43.269204 Current app_id: local-runner-app |  thread=8800297152 \n[INFO] 15:04:43.580198 Current model_id: local-runner-model |  thread=8800297152 \n[INFO] 15:04:46.328600 Current model version 9d38bb9398944de4bdef699835f17ec9 |  thread=8800297152 \n[INFO] 15:04:46.329168 Create the local runner tying this\n  alfrick/local-runner-app/models/local-runner-model model (version: 9d38bb9398944de4bdef699835f17ec9) to the\n  alfrick/local-runner-compute-cluster/local-runner-nodepool nodepool. |  thread=8800297152 \n[INFO] 15:04:47.564767 \nRunner created\ncode: SUCCESS\ndescription: "Ok"\nreq_id: "sdk-python-11.6.4-5a17241947ba4ac593f29eacecb4d61d"\n with id: 7dbd2b733acb4684a4cb8d3b11ee626a |  thread=8800297152 \n[INFO] 15:04:47.573245 Current runner_id: 7dbd2b733acb4684a4cb8d3b11ee626a |  thread=8800297152 \n[INFO] 15:04:47.828638 Failed to get deployment with ID local-runner-deployment: code: CONN_DOES_NOT_EXIST\ndescription: "Resource does not exist"\ndetails: "Deployment with ID \\\'local-runner-deployment\\\' not found. Check your request fields."\nreq_id: "sdk-python-11.6.4-1f51c07b0cc54f14893175401f6fda1d"\n |  thread=8800297152 \nDeployment not found. Do you want to create a new deployment alfrick/local-runner-compute-cluster/local-runner-nodepool/local-runner-deployment? (y/n): y\n[INFO] 15:04:50.307460 \nDeployment created\ncode: SUCCESS\ndescription: "Ok"\nreq_id: "sdk-python-11.6.4-6afe596327fb42bc818940fb324cc8bc"\n |  thread=8800297152 \n[INFO] 15:04:50.315366 Current deployment_id: local-runner-deployment |  thread=8800297152 \n[INFO] 15:04:50.315542 Full url for the model: https://clarifai.com/users/alfrick/apps/local-runner-app/models/local-runner-model/versions/9d38bb9398944de4bdef699835f17ec9 |  thread=8800297152 \n[INFO] 15:04:50.318223 Current model section of config.yaml: {\'app_id\': \'items-app\', \'id\': \'first-local-runner-model\', \'model_type_id\': \'text-to-text\', \'user_id\': \'alfrick\'} |  thread=8800297152 \nDo you want to backup config.yaml to config.yaml.bk then update the config.yaml with the new model information? (y/n): y\n[INFO] 15:04:53.150497 Hugging Face repo access validated |  thread=8800297152 \n[INFO] 15:05:00.624489 \n\nXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n# About to start up the local runner in this terminal...\n# Here is a code snippet to call this model once it start from another terminal:\n |  thread=8800297152 \n[INFO] 15:05:00.624556 \n# Clarifai Model Client Script\n# Set the environment variables `CLARIFAI_DEPLOYMENT_ID` and `CLARIFAI_PAT` to run this script.\n# Example usage:\nimport os\n\nfrom clarifai.client import Model\nfrom clarifai.runners.utils import data_types\n\nmodel = Model("https://clarifai.com/alfrick/local-runner-app/models/local-runner-model",\n    deployment_id = \'local-runner-deployment\', # Only needed for dedicated deployed models\n    base_url=\'https://api.clarifai.com\',\n )\n\n    \n# Example model prediction from different model methods: \n\nresponse = model.predict(prompt="What is the future of AI?", max_tokens=512, temperature=0.7, top_p=0.8)\nprint(response)\n\nresponse = model.generate(prompt="What is the future of AI?", max_tokens=512, temperature=0.7, top_p=0.8)\nfor res in response:\n    print(res)\n\nresponse = model.chat(max_tokens=512, temperature=0.7, top_p=0.8)\nfor res in response:\n    print(res)\n\n |  thread=8800297152 \n[INFO] 15:05:00.624593 Now starting the local runner... |  thread=8800297152 \n[INFO] 15:05:01.132806 Hugging Face repo access validated |  thread=8800297152 \n[INFO] 15:05:01.170311 Running on device: mps |  thread=8800297152 \n[INFO] 15:05:01.790161 Hugging Face repo access validated |  thread=8800297152 \n[INFO] 15:05:01.791351 Loading model from: unsloth/Llama-3.2-1B-Instruct |  thread=8800297152 \ntokenizer_config.json: 54.7kB [00:00, 49.0MB/s]\ntokenizer.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 17.2M/17.2M [00:07<00:00, 2.27MB/s]\nspecial_tokens_map.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 454/454 [00:00<00:00, 1.33MB/s]\nchat_template.jinja: 3.83kB [00:00, 7.36MB/s]\nconfig.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 894/894 [00:00<00:00, 3.20MB/s]\n`torch_dtype` is deprecated! Use `dtype` instead!\n',g='# Before running this script, set the environment variables:\n#   CLARIFAI_DEPLOYMENT_ID  (optional \u2013 only required for dedicated deployments)\n#   CLARIFAI_PAT            (your Personal Access Token)\n\nfrom clarifai.client import Model\n\n# Initialize the model\nmodel = Model(\n    "https://clarifai.com/<user-id>/local-runner-app/models/local-runner-model",\n    # deployment_id="local-runner-deployment",  # Uncomment if using a deployed model\n)\n\n# Run a basic prediction\nresponse = model.predict(\n    prompt="What is the future of AI?",\n    max_tokens=512,\n    temperature=0.7,\n    top_p=0.8,\n)\n\nprint(response)\n\n\'\'\'\n--- Additional examples ---\n\n# Using the generate method\nresponse = model.generate(\n    prompt="What is the future of AI?",\n    max_tokens=512,\n    temperature=0.7,\n    top_p=0.8,\n)\nfor res in response:\n    print(res)\n\n# Using the chat method\nresponse = model.chat(\n    max_tokens=512,\n    temperature=0.7,\n    top_p=0.8,\n)\nfor res in response:\n    print(res)\n\'\'\'\n',x={description:"Download and run Hugging Face models locally and make them available via a public API",sidebar_position:3},y="Hugging Face",b={},j=[{value:"Step 1: Perform Prerequisites",id:"step-1-perform-prerequisites",level:2},{value:"Sign Up or Log In",id:"sign-up-or-log-in",level:3},{value:"Install Clarifai CLI",id:"install-clarifai-cli",level:3},{value:"Get Hugging Face Token",id:"get-hugging-face-token",level:3},{value:"Install Hugging Face Hub",id:"install-hugging-face-hub",level:3},{value:"Step 2: Initialize a Model",id:"step-2-initialize-a-model",level:2},{value:"<code>model.py</code>",id:"modelpy",level:3},{value:"<code>config.yaml</code>",id:"configyaml",level:3},{value:"<code>requirements.txt</code>",id:"requirementstxt",level:3},{value:"Step 3: Log In to Clarifai",id:"step-3-log-in-to-clarifai",level:2},{value:"Step 4: Start Your Local Runner",id:"step-4-start-your-local-runner",level:2},{value:"Step 5: Test Your Runner",id:"step-5-test-your-runner",level:2}];function _(e){const n={a:"a",admonition:"admonition",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components},{Details:t}=n;return t||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"hugging-face",children:"Hugging Face"})}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Download and run Hugging Face models locally and make them available via a public API"})}),"\n",(0,o.jsx)("hr",{}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.a,{href:"https://huggingface.co/",children:"Hugging Face"})," is an open-source platform for sharing, exploring, and collaborating on a wide range of pre-trained models and related assets."]}),"\n",(0,o.jsxs)(n.p,{children:["With Clarifai\u2019s ",(0,o.jsx)(n.a,{href:"/compute/toolkits/",children:"Local Runners"}),", you can run these models directly on your machine, expose them securely via a public URL, and tap into Clarifai\u2019s powerful platform \u2014 all while preserving the speed, privacy, and control of local deployment."]}),"\n",(0,o.jsxs)(n.blockquote,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Note:"})," After initializing a model using the Hugging Face toolkit, you can ",(0,o.jsx)(n.a,{href:"https://docs.clarifai.com/compute/upload/#step-4-upload-the-model-to-clarifai",children:"upload"})," it to Clarifai to leverage the platform\u2019s capabilities."]}),"\n"]}),"\n","\n","\n",(0,o.jsx)(n.h2,{id:"step-1-perform-prerequisites",children:"Step 1: Perform Prerequisites"}),"\n",(0,o.jsx)(n.h3,{id:"sign-up-or-log-in",children:"Sign Up or Log In"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.a,{href:"https://clarifai.com/login",children:"Log in"})," to your existing Clarifai account or ",(0,o.jsx)(n.a,{href:"https://clarifai.com/signup",children:"sign up"})," for a new one. Once logged in, you\u2019ll need the following credentials for setup:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"App ID"})," \u2013 Navigate to the application you want to use to run the model and select the ",(0,o.jsx)(n.strong,{children:(0,o.jsx)(n.a,{href:"https://docs.clarifai.com/create/applications/manage/#app-overview",children:"Overview"})})," option in the collapsible left sidebar. Get the app ID from there."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"User ID"})," \u2013 In the collapsible left sidebar, select ",(0,o.jsx)(n.strong,{children:"Settings"})," and choose ",(0,o.jsx)(n.strong,{children:"Account"})," from the dropdown list. Then, locate your user ID."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Personal Access Token (PAT)"})," \u2013 From the same ",(0,o.jsx)(n.strong,{children:"Settings"})," option, choose ",(0,o.jsx)(n.strong,{children:"Secrets"})," to generate or copy your ",(0,o.jsx)(n.a,{href:"https://docs.clarifai.com/control/authentication/pat",children:"PAT"}),". This token is used to authenticate your connection with the Clarifai platform."]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["You can then set the PAT as an environment variable using ",(0,o.jsx)(n.code,{children:"CLARIFAI_PAT"}),"."]}),"\n",(0,o.jsxs)(i.A,{groupId:"code",children:[(0,o.jsx)(s.A,{value:"bash",label:"Unix-Like Systems",children:(0,o.jsx)(l.A,{className:"language-bash",children:"export CLARIFAI_PAT=YOUR_PERSONAL_ACCESS_TOKEN_HERE"})}),(0,o.jsx)(s.A,{value:"bash2",label:"Windows",children:(0,o.jsx)(l.A,{className:"language-bash",children:"set CLARIFAI_PAT=YOUR_PERSONAL_ACCESS_TOKEN_HERE"})})]}),"\n",(0,o.jsx)(n.h3,{id:"install-clarifai-cli",children:"Install Clarifai CLI"}),"\n",(0,o.jsxs)(n.p,{children:["Install the latest version of the ",(0,o.jsx)(n.a,{href:"https://docs.clarifai.com/sdk/cli",children:"Clarifai CLI"})," tool. It includes built-in support for Local Runners."]}),"\n",(0,o.jsx)(i.A,{groupId:"code",children:(0,o.jsx)(s.A,{value:"bash",label:"Bash",children:(0,o.jsx)(l.A,{className:"language-bash",children:"pip install --upgrade clarifai"})})}),"\n",(0,o.jsxs)(n.blockquote,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Note:"})," You'll need ",(0,o.jsx)(n.strong,{children:(0,o.jsx)(n.a,{href:"https://docs.clarifai.com/resources/api-overview/python-sdk#python-requirements",children:"Python 3.11 or 3.12"})})," installed to successfully run the Local Runners."]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"get-hugging-face-token",children:"Get Hugging Face Token"}),"\n",(0,o.jsx)(n.p,{children:"A Hugging Face access token is required to authenticate with Hugging Face services, especially when downloading models from private or restricted repositories."}),"\n",(0,o.jsxs)(n.p,{children:["You can create one by following ",(0,o.jsx)(n.a,{href:"https://huggingface.co/docs/hub/en/security-tokens",children:"these instructions"}),". Once you have it, provide the token either in your model\u2019s ",(0,o.jsx)(n.code,{children:"config.yaml"})," file (as described ",(0,o.jsx)(n.a,{href:"#configyaml",children:"below"}),") or as an environment variable."]}),"\n",(0,o.jsxs)(n.blockquote,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Note"}),": If ",(0,o.jsx)(n.code,{children:"hf_token"})," is not specified in the ",(0,o.jsx)(n.code,{children:"config.yaml"})," file, the CLI automatically falls back to the ",(0,o.jsx)(n.code,{children:"HF_TOKEN"})," environment variable to authenticate with Hugging Face."]}),"\n"]}),"\n",(0,o.jsxs)(i.A,{groupId:"code",children:[(0,o.jsx)(s.A,{value:"bash",label:"Unix-Like Systems",children:(0,o.jsx)(l.A,{className:"language-bash",children:'export HF_TOKEN="YOUR_HF_ACCESS_TOKEN_HERE"'})}),(0,o.jsx)(s.A,{value:"bash2",label:"Windows",children:(0,o.jsx)(l.A,{className:"language-bash",children:'set HF_TOKEN="YOUR_HF_ACCESS_TOKEN_HERE"'})})]}),"\n",(0,o.jsx)(n.h3,{id:"install-hugging-face-hub",children:"Install Hugging Face Hub"}),"\n",(0,o.jsxs)(n.p,{children:["The ",(0,o.jsx)(n.a,{href:"https://github.com/huggingface/huggingface_hub",children:(0,o.jsx)(n.code,{children:"huggingface_hub"})})," library is used under the hood to fetch files from the Hugging Face Hub. While you won\u2019t interact with it directly, it\u2019s required for downloading the models and resources automatically."]}),"\n",(0,o.jsx)(i.A,{groupId:"code",children:(0,o.jsx)(s.A,{value:"bash",label:"Bash",children:(0,o.jsx)(l.A,{className:"language-bash",children:"pip install huggingface_hub"})})}),"\n",(0,o.jsx)(n.h2,{id:"step-2-initialize-a-model",children:"Step 2: Initialize a Model"}),"\n",(0,o.jsx)(n.p,{children:"With the Clarifai CLI, you can download and set up any supported Hugging Face model directly in your local environment."}),"\n",(0,o.jsxs)(n.p,{children:["For example, the command below initializes the default model (",(0,o.jsx)(n.a,{href:"https://huggingface.co/unsloth/Llama-3.2-1B-Instruct",children:(0,o.jsx)(n.code,{children:"unsloth/Llama-3.2-1B-Instruct"})}),") in your current directory."]}),"\n",(0,o.jsx)(i.A,{groupId:"code",children:(0,o.jsx)(s.A,{value:"bash",label:"Bash",children:(0,o.jsx)(l.A,{className:"language-bash",children:"clarifai model init --toolkit huggingface"})})}),"\n",(0,o.jsxs)(n.blockquote,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Note:"})," You can initialize a model in a specific location by passing a ",(0,o.jsx)(n.a,{href:"https://docs.clarifai.com/resources/api-overview/cli#clarifai-model-init",children:(0,o.jsx)(n.code,{children:"MODEL_PATH"})}),"."]}),"\n"]}),"\n",(0,o.jsxs)(t,{children:[(0,o.jsx)("summary",{children:"Example Output"}),(0,o.jsx)(l.A,{className:"language-text",children:m})]}),"\n",(0,o.jsx)(n.p,{children:"The command above generates a new model directory structure that is compatible with the Clarifai platform. You can customize or optimize the model by editing the generated files as needed."}),"\n",(0,o.jsxs)(n.admonition,{type:"tip",children:[(0,o.jsxs)(n.p,{children:["You can use the ",(0,o.jsx)(n.code,{children:"--model-name"})," parameter to initialize any supported Hugging Face model."]}),(0,o.jsx)(i.A,{groupId:"code",children:(0,o.jsx)(s.A,{value:"bash",label:"Bash",children:(0,o.jsx)(l.A,{className:"language-bash",children:"clarifai model init --toolkit huggingface --model-name Qwen/Qwen2-0.5B"})})}),(0,o.jsxs)(t,{children:[(0,o.jsx)("summary",{children:"Supported Models"}),(0,o.jsx)(l.A,{className:"language-text",children:u})]})]}),"\n",(0,o.jsxs)(n.blockquote,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Note:"})," Some models are quite large and require substantial memory or GPU resources. Ensure your machine has sufficient compute capacity to load and run the model locally before initializing it."]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"The generated structure includes:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"\u251c\u2500\u2500 1/\n\u2502   \u2514\u2500\u2500 model.py\n\u251c\u2500\u2500 requirements.txt\n\u2514\u2500\u2500 config.yaml\n"})}),"\n",(0,o.jsx)(n.h3,{id:"modelpy",children:(0,o.jsx)(n.code,{children:"model.py"})}),"\n",(0,o.jsxs)(t,{children:[(0,o.jsx)("summary",{children:"Example: model.py"}),(0,o.jsx)(l.A,{className:"language-text",children:c})]}),"\n",(0,o.jsxs)(n.p,{children:["The ",(0,o.jsx)(n.a,{href:"https://docs.clarifai.com/compute/upload/#prepare-modelpy",children:(0,o.jsx)(n.code,{children:"model.py"})})," file, which is located inside the ",(0,o.jsx)(n.code,{children:"1"})," folder, defines the logic for the Hugging Face model, including how predictions are made."]}),"\n",(0,o.jsx)(n.h3,{id:"configyaml",children:(0,o.jsx)(n.code,{children:"config.yaml"})}),"\n",(0,o.jsxs)(t,{children:[(0,o.jsx)("summary",{children:"Example: config.yaml"}),(0,o.jsx)(l.A,{className:"language-text",children:d})]}),"\n",(0,o.jsxs)(n.p,{children:["The ",(0,o.jsx)(n.code,{children:"config.yaml"})," file specifies the model\u2019s configuration, including compute resource requirements, checkpoints, and other essential settings."]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:["In the ",(0,o.jsx)(n.code,{children:"model"})," section, you need to specify a unique model ID (any name you choose) along with an app ID. Your Clarifai user ID is set by default from your ",(0,o.jsx)(n.a,{href:"https://docs.clarifai.com/resources/api-overview/cli#clarifai-config",children:"active context"}),". These together determine where your model will run on the Clarifai platform."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:["In the ",(0,o.jsx)(n.code,{children:"checkpoints"})," section, you can provide your Hugging Face token using the ",(0,o.jsx)(n.code,{children:"hf_token"})," parameter if you need to access private or restricted repositories. This section also includes the ",(0,o.jsx)(n.code,{children:"when"})," parameter, which controls when model checkpoints are downloaded and stored. The available options are ",(0,o.jsx)(n.code,{children:"runtime"})," (the default), which downloads checkpoints when the model is loaded; ",(0,o.jsx)(n.code,{children:"build"}),", which downloads checkpoints during the image build process; and ",(0,o.jsx)(n.code,{children:"upload"}),", which downloads checkpoints before the model is uploaded."]}),"\n",(0,o.jsxs)(n.blockquote,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Note:"})," For large models, it is strongly recommended to set ",(0,o.jsx)(n.code,{children:"when: runtime"}),". Doing so helps prevent image sizes from becoming unnecessarily large, which keeps build times shorter, uploads faster, and inference more efficient on the Clarifai platform. By contrast, choosing ",(0,o.jsx)(n.code,{children:"build"})," or ",(0,o.jsx)(n.code,{children:"upload"})," can significantly increase the image size, leading to slower uploads and higher cold start latency."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"requirementstxt",children:(0,o.jsx)(n.code,{children:"requirements.txt"})}),"\n",(0,o.jsxs)(t,{children:[(0,o.jsx)("summary",{children:"Example: requirements.txt"}),(0,o.jsx)(l.A,{className:"language-text",children:p})]}),"\n",(0,o.jsxs)(n.p,{children:["The ",(0,o.jsx)(n.code,{children:"requirements.txt"})," file lists Python dependencies needed by your model. If you haven\u2019t installed them yet, run the following command to install the dependencies:"]}),"\n",(0,o.jsx)(i.A,{groupId:"code",children:(0,o.jsx)(s.A,{value:"bash",label:"Bash",children:(0,o.jsx)(l.A,{className:"language-bash",children:"pip install -r requirements.txt"})})}),"\n",(0,o.jsx)(n.h2,{id:"step-3-log-in-to-clarifai",children:"Step 3: Log In to Clarifai"}),"\n",(0,o.jsxs)(n.p,{children:["Run the following command to log in to the Clarifai platform, create a configuration ",(0,o.jsx)(n.a,{href:"https://docs.clarifai.com/compute/local-runners/#step-2-create-a-context-optional",children:"context"}),", and establish a connection:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"clarifai login\n"})}),"\n",(0,o.jsx)(n.p,{children:"You\u2019ll be prompted to provide a few details for authentication:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"User ID"})," \u2013 Enter your Clarifai user ID."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"PAT"})," \u2013 Enter your Clarifai PAT. If you\u2019ve already set the ",(0,o.jsx)(n.code,{children:"CLARIFAI_PAT"})," environment variable, type ",(0,o.jsx)(n.code,{children:"ENVVAR"})," to use it automatically."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Context name"})," \u2013 Assign a custom name to this configuration context, or press Enter to accept the default name, ",(0,o.jsx)(n.code,{children:'"default"'}),". This is helpful if you manage multiple environments or configurations."]}),"\n"]}),"\n",(0,o.jsxs)(t,{children:[(0,o.jsx)("summary",{children:"Example Output"}),(0,o.jsx)(l.A,{className:"language-text",children:h})]}),"\n",(0,o.jsx)(n.h2,{id:"step-4-start-your-local-runner",children:"Step 4: Start Your Local Runner"}),"\n",(0,o.jsx)(n.p,{children:"Start a local runner with the following command:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"clarifai model local-runner\n"})}),"\n",(0,o.jsx)(n.p,{children:"If the required context configurations aren\u2019t found, the CLI will walk you through creating them with default values."}),"\n",(0,o.jsxs)(n.p,{children:["This process ensures that all necessary components \u2014 such as compute clusters, nodepools, and deployments \u2014 are included in your configuration context, which are described ",(0,o.jsx)(n.a,{href:"https://docs.clarifai.com/compute/local-runners/#step-2-create-a-context-optional",children:"here"}),"."]}),"\n",(0,o.jsx)(n.p,{children:"Simply review each prompt and confirm to continue."}),"\n",(0,o.jsxs)(t,{children:[(0,o.jsx)("summary",{children:"Example Output"}),(0,o.jsx)(l.A,{className:"language-text",children:f})]}),"\n",(0,o.jsx)(n.h2,{id:"step-5-test-your-runner",children:"Step 5: Test Your Runner"}),"\n",(0,o.jsx)(n.p,{children:"Once the local runner starts, it provides a sample client code snippet you can use for quick testing."}),"\n",(0,o.jsx)(n.p,{children:"You can run the snippet in a separate terminal within the same directory to see the model\u2019s response."}),"\n",(0,o.jsx)(n.p,{children:"Here\u2019s an example snippet:"}),"\n",(0,o.jsx)(i.A,{groupId:"code",children:(0,o.jsx)(s.A,{value:"python",label:"Python SDK",children:(0,o.jsx)(l.A,{className:"language-python",children:g})})})]})}function k(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(_,{...e})}):_(e)}}}]);
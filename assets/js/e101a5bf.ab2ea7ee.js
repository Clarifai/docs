"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[6126],{37506:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>v,contentTitle:()=>p,default:()=>b,frontMatter:()=>m,metadata:()=>n,toc:()=>f});const n=JSON.parse('{"id":"sdk/model-train-and-eval","title":"Model Training And Evaluation Overview","description":"Get a brief overview about model training and evaluation using Clarifai Python SDK","source":"@site/docs/sdk/model-train-and-eval.md","sourceDirName":"sdk","slug":"/sdk/model-train-and-eval","permalink":"/sdk/model-train-and-eval","draft":false,"unlisted":false,"editUrl":"https://github.com/Clarifai/docs/blob/main/docs/sdk/model-train-and-eval.md","tags":[],"version":"current","sidebarPosition":7,"frontMatter":{"sidebar_position":7},"sidebar":"tutorialSidebar","previous":{"title":"Inference from Workflows","permalink":"/sdk/Building-Workflow-Graphs/Inference-from-Workflows/"},"next":{"title":"Advanced Model Operations","permalink":"/sdk/advance-model-operations/"}}');var r=a(74848),s=a(28453),l=a(65537),o=a(79329),i=a(58069);const d="#importing load_module_dataloader for calling the dataloader object in dataset.py in the local data folder\nfrom clarifai.datasets.upload.utils import load_module_dataloader\n\n\n# Evaluate the model using the specified dataset ID 'text_dataset' and evaluation ID 'one'.\nmodel.evaluate(dataset_id='text_dataset', eval_id='one')\n\n# Retrieve the evaluation result for the evaluation ID 'one'.\ntrain_result = model.get_eval_by_id(eval_id=\"one\")\n\n# Construct the path to the test dataset folder\nCSV_PATH = os.path.join(os.getcwd().split('/models/model_train')[0],'datasets/upload/data/test_imdb.csv')\n\n# Create a Clarifai dataset with the specified dataset_id\ntest_dataset = app.create_dataset(dataset_id=\"test_text_dataset\")\n# Upload the dataset using the provided dataloader and get the upload status\ntest_dataset.upload_from_csv(csv_path=CSV_PATH,input_type='text',csv_type='raw', labels=True)\n\n# Evaluate the model using the specified test text dataset identified as 'test_text_dataset'\n# and the evaluation identifier 'two'.\nmodel.evaluate(dataset_id='test_text_dataset', eval_id='two')\n\n# Retrieve the evaluation result with the identifier 'two'.\ntest_result = model.get_eval_by_id(\"two\")\n\n# Print the summary of the evaluation result.\nprint(\"train result:\",train_result.summary)\nprint(\"test result:\",test_result.summary)\n\n",u="from clarifai.utils.evaluation import EvalResultCompare\n\n# Creating an instance of EvalResultCompare class with specified models and datasets\neval_result = EvalResultCompare(models=[model], datasets=[dataset, test_dataset])\n\n# Printing a detailed summary of the evaluation result\nprint(eval_result.detailed_summary())",c="train result:\nmacro_avg_roc_auc: 0.6499999761581421\nmacro_std_roc_auc: 0.07468751072883606\nmacro_avg_f1_score: 0.75\nmacro_avg_precision: 0.6000000238418579\nmacro_avg_recall: 0.5\ntest result:\nmacro_avg_roc_auc: 0.6161290407180786\nmacro_std_roc_auc: 0.1225806474685669\nmacro_avg_f1_score: 0.7207207679748535\nmacro_avg_precision: 0.5633803009986877\nmacro_avg_recall: 0.5\n",h="(  Concept  Accuracy (ROC AUC)  Total Labeled  Total Predicted  True Positives  \\\n 0  id-pos               0.725             80                0               0   \n 0  id-neg               0.575            120              200             120   \n 0  id-pos               0.739             31                0               0   \n 0  id-neg               0.494             40               71              40   \n \n    False Negatives  False Positives  Recall  Precision        F1  \\\n 0               80                0     0.0     1.0000  0.000000   \n 0                0               80     1.0     0.6000  0.750000   \n 0               31                0     0.0     1.0000  0.000000   \n 0                0               31     1.0     0.5634  0.720737   \n \n               Dataset  \n 0       text_dataset2  \n 0       text_dataset2  \n 0  test_text_dataset3  \n 0  test_text_dataset3  ,\n                 Total Concept  Accuracy (ROC AUC)  Total Labeled  \\\n 0       Dataset:text_dataset2            0.650000            200   \n 0  Dataset:test_text_dataset3            0.616129             71   \n \n    Total Predicted  True Positives  False Negatives  False Positives   Recall  \\\n 0              200             120               80               80  0.60000   \n 0               71              40               31               31  0.56338   \n \n    Precision        F1  \n 0   0.760000  0.670588  \n 0   0.754028  0.644909  )",m={sidebar_position:7},p="Model Training And Evaluation Overview",v={},f=[{value:"Model Evaluation",id:"model-evaluation",level:2}];function _(e){const t={admonition:"admonition",code:"code",h1:"h1",h2:"h2",header:"header",p:"p",strong:"strong",...(0,s.R)(),...e.components},{Details:a}=t;return a||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(t.header,{children:(0,r.jsx)(t.h1,{id:"model-training-and-evaluation-overview",children:"Model Training And Evaluation Overview"})}),"\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.strong,{children:"Get a brief overview about model training and evaluation using Clarifai Python SDK"})}),"\n",(0,r.jsx)("hr",{}),"\n",(0,r.jsx)(t.h2,{id:"model-evaluation",children:"Model Evaluation"}),"\n",(0,r.jsxs)(t.p,{children:["Model evaluation is the process by which we monitor the model's performance on the dataset. The Clarifai Python SDK allows you to evaluate the model in two ways. Firstly, you can receive evaluation metrics for each dataset split separately.  The ",(0,r.jsx)(t.code,{children:"Mode.evaluate()"})," method will run the evaluation on the model by using the dataset passed as a parameter. Each evaluation is marked by ",(0,r.jsx)(t.code,{children:"eval_id"}),". This allows users to run multiple evaluations using different datasets."]}),"\n",(0,r.jsx)(t.admonition,{type:"info",children:(0,r.jsx)(t.p,{children:"Evaluation is currently supported for the following model types: Embedding Classifier, Text Classifier, Visual Classifier, and Visual Detector."})}),"\n",(0,r.jsx)(l.A,{children:(0,r.jsx)(o.A,{value:"python",label:"Python",children:(0,r.jsx)(i.A,{className:"language-python",children:d})})}),"\n",(0,r.jsxs)(a,{children:[(0,r.jsx)("summary",{children:"Output"}),(0,r.jsx)(i.A,{className:"language-text",children:c})]}),"\n",(0,r.jsxs)(t.p,{children:["The SDK also has a feature called ",(0,r.jsx)(t.code,{children:"EvalResultCompare"}),". This method allows users to compare the outputs from different evaluations."]}),"\n",(0,r.jsx)(l.A,{children:(0,r.jsx)(o.A,{value:"python",label:"Python",children:(0,r.jsx)(i.A,{className:"language-python",children:u})})}),"\n",(0,r.jsxs)(a,{children:[(0,r.jsx)("summary",{children:"Output"}),(0,r.jsx)(i.A,{className:"language-text",children:h})]})]})}function b(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,r.jsx)(t,{...e,children:(0,r.jsx)(_,{...e})}):_(e)}},65537:(e,t,a)=>{a.d(t,{A:()=>w});var n=a(96540),r=a(18215),s=a(65627),l=a(56347),o=a(50372),i=a(30604),d=a(11861),u=a(78749);function c(e){return n.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,n.isValidElement)(e)&&function(e){const{props:t}=e;return!!t&&"object"==typeof t&&"value"in t}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function h(e){const{values:t,children:a}=e;return(0,n.useMemo)((()=>{const e=t??function(e){return c(e).map((e=>{let{props:{value:t,label:a,attributes:n,default:r}}=e;return{value:t,label:a,attributes:n,default:r}}))}(a);return function(e){const t=(0,d.XI)(e,((e,t)=>e.value===t.value));if(t.length>0)throw new Error(`Docusaurus error: Duplicate values "${t.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[t,a])}function m(e){let{value:t,tabValues:a}=e;return a.some((e=>e.value===t))}function p(e){let{queryString:t=!1,groupId:a}=e;const r=(0,l.W6)(),s=function(e){let{queryString:t=!1,groupId:a}=e;if("string"==typeof t)return t;if(!1===t)return null;if(!0===t&&!a)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return a??null}({queryString:t,groupId:a});return[(0,i.aZ)(s),(0,n.useCallback)((e=>{if(!s)return;const t=new URLSearchParams(r.location.search);t.set(s,e),r.replace({...r.location,search:t.toString()})}),[s,r])]}function v(e){const{defaultValue:t,queryString:a=!1,groupId:r}=e,s=h(e),[l,i]=(0,n.useState)((()=>function(e){let{defaultValue:t,tabValues:a}=e;if(0===a.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(t){if(!m({value:t,tabValues:a}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${t}" but none of its children has the corresponding value. Available values are: ${a.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return t}const n=a.find((e=>e.default))??a[0];if(!n)throw new Error("Unexpected error: 0 tabValues");return n.value}({defaultValue:t,tabValues:s}))),[d,c]=p({queryString:a,groupId:r}),[v,f]=function(e){let{groupId:t}=e;const a=function(e){return e?`docusaurus.tab.${e}`:null}(t),[r,s]=(0,u.Dv)(a);return[r,(0,n.useCallback)((e=>{a&&s.set(e)}),[a,s])]}({groupId:r}),_=(()=>{const e=d??v;return m({value:e,tabValues:s})?e:null})();(0,o.A)((()=>{_&&i(_)}),[_]);return{selectedValue:l,selectValue:(0,n.useCallback)((e=>{if(!m({value:e,tabValues:s}))throw new Error(`Can't select invalid tab value=${e}`);i(e),c(e),f(e)}),[c,f,s]),tabValues:s}}var f=a(9136);const _={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var b=a(74848);function g(e){let{className:t,block:a,selectedValue:n,selectValue:l,tabValues:o}=e;const i=[],{blockElementScrollPositionUntilNextRender:d}=(0,s.a_)(),u=e=>{const t=e.currentTarget,a=i.indexOf(t),r=o[a].value;r!==n&&(d(t),l(r))},c=e=>{let t=null;switch(e.key){case"Enter":u(e);break;case"ArrowRight":{const a=i.indexOf(e.currentTarget)+1;t=i[a]??i[0];break}case"ArrowLeft":{const a=i.indexOf(e.currentTarget)-1;t=i[a]??i[i.length-1];break}}t?.focus()};return(0,b.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,r.A)("tabs",{"tabs--block":a},t),children:o.map((e=>{let{value:t,label:a,attributes:s}=e;return(0,b.jsx)("li",{role:"tab",tabIndex:n===t?0:-1,"aria-selected":n===t,ref:e=>{i.push(e)},onKeyDown:c,onClick:u,...s,className:(0,r.A)("tabs__item",_.tabItem,s?.className,{"tabs__item--active":n===t}),children:a??t},t)}))})}function x(e){let{lazy:t,children:a,selectedValue:s}=e;const l=(Array.isArray(a)?a:[a]).filter(Boolean);if(t){const e=l.find((e=>e.props.value===s));return e?(0,n.cloneElement)(e,{className:(0,r.A)("margin-top--md",e.props.className)}):null}return(0,b.jsx)("div",{className:"margin-top--md",children:l.map(((e,t)=>(0,n.cloneElement)(e,{key:t,hidden:e.props.value!==s})))})}function y(e){const t=v(e);return(0,b.jsxs)("div",{className:(0,r.A)("tabs-container",_.tabList),children:[(0,b.jsx)(g,{...t,...e}),(0,b.jsx)(x,{...t,...e})]})}function w(e){const t=(0,f.A)();return(0,b.jsx)(y,{...e,children:c(e.children)},String(t))}},79329:(e,t,a)=>{a.d(t,{A:()=>l});a(96540);var n=a(18215);const r={tabItem:"tabItem_Ymn6"};var s=a(74848);function l(e){let{children:t,hidden:a,className:l}=e;return(0,s.jsx)("div",{role:"tabpanel",className:(0,n.A)(r.tabItem,l),hidden:a,children:t})}}}]);
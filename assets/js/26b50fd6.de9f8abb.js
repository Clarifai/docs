"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[2272],{11470:(e,n,t)=>{t.d(n,{A:()=>j});var o=t(96540),r=t(18215),s=t(23104),l=t(56347),a=t(205),i=t(57485),c=t(31682),d=t(70679);function u(e){return o.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,o.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function h(e){const{values:n,children:t}=e;return(0,o.useMemo)(()=>{const e=n??function(e){return u(e).map(({props:{value:e,label:n,attributes:t,default:o}})=>({value:e,label:n,attributes:t,default:o}))}(t);return function(e){const n=(0,c.XI)(e,(e,n)=>e.value===n.value);if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[n,t])}function p({value:e,tabValues:n}){return n.some(n=>n.value===e)}function m({queryString:e=!1,groupId:n}){const t=(0,l.W6)(),r=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,i.aZ)(r),(0,o.useCallback)(e=>{if(!r)return;const n=new URLSearchParams(t.location.search);n.set(r,e),t.replace({...t.location,search:n.toString()})},[r,t])]}function f(e){const{defaultValue:n,queryString:t=!1,groupId:r}=e,s=h(e),[l,i]=(0,o.useState)(()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!p({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const t=n.find(e=>e.default)??n[0];if(!t)throw new Error("Unexpected error: 0 tabValues");return t.value}({defaultValue:n,tabValues:s})),[c,u]=m({queryString:t,groupId:r}),[f,g]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[t,r]=(0,d.Dv)(n);return[t,(0,o.useCallback)(e=>{n&&r.set(e)},[n,r])]}({groupId:r}),x=(()=>{const e=c??f;return p({value:e,tabValues:s})?e:null})();(0,a.A)(()=>{x&&i(x)},[x]);return{selectedValue:l,selectValue:(0,o.useCallback)(e=>{if(!p({value:e,tabValues:s}))throw new Error(`Can't select invalid tab value=${e}`);i(e),u(e),g(e)},[u,g,s]),tabValues:s}}var g=t(92303);const x={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var b=t(74848);function y({className:e,block:n,selectedValue:t,selectValue:o,tabValues:l}){const a=[],{blockElementScrollPositionUntilNextRender:i}=(0,s.a_)(),c=e=>{const n=e.currentTarget,r=a.indexOf(n),s=l[r].value;s!==t&&(i(n),o(s))},d=e=>{let n=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const t=a.indexOf(e.currentTarget)+1;n=a[t]??a[0];break}case"ArrowLeft":{const t=a.indexOf(e.currentTarget)-1;n=a[t]??a[a.length-1];break}}n?.focus()};return(0,b.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,r.A)("tabs",{"tabs--block":n},e),children:l.map(({value:e,label:n,attributes:o})=>(0,b.jsx)("li",{role:"tab",tabIndex:t===e?0:-1,"aria-selected":t===e,ref:e=>{a.push(e)},onKeyDown:d,onClick:c,...o,className:(0,r.A)("tabs__item",x.tabItem,o?.className,{"tabs__item--active":t===e}),children:n??e},e))})}function I({lazy:e,children:n,selectedValue:t}){const s=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=s.find(e=>e.props.value===t);return e?(0,o.cloneElement)(e,{className:(0,r.A)("margin-top--md",e.props.className)}):null}return(0,b.jsx)("div",{className:"margin-top--md",children:s.map((e,n)=>(0,o.cloneElement)(e,{key:n,hidden:e.props.value!==t}))})}function _(e){const n=f(e);return(0,b.jsxs)("div",{className:(0,r.A)("tabs-container",x.tabList),children:[(0,b.jsx)(y,{...n,...e}),(0,b.jsx)(I,{...n,...e})]})}function j(e){const n=(0,g.A)();return(0,b.jsx)(_,{...e,children:u(e.children)},String(n))}},19365:(e,n,t)=>{t.d(n,{A:()=>l});t(96540);var o=t(18215);const r={tabItem:"tabItem_Ymn6"};var s=t(74848);function l({children:e,hidden:n,className:t}){return(0,s.jsx)("div",{role:"tabpanel",className:(0,o.A)(r.tabItem,t),hidden:n,children:e})}},21512:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>b,contentTitle:()=>x,default:()=>_,frontMatter:()=>g,metadata:()=>o,toc:()=>y});const o=JSON.parse('{"id":"compute/local-runners/lmstudio","title":"LM Studio","description":"Download and run LM Studio models locally and make them available via a public API","source":"@site/docs/compute/local-runners/lmstudio.md","sourceDirName":"compute/local-runners","slug":"/compute/local-runners/lmstudio","permalink":"/compute/local-runners/lmstudio","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"description":"Download and run LM Studio models locally and make them available via a public API","sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Hugging Face","permalink":"/compute/local-runners/hf"},"next":{"title":"vLLM","permalink":"/compute/local-runners/vllm"}}');var r=t(74848),s=t(28453),l=t(11470),a=t(19365),i=t(73748);const c="clarifai model init --toolkit lmstudio\n[INFO] 09:07:03.086018 Parsed GitHub repository: owner=Clarifai, repo=runners-examples, branch=lmstudio, folder_path= |  thread=8309383360 \n[INFO] 09:07:05.331174 Files to be downloaded are:\n1. 1/model.py\n2. config.yaml\n3. requirements.txt |  thread=8309383360 \nPress Enter to continue...\n[INFO] 09:07:09.895510 Initializing model from GitHub repository: https://github.com/Clarifai/runners-examples |  thread=8309383360 \n[INFO] 09:07:37.976873 Successfully cloned repository from https://github.com/Clarifai/runners-examples (branch: lmstudio) |  thread=8309383360 \n[INFO] 09:07:37.980528 Model initialization complete with GitHub repository |  thread=8309383360 \n[INFO] 09:07:37.980580 Next steps: |  thread=8309383360 \n[INFO] 09:07:37.980603 1. Review the model configuration |  thread=8309383360 \n[INFO] 09:07:37.980619 2. Install any required dependencies manually |  thread=8309383360 \n[INFO] 09:07:37.980635 3. Test the model locally using 'clarifai model local-test' |  thread=8309383360 ",d="clarifai login\nEnter your Clarifai user ID: alfrick\n> To authenticate, you'll need a Personal Access Token (PAT).\n> You can create one from your account settings: https://clarifai.com/alfrick/settings/security\n\nEnter your Personal Access Token (PAT) value (or type \"ENVVAR\" to use an environment variable): ENVVAR\n\n> Verifying token...\n[INFO] 09:38:03.867057 Validating the Context Credentials... |  thread=8309383360 \n[INFO] 09:38:05.176881 \u2705 Context is valid |  thread=8309383360 \n\n> Let's save these credentials to a new context.\n> You can have multiple contexts to easily switch between accounts or projects.\n\nEnter a name for this context [default]: \n\u2705 Success! You are now logged in.\nCredentials saved to the 'default' context.\n\n\ud83d\udca1 To switch contexts later, use `clarifai config use-context <name>`.\n[INFO] 09:38:10.706639 Login successful for user 'alfrick' in context 'default' |  thread=8309383360 \n",u='clarifai model local-runner\n[INFO] 09:40:36.097539 Detected OpenAI chat completions for Clarifai model streaming - validating stream_options... |  thread=8309383360 \n[INFO] 09:40:36.098189 > Checking local runner requirements... |  thread=8309383360 \n[INFO] 09:40:36.118322 Checking 2 dependencies... |  thread=8309383360 \n[INFO] 09:40:36.118807 \u2705 All 2 dependencies are installed! |  thread=8309383360 \n[INFO] 09:40:36.119033 > Verifying local runner setup... |  thread=8309383360 \n[INFO] 09:40:36.119083 Current context: default |  thread=8309383360 \n[INFO] 09:40:36.119120 Current user_id: alfrick |  thread=8309383360 \n[INFO] 09:40:36.119150 Current PAT: d6570**** |  thread=8309383360 \n[INFO] 09:40:36.121055 Current compute_cluster_id: local-runner-compute-cluster |  thread=8309383360 \n[WARNING] 09:40:37.622490 Failed to get compute cluster with ID \'local-runner-compute-cluster\':\ncode: CONN_DOES_NOT_EXIST\ndescription: "Resource does not exist"\ndetails: "ComputeCluster with ID \\\'local-runner-compute-cluster\\\' not found. Check your request fields."\nreq_id: "sdk-python-11.8.2-c324cbe5deb248e19d5d0ed1e32e49d0"\n |  thread=8309383360 \nCompute cluster not found. Do you want to create a new compute cluster alfrick/local-runner-compute-cluster? (y/n): y\n[INFO] 09:40:44.198312 Compute Cluster with ID \'local-runner-compute-cluster\' is created:\ncode: SUCCESS\ndescription: "Ok"\nreq_id: "sdk-python-11.8.2-e5b312b4a46f4e2984efc65abb5124c5"\n |  thread=8309383360 \n[INFO] 09:40:44.203633 Current nodepool_id: local-runner-nodepool |  thread=8309383360 \n[WARNING] 09:40:46.398631 Failed to get nodepool with ID \'local-runner-nodepool\':\ncode: CONN_DOES_NOT_EXIST\ndescription: "Resource does not exist"\ndetails: "Nodepool not found. Check your request fields."\nreq_id: "sdk-python-11.8.2-1062d71d21574bce99bd4472a9fdc6ef"\n |  thread=8309383360 \nNodepool not found. Do you want to create a new nodepool alfrick/local-runner-compute-cluster/local-runner-nodepool? (y/n): y\n[INFO] 09:40:52.285792 Nodepool with ID \'local-runner-nodepool\' is created:\ncode: SUCCESS\ndescription: "Ok"\nreq_id: "sdk-python-11.8.2-66d76251237c4be38764837e639c6800"\n |  thread=8309383360 \n[INFO] 09:40:52.292983 Current app_id: local-runner-app |  thread=8309383360 \n[WARNING] 09:40:52.574021 Failed to get app with ID \'local-runner-app\':\ncode: CONN_DOES_NOT_EXIST\ndescription: "Resource does not exist"\ndetails: "app identified by path /users/alfrick/apps/local-runner-app not found"\nreq_id: "sdk-python-11.8.2-29b94532bf624596abbbaea66be198e2"\n |  thread=8309383360 \nApp not found. Do you want to create a new app alfrick/local-runner-app? (y/n): y\n[INFO] 09:40:56.302447 App with ID \'local-runner-app\' is created:\ncode: SUCCESS\ndescription: "Ok"\nreq_id: "sdk-python-11.8.2-b5066f7c64274944ba405ba01da11c1c"\n |  thread=8309383360 \n[INFO] 09:40:56.306934 Current model_id: local-runner-model |  thread=8309383360 \n[WARNING] 09:40:58.007139 Failed to get model with ID \'local-runner-model\':\ncode: MODEL_DOES_NOT_EXIST\ndescription: "Model does not exist"\ndetails: "Model \\\'local-runner-model\\\' does not exist."\nreq_id: "sdk-python-11.8.2-8b2717eb04624aca8bf119e03b94b5b4"\n |  thread=8309383360 \nModel not found. Do you want to create a new model alfrick/local-runner-app/models/local-runner-model? (y/n): y\n[INFO] 09:41:14.336510 Model with ID \'local-runner-model\' is created:\ncode: SUCCESS\ndescription: "Ok"\nreq_id: "sdk-python-11.8.2-f36c2684e1bc4d8f99e777d42e5c53f8"\n |  thread=8309383360 \n[WARNING] 09:41:17.182009 No model versions found. Creating a new version for local runner. |  thread=8309383360 \n[INFO] 09:41:17.510454 Model Version with ID \'fa82276f4cfa44c08745b028471bbfa5\' is created:\ncode: SUCCESS\ndescription: "Ok"\nreq_id: "sdk-python-11.8.2-0121fe726015400c86e4bd3959729787"\n |  thread=8309383360 \n[INFO] 09:41:17.517728 Current model version fa82276f4cfa44c08745b028471bbfa5 |  thread=8309383360 \n[INFO] 09:41:17.517802 Creating the local runner tying this \'alfrick/local-runner-app/models/local-runner-model\' model (version: fa82276f4cfa44c08745b028471bbfa5) to the \'alfrick/local-runner-compute-cluster/local-runner-nodepool\' nodepool. |  thread=8309383360 \n[INFO] 09:41:18.591818 Runner with ID \'649b39c737d84dd8a5e3d5af0b19c207\' is created:\ncode: SUCCESS\ndescription: "Ok"\nreq_id: "sdk-python-11.8.2-ced2523458a941519a709e6af082832a"\n |  thread=8309383360 \n[INFO] 09:41:18.598056 Current runner_id: 649b39c737d84dd8a5e3d5af0b19c207 |  thread=8309383360 \n[WARNING] 09:41:19.150091 Failed to get deployment with ID local-runner-deployment:\ncode: DEPLOYMENT_INVALID_REQUEST\ndescription: "Invalid deployment request"\ndetails: "Some of the deployment ids provided (local-runner-deployment) do not exist"\nreq_id: "sdk-python-11.8.2-9af7aa96a9a843e68f8f3ef898bf61c1"\n |  thread=8309383360 \nDeployment not found. Do you want to create a new deployment alfrick/local-runner-compute-cluster/local-runner-nodepool/local-runner-deployment? (y/n): y\n[INFO] 09:41:25.833184 Deployment with ID \'local-runner-deployment\' is created:\ncode: SUCCESS\ndescription: "Ok"\nreq_id: "sdk-python-11.8.2-38e4b0bd886e4979bc9b7324361e2c56"\n |  thread=8309383360 \n[INFO] 09:41:25.839987 Current deployment_id: local-runner-deployment |  thread=8309383360 \n[INFO] 09:41:25.841181 Current model section of config.yaml: {\'app_id\': \'local-runner-app\', \'id\': \'local-env-model\', \'model_type_id\': \'text-to-text\', \'user_id\': \'alfrick\'} |  thread=8309383360 \nDo you want to backup config.yaml to config.yaml.bk then update the config.yaml with the new model information? (y/n): y\n[INFO] 09:41:29.312446 Checking 2 dependencies... |  thread=8309383360 \n[INFO] 09:41:29.313228 \u2705 All 2 dependencies are installed! |  thread=8309383360 \n[INFO] 09:41:29.313325 \u2705 Starting local runner... |  thread=8309383360 \n[INFO] 09:41:29.313404 No secrets path configured, running without secrets |  thread=8309383360 \n[INFO] 09:41:30.647566 Detected OpenAI chat completions for Clarifai model streaming - validating stream_options... |  thread=8309383360 \n[INFO] 09:41:34.359410 Detected OpenAI chat completions for Clarifai model streaming - validating stream_options... |  thread=8309383360 \n[INFO] 09:41:34.359915 Running: lms get https://huggingface.co/LiquidAI/LFM2-1.2B --verbose |  thread=8309383360 \n[INFO] 09:41:34.625973 [lms logs] D Found local API server at ws://127.0.0.1:41343 |  thread=8309383360 \n[INFO] 09:41:34.633082 [lms logs] I Searching for models with the term https://huggingface.co/LiquidAI/LFM2-1.2B |  thread=8309383360 \n[INFO] 09:41:34.633891 [lms logs] D Searching for models with options { |  thread=8309383360 \n[INFO] 09:41:34.633919 [lms logs]   searchTerm: \'https://huggingface.co/LiquidAI/LFM2-1.2B\', |  thread=8309383360 \n[INFO] 09:41:34.633937 [lms logs]   compatibilityTypes: undefined, |  thread=8309383360 \n[INFO] 09:41:34.633950 [lms logs]   limit: undefined |  thread=8309383360 \n[INFO] 09:41:34.633963 [lms logs] } |  thread=8309383360 \n[INFO] 09:41:40.602478 [lms logs] D Found 10 result(s) |  thread=8309383360 \n[INFO] 09:41:40.602769 [lms logs] D Prompting user to choose a model |  thread=8309383360 \n[INFO] 09:41:40.602822 [lms logs] I No exact match found. Please choose a model from the list below. |  thread=8309383360 \n[INFO] 09:41:40.602867 [lms logs]  |  thread=8309383360 \n[INFO] 09:41:40.603408 [lms logs] ! Use the arrow keys to navigate, type to filter, and press enter to select. |  thread=8309383360 \n[INFO] 09:41:40.603520 [lms logs]  |  thread=8309383360 \n[INFO] 09:41:40.619671 [lms logs] ? Select a model to download Type to filter... |  thread=8309383360 \n[INFO] 09:41:40.619819 [lms logs] \u276f  LiquidAI/LFM2-1.2B-GGUF |  thread=8309383360 \n[INFO] 09:41:40.619874 [lms logs]    LiquidAI/LFM2-1.2B-Tool-GGUF |  thread=8309383360 \n[INFO] 09:41:40.619902 [lms logs]    LiquidAI/LFM2-1.2B-Extract-GGUF |  thread=8309383360 \n[INFO] 09:41:40.619946 [lms logs]    LiquidAI/LFM2-1.2B-RAG-GGUF |  thread=8309383360 \n[INFO] 09:41:40.619969 [lms logs]    DevQuasar/LiquidAI.LFM2-1.2B-GGUF |  thread=8309383360 \n[INFO] 09:41:40.619992 [lms logs]    bartowski/LiquidAI_LFM2-1.2B-Extract-GGUF |  thread=8309383360 \n[INFO] 09:41:40.620018 [lms logs]    bartowski/LiquidAI_LFM2-1.2B-RAG-GGUF |  thread=8309383360 \n[INFO] 09:41:40.620044 [lms logs]    bartowski/LiquidAI_LFM2-1.2B-Tool-GGUF |  thread=8309383360 \n[INFO] 09:41:40.620066 [lms logs]    DevQuasar/LiquidAI.LFM2-1.2B-RAG-GGUF |  thread=8309383360 \n\n',h='import os\nfrom openai import OpenAI\n\n# Initialize the OpenAI client, pointing to Clarifai\'s API\nclient = OpenAI(     \n    base_url="https://api.clarifai.com/v2/ext/openai/v1",  # Clarifai\'s OpenAI-compatible API endpoint\n    api_key=os.environ["CLARIFAI_PAT"]  # Ensure CLARIFAI_PAT is set as an environment variable\n)\n\n# Make a chat completion request to a Clarifai-hosted model\nresponse = client.chat.completions.create(    \n    model="https://clarifai.com/<user-id>/local-runner-app/models/local-runner-model",    \n    messages=[\n        {"role": "system", "content": "You are a helpful assistant."},\n        {"role": "user", "content": "What is the future of AI?"}\n    ],  \n)\n\n# Print the model\'s response\nprint(response.choices[0].message.content)',p='import sys\nimport time\nimport socket\nimport os\nimport json\nimport subprocess\nfrom typing import List, Iterator\n\nfrom clarifai.runners.models.openai_class import OpenAIModelClass\nfrom clarifai.runners.models.model_builder import ModelBuilder\nfrom clarifai.utils.logging import logger\n\nfrom clarifai.runners.utils.data_utils import Param\nfrom clarifai.runners.utils.data_types import Image\nfrom clarifai.runners.utils.openai_convertor import build_openai_messages\n\nfrom openai import OpenAI\n\n\nVERBOSE_LMSTUDIO = True # Set to True to see the output of the lmstudio server in the logs\n\ndef _stream_command(cmd: str, verbose: bool = True):\n    """\n    Run a shell command, streaming its combined stdout/stderr line by line.\n    Returns True on exit code 0, else raises RuntimeError.\n    """\n    logger.info(f"Running: {cmd}")\n    # Force line buffering from many tools by setting environment tweaks\n    env = os.environ.copy()\n    env["PYTHONUNBUFFERED"] = "1"\n    # Start process\n    process = subprocess.Popen(\n        cmd,\n        shell=True,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.STDOUT,\n        text=True,\n        bufsize=1,\n        env=env\n    )\n    if verbose and process.stdout:\n        for line in iter(process.stdout.readline, ""):\n            if line:  # strip trailing newline for cleaner log\n                logger.info(f"[lms logs] {line.rstrip()}")\n    ret = process.wait()\n    if ret != 0:\n        raise RuntimeError(f"Command failed ({ret}): {cmd}")\n    return True\n\ndef _wait_for_port(port: int, timeout: float = 30.0):\n    """\n    Wait until something is listening on localhost:port.\n    """\n    start = time.time()\n    while time.time() - start < timeout:\n        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n            sock.settimeout(1)\n            try:\n                if sock.connect_ex(("127.0.0.1", port)) == 0:\n                    return True\n            except Exception:\n                pass\n        time.sleep(0.5)\n    raise RuntimeError(f"Server did not start listening on port {port} within {timeout}s")\n\ndef run_lms_server(model_name: str = \'LiquidAI/LFM2-1.2B-GGUF\', port: int = 11434,\n                   context_length: int = 4096) -> None:\n    """\n    Start the lmstudio server with ordered, real\u2011time logs.\n    """\n    from clarifai.runners.utils.model_utils import terminate_process  # keep if needed elsewhere\n    try:\n        # 1. Pull model\n        _stream_command(f"lms get https://huggingface.co/{model_name} --verbose", verbose=VERBOSE_LMSTUDIO)\n        logger.info(f"Model {model_name} pulled successfully.")\n\n        # 2. Unload previous models\n        _stream_command("lms unload --all", verbose=VERBOSE_LMSTUDIO)\n        logger.info("All models unloaded successfully.")\n\n        # 3. Load target model\n        _stream_command(f"lms load {model_name} --verbose --context-length {context_length}",\n                        verbose=VERBOSE_LMSTUDIO)\n        logger.info(f"Model {model_name} loaded (context_length={context_length}).")\n\n        # 4. Start server (run in background so we return)\n        logger.info(f"Starting lmstudio server on port {port}...")\n        # Start server detached so we can still stream its startup output briefly if verbose.\n        server_proc = subprocess.Popen(\n            f"lms server start --port {port}",\n            shell=True,\n            stdout=None if not VERBOSE_LMSTUDIO else sys.stdout,\n            stderr=None if not VERBOSE_LMSTUDIO else sys.stderr\n        )\n\n        # 5. Wait for port to be open\n        _wait_for_port(port)\n        logger.info(f"lms server started successfully on port {port} (pid={server_proc.pid}).")\n\n    except Exception as e:\n        logger.error(f"Error starting lmstudio server: {e}")\n        raise RuntimeError(f"Failed to start lmstudio server: {e}")\n\n# Check if Image has content before building messages\ndef has_image_content(image: Image) -> bool:\n    """Check if Image object has either bytes or URL."""\n    return bool(getattr(image, \'url\', None) or getattr(image, \'bytes\', None))\n\nclass LMstudioModelClass(OpenAIModelClass):\n\n    client =  True\n    model = True\n\n    def load_model(self):\n        """\n        Load the lmstudio model.\n        """\n        model_path = os.path.dirname(os.path.dirname(__file__))\n        builder = ModelBuilder(model_path, download_validation_only=True)\n        self.model = builder.config[\'toolkit\'][\'model\']\n        self.port = builder.config[\'toolkit\'][\'port\']\n        self.context_length = builder.config[\'toolkit\'][\'context_length\']\n        \n        #start lmstudio server\n        run_lms_server(model_name=self.model, port=self.port, context_length=self.context_length)\n\n        self.client = OpenAI(\n                api_key="notset",\n                base_url= f"http://localhost:{self.port}/v1")\n\n        logger.info(f"LMstudio model loaded successfully: {self.model}")\n\n\n    @OpenAIModelClass.method\n    def predict(self,\n                prompt: str,\n                image: Image = None,\n                images: List[Image] = None,\n                chat_history: List[dict] = None,\n                tools: List[dict] = None,\n                tool_choice: str = None,\n                max_tokens: int = Param(default=2048, description="The maximum number of tokens to generate. Shorter token lengths will provide faster performance.", ),\n                temperature: float = Param(default=0.7, description="A decimal number that determines the degree of randomness in the response", ),\n                top_p: float = Param(default=0.95, description="An alternative to sampling with temperature, where the model considers the results of the tokens with top_p probability mass."),\n                ) -> str:\n      """\n      This method is used to predict the response for the given prompt and chat history using the model and tools.\n      """\n      if tools is not None and tool_choice is None:\n          tool_choice = "auto"\n\n      img_content = image if has_image_content(image) else None\n\n      messages = build_openai_messages(prompt=prompt, image=img_content, images=images, messages=chat_history)\n      response = self.client.chat.completions.create(\n          model=self.model,\n          messages=messages,\n          tools=tools,\n          tool_choice=tool_choice,\n          max_completion_tokens=max_tokens,\n          temperature=temperature,\n          top_p=top_p)\n\n      if response.usage is not None:\n            self.set_output_context(prompt_tokens=response.usage.prompt_tokens,\n                                    completion_tokens=response.usage.completion_tokens)\n            if len(response.choices) == 0:\n                # still need to send the usage back.\n                return ""\n\n      if response.choices[0] and response.choices[0].message.tool_calls:\n        # If the response contains tool calls, return as a string\n        tool_calls = response.choices[0].message.tool_calls\n        tool_calls_json = json.dumps([tc.to_dict() for tc in tool_calls], indent=2)\n        return tool_calls_json\n      else:\n        # Otherwise, return the content of the first choice\n        return response.choices[0].message.content\n\n\n    @OpenAIModelClass.method\n    def generate(self,\n                prompt: str,\n                image: Image = None,\n                images: List[Image] = None,\n                chat_history: List[dict] = None,\n                tools: List[dict] = None,\n                tool_choice: str = None,\n                max_tokens: int = Param(default=2048, description="The maximum number of tokens to generate. Shorter token lengths will provide faster performance.", ),\n                temperature: float = Param(default=0.7, description="A decimal number that determines the degree of randomness in the response", ),\n                top_p: float = Param(default=0.95, description="An alternative to sampling with temperature, where the model considers the results of the tokens with top_p probability mass.")) -> Iterator[str]:\n      """\n      This method is used to stream generated text tokens from a prompt + optional chat history and tools.\n      """\n      if tools is not None and tool_choice is None:\n          tool_choice = "auto"\n\n      img_content = image if has_image_content(image) else None\n\n      messages = build_openai_messages(prompt=prompt, image=img_content, images=images, messages=chat_history)\n      for chunk in self.client.chat.completions.create(\n            model=self.model,\n            messages=messages,\n            tools=tools,\n            tool_choice=tool_choice,\n            max_completion_tokens=max_tokens,\n            temperature=temperature,\n            top_p=top_p,\n            stream=True,\n            stream_options={"include_usage": True}\n            ):\n            if chunk.usage is not None:\n                if chunk.usage.prompt_tokens or chunk.usage.completion_tokens:\n                    self.set_output_context(prompt_tokens=chunk.usage.prompt_tokens, completion_tokens=chunk.usage.completion_tokens)\n                if len(chunk.choices) == 0: # still need to send the usage back.\n                    yield ""\n\n            if chunk.choices:\n                if chunk.choices[0].delta.tool_calls:\n                # If the response contains tool calls, return the first one as a string\n                    import json\n                    tool_calls = chunk.choices[0].delta.tool_calls\n                    tool_calls_json = [tc.to_dict() for tc in tool_calls]\n                    # Convert to JSON string\n                    json_string = json.dumps(tool_calls_json, indent=2)\n                    # Yield the JSON string\n                    yield json_string\n                else:\n                    # Otherwise, return the content of the first choice\n                    text = (chunk.choices[0].delta.content\n                            if (chunk and chunk.choices[0].delta.content) is not None else \'\')\n                    yield text\n',m="build_info:\n  python_version: '3.12'\ninference_compute_info:\n  cpu_limit: '3'\n  cpu_memory: 14Gi\n  num_accelerators: 0\nmodel:\n  app_id: local-runner-app\n  id: local-env-model\n  model_type_id: text-to-text\n  user_id: clarifai-user-id\ntoolkit:\n  provider: lmstudio\n  model: LiquidAI/LFM2-1.2B\n  port: 11434\n  context_length: 2048\n",f="clarifai\nopenai",g={description:"Download and run LM Studio models locally and make them available via a public API",sidebar_position:3},x="LM Studio",b={},y=[{value:"Step 1: Perform Prerequisites",id:"step-1-perform-prerequisites",level:2},{value:"Sign Up or Log In",id:"sign-up-or-log-in",level:3},{value:"Install the Clarifai CLI",id:"install-the-clarifai-cli",level:3},{value:"Install the OpenAI Package",id:"install-the-openai-package",level:3},{value:"Install LM Studio",id:"install-lm-studio",level:3},{value:"Step 2: Initialize a Model",id:"step-2-initialize-a-model",level:2},{value:"<code>model.py</code>",id:"modelpy",level:3},{value:"<code>config.yaml</code>",id:"configyaml",level:3},{value:"<code>requirements.txt</code>",id:"requirementstxt",level:3},{value:"Step 3: Log In to Clarifai",id:"step-3-log-in-to-clarifai",level:2},{value:"Step 4: Start the Local Runner",id:"step-4-start-the-local-runner",level:2},{value:"Step 5: Test Your Runner",id:"step-5-test-your-runner",level:2}];function I(e){const n={a:"a",admonition:"admonition",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components},{Details:t}=n;return t||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"lm-studio",children:"LM Studio"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Download and run LM Studio models locally and make them available via a public API"})}),"\n",(0,r.jsx)("hr",{}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"https://lmstudio.ai/",children:"LM Studio"})," is a desktop application that lets you run and chat with open-source large language models (LLMs) locally \u2014 no internet connection required."]}),"\n",(0,r.jsxs)(n.p,{children:["With Clarifai\u2019s ",(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/compute/local-runners/",children:"Local Runners"}),", you can take this a step further: run LM Studio models directly on your machine, expose them securely through a public URL, and leverage Clarifai\u2019s powerful AI platform \u2014 all while maintaining the speed, privacy, and control of local deployment."]}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Note:"})," After downloading the model using the LM Studio toolkit, you can ",(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/compute/upload/#step-4-upload-the-model-to-clarifai",children:"upload"})," it to Clarifai to leverage the platform\u2019s capabilities."]}),"\n"]}),"\n","\n","\n",(0,r.jsx)(n.h2,{id:"step-1-perform-prerequisites",children:"Step 1: Perform Prerequisites"}),"\n",(0,r.jsx)(n.h3,{id:"sign-up-or-log-in",children:"Sign Up or Log In"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"https://clarifai.com/login",children:(0,r.jsx)(n.strong,{children:"Log in"})})," to your existing Clarifai account or ",(0,r.jsx)(n.a,{href:"https://clarifai.com/signup",children:(0,r.jsx)(n.strong,{children:"sign up"})})," for a new one. After logging in, gather the following credentials for setup:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"App ID"})," \u2013 Go to the application you\u2019ll use to run your model and select ",(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/create/applications/manage/#app-overview",children:"Overview"})})," in the collapsible left sidebar. Get the app ID from there."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"User ID"})," \u2013 In the collapsible left sidebar, select ",(0,r.jsx)(n.strong,{children:"Settings"})," and select ",(0,r.jsx)(n.strong,{children:"Account"})," from the dropdown list. Then, find your user ID."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Personal Access Token (PAT)"})," \u2013 From the same ",(0,r.jsx)(n.strong,{children:"Settings"})," option, select ",(0,r.jsx)(n.strong,{children:"Secrets"})," to create or copy your ",(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/control/authentication/pat",children:"PAT"}),". This token is required to authenticate your connection with the Clarifai platform."]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Once you have your PAT, set it as an environment variable for secure authentication:"}),"\n",(0,r.jsxs)(l.A,{groupId:"code",children:[(0,r.jsx)(a.A,{value:"bash",label:"Unix-like Systems",children:(0,r.jsx)(i.A,{className:"language-bash",children:"export CLARIFAI_PAT=YOUR_PERSONAL_ACCESS_TOKEN_HERE"})}),(0,r.jsx)(a.A,{value:"bash2",label:"Windows",children:(0,r.jsx)(i.A,{className:"language-bash",children:"set CLARIFAI_PAT=YOUR_PERSONAL_ACCESS_TOKEN_HERE"})})]}),"\n",(0,r.jsx)(n.h3,{id:"install-the-clarifai-cli",children:"Install the Clarifai CLI"}),"\n",(0,r.jsxs)(n.p,{children:["Next, install the latest version of the ",(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/sdk/cli",children:(0,r.jsx)(n.strong,{children:"Clarifai CLI"})}),", which includes built-in support for Local Runners."]}),"\n",(0,r.jsx)(l.A,{groupId:"code",children:(0,r.jsx)(a.A,{value:"bash",label:"Bash",children:(0,r.jsx)(i.A,{className:"language-bash",children:"pip install --upgrade clarifai"})})}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Note:"})," Ensure you have ",(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/resources/api-overview/python-sdk#python-requirements",children:"Python 3.11 or 3.12"})})," installed to successfully run Local Runners."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"install-the-openai-package",children:"Install the OpenAI Package"}),"\n",(0,r.jsxs)(n.p,{children:["Install the ",(0,r.jsx)(n.code,{children:"openai"})," package \u2014 it\u2019s required to perform inference with LM Studio models that support the ",(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/compute/inference/#predict-with-openai-compatible-format",children:"OpenAI-compatible"})," format."]}),"\n",(0,r.jsx)(l.A,{groupId:"code",children:(0,r.jsx)(a.A,{value:"bash",label:"Bash",children:(0,r.jsx)(i.A,{className:"language-bash",children:"pip install openai"})})}),"\n",(0,r.jsx)(n.h3,{id:"install-lm-studio",children:"Install LM Studio"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"https://lmstudio.ai/download",children:"Download"})," and install the LM Studio desktop application to run open-source large language models locally."]}),"\n",(0,r.jsx)(n.p,{children:"Ensure the LM Studio remains open and running when you start a Clarifai Local Runner, as the runner relies on LM Studio\u2019s internal model runtime for successful execution."}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Note:"})," Currently, Clarifai Local Runners support running LLMs through LM Studio only on Apple devices (macOS)."]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"step-2-initialize-a-model",children:"Step 2: Initialize a Model"}),"\n",(0,r.jsxs)(n.p,{children:["Using the Clarifai CLI, you can download and set up any model available in the ",(0,r.jsx)(n.a,{href:"https://lmstudio.ai/models",children:"LM Studio Model Catalog"})," that supports the GGUF format."]}),"\n",(0,r.jsxs)(n.p,{children:["For example, the command below initializes the default model (",(0,r.jsx)(n.a,{href:"https://lmstudio.ai/models/liquid/lfm2-1.2b",children:"LiquidAI/LFM2-1.2B"}),") in your current directory:"]}),"\n",(0,r.jsx)(l.A,{groupId:"code",children:(0,r.jsx)(a.A,{value:"bash",label:"Bash",children:(0,r.jsx)(i.A,{className:"language-bash",children:"clarifai model init --toolkit lmstudio"})})}),"\n",(0,r.jsxs)(t,{children:[(0,r.jsx)("summary",{children:"Example Output"}),(0,r.jsx)(i.A,{className:"language-text",children:c})]}),"\n",(0,r.jsx)(n.p,{children:"Running this command creates a new model directory structure compatible with the Clarifai platform. You can further customize or optimize the model by modifying the generated files as needed."}),"\n",(0,r.jsxs)(n.admonition,{type:"tip",children:[(0,r.jsxs)(n.p,{children:["To initialize a specific LM Studio model that supports the GGUF format, use the ",(0,r.jsx)(n.code,{children:"--model-name"})," flag."]}),(0,r.jsx)(l.A,{groupId:"code",children:(0,r.jsx)(a.A,{value:"bash",label:"Bash",children:(0,r.jsx)(i.A,{className:"language-bash",children:"clarifai model init --toolkit lmstudio --model-name qwen/qwen3-4b-thinking-2507"})})})]}),"\n",(0,r.jsx)(n.p,{children:"The generated structure includes:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"\u251c\u2500\u2500 1/\n\u2502   \u2514\u2500\u2500 model.py\n\u251c\u2500\u2500 requirements.txt\n\u2514\u2500\u2500 config.yaml\n"})}),"\n",(0,r.jsx)(n.h3,{id:"modelpy",children:(0,r.jsx)(n.code,{children:"model.py"})}),"\n",(0,r.jsxs)(t,{children:[(0,r.jsx)("summary",{children:"Example: model.py"}),(0,r.jsx)(i.A,{className:"language-text",children:p})]}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/compute/upload/#prepare-modelpy",children:(0,r.jsx)(n.code,{children:"model.py"})})," file inside the ",(0,r.jsx)(n.code,{children:"1/"})," directory defines the model\u2019s logic \u2014 including how predictions are made and how inputs and outputs are handled."]}),"\n",(0,r.jsx)(n.h3,{id:"configyaml",children:(0,r.jsx)(n.code,{children:"config.yaml"})}),"\n",(0,r.jsxs)(t,{children:[(0,r.jsx)("summary",{children:"Example: config.yaml"}),(0,r.jsx)(i.A,{className:"language-text",children:m})]}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.code,{children:"config.yaml"})," file defines key configuration details, such as compute resource requirements and toolkit metadata."]}),"\n",(0,r.jsxs)(n.p,{children:["In the ",(0,r.jsx)(n.code,{children:"model"})," section, specify a unique model ID (any name of your choice) and your Clarifai user ID and app ID. These parameters determine where the model will be deployed on the Clarifai platform."]}),"\n",(0,r.jsx)(n.h3,{id:"requirementstxt",children:(0,r.jsx)(n.code,{children:"requirements.txt"})}),"\n",(0,r.jsxs)(t,{children:[(0,r.jsx)("summary",{children:"Example: requirements.txt"}),(0,r.jsx)(i.A,{className:"language-text",children:f})]}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.code,{children:"requirements.txt"})," file lists the Python dependencies your model needs. If you haven\u2019t installed them yet, run the following command to install the dependencies:"]}),"\n",(0,r.jsx)(l.A,{groupId:"code",children:(0,r.jsx)(a.A,{value:"bash",label:"Bash",children:(0,r.jsx)(i.A,{className:"language-bash",children:"pip install -r requirements.txt"})})}),"\n",(0,r.jsx)(n.h2,{id:"step-3-log-in-to-clarifai",children:"Step 3: Log In to Clarifai"}),"\n",(0,r.jsxs)(n.p,{children:["Use the Clarifai CLI to log in to your account and create a configuration ",(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/compute/local-runners/#step-2-create-a-context-optional",children:(0,r.jsx)(n.strong,{children:"context"})})," that securely connects your local environment to the Clarifai platform."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"clarifai login\n"})}),"\n",(0,r.jsx)(n.p,{children:"You\u2019ll be prompted to enter the following details:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"User ID"})," \u2013 Your Clarifai User ID."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"PAT"})," \u2013 Your Clarifai Personal Access Token.\nIf you\u2019ve already set the ",(0,r.jsx)(n.code,{children:"CLARIFAI_PAT"})," environment variable, type ",(0,r.jsx)(n.code,{children:"ENVVAR"})," to use it automatically."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Context name"})," \u2013 Optionally, specify a custom name for this configuration context, or press ",(0,r.jsx)(n.strong,{children:"Enter"})," to use the default ",(0,r.jsx)(n.code,{children:'"default"'}),".\nContexts are useful when working with multiple environments or projects."]}),"\n"]}),"\n",(0,r.jsxs)(t,{children:[(0,r.jsx)("summary",{children:"Example Output"}),(0,r.jsx)(i.A,{className:"language-text",children:d})]}),"\n",(0,r.jsx)(n.h2,{id:"step-4-start-the-local-runner",children:"Step 4: Start the Local Runner"}),"\n",(0,r.jsxs)(n.p,{children:["Next, start your ",(0,r.jsx)(n.strong,{children:"Local Runner"}),", which connects to the LM Studio runtime to execute your model locally."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"clarifai model local-runner\n"})}),"\n",(0,r.jsx)(n.p,{children:"If configuration contexts or defaults are missing, the CLI will guide you through setting them up automatically."}),"\n",(0,r.jsxs)(n.p,{children:["This setup ensures that all necessary components \u2014 such as compute clusters, nodepools, and deployments \u2014 are properly defined in your configuration context.\nFor more details, see ",(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/compute/local-runners/#step-2-create-a-context-optional",children:"here"}),"."]}),"\n",(0,r.jsxs)(t,{children:[(0,r.jsx)("summary",{children:"Example Output"}),(0,r.jsx)(i.A,{className:"language-text",children:u})]}),"\n",(0,r.jsx)(n.h2,{id:"step-5-test-your-runner",children:"Step 5: Test Your Runner"}),"\n",(0,r.jsxs)(n.p,{children:["After the Local Runner starts, you can use it to perform ",(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/compute/inference/clarifai/api",children:"inference"})," with your LM Studio\u2013based model."]}),"\n",(0,r.jsx)(n.p,{children:"You can run a snippet in a separate terminal, within the same directory, to confirm that your model is running and responding as expected."}),"\n",(0,r.jsx)(n.p,{children:"Here\u2019s an example snippet:"}),"\n",(0,r.jsx)(l.A,{groupId:"code",children:(0,r.jsx)(a.A,{value:"python",label:"Python (OpenAI)",children:(0,r.jsx)(i.A,{className:"language-python",children:h})})})]})}function _(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(I,{...e})}):I(e)}}}]);
"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[7048],{14795:(e,n,o)=>{o.d(n,{A:()=>j});o(96540);var t=o(18215),r=o(26972),i=o(28774),s=o(53465),a=o(16654),c=o(21312),l=o(51107);const d={cardContainer:"cardContainer_fWXF",cardTitle:"cardTitle_rnsV",cardDescription:"cardDescription_PWke"};var u=o(74848);function p({className:e,href:n,children:o}){return(0,u.jsx)(i.A,{href:n,className:(0,t.A)("card padding--lg",d.cardContainer,e),children:o})}function m({className:e,href:n,icon:o,title:r,description:i}){return(0,u.jsxs)(p,{href:n,className:e,children:[(0,u.jsxs)(l.A,{as:"h2",className:(0,t.A)("text--truncate",d.cardTitle),title:r,children:[o," ",r]}),i&&(0,u.jsx)("p",{className:(0,t.A)("text--truncate",d.cardDescription),title:i,children:i})]})}function h({item:e}){const n=(0,r.Nr)(e),o=function(){const{selectMessage:e}=(0,s.W)();return n=>e(n,(0,c.T)({message:"1 item|{count} items",id:"theme.docs.DocCard.categoryDescription.plurals",description:"The default description for a category card in the generated index about how many items this category includes"},{count:n}))}();return n?(0,u.jsx)(m,{className:e.className,href:n,icon:"\ud83d\uddc3\ufe0f",title:e.label,description:e.description??o(e.items.length)}):null}function f({item:e}){const n=(0,a.A)(e.href)?"\ud83d\udcc4\ufe0f":"\ud83d\udd17",o=(0,r.cC)(e.docId??void 0);return(0,u.jsx)(m,{className:e.className,href:e.href,icon:n,title:e.label,description:e.description??o?.description})}function g({item:e}){switch(e.type){case"link":return(0,u.jsx)(f,{item:e});case"category":return(0,u.jsx)(h,{item:e});default:throw new Error(`unknown item type ${JSON.stringify(e)}`)}}const y={docCardListItem:"docCardListItem_W1sv"};function x({className:e}){const n=(0,r.a4)();return(0,u.jsx)(j,{items:n,className:e})}function w({item:e}){return(0,u.jsx)("article",{className:(0,t.A)(y.docCardListItem,"col col--6"),children:(0,u.jsx)(g,{item:e})})}function j(e){const{items:n,className:o}=e;if(!n)return(0,u.jsx)(x,{...e});const i=(0,r.d1)(n);return(0,u.jsx)("section",{className:(0,t.A)("row",o),children:i.map((e,n)=>(0,u.jsx)(w,{item:e},n))})}},28453:(e,n,o)=>{o.d(n,{R:()=>s,x:()=>a});var t=o(96540);const r={},i=t.createContext(r);function s(e){const n=t.useContext(i);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),t.createElement(i.Provider,{value:n},e.children)}},53465:(e,n,o)=>{o.d(n,{W:()=>l});var t=o(96540),r=o(44586);const i=["zero","one","two","few","many","other"];function s(e){return i.filter(n=>e.includes(n))}const a={locale:"en",pluralForms:s(["one","other"]),select:e=>1===e?"one":"other"};function c(){const{i18n:{currentLocale:e}}=(0,r.A)();return(0,t.useMemo)(()=>{try{return function(e){const n=new Intl.PluralRules(e);return{locale:e,pluralForms:s(n.resolvedOptions().pluralCategories),select:e=>n.select(e)}}(e)}catch(n){return console.error(`Failed to use Intl.PluralRules for locale "${e}".\nDocusaurus will fallback to the default (English) implementation.\nError: ${n.message}\n`),a}},[e])}function l(){const e=c();return{selectMessage:(n,o)=>function(e,n,o){const t=e.split("|");if(1===t.length)return t[0];t.length>o.pluralForms.length&&console.error(`For locale=${o.locale}, a maximum of ${o.pluralForms.length} plural forms are expected (${o.pluralForms.join(",")}), but the message contains ${t.length}: ${e}`);const r=o.select(n),i=o.pluralForms.indexOf(r);return t[Math.min(i,t.length-1)]}(o,n,e)}}},60436:(e,n,o)=>{o.d(n,{A:()=>t});const t=o.p+"assets/images/intro-2-db3dc800522f04a3edfbe91b6b06a415.png"},82895:(e,n,o)=>{o.d(n,{A:()=>t});const t=o.p+"assets/images/intro-1-648018deea56189cde06b533be6f72f2.png"},95068:(e,n,o)=>{o.d(n,{$S:()=>t});o(44586);function t(...e){return o(48295).$S(...e)}},98401:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>d,contentTitle:()=>l,default:()=>m,frontMatter:()=>c,metadata:()=>t,toc:()=>u});const t=JSON.parse('{"id":"compute/overview","title":"Compute Orchestration","description":"Orchestrate your AI workloads better, avoid vendor lock-in, and use compute spend efficiently","source":"@site/docs/compute/overview.md","sourceDirName":"compute","slug":"/compute/overview","permalink":"/compute/overview","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"description":"Orchestrate your AI workloads better, avoid vendor lock-in, and use compute spend efficiently","sidebar_position":1,"toc_max_heading_level":3},"sidebar":"tutorialSidebar","previous":{"title":"Build and Upload a Model","permalink":"/getting-started/upload-model"},"next":{"title":"Deployments","permalink":"/compute/deployments/"}}');var r=o(74848),i=o(28453),s=o(14795),a=o(95068);const c={description:"Orchestrate your AI workloads better, avoid vendor lock-in, and use compute spend efficiently",sidebar_position:1,toc_max_heading_level:3},l="Compute Orchestration",d={},u=[{value:"Deployment Options",id:"deployment-options",level:2},{value:"Compute Clusters and Nodepools",id:"compute-clusters-and-nodepools",level:2},{value:"Benefits of Compute Orchestration",id:"benefits-of-compute-orchestration",level:2},{value:"1. Performance and Deployment Flexibility",id:"1-performance-and-deployment-flexibility",level:3},{value:"2. Enhanced Security",id:"2-enhanced-security",level:3},{value:"3. Use Compute Cost-Efficiently and Abstract Away Complexity",id:"3-use-compute-cost-efficiently-and-abstract-away-complexity",level:3},{value:"4. New Inference Capabilities and Developer Experience Improvements",id:"4-new-inference-capabilities-and-developer-experience-improvements",level:3}];function p(e){const n={a:"a",admonition:"admonition",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",p:"p",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"compute-orchestration",children:"Compute Orchestration"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Orchestrate your AI workloads better, avoid vendor lock-in, and use compute spend efficiently"})}),"\n",(0,r.jsx)("hr",{}),"\n",(0,r.jsx)("div",{style:{position:"relative",width:"100%",overflow:"hidden","padding-top":"56.25%"},children:(0,r.jsx)("iframe",{width:"900",height:"500",style:{position:"absolute",top:"0",left:"0",bottom:"0",right:"0",width:"100%",height:"100%"},src:"https://www.youtube.com/embed/BZYanHsxkSo",title:"Introducing Compute Orchestration - Any AI, On Any Compute!",frameborder:"0",allow:"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture",allowfullscreen:!0})}),"\n",(0,r.jsx)("br",{}),"\n",(0,r.jsx)(n.p,{children:"Clarifai\u2019s Compute Orchestration provides efficient capabilities for you to deploy any model on any compute infrastructure, at any scale."}),"\n",(0,r.jsx)(n.p,{children:"These new platform capabilities bring the convenience of serverless autoscaling to any environment, regardless of deployment location or hardware, and dynamically scale resources to meet workload demands."}),"\n",(0,r.jsxs)(n.p,{children:["With Compute Orchestration, we are providing users with the ability to organize and manage (",(0,r.jsx)(n.em,{children:"orchestrate"}),") the compute resources necessary for running their models and workflows."]}),"\n",(0,r.jsx)(n.p,{children:"These capabilities enable our enterprise customers to deploy production models with enhanced control, performance, and scalability \u2014 while addressing specific problems around compute costs, latency, and control over hosted models."}),"\n",(0,r.jsx)(n.p,{children:"Clarifai handles the containerization, model packing, time slicing, and other performance optimizations on your behalf."}),"\n",(0,r.jsx)(n.admonition,{title:"Quick Start",type:"tip",children:(0,r.jsxs)(n.p,{children:["Learn how to get started quickly with Compute Orchestration ",(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/getting-started/set-up-compute",children:"here"}),"."]})}),"\n",(0,r.jsx)(n.h2,{id:"deployment-options",children:"Deployment Options"}),"\n",(0,r.jsx)(n.p,{children:"Compute Orchestration allows us to provide multiple deployment options \u2014 all of which can be customized with your preferred settings for autoscaling, cold start, and more, ensuring maximum cost efficiency and performance."}),"\n",(0,r.jsx)(n.p,{children:"These are the deployment options we provide:"}),"\n",(0,r.jsx)(n.admonition,{title:"Shared SaaS (Serverless)",type:"note",children:(0,r.jsxs)(n.p,{children:["If you\u2019re not using Compute Orchestration for inference with models uploaded and owned by Clarifai, your requests will default to the Shared SaaS (Serverless) deployment. This serverless environment eliminates the need to manage infrastructure, allowing you to make predictions effortlessly \u2014 without configuring or maintaining any compute resources. Learn more ",(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/compute/models/inference/",children:"here"}),"."]})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Dedicated SaaS"})," \u2014 Provides access to Clarifai\u2011managed, isolated nodes with customizable configurations. For example, you can launch a dedicated deployment in AWS US\u2011East, with plans to expand to other cloud providers and hardware options in the future."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Self-Managed VPC (Virtual Private Cloud)"})," \u2014 Connect your own cloud provider\u2019s VPC to Clarifai, allowing Clarifai to run and orchestrate deployments within your environment. This approach lets you maintain full control of your infrastructure while leveraging existing cloud resources and spend commitments."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Self-Managed On-Premises"})," \u2014 Connect your own on\u2011premises or bare\u2011metal infrastructure to Clarifai, allowing you to utilize existing compute resources. Clarifai then orchestrates model deployments within your environment, making the most of your infrastructure investments."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Multi-Site Deployment"})," \u2014 Enables deployments across multiple self\u2011managed compute environments, with support for a mix of on\u2011premises, cloud, or edge locations \u2014 and a roadmap for future multi\u2011cloud or multi\u2011region SaaS options."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Full Platform Deployment"})," \u2014 Designed for organizations with stringent security and compliance needs, this option allows you to run both the Clarifai control and compute planes within your chosen cloud, on\u2011premises, or air\u2011gapped infrastructure, ensuring complete isolation and control."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:" ",src:o(82895).A+"",width:"4800",height:"2700"})}),"\n",(0,r.jsx)(n.h2,{id:"compute-clusters-and-nodepools",children:"Compute Clusters and Nodepools"}),"\n",(0,r.jsxs)(n.p,{children:["We use ",(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/portal-guide/compute-orchestration/set-up-compute",children:"clusters and nodepools"})," to organize and manage the compute resources required for the Compute Orchestration capabilities."]}),"\n",(0,r.jsx)(n.admonition,{title:"Cluster",type:"info",children:(0,r.jsx)(n.p,{children:"A compute cluster in Clarifai acts as the overarching computational environment where models are executed, whether for training or inference."})}),"\n",(0,r.jsx)(n.admonition,{title:"nodepool",type:"info",children:(0,r.jsx)(n.p,{children:"A nodepool refers to a set of dedicated nodes (virtual machine instances) within a cluster that share similar configurations and resources, such as CPU or GPU type, memory size, and other performance parameters for running your models."})}),"\n",(0,r.jsx)(n.p,{children:"Cluster configuration lets you specify where and how your models are run, ensuring better performance, lower latency, and adherence to regional regulations. You can specify a cloud provider, such as AWS, that will provide the underlying compute infrastructure for hosting and running your models. You can also specify the geographic location of the data center where the compute resources will be hosted."}),"\n",(0,r.jsx)(n.p,{children:"Nodepools are an important part of how compute resources are operated within a cluster. They provide flexibility in choosing the type of instances used to run your machine learning models and workflows and help determine how resources are scaled to meet demand."}),"\n",(0,r.jsx)(n.p,{children:"Nodepools specify the accelerator and instance that will run your models and other workloads. Accelerators are specialized hardware resources, such as GPUs or dedicated ML chips used for computation."}),"\n",(0,r.jsx)(n.p,{children:"Each nodepool can run containers or workloads, and you can have multiple nodepools within a single cluster to support different types of workloads or performance requirements. These nodes execute tasks like model training, inference, and workflow orchestration within a compute cluster."}),"\n",(0,r.jsx)(n.p,{children:"With compute orchestration, you can ensure these nodepools are properly scaled up or down depending on the workload's size, complexities, and costs."}),"\n",(0,r.jsx)(n.h2,{id:"benefits-of-compute-orchestration",children:"Benefits of Compute Orchestration"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:" ",src:o(60436).A+"",width:"1918",height:"1032"})}),"\n",(0,r.jsx)(n.h3,{id:"1-performance-and-deployment-flexibility",children:"1. Performance and Deployment Flexibility"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["It provides access to a wide range of ",(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/portal-guide/compute-orchestration/cloud-instances",children:"accelerator options"})," tailored to your use case. You can configure multiple compute clusters each tailored to your AI development stage, performance requirements, and budget. You can also run affordable proof of concepts or compute-heavy LLMs or LVMs in production all from a single product."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["It offers flexibility to ",(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/portal-guide/compute-orchestration/deploy-model",children:"make deployments"})," in any cloud service provider, on-premises, air-gapped, or Kubernetes-supported environment. Or, you can make deployments in Clarifai\u2019s compute to avoid having to worry about managing infrastructure.\nThis allows users to leverage their hardware of choice without being locked into a single vendor."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"You can customize auto-scaling settings to prevent cold-start issues and handle traffic swings; and scale down to zero for cost efficiency.  The ability to scale from zero to infinity ensures both flexibility and cost management."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["We ensure efficient resource usage and cost savings through ",(0,r.jsx)(n.a,{href:"https://www.clarifai.com/blog/gpu-fractioning-explained-how-to-run-multiple-ai-workloads-on-a-single-gpu",children:"GPU fractioning"})," (running multiple models per GPU), time slicing, and other optimizations."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"2-enhanced-security",children:"2. Enhanced Security"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Users can run compute planes within their own cloud service provider or on-premise environments and securely connect to Clarifai\u2019s control plane, while only having to open outbound ports for traffic. This reduces networking complexities and security risks compared to opening inbound access or configuring cloud Identity and Access Management (IAM) access roles within your VPC."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Nodepool-based compute allows users to keep their resources isolated and provides precise control over scaling models and nodes. This allows users to specify where models are executed, addressing compliance and security needs for regulated industries."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Clarifai offers fine-grained access control across apps, teams, users, and compute resources."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Users can group CPU and GPU types into dedicated scaling nodepools, enabling them to handle diverse workloads or team-specific requirements while enhancing security and resource management."}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"3-use-compute-cost-efficiently-and-abstract-away-complexity",children:"3. Use Compute Cost-Efficiently and Abstract Away Complexity"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"An intuitive control plane enables users to efficiently govern access to AI resources, monitor performance, and manage costs. Clarifai\u2019s expertly designed platform takes care of dependencies and optimizations, offering features like model packing, streamlined dependency management, and customizable autoscaling options \u2014 including scale-to-zero for both model replicas and compute nodes."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"The advanced optimizations deliver exceptional efficiency, with model packing reducing compute usage by up to 3.7x and enabling support for over 1.6 million inputs per second with an impressive 99.9997% reliability. Depending on the chosen configuration, customers can achieve cost savings of at least 60%, and in some cases, up to 90%."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Organizations with pre-committed cloud spend or compute contracts with major cloud service providers, like AWS, Azure, or GCP, or existing GPU and hardware investments, can efficiently leverage their compute using Clarifai Compute Orchestration."}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"4-new-inference-capabilities-and-developer-experience-improvements",children:"4. New Inference Capabilities and Developer Experience Improvements"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Resourceful features such as inference streaming improve time-to-first-token for LLM generations."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Faster cold starts and optimized frameworks improve performance for critical workloads."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Continuous batching is available to reduce costs by processing multiple inference requests in batches."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Clarifai containerizes your desired models into Docker images, ensuring model package requirements are encapsulated in a portable environment and dependencies are handled automatically."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Low-latency deployment minimizes gRPC hops, speeding up communication."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["New model types are easily supported with a unified protobuf format, and local inference runners allow users to ",(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/compute/local-runners",children:"test models"})," before deploying to the cloud."]}),"\n"]}),"\n"]}),"\n","\n",(0,r.jsx)(s.A,{items:(0,a.$S)().items})]})}function m(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(p,{...e})}):p(e)}}}]);
"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[1565],{11470:(e,n,l)=>{l.d(n,{A:()=>v});var a=l(96540),r=l(18215),t=l(23104),o=l(56347),i=l(205),s=l(57485),c=l(31682),d=l(70679);function u(e){return a.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,a.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function h(e){const{values:n,children:l}=e;return(0,a.useMemo)(()=>{const e=n??function(e){return u(e).map(({props:{value:e,label:n,attributes:l,default:a}})=>({value:e,label:n,attributes:l,default:a}))}(l);return function(e){const n=(0,c.XI)(e,(e,n)=>e.value===n.value);if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[n,l])}function p({value:e,tabValues:n}){return n.some(n=>n.value===e)}function m({queryString:e=!1,groupId:n}){const l=(0,o.W6)(),r=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,s.aZ)(r),(0,a.useCallback)(e=>{if(!r)return;const n=new URLSearchParams(l.location.search);n.set(r,e),l.replace({...l.location,search:n.toString()})},[r,l])]}function f(e){const{defaultValue:n,queryString:l=!1,groupId:r}=e,t=h(e),[o,s]=(0,a.useState)(()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!p({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const l=n.find(e=>e.default)??n[0];if(!l)throw new Error("Unexpected error: 0 tabValues");return l.value}({defaultValue:n,tabValues:t})),[c,u]=m({queryString:l,groupId:r}),[f,g]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[l,r]=(0,d.Dv)(n);return[l,(0,a.useCallback)(e=>{n&&r.set(e)},[n,r])]}({groupId:r}),x=(()=>{const e=c??f;return p({value:e,tabValues:t})?e:null})();(0,i.A)(()=>{x&&s(x)},[x]);return{selectedValue:o,selectValue:(0,a.useCallback)(e=>{if(!p({value:e,tabValues:t}))throw new Error(`Can't select invalid tab value=${e}`);s(e),u(e),g(e)},[u,g,t]),tabValues:t}}var g=l(92303);const x={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var b=l(74848);function y({className:e,block:n,selectedValue:l,selectValue:a,tabValues:o}){const i=[],{blockElementScrollPositionUntilNextRender:s}=(0,t.a_)(),c=e=>{const n=e.currentTarget,r=i.indexOf(n),t=o[r].value;t!==l&&(s(n),a(t))},d=e=>{let n=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const l=i.indexOf(e.currentTarget)+1;n=i[l]??i[0];break}case"ArrowLeft":{const l=i.indexOf(e.currentTarget)-1;n=i[l]??i[i.length-1];break}}n?.focus()};return(0,b.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,r.A)("tabs",{"tabs--block":n},e),children:o.map(({value:e,label:n,attributes:a})=>(0,b.jsx)("li",{role:"tab",tabIndex:l===e?0:-1,"aria-selected":l===e,ref:e=>{i.push(e)},onKeyDown:d,onClick:c,...a,className:(0,r.A)("tabs__item",x.tabItem,a?.className,{"tabs__item--active":l===e}),children:n??e},e))})}function j({lazy:e,children:n,selectedValue:l}){const t=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=t.find(e=>e.props.value===l);return e?(0,a.cloneElement)(e,{className:(0,r.A)("margin-top--md",e.props.className)}):null}return(0,b.jsx)("div",{className:"margin-top--md",children:t.map((e,n)=>(0,a.cloneElement)(e,{key:n,hidden:e.props.value!==l}))})}function I(e){const n=f(e);return(0,b.jsxs)("div",{className:(0,r.A)("tabs-container",x.tabList),children:[(0,b.jsx)(y,{...n,...e}),(0,b.jsx)(j,{...n,...e})]})}function v(e){const n=(0,g.A)();return(0,b.jsx)(I,{...e,children:u(e.children)},String(n))}},19365:(e,n,l)=>{l.d(n,{A:()=>o});l(96540);var a=l(18215);const r={tabItem:"tabItem_Ymn6"};var t=l(74848);function o({children:e,hidden:n,className:l}){return(0,t.jsx)("div",{role:"tabpanel",className:(0,a.A)(r.tabItem,l),hidden:n,children:e})}},67753:(e,n,l)=>{l.r(n),l.d(n,{assets:()=>p,contentTitle:()=>h,default:()=>g,frontMatter:()=>u,metadata:()=>a,toc:()=>m});const a=JSON.parse('{"id":"compute/local-runners/ollama","title":"Ollama","description":"Run Ollama models locally and make them available via a public API","source":"@site/docs/compute/local-runners/ollama.md","sourceDirName":"compute/local-runners","slug":"/compute/local-runners/ollama","permalink":"/compute/local-runners/ollama","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"description":"Run Ollama models locally and make them available via a public API","sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Local Runners","permalink":"/compute/local-runners/"},"next":{"title":"Hugging Face","permalink":"/compute/local-runners/hf"}}');var r=l(74848),t=l(28453),o=l(11470),i=l(19365),s=l(73748);const c="clarifai model init --toolkit ollama\n[INFO] 15:58:15.587351 Parsed GitHub repository: owner=Clarifai, repo=runners-examples, branch=ollama, folder_path= |  thread=8800297152 \n[INFO] 15:58:16.827976 Files to be downloaded are:\n1. 1/model.py\n2. config.yaml\n3. requirements.txt |  thread=8800297152 \nPress Enter to continue... \n[INFO] 15:58:24.007602 Initializing model from GitHub repository: https://github.com/Clarifai/runners-examples |  thread=8800297152 \n[INFO] 15:58:31.263139 Successfully cloned repository from https://github.com/Clarifai/runners-examples (branch: ollama) |  thread=8800297152 \n[INFO] 15:58:31.270469 Model initialization complete with GitHub repository |  thread=8800297152 \n[INFO] 15:58:31.270527 Next steps: |  thread=8800297152 \n[INFO] 15:58:31.270560 1. Review the model configuration |  thread=8800297152 \n[INFO] 15:58:31.270584 2. Install any required dependencies manually |  thread=8800297152 \n[INFO] 15:58:31.270608 3. Test the model locally using 'clarifai model local-test' |  thread=8800297152 \n",d='clarifai model local-runner\n[INFO] 16:01:28.904230 > Checking local runner requirements... |  thread=8800297152 \n[INFO] 16:01:28.928129 Checking 2 dependencies... |  thread=8800297152 \n[INFO] 16:01:28.928672 \u2705 All 2 dependencies are installed! |  thread=8800297152 \n[INFO] 16:01:28.928886 Verifying Ollama installation... |  thread=8800297152 \n[INFO] 16:01:29.004234 > Verifying local runner setup... |  thread=8800297152 \n[INFO] 16:01:29.004427 Current context: default |  thread=8800297152 \n[INFO] 16:01:29.004463 Current user_id: alfrick |  thread=8800297152 \n[INFO] 16:01:29.004490 Current PAT: d6570**** |  thread=8800297152 \n[INFO] 16:01:29.005945 Current compute_cluster_id: local-runner-compute-cluster |  thread=8800297152 \n[WARNING] 16:01:35.936440 Failed to get compute cluster with ID \'local-runner-compute-cluster\':\ncode: CONN_DOES_NOT_EXIST\ndescription: "Resource does not exist"\ndetails: "ComputeCluster with ID \\\'local-runner-compute-cluster\\\' not found. Check your request fields."\nreq_id: "sdk-python-11.8.2-75ca9226003a4b34a770885b119d5814"\n |  thread=8800297152 \nCompute cluster not found. Do you want to create a new compute cluster alfrick/local-runner-compute-cluster? (y/n): y\n[INFO] 16:01:58.382096 Compute Cluster with ID \'local-runner-compute-cluster\' is created:\ncode: SUCCESS\ndescription: "Ok"\nreq_id: "sdk-python-11.8.2-a0474b8ba93c4e069804500a188694db"\n |  thread=8800297152 \n[INFO] 16:01:58.391571 Current nodepool_id: local-runner-nodepool |  thread=8800297152 \n[WARNING] 16:02:00.633687 Failed to get nodepool with ID \'local-runner-nodepool\':\ncode: CONN_DOES_NOT_EXIST\ndescription: "Resource does not exist"\ndetails: "Nodepool not found. Check your request fields."\nreq_id: "sdk-python-11.8.2-62149d46e7104d35bcb2a36546710329"\n |  thread=8800297152 \nNodepool not found. Do you want to create a new nodepool alfrick/local-runner-compute-cluster/local-runner-nodepool? (y/n): y\n[INFO] 16:02:03.909005 Nodepool with ID \'local-runner-nodepool\' is created:\ncode: SUCCESS\ndescription: "Ok"\nreq_id: "sdk-python-11.8.2-05836a431940495b94b9c3691f6c6d4d"\n |  thread=8800297152 \n[INFO] 16:02:03.921694 Current app_id: local-runner-app |  thread=8800297152 \n[INFO] 16:02:04.203774 Current model_id: local-runner-model |  thread=8800297152 \n[WARNING] 16:02:10.933734 Attempting to patch latest version: 9d38bb9398944de4bdef699835f17ec9 |  thread=8800297152 \n[INFO] 16:02:14.195999 Successfully patched version 9d38bb9398944de4bdef699835f17ec9 |  thread=8800297152 \n[INFO] 16:02:14.197924 Current model version 9d38bb9398944de4bdef699835f17ec9 |  thread=8800297152 \n[WARNING] 16:02:18.679567 Failed to get runner with ID \'f3c46913186449ba99dedd38123d47a3\':\ncode: CONN_DOES_NOT_EXIST\ndescription: "Resource does not exist"\ndetails: "Runner not found. Check your request fields."\nreq_id: "sdk-python-11.8.2-0b3b241c76ef429290c8c54a318f2f21"\n |  thread=8800297152 \n[INFO] 16:02:18.679913 Creating the local runner tying this \'alfrick/local-runner-app/models/local-runner-model\' model (version: 9d38bb9398944de4bdef699835f17ec9) to the \'alfrick/local-runner-compute-cluster/local-runner-nodepool\' nodepool. |  thread=8800297152 \n[INFO] 16:02:19.757117 Runner with ID \'2f84d7194ee8464fad485fd058663fe5\' is created:\ncode: SUCCESS\ndescription: "Ok"\nreq_id: "sdk-python-11.8.2-8ac525dc13ec47629213f0283e89c6a7"\n |  thread=8800297152 \n[INFO] 16:02:19.765198 Current runner_id: 2f84d7194ee8464fad485fd058663fe5 |  thread=8800297152 \n[WARNING] 16:02:20.331980 Failed to get deployment with ID local-runner-deployment:\ncode: CONN_DOES_NOT_EXIST\ndescription: "Resource does not exist"\ndetails: "Deployment with ID \\\'local-runner-deployment\\\' not found. Check your request fields."\nreq_id: "sdk-python-11.8.2-864f55e6bc554614894ee031cc30cdb9"\n |  thread=8800297152 \nDeployment not found. Do you want to create a new deployment alfrick/local-runner-compute-cluster/local-runner-nodepool/local-runner-deployment? (y/n): y\n[INFO] 16:02:25.016935 Deployment with ID \'local-runner-deployment\' is created:\ncode: SUCCESS\ndescription: "Ok"\nreq_id: "sdk-python-11.8.2-b6bf9aad5aa545f8a041113608fc9365"\n |  thread=8800297152 \n[INFO] 16:02:25.024579 Current deployment_id: local-runner-deployment |  thread=8800297152 \n[INFO] 16:02:25.027108 Current model section of config.yaml: {\'app_id\': \'local-dev-runner-app\', \'id\': \'local-dev-model\', \'model_type_id\': \'text-to-text\', \'user_id\': \'clarifai-user-id\'} |  thread=8800297152 \nDo you want to backup config.yaml to config.yaml.bk then update the config.yaml with the new model information? (y/n): y\n[INFO] 16:02:27.407724 Checking 2 dependencies... |  thread=8800297152 \n[INFO] 16:02:27.408555 \u2705 All 2 dependencies are installed! |  thread=8800297152 \n[INFO] 16:02:27.451117 Customizing Ollama model with provided parameters... |  thread=8800297152 \n[INFO] 16:02:27.451785 \u2705 Starting local runner... |  thread=8800297152 \n[INFO] 16:02:27.451852 No secrets path configured, running without secrets |  thread=8800297152 \n[INFO] 16:02:30.020253 Detected OpenAI chat completions for Clarifai model streaming - validating stream_options... |  thread=8800297152 \n[INFO] 16:02:30.027464 Starting Ollama server in the host: 127.0.0.1:23333 |  thread=8800297152 \n[INFO] 16:02:30.040882 Model llama3.2 pulled successfully. |  thread=8800297152 \n[INFO] 16:02:30.041191 Ollama server started successfully on 127.0.0.1:23333 |  thread=8800297152 \n[INFO] 16:02:30.096053 Ollama model loaded successfully: llama3.2 |  thread=8800297152 \n[INFO] 16:02:30.096133 ModelServer initialized successfully |  thread=8800297152 \nException in thread Thread-1 (serve_health):\nTraceback (most recent call last):\n  File "/opt/homebrew/Cellar/python@3.12/3.12.11/Frameworks/Python.framework/Versions/3.12/lib/python3.12/threading.py", line 1075, in _bootstrap_inner\n[INFO] 16:02:30.100802 \u2705 Your model is running locally and is ready for requests from the API...\n |  thread=8800297152 \n[INFO] 16:02:30.100873 > Code Snippet: To call your model via the API, use this code snippet:\n\nimport os\n\nfrom openai import OpenAI\n\nclient = OpenAI(\n    base_url="https://api.clarifai.com/v2/ext/openai/v1",\n    api_key=os.environ[\'CLARIFAI_PAT\'],\n)\n\nresponse = client.chat.completions.create(\n    model="https://clarifai.com/alfrick/local-runner-app/models/local-runner-model",\n    messages=[\n        {"role": "system", "content": "Talk like a pirate."},\n        {\n            "role": "user",\n            "content": "How do I check if a Python object is an instance of a class?",\n        },\n    ],\n    temperature=1.0,\n    stream=False,  # stream=True also works, just iterator over the response\n)\nprint(response)\n |  thread=8800297152 \n[INFO] 16:02:30.100944 > Playground:   To chat with your model, visit: https://clarifai.com/playground?model=local-runner-model__9d38bb9398944de4bdef699835f17ec9&user_id=alfrick&app_id=local-runner-app\n |  thread=8800297152 \n    self.run()\n[INFO] 16:02:30.101006 > API URL:      To call your model via the API, use this model URL: https://clarifai.com/alfrick/local-runner-app/models/local-runner-model\n |  thread=8800297152 \n  File "/opt/homebrew/Cellar/python@3.12/3.12.11/Frameworks/Python.framework/Versions/3.12/lib/python3.12/threading.py", line 1012, in run\n[INFO] 16:02:30.101070 Press CTRL+C to stop the runner.\n |  thread=8800297152 \n[INFO] 16:02:30.101117 Starting 32 threads... |  thread=8800297152 \n',u={description:"Run Ollama models locally and make them available via a public API",sidebar_position:1},h="Ollama",p={},m=[{value:"Step 1: Perform Prerequisites",id:"step-1-perform-prerequisites",level:2},{value:"Install Ollama",id:"install-ollama",level:3},{value:"Sign Up or Log In",id:"sign-up-or-log-in",level:3},{value:"Install the Clarifai CLI",id:"install-the-clarifai-cli",level:3},{value:"Install OpenAI Package",id:"install-openai-package",level:3},{value:"Step 2: Initialize a Model From Ollama",id:"step-2-initialize-a-model-from-ollama",level:2},{value:"Step 3: Log In to Clarifai",id:"step-3-log-in-to-clarifai",level:2},{value:"Step 4: Start Your Local Runner",id:"step-4-start-your-local-runner",level:2},{value:"Step 5: Test Your Runner",id:"step-5-test-your-runner",level:2},{value:"Additional Examples",id:"additional-examples",level:2}];function f(e){const n={a:"a",admonition:"admonition",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components},{Details:a}=n;return a||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"ollama",children:"Ollama"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Download and run Ollama models locally and make them available via a public API"})}),"\n",(0,r.jsx)("hr",{}),"\n",(0,r.jsx)(n.p,{children:"Ollama is an open-source tool that allows you to download, run, and manage large language models (LLMs) directly on your local machine."}),"\n",(0,r.jsx)(n.p,{children:"When combined with Clarifai\u2019s Local Runners, it enables you to run Ollama models on your machine, expose them securely via a public URL, and tap into Clarifai\u2019s powerful platform \u2014 all while keeping the speed, privacy, and control of local deployment."}),"\n","\n","\n",(0,r.jsx)(n.h2,{id:"step-1-perform-prerequisites",children:"Step 1: Perform Prerequisites"}),"\n",(0,r.jsx)(n.h3,{id:"install-ollama",children:"Install Ollama"}),"\n",(0,r.jsxs)(n.p,{children:["Go to the ",(0,r.jsx)(n.a,{href:"https://ollama.com/download",children:"Ollama website"})," and choose the appropriate installer for your system (macOS, Windows, or Linux)."]}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Note:"})," If you're using Windows, make sure to restart your machine after installing Ollama to ensure that the updated environment variables are properly applied."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"sign-up-or-log-in",children:"Sign Up or Log In"}),"\n",(0,r.jsxs)(n.p,{children:["Start by ",(0,r.jsx)(n.a,{href:"https://clarifai.com/login",children:"logging in"})," to your existing Clarifai account or ",(0,r.jsx)(n.a,{href:"https://clarifai.com/signup",children:"signing up"})," for a new one. Once logged in, you'll need the following credentials for setup:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"User ID"})," \u2013 In the collapsible left sidebar, select ",(0,r.jsx)(n.strong,{children:"Settings"})," and choose ",(0,r.jsx)(n.strong,{children:"Account"})," from the dropdown list. Then, locate your user ID."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Personal Access Token (PAT)"})," \u2013 From the same ",(0,r.jsx)(n.strong,{children:"Settings"})," option, choose ",(0,r.jsx)(n.strong,{children:"Secrets"})," to generate or copy your ",(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/control/authentication/pat",children:"PAT"}),". This token is used to authenticate your connection with the Clarifai platform."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["You can then set the PAT as an environment variable using ",(0,r.jsx)(n.code,{children:"CLARIFAI_PAT"}),", which is important when running inference with your models."]}),"\n",(0,r.jsxs)(o.A,{groupId:"code",children:[(0,r.jsx)(i.A,{value:"bash",label:"Unix-Like Systems",children:(0,r.jsx)(s.A,{className:"language-bash",children:"export CLARIFAI_PAT=YOUR_PERSONAL_ACCESS_TOKEN_HERE"})}),(0,r.jsx)(i.A,{value:"bash2",label:"Windows",children:(0,r.jsx)(s.A,{className:"language-bash",children:"set CLARIFAI_PAT=YOUR_PERSONAL_ACCESS_TOKEN_HERE"})})]}),"\n",(0,r.jsx)(n.h3,{id:"install-the-clarifai-cli",children:"Install the Clarifai CLI"}),"\n",(0,r.jsxs)(n.p,{children:["Install the latest version of the ",(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/sdk/cli",children:"Clarifai CLI"}),", which includes built-in support for Local Runners."]}),"\n",(0,r.jsx)(o.A,{groupId:"code",children:(0,r.jsx)(i.A,{value:"bash",label:"Bash",children:(0,r.jsx)(s.A,{className:"language-bash",children:"pip install --upgrade clarifai"})})}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Note:"})," You must have ",(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/resources/api-overview/python-sdk#python-requirements",children:"Python 3.11 or 3.12"})}),"  installed to use Local Runners."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"install-openai-package",children:"Install OpenAI Package"}),"\n",(0,r.jsxs)(n.p,{children:["Install the ",(0,r.jsx)(n.code,{children:"openai"})," package, which is required when performing inference with models using the ",(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/compute/inference/#predict-with-openai-compatible-format",children:"OpenAI-compatible format"}),"."]}),"\n",(0,r.jsx)(o.A,{groupId:"code",children:(0,r.jsx)(i.A,{value:"bash",label:"Python",children:(0,r.jsx)(s.A,{className:"language-bash",children:" pip install openai "})})}),"\n",(0,r.jsx)(n.h2,{id:"step-2-initialize-a-model-from-ollama",children:"Step 2: Initialize a Model From Ollama"}),"\n",(0,r.jsx)(n.p,{children:"You can use the Clarifai CLI to download and initialize any model available in the Ollama library directly into your local environment."}),"\n",(0,r.jsxs)(n.p,{children:["For example, here's how to initialize the ",(0,r.jsx)(n.a,{href:"https://ollama.com/library/llama3.2",children:(0,r.jsx)(n.code,{children:"llama3.2"})})," model in your current directory:"]}),"\n",(0,r.jsx)(o.A,{groupId:"code",children:(0,r.jsx)(i.A,{value:"bash",label:"CLI",children:(0,r.jsx)(s.A,{className:"language-bash",children:"clarifai model init --toolkit ollama"})})}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Note:"})," The above command will create a new model directory structure that is compatible with the Clarifai platform. You can customize or optimize the generated model by modifying the ",(0,r.jsx)(n.code,{children:"1/model.py"})," file as needed."]}),"\n"]}),"\n",(0,r.jsxs)(a,{children:[(0,r.jsx)("summary",{children:"Example Output"}),(0,r.jsx)(s.A,{className:"language-text",children:c})]}),"\n",(0,r.jsx)(n.p,{children:"You can customize model initialization from the Ollama library using the Clarifai CLI with the following options:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"--model-name"})," \u2013 Name of the Ollama model to use (default: ",(0,r.jsx)(n.code,{children:"llama3.2"}),"). This lets you specify any model from the Ollama library. Example: ",(0,r.jsx)(n.code,{children:"clarifai model init --toolkit ollama --model-name gpt-oss:20b"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"--port"})," \u2013 Port to run the model on (default: ",(0,r.jsx)(n.code,{children:"23333"}),")"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"--context-length"})," \u2013 Context window size for the model in tokens (default: ",(0,r.jsx)(n.code,{children:"8192"}),")"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"--verbose"})," \u2013 Enables detailed Ollama logs during execution. By default, logs are suppressed unless this flag is provided."]}),"\n"]}),"\n",(0,r.jsx)(n.admonition,{title:"tip",type:"note",children:(0,r.jsxs)(n.p,{children:["You can use Ollama commands such as ",(0,r.jsx)(n.code,{children:"ollama list"})," to list downloaded models and ",(0,r.jsx)(n.code,{children:"ollama rm"})," to remove a model. Run ",(0,r.jsx)(n.code,{children:"ollama --help"})," to see the full list of available commands."]})}),"\n",(0,r.jsx)(n.h2,{id:"step-3-log-in-to-clarifai",children:"Step 3: Log In to Clarifai"}),"\n",(0,r.jsxs)(n.p,{children:["Use the following command to log in to the Clarifai platform to create a configuration ",(0,r.jsx)(n.a,{href:"/compute/local-runners/#step-2-create-a-context-optional",children:"context"})," and establish a connection:"]}),"\n",(0,r.jsx)(o.A,{groupId:"code",children:(0,r.jsx)(i.A,{value:"bash",label:"CLI",children:(0,r.jsx)(s.A,{className:"language-bash",children:"clarifai login"})})}),"\n",(0,r.jsx)(n.p,{children:"After running the command, you'll be prompted to provide a few details for authentication:"}),"\n",(0,r.jsx)(o.A,{groupId:"code",children:(0,r.jsx)(i.A,{value:"bash",label:"CLI",children:(0,r.jsx)(s.A,{className:"language-bash",children:(0,r.jsx)(n.p,{children:'context name (default: "default"):\nuser id:\npersonal access token value (default: "ENVVAR" to get our env var rather than config):'})})})}),"\n",(0,r.jsx)(n.p,{children:"Here\u2019s what each field means:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Context name"})," \u2013 You can assign a custom name to this configuration context, or simply press Enter to use the default name, ",(0,r.jsx)(n.code,{children:'"default"'}),". This is useful if you manage multiple environments or configurations."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"User ID"})," \u2013 Enter your Clarifai user ID."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Personal Access Token (PAT)"})," \u2013 Paste your Clarifai PAT here. If you've already set the ",(0,r.jsx)(n.code,{children:"CLARIFAI_PAT"})," environment variable, you can just press Enter to use it automatically."]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"step-4-start-your-local-runner",children:"Step 4: Start Your Local Runner"}),"\n",(0,r.jsx)(n.p,{children:"Start a local runner using the following command:"}),"\n",(0,r.jsx)(o.A,{groupId:"code",children:(0,r.jsx)(i.A,{value:"bash",label:"CLI",children:(0,r.jsx)(s.A,{className:"language-bash",children:"clarifai model local-runner"})})}),"\n",(0,r.jsx)(n.p,{children:"If the necessary context configurations aren\u2019t detected, the CLI will guide you through creating them using default values."}),"\n",(0,r.jsxs)(n.p,{children:["This setup ensures all required components \u2014 such as compute clusters, nodepools, and deployments \u2014 are properly included in your configuration context, which are described ",(0,r.jsx)(n.a,{href:"/compute/local-runners/#step-2-create-a-context-optional",children:"here"}),". Simply review each prompt and confirm to proceed."]}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Note"}),": Use the ",(0,r.jsx)(n.code,{children:"--verbose"})," option to show detailed logs from the Ollama server, which is helpful for debugging: ",(0,r.jsx)(n.code,{children:"clarifai model local-runner --verbose"}),"."]}),"\n"]}),"\n",(0,r.jsxs)(a,{children:[(0,r.jsx)("summary",{children:"Example Output"}),(0,r.jsx)(s.A,{className:"language-text",children:d})]}),"\n",(0,r.jsx)(n.h2,{id:"step-5-test-your-runner",children:"Step 5: Test Your Runner"}),"\n",(0,r.jsx)(n.p,{children:"When the local runner starts, it displays a public URL where your model is hosted and provides a sample client code snippet for quick testing."}),"\n",(0,r.jsx)(n.p,{children:"Pulling a model from Ollama may take some time depending on your machine\u2019s resources, but once the download finishes, you can run the snippet in a separate terminal within the same directory to get the model\u2019s response."}),"\n",(0,r.jsx)(n.p,{children:"Below is an example snippet for running inference using the OpenAI-compatible format:"}),"\n",(0,r.jsx)(o.A,{groupId:"code",children:(0,r.jsx)(i.A,{value:"python",label:"Python",children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import os\n\nfrom openai import OpenAI\n\nclient = OpenAI(\n    base_url="https://api.clarifai.com/v2/ext/openai/v1",\n    api_key=os.environ[\'CLARIFAI_PAT\'],\n)\n\nresponse = client.chat.completions.create(\n    model="https://clarifai.com/alfrick/local-runner-app/models/local-runner-model",\n    messages=[\n        {"role": "system", "content": "Talk like a pirate."},\n        {\n            "role": "user",\n            "content": "How do I check if a Python object is an instance of a class?",\n        },\n    ],\n    temperature=1.0,\n    stream=False,  # stream=True also works, just iterator over the response\n)\nprint(response)\n'})})})}),"\n",(0,r.jsxs)(n.p,{children:["The terminal also shows a link to the ",(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/getting-started/quickstart-playground",children:"AI Playground"}),", which you can copy to interact with the model directly."]}),"\n",(0,r.jsxs)(n.p,{children:["Alternatively, while your runner is active in the terminal, you can open the ",(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.a,{href:"https://clarifai.com/compute/runners",children:"Runners"})})," dashboard on the Clarifai platform, locate your runner in the table, and select ",(0,r.jsx)(n.strong,{children:"Open in Playground"})," from the three-dot menu to start chatting with the model."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{src:l(72140).A+"",width:"4200",height:"1700"})}),"\n",(0,r.jsx)(n.p,{children:"When you're done, just close the terminal running the local runner to shut it down."}),"\n",(0,r.jsx)(n.h2,{id:"additional-examples",children:"Additional Examples"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/ollama/ollama-python/tree/main/examples",children:"More examples of calling Ollama models"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/compute/inference/clarifai/api",children:"Clarifai-specific inference examples with Ollama models"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/Clarifai/runners-examples/tree/main/local-runners/ollama-model-upload",children:"Example for running Ollama models locally with Clarifai\u2019s Local Runners"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://www.youtube.com/watch?v=TfS2p8LZYBE",children:"YouTube video on running OpenAI\u2019s open-source GPT-OSS-20B model locally with Ollama"})}),"\n"]})]})}function g(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(f,{...e})}):f(e)}},72140:(e,n,l)=>{l.d(n,{A:()=>a});const a=l.p+"assets/images/runners-dashboard-ollama-efa680670a0005e6eb41d84ad88b3f2d.png"}}]);
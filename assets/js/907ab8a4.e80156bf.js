"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[5149],{89337:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>g,contentTitle:()=>A,default:()=>I,frontMatter:()=>_,metadata:()=>f,toc:()=>E});var a=n(74848),i=n(28453),s=n(11470),o=n(19365),r=n(21432);const l='##############################################################################################################\n# In this section, we set the user authentication, user and app ID, model details, and input details.\n# Change these values to run your own example.\n##############################################################################################################\n\n# Your PAT (Personal Access Token) can be found in the Account\'s Security section\nPAT = "YOUR_PAT_HERE"\n# Specify the correct user_id/app_id pairings\n# Since you\'re making inferences outside your app\'s scope\nUSER_ID = "openai"\nAPP_ID = "chat-completion"\n# Change these to whatever model and inputs you want to use\nMODEL_ID = "openai-gpt-4-vision"\nMODEL_VERSION_ID = "266df29bc09843e0aee9b7bf723c03c2"\nRAW_TEXT = "Write a caption for the image"\n# To use a hosted text file, assign the URL variable\n# TEXT_FILE_URL = "https://samples.clarifai.com/negative_sentence_12.txt"\n# Or, to use a local text file, assign the location variable\n# TEXT_FILE_LOCATION = "YOUR_TEXT_FILE_LOCATION_HERE"\nIMAGE_URL = "https://samples.clarifai.com/metro-north.jpg"\n# To use a local file, assign the location variable\n# IMAGE_FILE_LOCATION = "YOUR_IMAGE_FILE_LOCATION_HERE"\n\n############################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n############################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\nfrom google.protobuf.struct_pb2 import Struct\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (("authorization", "Key " + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\n# To use a local text file, uncomment the following lines\n# with open(TEXT_FILE_LOCATION, "rb") as f:\n#    file_bytes = f.read()\n\n# To use a local image file, uncomment the following lines\n# with open(IMAGE_FILE_LOCATION, "rb") as f:\n#     file_bytes = f.read()\n\nparams = Struct()\nparams.update(\n    {\n        "temperature": 0.5,\n        "max_tokens": 2048,\n        "top_p": 0.95,\n        # "api_key": "ADD_THIRD_PARTY_KEY_HERE"\n    }\n)\n\npost_model_outputs_response = stub.PostModelOutputs(\n    service_pb2.PostModelOutputsRequest(\n        user_app_id=userDataObject,  # The userDataObject is created in the overview and is required when using a PAT\n        model_id=MODEL_ID,\n        version_id=MODEL_VERSION_ID,  # This is optional. Defaults to the latest model version\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    text=resources_pb2.Text(\n                        raw=RAW_TEXT\n                        # url=TEXT_FILE_URL\n                        # raw=file_bytes\n                    ),\n                    image=resources_pb2.Image(\n                        url=IMAGE_URL\n                        # base64=file_bytes\n                    ),\n                )\n            )\n        ],\n        model=resources_pb2.Model(\n            model_version=resources_pb2.ModelVersion(\n                output_info=resources_pb2.OutputInfo(params=params)\n            )\n        ),\n    ),\n    metadata=metadata,\n)\nif post_model_outputs_response.status.code != status_code_pb2.SUCCESS:\n    print(post_model_outputs_response.status)\n    raise Exception(f"Post model outputs failed, status: {post_model_outputs_response.status.description}")\n\n# Since we have one input, one output will exist here\noutput = post_model_outputs_response.outputs[0]\n\nprint(output.data.text.raw)\n',u='\x3c!--index.html file--\x3e\n\n<script>\n    /////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, and input details.\n    // Change these values to run your own example.\n    /////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n    // Your PAT (Personal Access Token) can be found in the Account\'s Security section\n    const PAT = "YOUR_PAT_HERE";\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    const USER_ID = "openai";\n    const APP_ID = "chat-completion";\n    // Change these to whatever model and inputs you want to use\n    const MODEL_ID = "openai-gpt-4-vision";\n    const MODEL_VERSION_ID = "266df29bc09843e0aee9b7bf723c03c2";\n    const RAW_TEXT = "Write a caption for the image";\n    // To use a hosted text file, assign the URL variable\n    // const TEXT_FILE_URL = "https://samples.clarifai.com/negative_sentence_12.txt";\n    const IMAGE_URL = "https://samples.clarifai.com/metro-north.jpg";\n    // To use image bytes, assign its variable   \n    // const IMAGE_BYTES_STRING = "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAUDBAQEAwUEBAQFBQUGBwwIBwcHBw8LCwkMEQ8SEhEPERETFhwXExQaFRERGCEYGh0dHx8fExciJCIeJBweHx7/2wBDAQUFBQcGBw4ICA4eFBEUHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh7/wAARCAAoACgDASIAAhEBAxEB/8QAGQAAAgMBAAAAAAAAAAAAAAAAAAYDBQcE/8QAMBAAAQMDAwMDAgQHAAAAAAAAAQIDBAAFEQYSIQcTMTJBURRhCBYikSNScXKhsdH/xAAZAQACAwEAAAAAAAAAAAAAAAAFBgIDBAf/xAAtEQABAwMBBgQHAQAAAAAAAAABAgMRAAQhMQUSE0FRYQaBocEUFiJCcrHR8P/aAAwDAQACEQMRAD8A3+RYY1unSYzCS0ttZUkAgktn0q5yT7jPyDUC4wdGwycH5U2Kt9ZQ7VI1qw5PkvQy3CSVPpf7aQjuKyFH25xzn3pHn3TVNy01Hl2hyy6YdkSpKsS9sl/6RlI3rRu3dxWd6spwnAGPIJTfl925fcLaoSDHXvyo6i9SlCQrU9wKln3OyWiaDN1RAbW3kKbSd7gPtwMkH/tTWy9afuy1iPfnXMAblITwkE4yf08cn3pSbYt1uts24XH6fUbiLAuY1MWyGkLEmUW0rcCRvUpQ5CtwKQCPgi4S1ZbDe4sd9NntDEe79m3uOBLTr0IR9jzodSMqUpTu9JJ8owD7UTT4ZCfv9PbP7860m+s+HBSrejWRuz2kAxoesGYxTW/Zlpkwo1vkuSly3UgKWQUhHJUvIHsAaKTemF8XE6sWmxyZkiaZrMh1jv8ArQNpUVqB8FW0njHqx4zRVVhsph1KlKk5xQ+7uHmikaSJrQerMByet2IwvtuTLa4xv2k7Rk84H9x/esHv92d01boenLXGcuiWrFIhLlpbcaQ2/JdK3VJCkAq2pAR7Zz7YxWudY9fxNIdQbNGkR5TyX4aisNNpUMFZAzkj4NK0jq9ZpbLr0PSlzkhrlZDaQlP3P8Q4/ap3F87bPucJEkx/hHv60b2TYXLrKN5sramYECSQRk9M6c6zmJ+eb5Hi22M7cnWGIQgFLbX0zSo4PDa1YBcTgDyMjJ/qbGPabH08SJt1Uzc9QqRliGg5QySPKvgc+TyfYDmmTUWpNYz7ctxoQdPQshCktupckDJUPUcJT6DwMq8YyaQ9VL0pCS8zapcq4SVOBZmPDO8/cnknlWcDBwn4NYnPjLkQ+qE9OtOVlYpeVHDCEkkkJyT+SuQzy5Y0ru6Ez511/Efa5s1fdkOtyVurIxgdlQAA9gOKKPwolU7remU5hCGYEgo38KUv9I/0TRTDYJCWQBSF4rIN/CRgAR0iTpVD1j1g/qDqJcJqlKcjB9bcda142MpOEJAzgeMnjyTSyze5KEuNRpDoDvC0oe4X9iAeaKKFK+oya6fbOqYbDTeEiAPKpHdS3gBLYc7RQkp3ApQog+cq8nwPJrljzxnPZbUfnugn/NFFRgEVch9xKsH0H8pg6e3x3T3UC1ajaZITGkJLoS4MKbOUrzz/ACKVRRRVzVwtoQmhG1NkWu0HuI+JI8u/Kv/Z";\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    const raw = JSON.stringify({\n        "inputs": [\n            {\n                "data": {\n                    "text": {\n                        "raw": RAW_TEXT\n                        // "url": TEXT_FILE_URL\n                    },\n                    "image": {\n                        "url": IMAGE_URL\n                        // "base64": IMAGE_BYTES_STRING\n                    }\n                }\n            }\n        ],\n        "model": {\n            "model_version": {\n                "output_info": {\n                    "params": {\n                        "temperature": 0.5,\n                        "max_tokens": 2048,\n                        "top_p": 0.95,\n                        // "api_key": "ADD_THIRD_PARTY_KEY_HERE"\n                    }\n                }\n            }\n        }\n    });\n\n    const requestOptions = {\n        method: "POST",\n        headers: {\n            "Accept": "application/json",\n            "Authorization": "Key " + PAT\n        },\n        body: raw\n    };\n\n    fetch(`https://api.clarifai.com/v2/users/${USER_ID}/apps/${APP_ID}/models/${MODEL_ID}/versions/${MODEL_VERSION_ID}/outputs`, requestOptions)\n        .then(response => response.text())\n        .then(result => console.log(result))\n        .catch(error => console.log(\'error\', error));\n\n<\/script>',c='//index.js file\n\n////////////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, and input details.\n// Change these values to run your own example.\n////////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\nconst PAT = "YOUR_PAT_HERE";\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\nconst USER_ID = "openai";\nconst APP_ID = "chat-completion";\n// Change these to whatever model and inputs you want to use\nconst MODEL_ID = "openai-gpt-4-vision";\nconst MODEL_VERSION_ID = "266df29bc09843e0aee9b7bf723c03c2";\nconst RAW_TEXT = "Write a caption for the image";\n// To use a hosted text file, assign the URL variable\n// const TEXT_FILE_URL = "https://samples.clarifai.com/negative_sentence_12.txt";\n// Or, to use a local text file, assign the location variable\n// TEXT_FILE_LOCATION = "YOUR_TEXT_FILE_LOCATION_HERE";\nconst IMAGE_URL = "https://samples.clarifai.com/metro-north.jpg";\n// To use a local file, assign the location variable;\n// const IMAGE_FILE_LOCATION = "YOUR_IMAGE_FILE_LOCATION_HERE";\n\n/////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n/////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\n// To use a local text file, uncomment the following lines\n// const fs = require("fs");\n// const fileBytes = fs.readFileSync(TEXT_FILE_LOCATION);\n\n// To use a local image file, uncomment the following lines\n// const fs = require("fs");\n// const imageBytes = fs.readFileSync(IMAGE_FILE_LOCATION);\n\nstub.PostModelOutputs(\n    {\n        user_app_id: {\n            "user_id": USER_ID,\n            "app_id": APP_ID\n        },\n        model_id: MODEL_ID,\n        version_id: MODEL_VERSION_ID,  // This is optional. Defaults to the latest model version\n        inputs: [\n            {\n                "data": {\n                    "text": {\n                        "raw": RAW_TEXT,\n                        // url: TEXT_FILE_URL,\n                        // raw: fileBytes\n                    },\n                    "image": {\n                        "url": IMAGE_URL,\n                        // base64: imageBytes                      \n                    }\n                }\n            }\n        ],\n        model: {\n            "model_version": {\n                "output_info": {\n                    "params": {\n                        "temperature": 0.5,\n                        "max_tokens": 2048,\n                        "top_p": 0.95\n                        // "api_key": "ADD_THIRD_PARTY_KEY_HERE"\n                    }\n                }\n            }\n        }\n    },\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            throw new Error("Post models failed, status: " + response.status.description);\n        }\n        // Since we have one input, one output will exist here.\n        const output = response.outputs[0];\n\n        console.log(output.data.text.raw);\n    }\n);\n',p='package com.clarifai.example;\n\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.status.StatusCode;\nimport com.google.protobuf.Struct;\nimport com.google.protobuf.Value;\n\nimport com.google.protobuf.ByteString;\nimport java.io.File;\nimport java.io.IOException;\nimport java.nio.file.Files;\n\npublic class ClarifaiExample {\n\n    /////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, and input details.\n    // Change these values to run your own example.\n    /////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n    // Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    static final String USER_ID = "openai";\n    static final String APP_ID = "chat-completion";\n    // Change these to whatever model and inputs you want to use\n    static final String MODEL_ID = "openai-gpt-4-vision";\n    static final String MODEL_VERSION_ID = "266df29bc09843e0aee9b7bf723c03c2";\n    static final String RAW_TEXT = "Write a caption for the image";\n    // To use a hosted text file, assign the URL variable\n    // static final String TEXT_FILE_URL = "https://samples.clarifai.com/negative_sentence_12.txt";\n    // Or, to use a local text file, assign the location variable\n    // static final String TEXT_FILE_LOCATION = "YOUR_TEXT_FILE_LOCATION_HERE";\n    static final String IMAGE_URL = "https://samples.clarifai.com/metro-north.jpg";\n    // Or, to use a local image file, assign the location variable\n    //static final String IMAGE_FILE_LOCATION = "YOUR_IMAGE_FILE_LOCATION_HERE";\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\t\n\n    public static void main(String[] args) throws IOException {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n                .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        Struct.Builder params = Struct.newBuilder()\n                .putFields("temperature", Value.newBuilder().setNumberValue(0.5).build())\n                .putFields("max_tokens", Value.newBuilder().setNumberValue(2048).build())\n                .putFields("top_p", Value.newBuilder().setNumberValue(0.95).build());\n               // .putFields("api_key", Value.newBuilder().setNumberValue("ADD_THIRD_PARTY_KEY_HERE").build());\n\n        MultiOutputResponse postModelOutputsResponse = stub.postModelOutputs(\n                PostModelOutputsRequest.newBuilder()\n                        .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n                        .setModelId(MODEL_ID)\n                        .setVersionId(MODEL_VERSION_ID) // This is optional. Defaults to the latest model version\n                        .addInputs(\n                                Input.newBuilder().setData(\n                                        Data.newBuilder().setText(\n                                                Text.newBuilder().setRaw(RAW_TEXT)\n                                        // Text.newBuilder().setUrl(TEXT_FILE_URL)\n                                        // Text.newBuilder().setRawBytes(ByteString.copyFrom(Files.readAllBytes(\n                                        // new File(TEXT_FILE_LOCATION).toPath()\n                                        // )))\n                                        )\n                                                .setImage(\n                                                        Image.newBuilder().setUrl(IMAGE_URL)\n                                                // Image.newBuilder().setBase64(ByteString.copyFrom(Files.readAllBytes(\n                                                // new File(IMAGE_FILE_LOCATION).toPath()\n                                                // )))\n                                                )\n                                )\n                        )\n                        .setModel(Model.newBuilder()\n                                .setModelVersion(ModelVersion.newBuilder()\n                                        .setOutputInfo(OutputInfo.newBuilder().setParams(params))\n                                )\n                        )\n                        .build()\n        );\n\n        if (postModelOutputsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("Post model outputs failed, status: " + postModelOutputsResponse.getStatus());\n        }\n\n        // Since we have one input, one output will exist here\n        Output output = postModelOutputsResponse.getOutputs(0);\n\n        System.out.println(output.getData().getText().getRaw());\n\n    }\n}\n',d='<?php\n\nrequire __DIR__ . "/vendor/autoload.php";\n\n///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, and input details.\n// Change these values to run your own example.\n///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\n$PAT = "YOUR_PAT_HERE";\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\n$USER_ID = "openai";\n$APP_ID = "chat-completion";\n// Change these to whatever model and inputs you want to use\n$MODEL_ID = "openai-gpt-4-vision";\n$MODEL_VERSION_ID = "266df29bc09843e0aee9b7bf723c03c2";\n$RAW_TEXT = "Write a caption for the image";\n// To use a hosted text file, assign the URL variable\n// $TEXT_FILE_URL = "https://samples.clarifai.com/negative_sentence_12.txt";\n// Or, to use a local text file, assign the location variable\n// $TEXT_FILE_LOCATION = "YOUR_TEXT_FILE_LOCATION_HERE";\n$IMAGE_URL = "https://samples.clarifai.com/metro-north.jpg";\n// To use a local file, assign the location variable\n// $IMAGE_FILE_LOCATION = "YOUR_IMAGE_FILE_LOCATION_HERE";\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Text;\nuse Clarifai\\Api\\Image;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\Model;\nuse Clarifai\\Api\\ModelVersion;\nuse Clarifai\\Api\\OutputInfo;\nuse Clarifai\\Api\\PostModelOutputsRequest;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\nuse Google\\Protobuf\\Struct;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ["Authorization" => ["Key " . $PAT]];\n\n$userDataObject = new UserAppIDSet([\n    "user_id" => $USER_ID,\n    "app_id" => $APP_ID,\n]);\n\n// Create Struct instance\n$params = new Struct();\n$params->temperature = 0.5;\n$params->max_tokens = 2048;\n$params->top_p = 0.95;\n// $params->api_key = "ADD_THIRD_PARTY_KEY_HERE";\n\n// To use a local text file, uncomment the following lines\n// $textData = file_get_contents($TEXT_FILE_LOCATION); // Get the text bytes data from the location\n\n// To use a local image file, uncomment the following lines\n// $imageData = file_get_contents($IMAGE_FILE_LOCATION);\n\n// Let\'s make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client->PostModelOutputs(\n    // The request object carries the request along with the request status and other metadata related to the request itself\n    new PostModelOutputsRequest([\n        "user_app_id" => $userDataObject,\n        "model_id" => $MODEL_ID,\n        "version_id" => $MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n        "inputs" => [\n            new Input([\n                // The Input object wraps the Data object in order to meet the API specification\n                "data" => new Data([\n                    // The Data object is constructed around the Text object. It offers a container that has additional text independent\n                    // metadata. In this particular use case, no other metadata is needed to be specified\n                    "text" => new Text([\n                        // In the Clarifai platform, a text is defined by a special Text object\n                        "raw" => $RAW_TEXT,\n                        // "url" => $TEXT_FILE_URL\n                        // "raw" => $textData\n                    ]),\n                    "image" => new Image([\n                        // In the Clarifai platform, an image is defined by a special Image object\n                        "url" => $IMAGE_URL,\n                        // "base64" => $imageData,\n                    ]),\n                ]),\n            ]),\n        ],\n        "model" => new Model([\n            "model_version" => new ModelVersion([\n                "output_info" => new OutputInfo(["params" => $params]),\n            ]),\n        ]),\n    ]),\n    $metadata\n)->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception("Error: {$status->details}");\n}\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure\n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    throw new Exception("Failure response: " . $response->getStatus()->getDescription() . " " . $response->getStatus()->getDetails() );\n}\n\n# Since we have one input, one output will exist here\necho $response->getOutputs()[0]->getData()->getText()->getRaw();\n\n?>',h='curl -X POST "https://api.clarifai.com/v2/users/openai/apps/chat-completion/models/openai-gpt-4-vision/versions/266df29bc09843e0aee9b7bf723c03c2/outputs" \\\n    -H "Authorization: Key YOUR_PAT_HERE" \\\n    -H "Content-Type: application/json" \\\n    -d \'{\n    "inputs": [\n        {\n            "data": {\n                "text": {\n                    "raw": "Write a caption for the image"\n                },\n                "image": {\n                    "url": "https://samples.clarifai.com/metro-north.jpg"\n                }\n            }\n        }\n    ],\n    "model": {\n        "model_version": {\n            "output_info": {\n                "params": {\n                    "temperature": 0.5,\n                    "max_tokens": 2048,\n                    "top_p": 0.95,\n                    "api_key": "ADD_THIRD_PARTY_KEY_HERE"\n                }\n            }\n        }\n    }\n}\'\n',m='"Early morning solitude: A lone traveler waits on a snowy platform as dawn breaks."',_={description:"Make multimodal-to-text predictions",sidebar_position:6},A="Multimodal-to-Text",f={id:"api-guide/predict/multimodal-to-text",title:"Multimodal-to-Text",description:"Make multimodal-to-text predictions",source:"@site/docs/api-guide/predict/multimodal-to-text.md",sourceDirName:"api-guide/predict",slug:"/api-guide/predict/multimodal-to-text",permalink:"/api-guide/predict/multimodal-to-text",draft:!1,unlisted:!1,editUrl:"https://github.com/Clarifai/docs/blob/main/docs/api-guide/predict/multimodal-to-text.md",tags:[],version:"current",sidebarPosition:6,frontMatter:{description:"Make multimodal-to-text predictions",sidebar_position:6},sidebar:"tutorialSidebar",previous:{title:"Audio",permalink:"/api-guide/predict/audio"},next:{title:"Embeddings",permalink:"/api-guide/predict/embeddings"}},g={},E=[];function T(e){const t={a:"a",admonition:"admonition",h1:"h1",p:"p",strong:"strong",...(0,i.R)(),...e.components},{Details:n}=t;return n||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.h1,{id:"multimodal-to-text",children:"Multimodal-to-Text"}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.strong,{children:"Make multimodal-to-text predictions"})}),"\n",(0,a.jsx)("hr",{}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"Input"}),": Text, images, etc"]}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"Output"}),": Text"]}),"\n",(0,a.jsx)(t.p,{children:'Multimodal-to-text models allow you to generate textual descriptions or responses from multimodal inputs. "Multimodal" refers to the integration of information from multiple modalities, such as text, images, and/or other types of data.'}),"\n",(0,a.jsx)(t.p,{children:"A multimodal-to-text model might take as input a combination of textual data and images and generate a descriptive text that captures the content of both modalities. It can comprehend and generate a human-like response that encompasses multiple types of information."}),"\n",(0,a.jsxs)(t.admonition,{title:"Vision Language Models (VLMs)",type:"warning",children:[(0,a.jsxs)(t.p,{children:["Vision Language Models (VLMs), also called Large Vision Language Models (LVLMs), integrate capabilities from both Large Language Models (LLMs) and vision models. These models, such as ",(0,a.jsx)(t.a,{href:"https://clarifai.com/qwen/qwen-VL/models/qwen-VL-Chat",children:"Qwen-VL-Chat"}),", are designed to understand both text and visual content, enabling you to perform tasks that require interpreting multimodal information."]}),(0,a.jsx)(t.p,{children:"You can use LVLMs to process and understand data from different modalities, such as text and images. They can generate coherent textual outputs for your natural language processing needs."})]}),"\n",(0,a.jsx)(t.admonition,{type:"info",children:(0,a.jsxs)(t.p,{children:["The initialization code used in the following examples is outlined in detail on the ",(0,a.jsx)(t.a,{href:"https://docs.clarifai.com/api-guide/api-overview/api-clients/#client-installation-instructions",children:"client installation page."})]})}),"\n","\n","\n","\n",(0,a.jsx)(t.admonition,{type:"note",children:(0,a.jsxs)(t.p,{children:["You can use ",(0,a.jsx)(t.a,{href:"https://docs.clarifai.com/api-guide/predict/llms#use-hyperparameters-to-customize-llms",children:"hyperparameters"})," to customize the behavior of the models. You can also utilize the ",(0,a.jsx)(t.a,{href:"https://docs.clarifai.com/api-guide/predict/text#use-third-party-api-keys",children:"API keys"})," from a third-party model provider as an option\u2014in addition to using the default Clarifai keys."]})}),"\n",(0,a.jsxs)(t.p,{children:["Below is an example of how you would make a multimodal-to-text prediction using the ",(0,a.jsx)(t.a,{href:"https://clarifai.com/openai/chat-completion/models/openai-gpt-4-vision",children:"GPT-4 Vision"})," model."]}),"\n",(0,a.jsxs)(s.A,{children:[(0,a.jsx)(o.A,{value:"python",label:"Python",children:(0,a.jsx)(r.A,{className:"language-python",children:l})}),(0,a.jsx)(o.A,{value:"js_rest",label:"JavaScript (REST)",children:(0,a.jsx)(r.A,{className:"language-javascript",children:u})}),(0,a.jsx)(o.A,{value:"nodejs",label:"NodeJS",children:(0,a.jsx)(r.A,{className:"language-javascript",children:c})}),(0,a.jsx)(o.A,{value:"java",label:"Java",children:(0,a.jsx)(r.A,{className:"language-java",children:p})}),(0,a.jsx)(o.A,{value:"php",label:"PHP",children:(0,a.jsx)(r.A,{className:"language-php",children:d})}),(0,a.jsx)(o.A,{value:"curl",label:"cURL",children:(0,a.jsx)(r.A,{className:"language-bash",children:h})})]}),"\n",(0,a.jsxs)(n,{children:[(0,a.jsx)("summary",{children:"Text Output Example"}),(0,a.jsx)(r.A,{className:"language-text",children:m})]})]})}function I(e={}){const{wrapper:t}={...(0,i.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(T,{...e})}):T(e)}},19365:(e,t,n)=>{n.d(t,{A:()=>o});n(96540);var a=n(18215);const i={tabItem:"tabItem_Ymn6"};var s=n(74848);function o(e){let{children:t,hidden:n,className:o}=e;return(0,s.jsx)("div",{role:"tabpanel",className:(0,a.A)(i.tabItem,o),hidden:n,children:t})}},11470:(e,t,n)=>{n.d(t,{A:()=>b});var a=n(96540),i=n(18215),s=n(23104),o=n(56347),r=n(205),l=n(57485),u=n(31682),c=n(70679);function p(e){return a.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,a.isValidElement)(e)&&function(e){const{props:t}=e;return!!t&&"object"==typeof t&&"value"in t}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function d(e){const{values:t,children:n}=e;return(0,a.useMemo)((()=>{const e=t??function(e){return p(e).map((e=>{let{props:{value:t,label:n,attributes:a,default:i}}=e;return{value:t,label:n,attributes:a,default:i}}))}(n);return function(e){const t=(0,u.X)(e,((e,t)=>e.value===t.value));if(t.length>0)throw new Error(`Docusaurus error: Duplicate values "${t.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[t,n])}function h(e){let{value:t,tabValues:n}=e;return n.some((e=>e.value===t))}function m(e){let{queryString:t=!1,groupId:n}=e;const i=(0,o.W6)(),s=function(e){let{queryString:t=!1,groupId:n}=e;if("string"==typeof t)return t;if(!1===t)return null;if(!0===t&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:t,groupId:n});return[(0,l.aZ)(s),(0,a.useCallback)((e=>{if(!s)return;const t=new URLSearchParams(i.location.search);t.set(s,e),i.replace({...i.location,search:t.toString()})}),[s,i])]}function _(e){const{defaultValue:t,queryString:n=!1,groupId:i}=e,s=d(e),[o,l]=(0,a.useState)((()=>function(e){let{defaultValue:t,tabValues:n}=e;if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(t){if(!h({value:t,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${t}" but none of its children has the corresponding value. Available values are: ${n.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return t}const a=n.find((e=>e.default))??n[0];if(!a)throw new Error("Unexpected error: 0 tabValues");return a.value}({defaultValue:t,tabValues:s}))),[u,p]=m({queryString:n,groupId:i}),[_,A]=function(e){let{groupId:t}=e;const n=function(e){return e?`docusaurus.tab.${e}`:null}(t),[i,s]=(0,c.Dv)(n);return[i,(0,a.useCallback)((e=>{n&&s.set(e)}),[n,s])]}({groupId:i}),f=(()=>{const e=u??_;return h({value:e,tabValues:s})?e:null})();(0,r.A)((()=>{f&&l(f)}),[f]);return{selectedValue:o,selectValue:(0,a.useCallback)((e=>{if(!h({value:e,tabValues:s}))throw new Error(`Can't select invalid tab value=${e}`);l(e),p(e),A(e)}),[p,A,s]),tabValues:s}}var A=n(92303);const f={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var g=n(74848);function E(e){let{className:t,block:n,selectedValue:a,selectValue:o,tabValues:r}=e;const l=[],{blockElementScrollPositionUntilNextRender:u}=(0,s.a_)(),c=e=>{const t=e.currentTarget,n=l.indexOf(t),i=r[n].value;i!==a&&(u(t),o(i))},p=e=>{let t=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const n=l.indexOf(e.currentTarget)+1;t=l[n]??l[0];break}case"ArrowLeft":{const n=l.indexOf(e.currentTarget)-1;t=l[n]??l[l.length-1];break}}t?.focus()};return(0,g.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,i.A)("tabs",{"tabs--block":n},t),children:r.map((e=>{let{value:t,label:n,attributes:s}=e;return(0,g.jsx)("li",{role:"tab",tabIndex:a===t?0:-1,"aria-selected":a===t,ref:e=>l.push(e),onKeyDown:p,onClick:c,...s,className:(0,i.A)("tabs__item",f.tabItem,s?.className,{"tabs__item--active":a===t}),children:n??t},t)}))})}function T(e){let{lazy:t,children:n,selectedValue:i}=e;const s=(Array.isArray(n)?n:[n]).filter(Boolean);if(t){const e=s.find((e=>e.props.value===i));return e?(0,a.cloneElement)(e,{className:"margin-top--md"}):null}return(0,g.jsx)("div",{className:"margin-top--md",children:s.map(((e,t)=>(0,a.cloneElement)(e,{key:t,hidden:e.props.value!==i})))})}function I(e){const t=_(e);return(0,g.jsxs)("div",{className:(0,i.A)("tabs-container",f.tabList),children:[(0,g.jsx)(E,{...t,...e}),(0,g.jsx)(T,{...t,...e})]})}function b(e){const t=(0,A.A)();return(0,g.jsx)(I,{...e,children:p(e.children)},String(t))}}}]);
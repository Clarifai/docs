"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[860],{65537:(e,t,a)=>{a.d(t,{A:()=>y});var n=a(96540),r=a(18215),i=a(65627),s=a(56347),l=a(50372),o=a(30604),d=a(11861),c=a(78749);function u(e){return n.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,n.isValidElement)(e)&&function(e){const{props:t}=e;return!!t&&"object"==typeof t&&"value"in t}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function p(e){const{values:t,children:a}=e;return(0,n.useMemo)((()=>{const e=t??function(e){return u(e).map((e=>{let{props:{value:t,label:a,attributes:n,default:r}}=e;return{value:t,label:a,attributes:n,default:r}}))}(a);return function(e){const t=(0,d.XI)(e,((e,t)=>e.value===t.value));if(t.length>0)throw new Error(`Docusaurus error: Duplicate values "${t.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[t,a])}function h(e){let{value:t,tabValues:a}=e;return a.some((e=>e.value===t))}function m(e){let{queryString:t=!1,groupId:a}=e;const r=(0,s.W6)(),i=function(e){let{queryString:t=!1,groupId:a}=e;if("string"==typeof t)return t;if(!1===t)return null;if(!0===t&&!a)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return a??null}({queryString:t,groupId:a});return[(0,o.aZ)(i),(0,n.useCallback)((e=>{if(!i)return;const t=new URLSearchParams(r.location.search);t.set(i,e),r.replace({...r.location,search:t.toString()})}),[i,r])]}function f(e){const{defaultValue:t,queryString:a=!1,groupId:r}=e,i=p(e),[s,o]=(0,n.useState)((()=>function(e){let{defaultValue:t,tabValues:a}=e;if(0===a.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(t){if(!h({value:t,tabValues:a}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${t}" but none of its children has the corresponding value. Available values are: ${a.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return t}const n=a.find((e=>e.default))??a[0];if(!n)throw new Error("Unexpected error: 0 tabValues");return n.value}({defaultValue:t,tabValues:i}))),[d,u]=m({queryString:a,groupId:r}),[f,g]=function(e){let{groupId:t}=e;const a=function(e){return e?`docusaurus.tab.${e}`:null}(t),[r,i]=(0,c.Dv)(a);return[r,(0,n.useCallback)((e=>{a&&i.set(e)}),[a,i])]}({groupId:r}),v=(()=>{const e=d??f;return h({value:e,tabValues:i})?e:null})();(0,l.A)((()=>{v&&o(v)}),[v]);return{selectedValue:s,selectValue:(0,n.useCallback)((e=>{if(!h({value:e,tabValues:i}))throw new Error(`Can't select invalid tab value=${e}`);o(e),u(e),g(e)}),[u,g,i]),tabValues:i}}var g=a(9136);const v={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var b=a(74848);function x(e){let{className:t,block:a,selectedValue:n,selectValue:s,tabValues:l}=e;const o=[],{blockElementScrollPositionUntilNextRender:d}=(0,i.a_)(),c=e=>{const t=e.currentTarget,a=o.indexOf(t),r=l[a].value;r!==n&&(d(t),s(r))},u=e=>{let t=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const a=o.indexOf(e.currentTarget)+1;t=o[a]??o[0];break}case"ArrowLeft":{const a=o.indexOf(e.currentTarget)-1;t=o[a]??o[o.length-1];break}}t?.focus()};return(0,b.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,r.A)("tabs",{"tabs--block":a},t),children:l.map((e=>{let{value:t,label:a,attributes:i}=e;return(0,b.jsx)("li",{role:"tab",tabIndex:n===t?0:-1,"aria-selected":n===t,ref:e=>{o.push(e)},onKeyDown:u,onClick:c,...i,className:(0,r.A)("tabs__item",v.tabItem,i?.className,{"tabs__item--active":n===t}),children:a??t},t)}))})}function j(e){let{lazy:t,children:a,selectedValue:i}=e;const s=(Array.isArray(a)?a:[a]).filter(Boolean);if(t){const e=s.find((e=>e.props.value===i));return e?(0,n.cloneElement)(e,{className:(0,r.A)("margin-top--md",e.props.className)}):null}return(0,b.jsx)("div",{className:"margin-top--md",children:s.map(((e,t)=>(0,n.cloneElement)(e,{key:t,hidden:e.props.value!==i})))})}function _(e){const t=f(e);return(0,b.jsxs)("div",{className:(0,r.A)("tabs-container",v.tabList),children:[(0,b.jsx)(x,{...t,...e}),(0,b.jsx)(j,{...t,...e})]})}function y(e){const t=(0,g.A)();return(0,b.jsx)(_,{...e,children:u(e.children)},String(t))}},79329:(e,t,a)=>{a.d(t,{A:()=>s});a(96540);var n=a(18215);const r={tabItem:"tabItem_Ymn6"};var i=a(74848);function s(e){let{children:t,hidden:a,className:s}=e;return(0,i.jsx)("div",{role:"tabpanel",className:(0,n.A)(r.tabItem,s),hidden:a,children:t})}},94955:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>w,contentTitle:()=>y,default:()=>S,frontMatter:()=>_,metadata:()=>n,toc:()=>A});const n=JSON.parse('{"id":"create/models/deep-fine-tuning/visual-embedder","title":"Visual Embedder","description":"Learn about our visual embedder model type","source":"@site/docs/create/models/deep-fine-tuning/visual-embedder.md","sourceDirName":"create/models/deep-fine-tuning","slug":"/create/models/deep-fine-tuning/visual-embedder","permalink":"/create/models/deep-fine-tuning/visual-embedder","draft":false,"unlisted":false,"editUrl":"https://github.com/Clarifai/docs/blob/main/docs/create/models/deep-fine-tuning/visual-embedder.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"description":"Learn about our visual embedder model type","sidebar_position":6},"sidebar":"tutorialSidebar","previous":{"title":"Visual Anomaly","permalink":"/create/models/deep-fine-tuning/visual-anomaly"},"next":{"title":"Clusterer","permalink":"/create/models/deep-fine-tuning/clusterer"}}');var r=a(74848),i=a(28453),s=a(65537),l=a(79329),o=a(58069);const d='from clarifai.client.user import User\n#replace your "user_id"\nclient = User(user_id="user_id")\napp = client.create_app(app_id="demo_train", base_workflow="Universal")',c="#importing load_module_dataloader for calling the dataloader object in dataset.py in the local data folder\nfrom clarifai.datasets.upload.utils import load_module_dataloader\n\n\n# Construct the path to the dataset folder\nmodule_path = os.path.join(os.getcwd().split('/models/model_train')[0],'datasets/upload/image_classification/food-101')\n\n\n# Load the dataloader module using the provided function from your module\nfood101_dataloader = load_module_dataloader(module_path)\n\n# Create a Clarifai dataset with the specified dataset_id\ndataset = app.create_dataset(dataset_id=\"image_dataset\")\n\n# Upload the dataset using the provided dataloader and get the upload status\ndataset.upload_dataset(dataloader=food101_dataloader,get_upload_status=True)\n",u="print(app.list_trainable_model_types())",p='MODEL_ID = "model_visual_embedder"\nMODEL_TYPE_ID = "visual-embedder"\n\n# Create a model by passing the model name and model type as parameter\nmodel = app.create_model(model_id=MODEL_ID, model_type_id=MODEL_TYPE_ID)\n',h="print(model.list_training_templates())",m="# Get the params for the selected template\nmodel_params = model.get_params(template='Clarifai_ResNext')\n# list the concepts to add in the params\nconcepts = [concept.id for concept in app.list_concepts()]\nmodel.update_params(dataset_id = 'image_dataset',concepts = concepts)\nprint(model.training_params)",f='import time\n#Starting the training\nmodel_version_id = model.train()\n\n#Checking the status of training\nwhile True:\n    status = model.training_status(version_id=model_version_id,training_logs=False)\n    if status.code == 21106: #MODEL_TRAINING_FAILED\n        print(status)\n        break\n    elif status.code == 21100: #MODEL_TRAINED\n        print(status)\n        break\n    else:\n        print("Current Status:",status)\n        print("Waiting---")\n        time.sleep(120)',g="IMAGE_PATH = os.path.join(os.getcwd().split('/models')[0],'datasets/upload/image_classification/food-101/images/hamburger/139558.jpg')\nmodel_prediction = model.predict_by_filepath(IMAGE_PATH, input_type=\"image\")\n\n# Get the output\nprint(model_prediction.outputs[0].data.embeddings)",v="['visual-classifier',\n 'visual-detector',\n 'visual-segmenter',\n 'visual-embedder',\n 'clusterer',\n 'text-classifier',\n 'embedding-classifier',\n 'text-to-text']",b="['classification_angular_margin_embed',\n 'classification_basemodel_v1_embed',\n 'Clarifai_ResNet_AngularMargin',\n 'Clarifai_InceptionBatchNorm',\n 'Clarifai_ResNext']",x="{'dataset_id': 'image_dataset',\n 'dataset_version_id': '',\n 'concepts': ['id-hamburger', 'id-ramen', 'id-prime_rib', 'id-beignets'],\n 'train_params': {'invalid_data_tolerance_percent': 5.0,\n  'template': 'Clarifai_ResNext',\n  'logreg': 1.0,\n  'image_size': 256.0,\n  'batch_size': 64.0,\n  'init_epochs': 25.0,\n  'step_epochs': 7.0,\n  'num_epochs': 65.0,\n  'per_item_lrate': 7.8125e-05,\n  'num_items_per_epoch': 0.0}}",j="embeddings {\n\n  vector: 0.021010370925068855\n\n  vector: 0.011909130029380322\n\n  vector: 2.2577569325221702e-07\n\n  vector: 0.001307532424107194\n\n  vector: 0.04247743636369705\n\n  vector: 0.01022490207105875\n\n  vector: 0.0006444881437346339\n\n  vector: 0.027988344430923462\n\n  vector: 0.028407510370016098\n\n  vector: 5.129506917000981e-06\n\n  vector: 0.03279731422662735\n\n  vector: 0.016899824142456055\n\n  vector: 0.003125722287222743\n\n  vector: 0.0\n\n  vector: 0.024156155064702034\n\n  vector: 0.04975743591785431\n\n  vector: 0.010608416981995106\n\n  vector: 0.0006941271130926907\n\n  vector: 0.00018513976829126477\n\n  vector: 2.714529364311602e-05\n\n  vector: 0.0014789806446060538\n\n\u2026\u2026..\n\n}\n",_={description:"Learn about our visual embedder model type",sidebar_position:6},y="Visual Embedder",w={},A=[{value:"Create and Train a Visual Embedder",id:"create-and-train-a-visual-embedder",level:2},{value:"Step 1: App Creation",id:"step-1-app-creation",level:3},{value:"Step 2: Dataset Upload",id:"step-2-dataset-upload",level:3},{value:"Step 3: Model Creation",id:"step-3-model-creation",level:3},{value:"Step 4: Template Selection",id:"step-4-template-selection",level:3},{value:"Step 5: Set Up Model Parameters",id:"step-5-set-up-model-parameters",level:3},{value:"Step 6: Initiate Model Training",id:"step-6-initiate-model-training",level:3},{value:"Step 7: Model Prediction",id:"step-7-model-prediction",level:3}];function I(e){const t={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,i.R)(),...e.components},{Details:a}=t;return a||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(t.header,{children:(0,r.jsx)(t.h1,{id:"visual-embedder",children:"Visual Embedder"})}),"\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.strong,{children:"Learn about our visual embedder model type"})}),"\n",(0,r.jsx)("hr",{}),"\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"Input"}),": Images and videos"]}),"\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"Output"}),": Embeddings"]}),"\n",(0,r.jsx)(t.p,{children:"Visual embedder, also known as visual embedding, is a type of deep fine-tuned model specifically designed to generate meaningful numerical representations (embeddings) from images and video frames."}),"\n",(0,r.jsx)(t.p,{children:"The primary goal of a visual embedder model is to transform the raw pixel values of images or video frames into a compact and high-dimensional vector. These vectors capture essential features and patterns in the visual content, enabling the model to understand and process the data in a more structured and interpretable way."}),"\n",(0,r.jsx)(t.p,{children:"These vectors can then be used for a variety of tasks, such as:"}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Visual search"}),": This is the task of finding images or videos that are similar to a given query image or video. The visual embedder model can be used to create a similarity metric between images or videos, which can then be used to search for similar visual content in a vector database."]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Training on top of them"}),": The visual embedder model can also be used as a starting point for training other machine learning models. For example, a model that can classify images or videos can be trained on top of the visual embedder model."]}),"\n"]}),"\n",(0,r.jsx)(t.admonition,{type:"info",children:(0,r.jsxs)(t.p,{children:["The visual embedder model type also comes with various ",(0,r.jsx)(t.a,{href:"https://docs.clarifai.com/portal-guide/model/deep-training/visual-embedding-templates",children:"templates"})," that give you the control to choose the specific architecture used by your neural network, as well as define a set of hyperparameters you can use to fine-tune the way your model learns."]})}),"\n",(0,r.jsx)(t.p,{children:"You may choose a visual embedder model type in cases where:"}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsx)(t.li,{children:"You need a model that can accurately represent images and video frames as vectors. Once the model is trained, you can use it to embed new images or videos into vectors."}),"\n",(0,r.jsxs)(t.li,{children:['You need an embedding model to learn new features not recognized by the existing Clarifai models. In that case, you may need to "deep fine-tune" your custom model and integrate it directly within your ',(0,r.jsx)(t.a,{href:"https://docs.clarifai.com/portal-guide/workflows/",children:"workflows"}),"."]}),"\n",(0,r.jsx)(t.li,{children:"You have a custom-tailored dataset, accurate labels, and the expertise and time to fine-tune models."}),"\n"]}),"\n",(0,r.jsx)(t.h2,{id:"create-and-train-a-visual-embedder",children:"Create and Train a Visual Embedder"}),"\n",(0,r.jsx)(t.p,{children:"Let's demonstrate how to create and train a visual embedder model using our API."}),"\n",(0,r.jsx)(t.admonition,{type:"info",children:(0,r.jsxs)(t.p,{children:["Before using the ",(0,r.jsx)(t.a,{href:"https://docs.clarifai.com/additional-resources/api-overview/python-sdk",children:"Python SDK"}),", ",(0,r.jsx)(t.a,{href:"https://docs.clarifai.com/additional-resources/api-overview/nodejs-sdk",children:"Node.js SDK"}),", or any of our ",(0,r.jsx)(t.a,{href:"https://docs.clarifai.com/additional-resources/api-overview/grpc-clients",children:"gRPC clients"}),", ensure they are properly installed on your machine. Refer to their respective installation guides for instructions on how to install and initialize them."]})}),"\n","\n","\n","\n",(0,r.jsx)(t.h3,{id:"step-1-app-creation",children:"Step 1: App Creation"}),"\n",(0,r.jsxs)(t.p,{children:["Let's start by creating an ",(0,r.jsx)(t.a,{href:"https://docs.clarifai.com/create-manage/applications/create",children:"app"}),"."]}),"\n",(0,r.jsx)(s.A,{groupId:"code",children:(0,r.jsx)(l.A,{value:"python",label:"Python SDK",children:(0,r.jsx)(o.A,{className:"language-python",children:d})})}),"\n",(0,r.jsx)(t.h3,{id:"step-2-dataset-upload",children:"Step 2: Dataset Upload"}),"\n",(0,r.jsxs)(t.p,{children:["Next, let\u2019s upload the ",(0,r.jsx)(t.a,{href:"https://docs.clarifai.com/create-manage/datasets/upload",children:"dataset"})," that will be used to train the model to the app."]}),"\n",(0,r.jsxs)(t.p,{children:["You can find the dataset we used ",(0,r.jsx)(t.a,{href:"https://github.com/Clarifai/examples/tree/main/datasets/upload/image_classification",children:"here"}),"."]}),"\n",(0,r.jsx)(s.A,{groupId:"code",children:(0,r.jsx)(l.A,{value:"python",label:"Python SDK",children:(0,r.jsx)(o.A,{className:"language-python",children:c})})}),"\n",(0,r.jsx)(t.h3,{id:"step-3-model-creation",children:"Step 3: Model Creation"}),"\n",(0,r.jsx)(t.p,{children:"Let's list all the available trainable model types in the Clarifai platform."}),"\n",(0,r.jsx)(s.A,{groupId:"code",children:(0,r.jsx)(l.A,{value:"python",label:"Python SDK",children:(0,r.jsx)(o.A,{className:"language-python",children:u})})}),"\n",(0,r.jsxs)(a,{children:[(0,r.jsx)("summary",{children:"Output"}),(0,r.jsx)(o.A,{className:"language-text",children:v})]}),"\n",(0,r.jsxs)(t.p,{children:["Next, let's select the ",(0,r.jsx)(t.code,{children:"visual-embedder"})," model type and use it to create a model."]}),"\n",(0,r.jsx)(s.A,{groupId:"code",children:(0,r.jsx)(l.A,{value:"python",label:"Python SDK",children:(0,r.jsx)(o.A,{className:"language-python",children:p})})}),"\n",(0,r.jsx)(t.h3,{id:"step-4-template-selection",children:"Step 4: Template Selection"}),"\n",(0,r.jsx)(t.p,{children:"Let's list all the available training templates in the Clarifai platform."}),"\n",(0,r.jsx)(s.A,{groupId:"code",children:(0,r.jsx)(l.A,{value:"python",label:"Python SDK",children:(0,r.jsx)(o.A,{className:"language-python",children:h})})}),"\n",(0,r.jsxs)(a,{children:[(0,r.jsx)("summary",{children:"Output"}),(0,r.jsx)(o.A,{className:"language-text",children:b})]}),"\n",(0,r.jsxs)(t.p,{children:["Next, let's choose the ",(0,r.jsx)(t.code,{children:"'Clarifai_ResNext' "})," template to use for training our model, as demonstrated below."]}),"\n",(0,r.jsx)(t.h3,{id:"step-5-set-up-model-parameters",children:"Step 5: Set Up Model Parameters"}),"\n",(0,r.jsx)(t.p,{children:"You can customize the model parameters as needed before starting the training process."}),"\n",(0,r.jsx)(s.A,{groupId:"code",children:(0,r.jsx)(l.A,{value:"python",label:"Python SDK",children:(0,r.jsx)(o.A,{className:"language-python",children:m})})}),"\n",(0,r.jsxs)(a,{children:[(0,r.jsx)("summary",{children:"Output"}),(0,r.jsx)(o.A,{className:"language-text",children:x})]}),"\n",(0,r.jsx)(t.h3,{id:"step-6-initiate-model-training",children:"Step 6: Initiate Model Training"}),"\n",(0,r.jsxs)(t.p,{children:["To initiate the model training process, call the ",(0,r.jsx)(t.code,{children:"model.train()"})," method. The Clarifai API also provides features for monitoring training status and saving training logs to a local file."]}),"\n",(0,r.jsx)(t.admonition,{type:"note",children:(0,r.jsxs)(t.p,{children:["If the status code is ",(0,r.jsx)(t.code,{children:"MODEL-TRAINED"}),", it indicates that the model has been successfully trained and is ready for use."]})}),"\n",(0,r.jsx)(s.A,{groupId:"code",children:(0,r.jsx)(l.A,{value:"python",label:"Python SDK",children:(0,r.jsx)(o.A,{className:"language-python",children:f})})}),"\n",(0,r.jsxs)(a,{children:[(0,r.jsx)("summary",{children:"Output"}),(0,r.jsx)("img",{src:"/img/python-sdk/ve_imt.png"})]}),"\n",(0,r.jsx)(t.h3,{id:"step-7-model-prediction",children:"Step 7: Model Prediction"}),"\n",(0,r.jsx)(t.p,{children:"After the model is trained and ready to use, you can run some predictions with it."}),"\n",(0,r.jsx)(s.A,{groupId:"code",children:(0,r.jsx)(l.A,{value:"python",label:"Python SDK",children:(0,r.jsx)(o.A,{className:"language-python",children:g})})}),"\n",(0,r.jsxs)(a,{children:[(0,r.jsx)("summary",{children:"Output"}),(0,r.jsx)(o.A,{className:"language-text",children:j})]})]})}function S(e={}){const{wrapper:t}={...(0,i.R)(),...e.components};return t?(0,r.jsx)(t,{...e,children:(0,r.jsx)(I,{...e})}):I(e)}}}]);
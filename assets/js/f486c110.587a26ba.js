"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[9308],{6289:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>v,contentTitle:()=>A,default:()=>_,frontMatter:()=>b,metadata:()=>i,toc:()=>I});const i=JSON.parse('{"id":"compute/models/inference/open-ai","title":"OpenAI Inferences","description":"Generate predictions using your deployed models","source":"@site/docs/compute/models/inference/open-ai.md","sourceDirName":"compute/models/inference","slug":"/compute/models/inference/open-ai","permalink":"/compute/models/inference/open-ai","draft":false,"unlisted":false,"editUrl":"https://github.com/Clarifai/docs/blob/main/docs/compute/models/inference/open-ai.md","tags":[],"version":"current","sidebarPosition":1.1,"frontMatter":{"description":"Generate predictions using your deployed models","sidebar_position":1.1,"toc_max_heading_level":4},"sidebar":"tutorialSidebar","previous":{"title":"Inference via API","permalink":"/compute/models/inference/api"},"next":{"title":"Legacy Inference via API","permalink":"/compute/models/inference/api-legacy/"}}');var a=t(74848),l=t(28453),s=t(65537),o=t(79329),r=t(58069);const c='from openai import OpenAI\n\n# Initialize the OpenAI client, pointing to Clarifai\'s API\nclient = OpenAI(\n    api_key="YOUR_CLARIFAI_PAT_KEY_HERE",  \n    base_url="https://api.clarifai.com/v2/ext/openai/v1"  # Clarifai\'s OpenAI-compatible API endpoint\n)\n\n# Make a chat completion request to a Clarifai-hosted model\nresponse = client.chat.completions.create(\n    model="anthropic/completion/models/claude-sonnet-4",  # Clarifai model name\n    messages=[\n        {"role": "system", "content": "You are a helpful assistant."},\n        {"role": "user", "content": "Who are you?"}\n    ],\n    # You can also add other OpenAI-compatible parameters like temperature, max_tokens, etc.\n    max_completion_tokens=100,  # Limits the response length\n    temperature=0.7,  # Controls randomness of the output\n    stream=True  # Enables streaming the response token by token\n)\n\nprint("Assistant\'s Response:")\nfor chunk in response:\n    # Safely check if choices, delta, and content exist before accessing\n    if chunk.choices and \\\n       chunk.choices[0].delta and \\\n       chunk.choices[0].delta.content is not None:\n        print(chunk.choices[0].delta.content, end=\'\')\nprint("\\n")  ',d='import OpenAI from "openai";\n\nconst client = new OpenAI({\n  baseURL: "https://api.clarifai.com/v2/ext/openai/v1",\n  apiKey: process.env.CLARIFAI_PAT,\n});\n\nconst response = await client.chat.completions.create({\n  model: "https://clarifai.com/anthropic/completion/models/claude-sonnet-4",\n  messages: [\n    { role: "system", content: "You are a helpful assistant." },\n    { role: "user", content: "Who are you?" },\n  ],\n});\n\nconsole.log(response.choices?.[0]?.message.content);',h='from openai import OpenAI\n\n# Initialize the OpenAI-compatible client for Clarifai\nclient = OpenAI(\n    api_key="YOUR_CLARIFAI_PAT_KEY_HERE",\n    base_url="https://api.clarifai.com/v2/ext/openai/v1"   \n)\n\n# Define the external tools (functions) that the LLM can call.\n# In this example, it\'s a \'get_weather\' function.\ntools = [\n    {\n        "type": "function",\n        "function": {\n            "name": "get_weather",\n            "description": "Returns the current temperature for a given location.",\n            "parameters": {\n                "type": "object",\n                "properties": {\n                    "location": {\n                        "type": "string",\n                        "description": "City and country, e.g., \'Bogot\xe1, Colombia\'"\n                    }\n                },\n                "required": ["location"],\n                "additionalProperties": False # Ensures no extra parameters are passed\n            }\n        }\n    }\n]\n\n# Create a chat completion request with tool-calling enabled\nresponse = client.chat.completions.create(\n    model="anthropic/completion/models/claude-sonnet-4",  # Clarifai-compatible OpenAI model name\n    messages=[\n        {"role": "user", "content": "What is the weather like in New York today?"}\n    ],\n    tools=tools,\n    tool_choice=\'auto\' # Let the LLM decide if it needs to use a tool\n)\n\n# Print the tool call proposed by the model, if any\ntool_calls = response.choices[0].message.tool_calls\nprint("Tool calls:", tool_calls)\n',p='import OpenAI from "openai";\nimport type { ChatCompletionTool } from "openai/resources";\n\nconst client = new OpenAI({\n  baseURL: "https://api.clarifai.com/v2/ext/openai/v1",\n  apiKey: process.env.CLARIFAI_PAT,\n});\n\nconst tools: ChatCompletionTool[] = [\n  {\n    type: "function",\n    function: {\n      name: "get_weather",\n      description: "Get current temperature for a given location.",\n      parameters: {\n        type: "object",\n        properties: {\n          location: {\n            type: "string",\n            description: "City and country e.g. Bogot\xe1, Colombia",\n          },\n        },\n        required: ["location"],\n        additionalProperties: false,\n      },\n      strict: true,\n    },\n  },\n];\n\nconst toolCompletion = await client.chat.completions.create({\n  model: "https://clarifai.com/anthropic/completion/models/claude-sonnet-4",\n  messages: [\n    { role: "system", content: "You are a helpful assistant." },\n    { role: "user", content: "What is the weather in New York?" },\n  ],\n  tools,\n});\n\nconsole.log(toolCompletion.choices?.[0]?.message.tool_calls);\n',m='import json\nfrom openai import OpenAI\n\n# Initialize the OpenAI client, pointing to Clarifai\'s OpenAI-compatible API endpoint\nclient = OpenAI(\n    base_url="https://api.clarifai.com/v2/ext/openai/v1",\n    api_key="YOUR_CLARIFAI_PAT_KEY_HERE",\n)\n\n# Define the external tools (functions) that the LLM can call.\n# In this example, it\'s a \'get_weather\' function.\ntools = [{\n    "type": "function",\n    "function": {\n        "name": "get_weather",\n        "description": "Get current temperature for a given location.",\n        "parameters": {\n            "type": "object",\n            "properties": {\n                "location": {\n                    "type": "string",\n                    "description": "City and country, e.g., \'Bogot\xe1, Colombia\'"\n                }\n            },\n            "required": ["location"],\n            "additionalProperties": False # Ensures no extra parameters are passed\n        },\n        "strict": True # Enforces strict adherence to parameter schema\n    }\n}]\n\n## Simulate Tool Execution (for demonstration)\n\n# This function simulates calling an external weather API.\n# In a real application, this would make an actual API request.\ndef get_weather(location: str):\n    """Simulates fetching weather for a given location."""\n    # Placeholder data for demonstration\n    if "New York" in location:\n        return {"location": "New York", "temperature": "20\xb0C", "conditions": "Partly cloudy"}\n    elif "London" in location:\n        return {"location": "London", "temperature": "15\xb0C", "conditions": "Rainy"}\n    else:\n        return {"location": location, "temperature": "N/A", "conditions": "Unknown"}\n\n## LLM Call with Tooling\n\n# First API call: The LLM decides if a tool needs to be called.\nprint("--- Initial LLM Call (Tool Recommendation) ---")\nfirst_response = client.chat.completions.create(\n    model="anthropic/completion/models/claude-sonnet-4", # Ensure this model supports tool calling on Clarifai\'s platform\n    messages=[\n        {"role": "user", "content": "What is the weather like in New York today?"}\n    ],\n    tools=tools, # Provide the list of available tools\n    tool_choice="auto", # Let the LLM decide if it needs to use a tool\n)\n\n\n## Process LLM\'s Response and Execute Tool (if recommended)\n\n# Check if the LLM decided to call a tool\nif first_response.choices[0].message.tool_calls:\n    tool_calls = first_response.choices[0].message.tool_calls\n    print(f"\\nLLM recommended tool calls: {tool_calls}")\n\n    # Execute each recommended tool call\n    available_functions = {\n        "get_weather": get_weather, # Map function name to actual Python function\n    }\n\n    messages = [\n        {"role": "user", "content": "What is the weather like in New York today?"}\n    ]\n    messages.append(first_response.choices[0].message) # Add LLM\'s tool call suggestion to messages\n\n    for tool_call in tool_calls:\n        function_name = tool_call.function.name\n        function_to_call = available_functions[function_name]\n        function_args = json.loads(tool_call.function.arguments)\n\n        # Call the actual Python function\n        function_response = function_to_call(**function_args)\n        print(f"\\nExecuting tool: {function_name}({function_args}) -> {function_response}")\n\n        # Add the tool\'s output to the conversation for the LLM to process\n        messages.append(\n            {\n                "tool_call_id": tool_call.id,\n                "role": "tool",\n                "name": function_name,\n                "content": json.dumps(function_response),\n            }\n        )\n\n    # ---\n    ## Second LLM Call (Summarize Tool Output)\n    \n\n    # Now, send the tool\'s output back to the LLM to get a natural language response\n    print("\\n--- Second LLM Call (Summarizing Tool Output) ---")\n    second_response = client.chat.completions.create(\n        model="anthropic/completion/models/claude-sonnet-4",\n        messages=messages, # Continue the conversation with tool output\n    )\n\n    print("\\nFinal Assistant\'s Response:")\n    print(second_response.choices[0].message.content)\n\nelse:\n    print("\\nLLM did not recommend any tool calls.")\n    print("Assistant\'s direct response:")\n    print(first_response.choices[0].message.content)',u="--- Initial LLM Call (Tool Recommendation) ---\n\nLLM recommended tool calls: [ChatCompletionMessageToolCall(id='toolu_01Mhqb1c7ne4GPKWY9eZtgxd', function=Function(arguments='{\"location\": \"New York, United States\"}', name='get_weather'), type='function')]\n\nExecuting tool: get_weather({'location': 'New York, United States'}) -> {'location': 'New York', 'temperature': '20\xb0C', 'conditions': 'Partly cloudy'}\n\n--- Second LLM Call (Summarizing Tool Output) ---\n\nFinal Assistant's Response:\nThe weather in New York today is:\n- **Temperature:** 20\xb0C (68\xb0F)\n- **Conditions:** Partly cloudy\n\nIt's a pleasant day with mild temperatures and partly cloudy skies!",x='import litellm\n\n# Call litellm.completion or litellm.chat_completion to send requests\nfor chunk in litellm.completion(\n    model="openai/https://clarifai.com/openai/chat-completion/models/o4-mini",\n    api_key="CLARIFAI_PAT",\n    api_base="https://api.clarifai.com/v2/ext/openai/v1",\n    # Message formatting is consistent with OpenAI\'s schema ({"role": ..., "content": ...}).\n    messages=[\n        {"role": "user", "content": "Tell me a fun fact about space."}\n    ],\n    stream=True, # Supports streaming responses\n):\n    print(chunk.choices[0].delta)\n',f='import litellm\n\ntools = [{\n    "type": "function",\n    "function": {\n        "name": "get_weather",\n        "description": "Get current temperature for a given location.",\n        "parameters": {\n            "type": "object",\n            "properties": {\n                "location": {\n                    "type": "string",\n                    "description": "City and country e.g. Tokyo, Japan"\n                }\n            },\n            "required": ["location"],\n            "additionalProperties": False\n        },\n    }\n}]\n\nresponse = litellm.completion(\n    model="openai/https://clarifai.com/openai/chat-completion/models/o4-mini",\n    api_key="CLARIFAI_PAT",\n    api_base="https://api.clarifai.com/v2/ext/openai/v1",\n    messages=[{"role": "user", "content": "What is the weather in Paris today?"}],\n    tools=tools,\n)\n\nprint(response.choices[0].message.tool_calls)\n',j='import { createOpenAICompatible } from "@ai-sdk/openai-compatible";\nimport { generateText } from "ai";\n\nconst clarifai = createOpenAICompatible({\n  baseURL: "https://api.clarifai.com/v2/ext/openai/v1",\n  apiKey: process.env.CLARIFAI_PAT,\n});\n\nconst model = clarifai(\n  "https://clarifai.com/anthropic/completion/models/claude-sonnet-4",\n);\n\nconst { text } = await generateText({\n  model,\n  messages: [\n    { role: "system", content: "You are a helpful assistant." },\n    { role: "user", content: "What is photosynthesis?" },\n  ],\n});\n\nconsole.log(text);\n',g='import { createOpenAICompatible } from "@ai-sdk/openai-compatible";\nimport { streamText } from "ai";\n\nconst clarifai = createOpenAICompatible({\n  baseURL: "https://api.clarifai.com/v2/ext/openai/v1",\n  apiKey: process.env.CLARIFAI_PAT,\n});\n\nconst model = clarifai(\n  "https://clarifai.com/anthropic/completion/models/claude-sonnet-4",\n);\n\nconst stream = streamText({\n  model,\n  messages: [\n    { role: "system", content: "You are a helpful assistant." },\n    { role: "user", content: "What is photosynthesis?" },\n  ],\n});\n\nfor await (const part of stream.textStream) {\n  console.log(part);\n}\n',y='import { createOpenAICompatible } from "@ai-sdk/openai-compatible";\nimport { generateText, tool } from "ai";\nimport { z } from "zod";\n\nconst clarifai = createOpenAICompatible({\n  baseURL: "https://api.clarifai.com/v2/ext/openai/v1",\n  apiKey: process.env.CLARIFAI_PAT,\n});\n\nconst model = clarifai(\n  "https://clarifai.com/anthropic/completion/models/claude-sonnet-4",\n);\n\nconst result = await generateText({\n  model,\n  tools: {\n    weather: tool({\n      description: "Get the weather in a location",\n      parameters: z.object({\n        location: z.string().describe("The location to get the weather for"),\n      }),\n      execute: async ({ location }) => ({\n        location,\n        temperature: 72 + Math.floor(Math.random() * 21) - 10,\n      }),\n    }),\n    cityAttractions: tool({\n      parameters: z.object({ city: z.string() }),\n    }),\n  },\n  prompt:\n    "What is the weather in San Francisco and what attractions should I visit?",\n});\n\nconsole.log(result.toolResults);\n',b={description:"Generate predictions using your deployed models",sidebar_position:1.1,toc_max_heading_level:4},A="OpenAI Inferences",v={},I=[{value:"OpenAI",id:"openai",level:2},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Install Clarifai Package",id:"install-clarifai-package",level:3},{value:"Get a PAT Key",id:"get-a-pat-key",level:4},{value:"Install Openai Package",id:"install-openai-package",level:4},{value:"Example",id:"example",level:3},{value:"Tool Calling",id:"tool-calling",level:3},{value:"Vercel AI SDK",id:"vercel-ai-sdk",level:2},{value:"Example",id:"example-1",level:3},{value:"LiteLLM",id:"litellm",level:2},{value:"Example",id:"example-2",level:3},{value:"Tool Calling",id:"tool-calling-1",level:3}];function w(e){const n={a:"a",admonition:"admonition",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,l.R)(),...e.components},{Details:t}=n;return t||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"openai-inferences",children:"OpenAI Inferences"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Make inferences with Clarifai using an OpenAI-compatible format"})}),"\n",(0,a.jsx)("hr",{}),"\n",(0,a.jsx)(n.p,{children:"Clarifai provides an OpenAI-compatible API endpoint, which allows you to apply the widely adopted OpenAI standard to interact with Clarifai models. Any OpenAI-compatible library can use this endpoint to send requests to Clarifai."}),"\n",(0,a.jsx)(n.p,{children:"With this integration capability, your existing OpenAI-compatible libraries can seamlessly connect with Clarifai's models, requiring minimal code changes."}),"\n",(0,a.jsx)(n.admonition,{type:"info",children:(0,a.jsxs)(n.p,{children:["Base URL for Clarifai's OpenAI endpoint: ",(0,a.jsx)(n.code,{children:"https://api.clarifai.com/v2/ext/openai/v1"}),"."]})}),"\n",(0,a.jsx)(n.p,{children:"This endpoint offers several advantages, including:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Access to diverse models"})," \u2014 Harness Clarifai's rich array of models directly within your OpenAI projects, expanding your AI capabilities."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Standardized interaction"})," \u2014 Interact with Clarifai-hosted models using familiar OpenAI API patterns and interfaces, reducing the learning curve and streamlining development."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Enhanced flexibility"})," \u2014 Leverage the power of Clarifai's platform while maintaining the flexibility of your chosen OpenAI development environment."]}),"\n"]}),"\n",(0,a.jsxs)(n.blockquote,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Note"})," Usage-based billing is handled directly through Clarifai \u2014 not through OpenAI or any other external tool. Also, while most OpenAI parameters are supported, certain advanced features may be unavailable depending on the specific model or endpoint."]}),"\n"]}),"\n","\n","\n","\n","\n",(0,a.jsx)(n.h2,{id:"openai",children:"OpenAI"}),"\n",(0,a.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,a.jsx)(n.h3,{id:"install-clarifai-package",children:"Install Clarifai Package"}),"\n",(0,a.jsxs)(n.p,{children:["Install the latest version of the Clarifai ",(0,a.jsx)(n.a,{href:"https://github.com/Clarifai/clarifai-python/",children:"Python"})," SDK package:"]}),"\n",(0,a.jsx)(s.A,{children:(0,a.jsx)(o.A,{value:"bash",label:"Bash",children:(0,a.jsx)(r.A,{className:"language-bash",children:" pip install --upgrade clarifai "})})}),"\n",(0,a.jsx)(n.h4,{id:"get-a-pat-key",children:"Get a PAT Key"}),"\n",(0,a.jsxs)(n.p,{children:["You need a PAT (Personal Access Token) key to authenticate your connection to the Clarifai platform. You can generate the PAT key in your personal settings page by navigating to the ",(0,a.jsx)(n.a,{href:"https://clarifai.com/settings/security",children:"Security section"}),"."]}),"\n",(0,a.jsx)(n.h4,{id:"install-openai-package",children:"Install Openai Package"}),"\n",(0,a.jsxs)(n.p,{children:["Install the ",(0,a.jsx)(n.code,{children:"openai"})," package:"]}),"\n",(0,a.jsxs)(s.A,{children:[(0,a.jsx)(o.A,{value:"python",label:"Python",children:(0,a.jsx)(r.A,{className:"language-bash",children:" pip install openai "})}),(0,a.jsx)(o.A,{value:"node.js",label:"Node.js",children:(0,a.jsx)(r.A,{className:"language-bash",children:" npm install openai "})})]}),"\n",(0,a.jsx)(n.h3,{id:"example",children:"Example"}),"\n",(0,a.jsx)(n.p,{children:"Here is an example that uses the OpenAI Python client library to interact with a Clarifai model via Clarifai's OpenAI-compatible API endpoint."}),"\n",(0,a.jsxs)(s.A,{children:[(0,a.jsx)(o.A,{value:"python",label:"Python SDK",children:(0,a.jsx)(r.A,{className:"language-python",children:c})}),(0,a.jsx)(o.A,{value:"typescript",label:"TypeScript",children:(0,a.jsx)(r.A,{className:"language-typescript",children:d})})]}),"\n",(0,a.jsxs)(t,{children:[(0,a.jsx)("summary",{children:"Example Output"}),(0,a.jsx)(r.A,{className:"language-text",children:"Assistant's Response:\nI'm Claude, an AI assistant created by Anthropic. I'm here to help with a wide variety of tasks like answering questions, helping with analysis and research, creative projects, math and coding, and having conversations. Is there something specific I can help you with today?"})]}),"\n",(0,a.jsx)(n.h3,{id:"tool-calling",children:"Tool Calling"}),"\n",(0,a.jsx)(n.p,{children:"Tool calling (formerly known as function calling) enables large language models (LLMs) to autonomously decide when and how to invoke external tools \u2014 such as APIs or custom functions \u2014 based on user input."}),"\n",(0,a.jsx)(n.p,{children:"With Clarifai\u2019s support for OpenAI-compatible APIs, you can seamlessly integrate tool-calling capabilities using your existing OpenAI workflows, while leveraging Clarifai-hosted or custom models."}),"\n",(0,a.jsx)(n.p,{children:'Here is an example code that sets up a basic tool-calling interaction. It simulates a weather API and shows how the LLM would "call" that tool when asked about the weather.'}),"\n",(0,a.jsxs)(s.A,{children:[(0,a.jsx)(o.A,{value:"python",label:"Python SDK",children:(0,a.jsx)(r.A,{className:"language-python",children:h})}),(0,a.jsx)(o.A,{value:"typescript",label:"TypeScript",children:(0,a.jsx)(r.A,{className:"language-typescript",children:p})})]}),"\n",(0,a.jsxs)(t,{children:[(0,a.jsx)("summary",{children:"Tool Calling Implementation Example"}),(0,a.jsx)(r.A,{className:"language-python",children:m}),(0,a.jsx)(r.A,{className:"language-text",children:u})]}),"\n",(0,a.jsx)(n.h2,{id:"vercel-ai-sdk",children:"Vercel AI SDK"}),"\n",(0,a.jsxs)(n.p,{children:["The ",(0,a.jsx)(n.a,{href:"https://vercel.com/docs/ai-sdk",children:"Vercel AI SDK"})," provides a convenient way to interact with Clarifai's OpenAI-compatible API. You can leverage the ",(0,a.jsx)(n.a,{href:"https://www.npmjs.com/package/@ai-sdk/openai-compatible",children:"@ai-sdk/openai-compatible"})," to interact with Clarifai models."]}),"\n",(0,a.jsx)(n.h3,{id:"example-1",children:"Example"}),"\n",(0,a.jsxs)(s.A,{children:[(0,a.jsx)(o.A,{value:"text-response",label:"Text Response",children:(0,a.jsx)(r.A,{className:"language-typescript",children:j})}),(0,a.jsx)(o.A,{value:"streaming",label:"Streaming",children:(0,a.jsx)(r.A,{className:"language-typescript",children:g})}),(0,a.jsx)(o.A,{value:"tool-calling",label:"Tool Calling",children:(0,a.jsx)(r.A,{className:"language-typescript",children:y})})]}),"\n",(0,a.jsxs)(t,{children:[(0,a.jsx)("summary",{children:"Vercel AI SDK model capabilities list"}),(0,a.jsxs)(n.table,{children:[(0,a.jsx)(n.thead,{children:(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.th,{children:"Model"}),(0,a.jsx)(n.th,{children:"Image Input"}),(0,a.jsx)(n.th,{children:"Tool Usage"}),(0,a.jsx)(n.th,{children:"Tool Streaming"})]})}),(0,a.jsxs)(n.tbody,{children:[(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.a,{href:"https://clarifai.com/deepseek-ai/deepseek-chat/models/DeepSeek-R1-0528-Qwen3-8B",children:"DeepSeek R1 0528 Qwen3 8B"})}),(0,a.jsx)(n.td,{children:"\u2705"}),(0,a.jsx)(n.td,{children:"\u2705"}),(0,a.jsx)(n.td,{children:"\u2705"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.a,{href:"https://clarifai.com/meta/Llama-3/models/Llama-3_2-3B-Instruct",children:"Llama 3.2 3B Instruct"})}),(0,a.jsx)(n.td,{children:"\u2705"}),(0,a.jsx)(n.td,{children:"\u2705"}),(0,a.jsx)(n.td,{children:"\u2705"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.a,{href:"https://clarifai.com/anthropic/completion/models/claude-sonnet-4",children:"claude Sonnet 4"})}),(0,a.jsx)(n.td,{children:"\u2705"}),(0,a.jsx)(n.td,{children:"\u2705"}),(0,a.jsx)(n.td,{children:"\u2705"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.a,{href:"https://clarifai.com/qwen/qwenLM/models/Qwen3-14B",children:"Qwen3 14B"})}),(0,a.jsx)(n.td,{children:"\u2705"}),(0,a.jsx)(n.td,{children:"\u2705"}),(0,a.jsx)(n.td,{children:"\u2705"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.a,{href:"https://clarifai.com/mistralai/completion/models/Devstral-Small-2505_gguf-4bit",children:"Devstral Small 2505.gguf 4bit"})}),(0,a.jsx)(n.td,{children:"\u2705"}),(0,a.jsx)(n.td,{children:"\u2705"}),(0,a.jsx)(n.td,{children:"\u2705"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.a,{href:"https://clarifai.com/xai/chat-completion/models/grok-3",children:"grok 3"})}),(0,a.jsx)(n.td,{children:"\u274c"}),(0,a.jsx)(n.td,{children:"\u2705"}),(0,a.jsx)(n.td,{children:"\u2705"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.a,{href:"https://clarifai.com/openai/chat-completion/models/gpt-4o",children:"gpt 4o"})}),(0,a.jsx)(n.td,{children:"\u2705"}),(0,a.jsx)(n.td,{children:"\u2705"}),(0,a.jsx)(n.td,{children:"\u2705"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.a,{href:"https://clarifai.com/openai/chat-completion/models/gpt-4_1",children:"gpt 4.1"})}),(0,a.jsx)(n.td,{children:"\u2705"}),(0,a.jsx)(n.td,{children:"\u2705"}),(0,a.jsx)(n.td,{children:"\u2705"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.a,{href:"https://clarifai.com/gcp/generate/models/gemini-2_5-flash",children:"gemini 2.5 Flash"})}),(0,a.jsx)(n.td,{children:"\u2705"}),(0,a.jsx)(n.td,{children:"\u2705"}),(0,a.jsx)(n.td,{children:"\u2705"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.a,{href:"https://clarifai.com/anthropic/completion/models/claude-3_5-haiku",children:"claude 3.5 Haiku"})}),(0,a.jsx)(n.td,{children:"\u2705"}),(0,a.jsx)(n.td,{children:"\u2705"}),(0,a.jsx)(n.td,{children:"\u2705"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.a,{href:"https://clarifai.com/qwen/qwenLM/models/Qwen3-30B-A3B-GGUF",children:"Qwen3 30B A3B GGUF"})}),(0,a.jsx)(n.td,{children:"\u2705"}),(0,a.jsx)(n.td,{children:"\u2705"}),(0,a.jsx)(n.td,{children:"\u2705"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.a,{href:"https://clarifai.com/gcp/generate/models/gemini-2_0-flash",children:"gemini 2.0 Flash"})}),(0,a.jsx)(n.td,{children:"\u2705"}),(0,a.jsx)(n.td,{children:"\u2705"}),(0,a.jsx)(n.td,{children:"\u2705"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.a,{href:"https://clarifai.com/gcp/generate/models/gemma-3-12b-it",children:"gemma 3 12b It"})}),(0,a.jsx)(n.td,{children:"\u2705"}),(0,a.jsx)(n.td,{children:"\u2705"}),(0,a.jsx)(n.td,{children:"\u2705"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.a,{href:"https://clarifai.com/microsoft/text-generation/models/Phi-4-reasoning-plus",children:"Phi 4 Reasoning Plus"})}),(0,a.jsx)(n.td,{children:"\u2705"}),(0,a.jsx)(n.td,{children:"\u2705"}),(0,a.jsx)(n.td,{children:"\u2705"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.a,{href:"https://clarifai.com/microsoft/text-generation/models/phi-4-mini-instruct",children:"phi 4 Mini Instruct"})}),(0,a.jsx)(n.td,{children:"\u2705"}),(0,a.jsx)(n.td,{children:"\u2705"}),(0,a.jsx)(n.td,{children:"\u2705"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.a,{href:"https://clarifai.com/qwen/qwen-VL/models/Qwen2_5-VL-7B-Instruct",children:"Qwen2.5 VL 7B Instruct"})}),(0,a.jsx)(n.td,{children:"\u2705"}),(0,a.jsx)(n.td,{children:"\u274c"}),(0,a.jsx)(n.td,{children:"\u274c"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.a,{href:"https://clarifai.com/microsoft/text-generation/models/phi-4",children:"phi 4"})}),(0,a.jsx)(n.td,{children:"\u2705"}),(0,a.jsx)(n.td,{children:"\u2705"}),(0,a.jsx)(n.td,{children:"\u2705"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.a,{href:"https://clarifai.com/xai/chat-completion/models/grok-2-vision-1212",children:"grok 2 Vision 1212"})}),(0,a.jsx)(n.td,{children:"\u2705"}),(0,a.jsx)(n.td,{children:"\u2705"}),(0,a.jsx)(n.td,{children:"\u2705"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.a,{href:"https://clarifai.com/xai/chat-completion/models/grok-2-1212",children:"grok 2 1212"})}),(0,a.jsx)(n.td,{children:"\u274c"}),(0,a.jsx)(n.td,{children:"\u2705"}),(0,a.jsx)(n.td,{children:"\u2705"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.a,{href:"https://clarifai.com/qwen/qwenLM/models/QwQ-32B-AWQ",children:"QwQ 32B AWQ"})}),(0,a.jsx)(n.td,{children:"\u2705"}),(0,a.jsx)(n.td,{children:"\u2705"}),(0,a.jsx)(n.td,{children:"\u2705"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.a,{href:"https://clarifai.com/gcp/generate/models/gemini-2_0-flash-lite",children:"gemini 2.0 Flash Lite"})}),(0,a.jsx)(n.td,{children:"\u2705"}),(0,a.jsx)(n.td,{children:"\u2705"}),(0,a.jsx)(n.td,{children:"\u2705"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.a,{href:"https://clarifai.com/anthropic/completion/models/claude-opus-4",children:"claude Opus 4"})}),(0,a.jsx)(n.td,{children:"\u2705"}),(0,a.jsx)(n.td,{children:"\u2705"}),(0,a.jsx)(n.td,{children:"\u2705"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.a,{href:"https://clarifai.com/openai/chat-completion/models/o4-mini",children:"o4 Mini"})}),(0,a.jsx)(n.td,{children:"\u2705"}),(0,a.jsx)(n.td,{children:"\u2705"}),(0,a.jsx)(n.td,{children:"\u2705"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.a,{href:"https://clarifai.com/openai/chat-completion/models/o3",children:"o3"})}),(0,a.jsx)(n.td,{children:"\u2705"}),(0,a.jsx)(n.td,{children:"\u2705"}),(0,a.jsx)(n.td,{children:"\u2705"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.a,{href:"https://clarifai.com/openbmb/miniCPM/models/MiniCPM-o-2_6-language",children:"MiniCPM-o 2.6 Language"})}),(0,a.jsx)(n.td,{children:"\u2705"}),(0,a.jsx)(n.td,{children:"\u274c"}),(0,a.jsx)(n.td,{children:"\u274c"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.a,{href:"https://clarifai.com/deepseek-ai/deepseek-chat/models/DeepSeek-R1-Distill-Qwen-7B",children:"DeepSeek R1 Distill Qwen 7B"})}),(0,a.jsx)(n.td,{children:"\u2705"}),(0,a.jsx)(n.td,{children:"\u274c"}),(0,a.jsx)(n.td,{children:"\u274c"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.a,{href:"https://clarifai.com/qwen/qwenCoder/models/Qwen2_5-Coder-7B-Instruct",children:"Qwen2.5 Coder 7B Instruct"})}),(0,a.jsx)(n.td,{children:"\u2705"}),(0,a.jsx)(n.td,{children:"\u2705"}),(0,a.jsx)(n.td,{children:"\u2705"})]})]})]})]}),"\n",(0,a.jsx)(n.h2,{id:"litellm",children:"LiteLLM"}),"\n",(0,a.jsxs)(n.p,{children:["You can use the ",(0,a.jsx)(n.a,{href:"https://github.com/BerriAI/litellm",children:"LiteLLM Python SDK"})," to directly route inference requests to their Clarifai-hosted models. This provides a lightweight, OpenAI-compatible interface for interacting with Clarifai's powerful LLMs using a single, unified API."]}),"\n",(0,a.jsxs)(n.p,{children:["To use Clarifai models via ",(0,a.jsx)(n.a,{href:"https://docs.litellm.ai/docs/providers/clarifai",children:"LiteLLM"}),", you'll need to:"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Install the Clarifai package and get a PAT key as mentioned earlier."}),"\n",(0,a.jsxs)(n.li,{children:["Install LiteLLM by running ",(0,a.jsx)(n.code,{children:"pip install litellm"}),"."]}),"\n",(0,a.jsxs)(n.li,{children:["Specify Clarifai models by using the model path prefixed with ",(0,a.jsx)(n.code,{children:"openai/"})," followed by the Clarifai model URL (e.g., ",(0,a.jsx)(n.code,{children:"openai/https://clarifai.com/..."}),")."]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"example-2",children:"Example"}),"\n",(0,a.jsx)(s.A,{children:(0,a.jsx)(o.A,{value:"python",label:"Python SDK",children:(0,a.jsx)(r.A,{className:"language-python",children:x})})}),"\n",(0,a.jsx)(n.h3,{id:"tool-calling-1",children:"Tool Calling"}),"\n",(0,a.jsx)(n.p,{children:"Clarifai models accessed via LiteLLM also support tool calling."}),"\n",(0,a.jsx)(s.A,{children:(0,a.jsx)(o.A,{value:"python",label:"Python SDK",children:(0,a.jsx)(r.A,{className:"language-python",children:f})})})]})}function _(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(w,{...e})}):w(e)}},65537:(e,n,t)=>{t.d(n,{A:()=>v});var i=t(96540),a=t(18215),l=t(65627),s=t(56347),o=t(50372),r=t(30604),c=t(11861),d=t(78749);function h(e){return i.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,i.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function p(e){const{values:n,children:t}=e;return(0,i.useMemo)((()=>{const e=n??function(e){return h(e).map((e=>{let{props:{value:n,label:t,attributes:i,default:a}}=e;return{value:n,label:t,attributes:i,default:a}}))}(t);return function(e){const n=(0,c.XI)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,t])}function m(e){let{value:n,tabValues:t}=e;return t.some((e=>e.value===n))}function u(e){let{queryString:n=!1,groupId:t}=e;const a=(0,s.W6)(),l=function(e){let{queryString:n=!1,groupId:t}=e;if("string"==typeof n)return n;if(!1===n)return null;if(!0===n&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:n,groupId:t});return[(0,r.aZ)(l),(0,i.useCallback)((e=>{if(!l)return;const n=new URLSearchParams(a.location.search);n.set(l,e),a.replace({...a.location,search:n.toString()})}),[l,a])]}function x(e){const{defaultValue:n,queryString:t=!1,groupId:a}=e,l=p(e),[s,r]=(0,i.useState)((()=>function(e){let{defaultValue:n,tabValues:t}=e;if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(n){if(!m({value:n,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${n}" but none of its children has the corresponding value. Available values are: ${t.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return n}const i=t.find((e=>e.default))??t[0];if(!i)throw new Error("Unexpected error: 0 tabValues");return i.value}({defaultValue:n,tabValues:l}))),[c,h]=u({queryString:t,groupId:a}),[x,f]=function(e){let{groupId:n}=e;const t=function(e){return e?`docusaurus.tab.${e}`:null}(n),[a,l]=(0,d.Dv)(t);return[a,(0,i.useCallback)((e=>{t&&l.set(e)}),[t,l])]}({groupId:a}),j=(()=>{const e=c??x;return m({value:e,tabValues:l})?e:null})();(0,o.A)((()=>{j&&r(j)}),[j]);return{selectedValue:s,selectValue:(0,i.useCallback)((e=>{if(!m({value:e,tabValues:l}))throw new Error(`Can't select invalid tab value=${e}`);r(e),h(e),f(e)}),[h,f,l]),tabValues:l}}var f=t(9136);const j={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var g=t(74848);function y(e){let{className:n,block:t,selectedValue:i,selectValue:s,tabValues:o}=e;const r=[],{blockElementScrollPositionUntilNextRender:c}=(0,l.a_)(),d=e=>{const n=e.currentTarget,t=r.indexOf(n),a=o[t].value;a!==i&&(c(n),s(a))},h=e=>{let n=null;switch(e.key){case"Enter":d(e);break;case"ArrowRight":{const t=r.indexOf(e.currentTarget)+1;n=r[t]??r[0];break}case"ArrowLeft":{const t=r.indexOf(e.currentTarget)-1;n=r[t]??r[r.length-1];break}}n?.focus()};return(0,g.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,a.A)("tabs",{"tabs--block":t},n),children:o.map((e=>{let{value:n,label:t,attributes:l}=e;return(0,g.jsx)("li",{role:"tab",tabIndex:i===n?0:-1,"aria-selected":i===n,ref:e=>{r.push(e)},onKeyDown:h,onClick:d,...l,className:(0,a.A)("tabs__item",j.tabItem,l?.className,{"tabs__item--active":i===n}),children:t??n},n)}))})}function b(e){let{lazy:n,children:t,selectedValue:l}=e;const s=(Array.isArray(t)?t:[t]).filter(Boolean);if(n){const e=s.find((e=>e.props.value===l));return e?(0,i.cloneElement)(e,{className:(0,a.A)("margin-top--md",e.props.className)}):null}return(0,g.jsx)("div",{className:"margin-top--md",children:s.map(((e,n)=>(0,i.cloneElement)(e,{key:n,hidden:e.props.value!==l})))})}function A(e){const n=x(e);return(0,g.jsxs)("div",{className:(0,a.A)("tabs-container",j.tabList),children:[(0,g.jsx)(y,{...n,...e}),(0,g.jsx)(b,{...n,...e})]})}function v(e){const n=(0,f.A)();return(0,g.jsx)(A,{...e,children:h(e.children)},String(n))}},79329:(e,n,t)=>{t.d(n,{A:()=>s});t(96540);var i=t(18215);const a={tabItem:"tabItem_Ymn6"};var l=t(74848);function s(e){let{children:n,hidden:t,className:s}=e;return(0,l.jsx)("div",{role:"tabpanel",className:(0,i.A)(a.tabItem,s),hidden:t,children:n})}}}]);
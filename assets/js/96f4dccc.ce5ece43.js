"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[349],{19365:(e,n,t)=>{t.d(n,{A:()=>i});var r=t(96540),o=t(20053);const a={tabItem:"tabItem_Ymn6"};function i(e){let{children:n,hidden:t,className:i}=e;return r.createElement("div",{role:"tabpanel",className:(0,o.A)(a.tabItem,i),hidden:t},n)}},11470:(e,n,t)=>{t.d(n,{A:()=>I});var r=t(58168),o=t(96540),a=t(20053),i=t(23104),l=t(56347),s=t(57485),c=t(31682),d=t(89466);function p(e){return function(e){return o.Children.map(e,(e=>{if(!e||(0,o.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}(e).map((e=>{let{props:{value:n,label:t,attributes:r,default:o}}=e;return{value:n,label:t,attributes:r,default:o}}))}function m(e){const{values:n,children:t}=e;return(0,o.useMemo)((()=>{const e=n??p(t);return function(e){const n=(0,c.X)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,t])}function u(e){let{value:n,tabValues:t}=e;return t.some((e=>e.value===n))}function y(e){let{queryString:n=!1,groupId:t}=e;const r=(0,l.W6)(),a=function(e){let{queryString:n=!1,groupId:t}=e;if("string"==typeof n)return n;if(!1===n)return null;if(!0===n&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:n,groupId:t});return[(0,s.aZ)(a),(0,o.useCallback)((e=>{if(!a)return;const n=new URLSearchParams(r.location.search);n.set(a,e),r.replace({...r.location,search:n.toString()})}),[a,r])]}function g(e){const{defaultValue:n,queryString:t=!1,groupId:r}=e,a=m(e),[i,l]=(0,o.useState)((()=>function(e){let{defaultValue:n,tabValues:t}=e;if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(n){if(!u({value:n,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${n}" but none of its children has the corresponding value. Available values are: ${t.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return n}const r=t.find((e=>e.default))??t[0];if(!r)throw new Error("Unexpected error: 0 tabValues");return r.value}({defaultValue:n,tabValues:a}))),[s,c]=y({queryString:t,groupId:r}),[p,g]=function(e){let{groupId:n}=e;const t=function(e){return e?`docusaurus.tab.${e}`:null}(n),[r,a]=(0,d.Dv)(t);return[r,(0,o.useCallback)((e=>{t&&a.set(e)}),[t,a])]}({groupId:r}),f=(()=>{const e=s??p;return u({value:e,tabValues:a})?e:null})();(0,o.useLayoutEffect)((()=>{f&&l(f)}),[f]);return{selectedValue:i,selectValue:(0,o.useCallback)((e=>{if(!u({value:e,tabValues:a}))throw new Error(`Can't select invalid tab value=${e}`);l(e),c(e),g(e)}),[c,g,a]),tabValues:a}}var f=t(92303);const h={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};function _(e){let{className:n,block:t,selectedValue:l,selectValue:s,tabValues:c}=e;const d=[],{blockElementScrollPositionUntilNextRender:p}=(0,i.a_)(),m=e=>{const n=e.currentTarget,t=d.indexOf(n),r=c[t].value;r!==l&&(p(n),s(r))},u=e=>{let n=null;switch(e.key){case"Enter":m(e);break;case"ArrowRight":{const t=d.indexOf(e.currentTarget)+1;n=d[t]??d[0];break}case"ArrowLeft":{const t=d.indexOf(e.currentTarget)-1;n=d[t]??d[d.length-1];break}}n?.focus()};return o.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,a.A)("tabs",{"tabs--block":t},n)},c.map((e=>{let{value:n,label:t,attributes:i}=e;return o.createElement("li",(0,r.A)({role:"tab",tabIndex:l===n?0:-1,"aria-selected":l===n,key:n,ref:e=>d.push(e),onKeyDown:u,onClick:m},i,{className:(0,a.A)("tabs__item",h.tabItem,i?.className,{"tabs__item--active":l===n})}),t??n)})))}function v(e){let{lazy:n,children:t,selectedValue:r}=e;const a=(Array.isArray(t)?t:[t]).filter(Boolean);if(n){const e=a.find((e=>e.props.value===r));return e?(0,o.cloneElement)(e,{className:"margin-top--md"}):null}return o.createElement("div",{className:"margin-top--md"},a.map(((e,n)=>(0,o.cloneElement)(e,{key:n,hidden:e.props.value!==r}))))}function b(e){const n=g(e);return o.createElement("div",{className:(0,a.A)("tabs-container",h.tabList)},o.createElement(_,(0,r.A)({},e,n)),o.createElement(v,(0,r.A)({},e,n)))}function I(e){const n=(0,f.A)();return o.createElement(b,(0,r.A)({key:String(n)},e))}},99980:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>_,contentTitle:()=>f,default:()=>x,frontMatter:()=>g,metadata:()=>h,toc:()=>v});var r=t(58168),o=(t(96540),t(15680)),a=t(11470),i=t(19365),l=t(77964);const s='# Import necessary libraries\nimport os\nfrom clarifai.client.model import Model\n\n# Set the Clarifai API key as an environment variable (replace with your actual key)\nos.environ[\'CLARIFAI_PAT\'] = "YOUR_PAT" \n\n# Create a Model object using the model URL from the Clarifai portal\nmodel = Model("Model URL From Portal")\n\n# Set the model version ID (replace with the ID from the Clarifai portal)\nmodel.model_version.id = "Model Version ID From Portal" \n\n# Export the model to the specified path\nmodel.export("path to save .tar file")',c="import onnx  # Library for working with ONNX models\nimport onnxruntime as ort  # Library for running ONNX models\nimport numpy as np  # Library for numerical operations\nimport cv2  # Library for image processing\n\n# Commented-out code for model verification (uncomment if needed)\n# onnx_model = onnx.load(\"model/1/model.onnx\")\n# onnx.checker.check_model(onnx_model)\n\n# Load the input image using OpenCV\ninput_image = cv2.imread('ramen.png')\n\n# Expand the image dimension to match model input requirements \ninput_array = np.expand_dims(input_image, axis=0)\n\n# Swap axes and convert to float32 for potential model input requirements\ninput_array = np.swapaxes(input_array, 1, 3).astype(np.float32)\n\n# Create an inference session using the ONNX model\nort_sess = ort.InferenceSession('model/1/model.onnx') # replace with correct path to onnx model file\n\n# Run inference on the model with the preprocessed input\noutputs = ort_sess.run(None, {'input': input_array})\n\n# Extract the predicted class index from the output\npredicted = outputs[0][0].argmax(0)\n\n# Read class labels from a text file (replace filename if different)\nwith open(f'model/labels.txt') as f:\n    labels = f.readlines()\n\n# Print the predicted class label based on the index and labels list\nprint(labels[predicted])",d="# Import necessary libraries\nimport numpy as np\nimport cv2\nfrom tritonclient.grpc import (\n    InferenceServerClient,  # Client for interacting with Triton server\n    InferInput,  # Represents an input to the model\n    InferRequestedOutput,  # Represents an output requested for inference\n)\n\n# Define model name in our case the name is 'model'\nmodel_name = 'model'\n\n# Connect to Triton server running on localhost \ntriton_client = InferenceServerClient('127.0.0.1:9001')\n\n# Load the specified model onto the Triton server\ntriton_client.load_model(model_name)\n\n\n# Load the input image using OpenCV\ninput_image = cv2.imread('ramen.png') # replace with any image file\n\n# Expand the image dimension to match model input requirements\ninput_array = np.expand_dims(input_image, axis=0)\n\n# Define the model input object with its name, shape, and data type\nmodel_input = InferInput('input', input_array.shape, 'UINT8')\n\n# Set the data for the model input from the NumPy array\nmodel_input.set_data_from_numpy(input_array)\n\n# Run inference on the model with the provided input\nres = triton_client.infer(\n    model_name=model_name,  # Specify the model to use\n    inputs=[model_input],  # List of input objects\n    outputs=[InferRequestedOutput('probs')],  # List of requested outputs\n)\n\n# Read class labels from a text file \nwith open(f'{model_name}/labels.txt') as f:\n    labels = f.readlines()\n\n# Get the index of the class with the highest probability from the output\npredicted_class_index = np.argmax(res.as_numpy('probs'))\n\n# Print the predicted class label based on the index and labels list\nprint(labels[predicted_class_index])",p="Exporting model: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 912M/912M [00:22<00:00, 39.8MB/s]\n2024-04-18 11:44:30 INFO     clarifai.client.model:  Model ID model_classifier model.py:991\n                             with version 78a14f11871a4d5fa9dfa462fc81c1aa                 \n                             exported successfully to                                      \n                             /home/adithyansukumar/work/output/model.tar    ",m="id-ramen",u="root@c1ed01adc0c6:/run# ",y="I0418 10:22:19.527473 16089 server.cc:610] \n+---------+---------------------------------------+---------------------------------------+\n| Backend | Path                                  | Config                                |\n+---------+---------------------------------------+---------------------------------------+\n| python  | /opt/tritonserver/backends/python/lib | {\"cmdline\":{\"auto-complete-config\":\"f |\n|         | triton_python.so                      | alse\",\"min-compute-capability\":\"6.000 |\n|         |                                       | 000\",\"backend-directory\":\"/opt/triton |\n|         |                                       | server/backends\",\"default-max-batch-s |\n|         |                                       | ize\":\"4\"}}                            |\n|         |                                       |                                       |\n+---------+---------------------------------------+---------------------------------------+\n\nI0418 10:22:19.527498 16089 model_lifecycle.cc:264] ModelStates()\nI0418 10:22:19.527539 16089 server.cc:653] \n+-------+---------+--------+\n| Model | Version | Status |\n+-------+---------+--------+\n| model | 1       | READY  |\n+-------+---------+--------+\n\nI0418 10:22:19.652489 16089 metrics.cc:747] Collecting metrics for GPU 0: NVIDIA RTX 6000 Ada Generation\nI0418 10:22:19.652537 16089 metrics.cc:747] Collecting metrics for GPU 1: NVIDIA RTX 6000 Ada Generation\nI0418 10:22:19.652547 16089 metrics.cc:747] Collecting metrics for GPU 2: NVIDIA RTX 6000 Ada Generation\nI0418 10:22:19.652557 16089 metrics.cc:747] Collecting metrics for GPU 3: NVIDIA RTX 6000 Ada Generation\nI0418 10:22:19.652564 16089 metrics.cc:747] Collecting metrics for GPU 4: NVIDIA RTX 6000 Ada Generation\nI0418 10:22:19.652573 16089 metrics.cc:747] Collecting metrics for GPU 5: NVIDIA RTX 6000 Ada Generation\nI0418 10:22:19.652581 16089 metrics.cc:747] Collecting metrics for GPU 6: NVIDIA RTX 6000 Ada Generation\nI0418 10:22:19.652588 16089 metrics.cc:747] Collecting metrics for GPU 7: NVIDIA RTX 6000 Ada Generation\nI0418 10:22:19.653107 16089 metrics.cc:640] Collecting CPU metrics\nI0418 10:22:19.653341 16089 tritonserver.cc:2364] \n+----------------------------------+------------------------------------------------------+\n| Option                           | Value                                                |\n+----------------------------------+------------------------------------------------------+\n| server_id                        | triton                                               |\n| server_version                   | 2.32.0                                               |\n| server_extensions                | classification sequence model_repository model_repos |\n|                                  | itory(unload_dependents) schedule_policy model_confi |\n|                                  | guration system_shared_memory cuda_shared_memory bin |\n|                                  | ary_tensor_data parameters statistics trace logging  |\n| model_repository_path[0]         | /run                                                 |\n| model_control_mode               | MODE_EXPLICIT                                        |\n| startup_models_0                 | model                                                |\n| strict_model_config              | 1                                                    |\n| rate_limit                       | OFF                                                  |\n| pinned_memory_pool_byte_size     | 268435456                                            |\n| cuda_memory_pool_byte_size{0}    | 67108864                                             |\n| cuda_memory_pool_byte_size{1}    | 67108864                                             |\n| cuda_memory_pool_byte_size{2}    | 67108864                                             |\n| cuda_memory_pool_byte_size{3}    | 67108864                                             |\n| cuda_memory_pool_byte_size{4}    | 67108864                                             |\n| cuda_memory_pool_byte_size{5}    | 67108864                                             |\n| cuda_memory_pool_byte_size{6}    | 67108864                                             |\n| cuda_memory_pool_byte_size{7}    | 67108864                                             |\n| min_supported_compute_capability | 6.0                                                  |\n| strict_readiness                 | 1                                                    |\n| exit_timeout                     | 30                                                   |\n| cache_enabled                    | 0                                                    |\n+----------------------------------+------------------------------------------------------+\n\nI0418 10:22:19.654269 16089 grpc_server.cc:4888] === GRPC KeepAlive Options ===\nI0418 10:22:19.654283 16089 grpc_server.cc:4889] keepalive_time_ms: 7200000\nI0418 10:22:19.654288 16089 grpc_server.cc:4891] keepalive_timeout_ms: 20000\nI0418 10:22:19.654293 16089 grpc_server.cc:4893] keepalive_permit_without_calls: 0\nI0418 10:22:19.654299 16089 grpc_server.cc:4895] http2_max_pings_without_data: 2\nI0418 10:22:19.654305 16089 grpc_server.cc:4897] http2_min_recv_ping_interval_without_data_ms: 300000\nI0418 10:22:19.654312 16089 grpc_server.cc:4900] http2_max_ping_strikes: 2\nI0418 10:22:19.654320 16089 grpc_server.cc:4902] ==============================\nI0418 10:22:19.655139 16089 grpc_server.cc:227] Ready for RPC 'Check', 0\nI0418 10:22:19.655203 16089 grpc_server.cc:227] Ready for RPC 'ServerLive', 0\nI0418 10:22:19.655219 16089 grpc_server.cc:227] Ready for RPC 'ServerReady', 0\nI0418 10:22:19.655228 16089 grpc_server.cc:227] Ready for RPC 'ModelReady', 0\nI0418 10:22:19.655237 16089 grpc_server.cc:227] Ready for RPC 'ServerMetadata', 0\nI0418 10:22:19.655246 16089 grpc_server.cc:227] Ready for RPC 'ModelMetadata', 0\nI0418 10:22:19.655257 16089 grpc_server.cc:227] Ready for RPC 'ModelConfig', 0\nI0418 10:22:19.655270 16089 grpc_server.cc:227] Ready for RPC 'SystemSharedMemoryStatus', 0\nI0418 10:22:19.655277 16089 grpc_server.cc:227] Ready for RPC 'SystemSharedMemoryRegister', 0\nI0418 10:22:19.655286 16089 grpc_server.cc:227] Ready for RPC 'SystemSharedMemoryUnregister', 0\nI0418 10:22:19.655293 16089 grpc_server.cc:227] Ready for RPC 'CudaSharedMemoryStatus', 0\nI0418 10:22:19.655299 16089 grpc_server.cc:227] Ready for RPC 'CudaSharedMemoryRegister', 0\nI0418 10:22:19.655307 16089 grpc_server.cc:227] Ready for RPC 'CudaSharedMemoryUnregister', 0\nI0418 10:22:19.655316 16089 grpc_server.cc:227] Ready for RPC 'RepositoryIndex', 0\nI0418 10:22:19.655322 16089 grpc_server.cc:227] Ready for RPC 'RepositoryModelLoad', 0\nI0418 10:22:19.655330 16089 grpc_server.cc:227] Ready for RPC 'RepositoryModelUnload', 0\nI0418 10:22:19.655340 16089 grpc_server.cc:227] Ready for RPC 'ModelStatistics', 0\nI0418 10:22:19.655348 16089 grpc_server.cc:227] Ready for RPC 'Trace', 0\nI0418 10:22:19.655355 16089 grpc_server.cc:227] Ready for RPC 'Logging', 0\nI0418 10:22:19.655371 16089 grpc_server.cc:445] Thread started for CommonHandler\nI0418 10:22:19.655525 16089 grpc_server.cc:3952] New request handler for ModelInferHandler, 0\nI0418 10:22:19.655567 16089 grpc_server.cc:2844] Thread started for ModelInferHandler\nI0418 10:22:19.655706 16089 grpc_server.cc:3952] New request handler for ModelInferHandler, 0\nI0418 10:22:19.655748 16089 grpc_server.cc:2844] Thread started for ModelInferHandler\nI0418 10:22:19.655870 16089 grpc_server.cc:4348] New request handler for ModelStreamInferHandler, 0\nI0418 10:22:19.655901 16089 grpc_server.cc:2844] Thread started for ModelStreamInferHandler\nI0418 10:22:19.655909 16089 grpc_server.cc:4977] Started GRPCInferenceService at 0.0.0.0:9001\nI0418 10:22:19.656156 16089 http_server.cc:3518] Started HTTPService at 0.0.0.0:9000\nI0418 10:22:19.726777 16089 http_server.cc:186] Started Metrics Service at 0.0.0.0:9002\n",g={},f="Model Export",h={unversionedId:"python-sdk/advance-model-operations/model-export",id:"python-sdk/advance-model-operations/model-export",title:"Model Export",description:"Learn how to perform model export using Clarifai Python SDK",source:"@site/docs/python-sdk/advance-model-operations/model-export.md",sourceDirName:"python-sdk/advance-model-operations",slug:"/python-sdk/advance-model-operations/model-export",permalink:"/python-sdk/advance-model-operations/model-export",draft:!1,editUrl:"https://github.com/Clarifai/docs/blob/main/docs/python-sdk/advance-model-operations/model-export.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Model Upload",permalink:"/python-sdk/advance-model-operations/model-upload"},next:{title:"Building RAG Applications",permalink:"/python-sdk/rag"}},_={},v=[{value:"Model Inference Using ONNX",id:"model-inference-using-onnx",level:2},{value:"Deployment Using Nvidia Triton",id:"deployment-using-nvidia-triton",level:2}],b={toc:v},I="wrapper";function x(e){let{components:n,...t}=e;return(0,o.yg)(I,(0,r.A)({},b,t,{components:n,mdxType:"MDXLayout"}),(0,o.yg)("h1",{id:"model-export"},"Model Export"),(0,o.yg)("p",null,(0,o.yg)("strong",{parentName:"p"},"Learn how to perform model export using Clarifai Python SDK")),(0,o.yg)("hr",null),(0,o.yg)("admonition",{type:"info"},(0,o.yg)("p",{parentName:"admonition"},"The ability to export the model is exclusively available to Enterprise users. Learn more ",(0,o.yg)("a",{parentName:"p",href:"https://www.clarifai.com/pricing"},"here"),".")),(0,o.yg)("p",null,"Using the Clarifai Python SDK, you can export the model you have trained on the Clarifai portal into a .tar file by specifying the model URL. This feature allows users to version control their trained models and seamlessly integrate them into different environments. The exported .tar file encapsulates the model architecture, weights, and any additional training artifacts, making it a portable archive for deployment. Overall, the ability to export models via the Clarifai Python SDK empowers users with greater flexibility and control over their machine-learning workflows."),(0,o.yg)(a.A,{mdxType:"Tabs"},(0,o.yg)(i.A,{value:"python",label:"Python",mdxType:"TabItem"},(0,o.yg)(l.A,{className:"language-python",mdxType:"CodeBlock"},s))),(0,o.yg)("details",null,(0,o.yg)("summary",null,"Output"),(0,o.yg)(l.A,{className:"language-python",mdxType:"CodeBlock"},p)),(0,o.yg)("p",null,"Before moving on to deployment, unpack the ",(0,o.yg)("inlineCode",{parentName:"p"},"model.tar")," file to get the required files.\nThe unpacked ",(0,o.yg)("inlineCode",{parentName:"p"},"model.tar")," folder structure will look like this,"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre"},"\u251c\u2500\u2500 model\n   \u251c\u2500\u2500 1\n   \u2502   \u251c\u2500\u2500 lib\n   \u2502   \u251c\u2500\u2500 model.onnx\n   \u2502   \u251c\u2500\u2500 model.py\n   \u251c\u2500\u2500 config.pbtxt\n   \u251c\u2500\u2500 labels.txt\n   \u251c\u2500\u2500 requirements.txt\n   \u251c\u2500\u2500 triton_conda-cp3.8-72f240d2.tar.gz\n   \u2514\u2500\u2500 triton_server_info.proto\n")),(0,o.yg)("h2",{id:"model-inference-using-onnx"},"Model Inference Using ONNX"),(0,o.yg)("p",null,"ONNX inference equips developers and data scientists with a standardized and efficient approach to deploying machine learning models in production environments. It fosters flexibility, simplifies deployment, and offers the potential for performance improvements, making it a valuable tool for unlocking the power of machine-learning models across a wide range of applications. ONNX inference acts as a bridge, enabling the seamless execution of this model across diverse platforms and frameworks that support ONNX. This eliminates the need for repetitive model retraining or framework-specific conversions, resulting in significant time and resource savings."),(0,o.yg)("p",null,"Visit ",(0,o.yg)("a",{parentName:"p",href:"https://onnxruntime.ai/docs/get-started/with-python.html"},"this")," page to learn more about ONNX."),(0,o.yg)("admonition",{type:"note"},(0,o.yg)("p",{parentName:"admonition"},"Install the ",(0,o.yg)("inlineCode",{parentName:"p"},"requirements.txt")," file with ",(0,o.yg)("inlineCode",{parentName:"p"},"pip install requirements.txt"),".")),(0,o.yg)("p",null,"Below is an example of running predictions on a model using ONNX runtime. We are going to use ",(0,o.yg)("inlineCode",{parentName:"p"},"model.onnx")," file we received after unpacking the ",(0,o.yg)("inlineCode",{parentName:"p"},"model.tar")," file."),(0,o.yg)(a.A,{mdxType:"Tabs"},(0,o.yg)(i.A,{value:"python",label:"Python",mdxType:"TabItem"},(0,o.yg)(l.A,{className:"language-python",mdxType:"CodeBlock"},c))),(0,o.yg)("details",null,(0,o.yg)("summary",null,"Output"),(0,o.yg)(l.A,{className:"language-python",mdxType:"CodeBlock"},m)),(0,o.yg)("h2",{id:"deployment-using-nvidia-triton"},"Deployment Using Nvidia Triton"),(0,o.yg)("p",null,"Imagine you've trained powerful machine learning models and want to deploy them efficiently for real-world applications. NVIDIA Triton Inference Server acts as the bridge between your trained models and the real world. It's an open-source software specifically designed to optimize and streamline the process of deploying and running machine learning models for inference tasks."),(0,o.yg)("p",null,"Click here to ",(0,o.yg)("a",{parentName:"p",href:"https://github.com/triton-inference-server/python_backend"},"learn")," more about Nvidia Triton."),(0,o.yg)("p",null,"Before we deploy our model we have to first set up the triton package on our local machine."),(0,o.yg)("admonition",{type:"info"},(0,o.yg)("p",{parentName:"admonition"},"Make sure that you have Docker installed on your system. Follow the steps in this ",(0,o.yg)("a",{parentName:"p",href:"https://docs.docker.com/engine/install"},"page")," to install Docker.")),(0,o.yg)("p",null,"Execute the following command to run the triton inference container in your machine,"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre"},"docker run --shm-size=1g --ulimit memlock=-1 -p 9000:9000 -p 9001:9001 -p 9002:9002 --ulimit stack=67108864 -ti nvcr.io/nvidia/tritonserver:23.03-py3 -v $(pwd):/run\n")),(0,o.yg)("details",null,(0,o.yg)("summary",null,"Output"),(0,o.yg)(l.A,{className:"language-python",mdxType:"CodeBlock"},u)),(0,o.yg)("admonition",{type:"note"},(0,o.yg)("p",{parentName:"admonition"},"Use a common directory to run the container and to extract the ",(0,o.yg)("inlineCode",{parentName:"p"},"model.tar")," file.")),(0,o.yg)("p",null,"Once you are inside the container then execute the following to start the triton server,"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre"},"cd /run\n")),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre"},"tritonserver --model-repository=/run --model-control-mode=explicit --disable-auto-complete-config --backend-config=python3,python-runtime=/usr/bin/python3 --backend-directory=/opt/tritonserver/backends --http-port=9000 --grpc-port=9001 --metrics-port=9002 --log-verbose=5 --load-model=model\n")),(0,o.yg)("p",null,"If you have followed the steps correctly, you should receive an output that looks similar to the one shown here,"),(0,o.yg)("details",null,(0,o.yg)("summary",null,"Output"),(0,o.yg)(l.A,{className:"language-python",mdxType:"CodeBlock"},y)),(0,o.yg)("p",null,"Since the inference server is up and running successfully, let's create an inference script that will communicate with the server and return the prediction.\nBelow is an example inference script that does image classification using the exported model,"),(0,o.yg)(a.A,{mdxType:"Tabs"},(0,o.yg)(i.A,{value:"python",label:"Python",mdxType:"TabItem"},(0,o.yg)(l.A,{className:"language-python",mdxType:"CodeBlock"},d))),(0,o.yg)("br",null),(0,o.yg)("details",null,(0,o.yg)("summary",null,"Output"),(0,o.yg)(l.A,{className:"language-python",mdxType:"CodeBlock"},m)))}x.isMDXComponent=!0}}]);
"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[5336],{90454:(e,i,t)=>{t.r(i),t.d(i,{assets:()=>c,contentTitle:()=>r,default:()=>u,frontMatter:()=>n,metadata:()=>l,toc:()=>h});var a=t(74848),s=t(28453),o=t(47242);const n={description:"How to Evaluate An Image Classification Model",sidebar_position:3},r="How to Evaluate An Image Classification Model",l={id:"tutorials/how-to-evaluate-an-image-classification-model",title:"How to Evaluate An Image Classification Model",description:"How to Evaluate An Image Classification Model",source:"@site/docs/tutorials/how-to-evaluate-an-image-classification-model.mdx",sourceDirName:"tutorials",slug:"/tutorials/how-to-evaluate-an-image-classification-model",permalink:"/tutorials/how-to-evaluate-an-image-classification-model",draft:!1,unlisted:!1,editUrl:"https://github.com/Clarifai/docs/blob/main/docs/tutorials/how-to-evaluate-an-image-classification-model.mdx",tags:[],version:"current",sidebarPosition:3,frontMatter:{description:"How to Evaluate An Image Classification Model",sidebar_position:3},sidebar:"tutorialSidebar",previous:{title:"Add AI to a Node.js Web App",permalink:"/tutorials/node-js-tutorial"},next:{title:"Image classification vs detection vs segmentation",permalink:"/tutorials/image-classification-detection-segmentation"}},c={},h=[{value:"Accuracy",id:"accuracy",level:2},{value:"Precision &amp; Recall",id:"precision--recall",level:2},{value:"Precision-Recall Curve",id:"precision-recall-curve",level:2},{value:"F1-Score",id:"f1-score",level:2},{value:"ROC Curve",id:"roc-curve",level:2},{value:"AUC Score",id:"auc-score",level:2},{value:"Confusion Matrix",id:"confusion-matrix",level:2}];function d(e){const i={h1:"h1",h2:"h2",p:"p",strong:"strong",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(i.h1,{id:"how-to-evaluate-an-image-classification-model",children:"How to Evaluate An Image Classification Model"}),"\n",(0,a.jsx)(i.p,{children:(0,a.jsx)(i.strong,{children:"by Minhajul Hoque"})}),"\n",(0,a.jsx)("hr",{}),"\n",(0,a.jsx)(o.A,{caption:"Image Classification Example",src:"img/tutorial/how-to-evaluate-an-image-classification-model/image_classification_example.webp"}),"\n",(0,a.jsx)(i.h1,{id:"what-is-an-image-classification-model",children:"What is an Image Classification Model?"}),"\n",(0,a.jsx)(i.p,{children:"An image classification model is a type of computer program that has learned to recognize and sort images into different categories or labels."}),"\n",(0,a.jsxs)(i.p,{children:["However, it\u2019s different from a detection model, which can identify ",(0,a.jsx)(i.strong,{children:"where"})," specific objects are ",(0,a.jsx)(i.strong,{children:"located"})," within an image. Instead, a classification model only tells you whether it believes a certain category or label is ",(0,a.jsx)(i.strong,{children:"present"})," in the image, but not precisely where in the image it is located."]}),"\n",(0,a.jsx)(i.h1,{id:"what-metrics-can-we-use",children:"What Metrics Can We Use?"}),"\n",(0,a.jsx)(i.p,{children:"When we\u2019re trying to determine how well an image classification model is performing, there are different ways to measure its success. In this article, we\u2019ll explore a few of the most commonly used metrics in the industry and look at the advantages and disadvantages of each. This will help us better understand how to assess the model\u2019s performance."}),"\n",(0,a.jsx)(i.h2,{id:"accuracy",children:"Accuracy"}),"\n",(0,a.jsx)(i.p,{children:"Accuracy is like a teacher grading a multiple-choice test. If you get most of the questions right, your grade is high, and if you get most of them wrong, your grade is low."}),"\n",(0,a.jsx)(i.p,{children:"In the world of image classification, the \u201cstudent\u201d is the model, and the \u201cquestions\u201d are the images. The model looks at the image and predicts what it is \u2014 if it gets it right, it gets a point, and if it gets it wrong, it gets a big red X."}),"\n",(0,a.jsx)(i.p,{children:"So, accuracy is all about counting up the number of points and dividing it by the total number of questions to get a score between 0 and 1. If your model gets a high accuracy score, it means it\u2019s acing the test and making mostly correct predictions. But if it gets a low accuracy score, it means it\u2019s failing the test and making mostly incorrect predictions."}),"\n",(0,a.jsx)(o.A,{caption:"Accuracy Formula",src:"img/tutorial/how-to-evaluate-an-image-classification-model/accuracy_formula.webp"}),"\n",(0,a.jsxs)(i.p,{children:["However, like a bad student, the model can ",(0,a.jsx)(i.strong,{children:"\u201ccheat\u201d"}),". Accuracy can be misleading when working with imbalanced datasets, where the number of samples in each class is ",(0,a.jsx)(i.strong,{children:"significantly different"}),". For example, if you are training a spam classifier on a dataset with 99% non-spam and 1% spam emails, a model that always predicts non-spam will have a high accuracy of 99%. This can give a false impression of a well-performing model. Therefore, in such cases, it\u2019s important to consider other metrics to better evaluate the performance of the model."]}),"\n",(0,a.jsx)(i.h2,{id:"precision--recall",children:"Precision & Recall"}),"\n",(0,a.jsx)(i.p,{children:"Precision and recall are like Batman and the Joker, but for computer vision metrics. They are constantly fighting for supremacy, and when one wins, the other loses. It\u2019s like a game of tug-of-war, except the rope is your data and the two teams are precision and recall."}),"\n",(0,a.jsx)(o.A,{caption:"Precision & Recall Formulas",src:"img/tutorial/how-to-evaluate-an-image-classification-model/precision_recall_formulas.webp"}),"\n",(0,a.jsx)(i.p,{children:"Precision is like a sniper rifle \u2014 if your model is precise, it hits the target with accuracy, and you won\u2019t end up shooting innocent bystanders (many false positives). However, if your model is not precise, it\u2019s like using a shotgun \u2014 you\u2019ll hit everything in sight, including things you didn\u2019t mean to hit. In use cases like spam classification, you want to be like the sniper and have high precision, so that you don\u2019t end up marking important emails as spam."}),"\n",(0,a.jsx)(i.p,{children:"Recall is like a treasure hunter \u2014 if your model has high recall, it\u2019s able to find all the hidden gems and bring them to the surface (find all the true positives). However, if your model has low recall, it\u2019s like a forgetful pirate who misses half of the treasure. In use cases like cancer classification, you want to be like the treasure hunter and have high recall, so that you don\u2019t miss any cases where cancer is present."}),"\n",(0,a.jsx)(i.h2,{id:"precision-recall-curve",children:"Precision-Recall Curve"}),"\n",(0,a.jsx)(i.p,{children:"Imagine you\u2019re trying to catch fish with a net. The precision-recall curve is like trying to find the perfect size of the net so that you catch the most fish without letting any escape."}),"\n",(0,a.jsx)(i.p,{children:"The net\u2019s size represents the threshold for making predictions. If you have a really small net (high threshold), you\u2019ll catch very few fish, but you can be pretty sure that the ones you catch are the right ones. This is high precision, but low recall. On the other hand, if you have a really big net (low threshold), you\u2019ll catch a lot of fish, but you\u2019ll also catch a lot of other things that aren\u2019t fish. This is high recall, but low precision."}),"\n",(0,a.jsx)(i.p,{children:"So, the precision-recall curve is all about finding the sweet spot \u2014 the perfect size of the net that catches as many fish as possible without catching any non-fish items. It\u2019s a trade-off between precision and recall, and it helps you visualize the performance of your model across different thresholds."}),"\n",(0,a.jsx)(o.A,{caption:"Precision-Recall Curve",src:"img/tutorial/how-to-evaluate-an-image-classification-model/precision_recall_curve.webp"}),"\n",(0,a.jsx)(i.h2,{id:"f1-score",children:"F1-Score"}),"\n",(0,a.jsx)(i.p,{children:"The F1 score is like an average grade for your model, but instead of getting an A or a B, you get a number between 0 and 1 that tells you how well your model is doing."}),"\n",(0,a.jsx)(i.p,{children:"Think of it like a pizza party. If you have a bunch of friends who like different toppings, you want to order a pizza that everyone will like. The F1 score is like trying to find the perfect pizza that everyone will enjoy."}),"\n",(0,a.jsx)(i.p,{children:"The \u201cF\u201d stands for \u201cf-measure,\u201d which is a combination of precision and recall. Precision is like making sure the pizza only has the toppings your friends like, while recall is like making sure you don\u2019t forget any of the toppings that they like. The F1 score combines these two measures into one metric to give you an overall idea of how well your model is performing."}),"\n",(0,a.jsx)(o.A,{caption:"F1-Score Formula",src:"img/tutorial/how-to-evaluate-an-image-classification-model/f1_score_formula.webp"}),"\n",(0,a.jsx)(i.h2,{id:"roc-curve",children:"ROC Curve"}),"\n",(0,a.jsx)(i.p,{children:"The ROC (Receiver Operating Characteristic) probability curve is a tool that helps us evaluate the performance of binary classification models. Think of it like a musical performance \u2014 the ROC curve tells us how well our model is singing its tune."}),"\n",(0,a.jsx)(i.p,{children:"In a ROC curve, we plot the True Positive Rate (TPR) against the False Positive Rate (FPR) at various thresholds. The TPR measures how many of the positive cases our model correctly identifies, while the FPR measures how many of the negative cases are falsely identified as positive. Essentially, the ROC curve is a plot of the model\u2019s sensitivity (how well it detects the positive cases) versus its specificity (how well it rejects the negative cases)."}),"\n",(0,a.jsx)(o.A,{caption:"ROC Curve Example",src:"img/tutorial/how-to-evaluate-an-image-classification-model/roc_curve_example.webp"}),"\n",(0,a.jsx)(i.p,{children:"ROC curve is not necessarily a metric, but it is a useful tool to evaluate your model and to calculate the AUC score. We can plot the ROC curve for a multi-class use case by plotting N number of ROC curves using the One vs All method."}),"\n",(0,a.jsx)(i.h2,{id:"auc-score",children:"AUC Score"}),"\n",(0,a.jsxs)(i.p,{children:["The AUC, or area under the ROC curve, is a metric used to measure the performance of a binary classifier, such as a spam filter or fraud detector. It\u2019s a numerical value between 0 and 1 that represents the ",(0,a.jsx)(i.strong,{children:"overall performance of the classifier and its degree of separability"}),", where 1 means the classifier is perfect at distinguishing between two classes, and 0.5 means it\u2019s no better than a coin flip."]}),"\n",(0,a.jsx)(i.p,{children:"Think of it like a doctor who is trying to diagnose patients with a particular disease. A high AUC score means the doctor is doing a great job of distinguishing between patients who have the disease and those who don\u2019t. Just like a good doctor, a good model with a high AUC score can accurately diagnose positive and negative samples and help make the right decisions based on that diagnosis."}),"\n",(0,a.jsx)(o.A,{caption:"ROC and AUC Example",src:"img/tutorial/how-to-evaluate-an-image-classification-model/roc_auc_example.webp"}),"\n",(0,a.jsx)(i.h2,{id:"confusion-matrix",children:"Confusion Matrix"}),"\n",(0,a.jsx)(i.p,{children:"A confusion matrix is a useful representation of your model results. You can think of it as a report card for your model\u2019s performance. It lets you see where your model is struggling and where it\u2019s acing the test. Imagine you\u2019re a teacher grading a class of students. You might notice that some students confuse the concepts of multiplication and division, just like how our model is confusing dogs and cats in the CIFAR10 dataset. It\u2019s important to identify these confusions to help our model learn and improve."}),"\n",(0,a.jsx)(o.A,{caption:"Confusion Matrix for The CIFAR10 Dataset",src:"img/tutorial/how-to-evaluate-an-image-classification-model/cifar10_confusion_matrix.webp"}),"\n",(0,a.jsx)(i.p,{children:"It is not necessarily a metric, but a useful tool that many data scientists use."})]})}function u(e={}){const{wrapper:i}={...(0,s.R)(),...e.components};return i?(0,a.jsx)(i,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},47242:(e,i,t)=>{t.d(i,{A:()=>o});t(96540);var a=t(86025),s=t(74848);function o(e){let{src:i,caption:t}=e;return(0,s.jsxs)("figure",{style:{border:"1px dashed rgba(0, 0, 0, .1)",padding:20,borderRadius:"15px"},children:[(0,s.jsx)("img",{src:(0,a.Ay)(i),alt:t}),(0,s.jsx)("hr",{style:{margin:"5px",backgroundColor:"rgba(0, 0, 0, .2)"}}),(0,s.jsx)("figcaption",{style:{marginTop:"0px"},children:`${t}`})]})}},28453:(e,i,t)=>{t.d(i,{R:()=>n,x:()=>r});var a=t(96540);const s={},o=a.createContext(s);function n(e){const i=a.useContext(o);return a.useMemo((function(){return"function"==typeof e?e(i):{...i,...e}}),[i,e])}function r(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:n(e.components),a.createElement(o.Provider,{value:i},e.children)}}}]);
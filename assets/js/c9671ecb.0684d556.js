"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[7334],{3905:(e,t,n)=>{n.d(t,{Zo:()=>u,kt:()=>g});var a=n(67294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,i=function(e,t){if(null==e)return{};var n,a,i={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var l=a.createContext({}),m=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},u=function(e){var t=m(e.components);return a.createElement(l.Provider,{value:t},e.children)},c="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},p=a.forwardRef((function(e,t){var n=e.components,i=e.mdxType,r=e.originalType,l=e.parentName,u=s(e,["components","mdxType","originalType","parentName"]),c=m(n),p=i,g=c["".concat(l,".").concat(p)]||c[p]||d[p]||r;return n?a.createElement(g,o(o({ref:t},u),{},{components:n})):a.createElement(g,o({ref:t},u))}));function g(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var r=n.length,o=new Array(r);o[0]=p;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[c]="string"==typeof e?e:i,o[1]=s;for(var m=2;m<r;m++)o[m]=n[m];return a.createElement.apply(null,o)}return a.createElement.apply(null,n)}p.displayName="MDXCreateElement"},81077:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>r,metadata:()=>s,toc:()=>m});var a=n(87462),i=(n(67294),n(3905));const r={description:"Learn about our visual segmenter model type",sidebar_position:4,keywords:["visual segmenter","image segmentation","visual segmentation models","AI image segmentation","machine learning image segmenter","computer vision image segmentation","visual segmentation AI","AI model for image segmentation","deep learning image segmentation"]},o="Visual Segmenter",s={unversionedId:"portal-guide/model/model-types/visual-segmenter",id:"portal-guide/model/model-types/visual-segmenter",title:"Visual Segmenter",description:"Learn about our visual segmenter model type",source:"@site/docs/portal-guide/model/model-types/visual-segmenter.md",sourceDirName:"portal-guide/model/model-types",slug:"/portal-guide/model/model-types/visual-segmenter",permalink:"/portal-guide/model/model-types/visual-segmenter",draft:!1,editUrl:"https://github.com/Clarifai/docs/blob/main/docs/portal-guide/model/model-types/visual-segmenter.md",tags:[],version:"current",sidebarPosition:4,frontMatter:{description:"Learn about our visual segmenter model type",sidebar_position:4,keywords:["visual segmenter","image segmentation","visual segmentation models","AI image segmentation","machine learning image segmenter","computer vision image segmentation","visual segmentation AI","AI model for image segmentation","deep learning image segmentation"]},sidebar:"tutorialSidebar",previous:{title:"Visual Detector",permalink:"/portal-guide/model/model-types/visual-detector"},next:{title:"Visual Anomaly",permalink:"/portal-guide/model/model-types/visual-anomaly"}},l={},m=[{value:"Example use case",id:"example-use-case",level:2}],u={toc:m},c="wrapper";function d(e){let{components:t,...n}=e;return(0,i.kt)(c,(0,a.Z)({},u,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"visual-segmenter"},"Visual Segmenter"),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Learn about our visual segmenter model type")),(0,i.kt)("hr",null),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Input"),": Images and videos"),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Output"),": Regions"),(0,i.kt)("p",null,"Visual segmenter, also known as semantic segmentation, is a type of deep fine-tuned model used in image analysis and understanding tasks."),(0,i.kt)("p",null,"It aims to achieve a fine-grained understanding of the content within an image by associating each pixel with a particular class label. This is more detailed than traditional object detection, which typically identifies bounding boxes around objects."),(0,i.kt)("p",null,"The primary task of a visual segmenter model is twofold:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Semantic segmentation"),": The model segments an input image into per-pixel masks, where each mask corresponds to a particular object or region of interest. Each pixel in the image is assigned a label that indicates the class of the object it belongs to. This process effectively divides the image into segments based on the objects or regions present in it."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Object classification or labeling"),": Once the semantic segmentation is done, the model can then classify the segmented objects or regions into specific categories, descriptive words, or topics. This classification step involves assigning labels or tags to the segmented areas, indicating what each segment represents.")),(0,i.kt)("admonition",{type:"info"},(0,i.kt)("p",{parentName:"admonition"},"The visual segmenter model type also comes with various ",(0,i.kt)("a",{parentName:"p",href:"https://docs.clarifai.com/portal-guide/model/deep-training/visual-segmenter-templates"},"templates")," that give you the control to choose the specific architecture used by your neural network, as well as define a set of hyperparameters you can use to fine-tune the way your model learns.")),(0,i.kt)("p",null,"Visual Segmenter models are used in a wide variety of applications, including:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Self-driving cars"),": Visual Segmenter models can be used to identify objects in the road and surroundings, such as other cars, pedestrians, and traffic signs. This information can be used to help self-driving cars navigate safely."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Robotics"),": Visual Segmenter models can be used to help robots interact with the physical world. For example, a robot could use a Visual Segmenter model to identify objects in its environment and then plan a path to avoid those objects."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Image editing"),": Visual segmenter models can assist in automatic background removal, object manipulation, and other image editing tasks."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Augmented reality"),": In AR applications, semantic segmentation helps in understanding the scene and integrating virtual objects more realistically.")),(0,i.kt)("p",null,"You may choose a visual segmenter model type in cases where:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Your application requires high accuracy, and you're willing to sacrifice speed and ease of use. These models tend to be computationally intensive due to their per-pixel processing. "),(0,i.kt)("li",{parentName:"ul"},'You need a segmentation model to learn new features not recognized by the existing Clarifai models, especially if your application requires a detailed understanding of the content within an image at a per-pixel level. In that case, you may need to "deep fine-tune" your custom segmenter model and integrate it directly within your ',(0,i.kt)("a",{parentName:"li",href:"https://docs.clarifai.com/portal-guide/workflows/"},"workflows"),"."),(0,i.kt)("li",{parentName:"ul"},"You have a custom-tailored dataset, accurate labels, and the expertise and time to fine-tune models.")),(0,i.kt)("h2",{id:"example-use-case"},"Example use case"),(0,i.kt)("p",null,'Given an image of a street scene, a visual segmenter model could segment the image into per-pixel masks representing cars, pedestrians, buildings, roads, and other objects. Then, for each segmented area, the model could classify the objects into categories like "sedan," "person," "skyscraper," and "asphalt road.\u201d'))}d.isMDXComponent=!0}}]);
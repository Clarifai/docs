"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[239],{11470:(e,n,t)=>{t.d(n,{A:()=>I});var o=t(96540),l=t(18215),a=t(17559),r=t(23104),s=t(56347),i=t(205),c=t(57485),d=t(31682),h=t(70679);function u(e){return o.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,o.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function m(e){const{values:n,children:t}=e;return(0,o.useMemo)(()=>{const e=n??function(e){return u(e).map(({props:{value:e,label:n,attributes:t,default:o}})=>({value:e,label:n,attributes:t,default:o}))}(t);return function(e){const n=(0,d.XI)(e,(e,n)=>e.value===n.value);if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[n,t])}function p({value:e,tabValues:n}){return n.some(n=>n.value===e)}function f({queryString:e=!1,groupId:n}){const t=(0,s.W6)(),l=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,c.aZ)(l),(0,o.useCallback)(e=>{if(!l)return;const n=new URLSearchParams(t.location.search);n.set(l,e),t.replace({...t.location,search:n.toString()})},[l,t])]}function g(e){const{defaultValue:n,queryString:t=!1,groupId:l}=e,a=m(e),[r,s]=(0,o.useState)(()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!p({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const t=n.find(e=>e.default)??n[0];if(!t)throw new Error("Unexpected error: 0 tabValues");return t.value}({defaultValue:n,tabValues:a})),[c,d]=f({queryString:t,groupId:l}),[u,g]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[t,l]=(0,h.Dv)(n);return[t,(0,o.useCallback)(e=>{n&&l.set(e)},[n,l])]}({groupId:l}),x=(()=>{const e=c??u;return p({value:e,tabValues:a})?e:null})();(0,i.A)(()=>{x&&s(x)},[x]);return{selectedValue:r,selectValue:(0,o.useCallback)(e=>{if(!p({value:e,tabValues:a}))throw new Error(`Can't select invalid tab value=${e}`);s(e),d(e),g(e)},[d,g,a]),tabValues:a}}var x=t(92303);const y={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var j=t(74848);function b({className:e,block:n,selectedValue:t,selectValue:o,tabValues:a}){const s=[],{blockElementScrollPositionUntilNextRender:i}=(0,r.a_)(),c=e=>{const n=e.currentTarget,l=s.indexOf(n),r=a[l].value;r!==t&&(i(n),o(r))},d=e=>{let n=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const t=s.indexOf(e.currentTarget)+1;n=s[t]??s[0];break}case"ArrowLeft":{const t=s.indexOf(e.currentTarget)-1;n=s[t]??s[s.length-1];break}}n?.focus()};return(0,j.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,l.A)("tabs",{"tabs--block":n},e),children:a.map(({value:e,label:n,attributes:o})=>(0,j.jsx)("li",{role:"tab",tabIndex:t===e?0:-1,"aria-selected":t===e,ref:e=>{s.push(e)},onKeyDown:d,onClick:c,...o,className:(0,l.A)("tabs__item",y.tabItem,o?.className,{"tabs__item--active":t===e}),children:n??e},e))})}function _({lazy:e,children:n,selectedValue:t}){const a=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=a.find(e=>e.props.value===t);return e?(0,o.cloneElement)(e,{className:(0,l.A)("margin-top--md",e.props.className)}):null}return(0,j.jsx)("div",{className:"margin-top--md",children:a.map((e,n)=>(0,o.cloneElement)(e,{key:n,hidden:e.props.value!==t}))})}function O(e){const n=g(e);return(0,j.jsxs)("div",{className:(0,l.A)(a.G.tabs.container,"tabs-container",y.tabList),children:[(0,j.jsx)(b,{...n,...e}),(0,j.jsx)(_,{...n,...e})]})}function I(e){const n=(0,x.A)();return(0,j.jsx)(O,{...e,children:u(e.children)},String(n))}},19365:(e,n,t)=>{t.d(n,{A:()=>r});t(96540);var o=t(18215);const l={tabItem:"tabItem_Ymn6"};var a=t(74848);function r({children:e,hidden:n,className:t}){return(0,a.jsx)("div",{role:"tabpanel",className:(0,o.A)(l.tabItem,t),hidden:n,children:e})}},32809:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>g,contentTitle:()=>f,default:()=>j,frontMatter:()=>p,metadata:()=>o,toc:()=>x});const o=JSON.parse('{"id":"compute/toolkits/ollama","title":"Ollama","description":"Download and run Ollama models locally and expose them via a public API","source":"@site/docs/compute/toolkits/ollama.md","sourceDirName":"compute/toolkits","slug":"/compute/toolkits/ollama","permalink":"/compute/toolkits/ollama","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"description":"Download and run Ollama models locally and expose them via a public API","sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"OpenAI","permalink":"/compute/toolkits/openai"},"next":{"title":"Hugging Face","permalink":"/compute/toolkits/hf"}}');var l=t(74848),a=t(28453),r=t(11470),s=t(19365),i=t(88149);const c="clarifai model init --toolkit ollama\n[INFO] 15:58:15.587351 Parsed GitHub repository: owner=Clarifai, repo=runners-examples, branch=ollama, folder_path= |  thread=8800297152 \n[INFO] 15:58:16.827976 Files to be downloaded are:\n1. 1/model.py\n2. config.yaml\n3. requirements.txt |  thread=8800297152 \nPress Enter to continue... \n[INFO] 15:58:24.007602 Initializing model from GitHub repository: https://github.com/Clarifai/runners-examples |  thread=8800297152 \n[INFO] 15:58:31.263139 Successfully cloned repository from https://github.com/Clarifai/runners-examples (branch: ollama) |  thread=8800297152 \n[INFO] 15:58:31.270469 Model initialization complete with GitHub repository |  thread=8800297152 \n[INFO] 15:58:31.270527 Next steps: |  thread=8800297152 \n[INFO] 15:58:31.270560 1. Review the model configuration |  thread=8800297152 \n[INFO] 15:58:31.270584 2. Install any required dependencies manually |  thread=8800297152 \n[INFO] 15:58:31.270608 3. Test the model locally using 'clarifai model local-test' |  thread=8800297152 \n",d='clarifai model local-runner\n[INFO] 16:01:28.904230 > Checking local runner requirements... |  thread=8800297152 \n[INFO] 16:01:28.928129 Checking 2 dependencies... |  thread=8800297152 \n[INFO] 16:01:28.928672 \u2705 All 2 dependencies are installed! |  thread=8800297152 \n[INFO] 16:01:28.928886 Verifying Ollama installation... |  thread=8800297152 \n[INFO] 16:01:29.004234 > Verifying local runner setup... |  thread=8800297152 \n[INFO] 16:01:29.004427 Current context: default |  thread=8800297152 \n[INFO] 16:01:29.004463 Current user_id: alfrick |  thread=8800297152 \n[INFO] 16:01:29.004490 Current PAT: d6570**** |  thread=8800297152 \n[INFO] 16:01:29.005945 Current compute_cluster_id: local-runner-compute-cluster |  thread=8800297152 \n[WARNING] 16:01:35.936440 Failed to get compute cluster with ID \'local-runner-compute-cluster\':\ncode: CONN_DOES_NOT_EXIST\ndescription: "Resource does not exist"\ndetails: "ComputeCluster with ID \\\'local-runner-compute-cluster\\\' not found. Check your request fields."\nreq_id: "sdk-python-11.8.2-75ca9226003a4b34a770885b119d5814"\n |  thread=8800297152 \nCompute cluster not found. Do you want to create a new compute cluster alfrick/local-runner-compute-cluster? (y/n): y\n[INFO] 16:01:58.382096 Compute Cluster with ID \'local-runner-compute-cluster\' is created:\ncode: SUCCESS\ndescription: "Ok"\nreq_id: "sdk-python-11.8.2-a0474b8ba93c4e069804500a188694db"\n |  thread=8800297152 \n[INFO] 16:01:58.391571 Current nodepool_id: local-runner-nodepool |  thread=8800297152 \n[WARNING] 16:02:00.633687 Failed to get nodepool with ID \'local-runner-nodepool\':\ncode: CONN_DOES_NOT_EXIST\ndescription: "Resource does not exist"\ndetails: "Nodepool not found. Check your request fields."\nreq_id: "sdk-python-11.8.2-62149d46e7104d35bcb2a36546710329"\n |  thread=8800297152 \nNodepool not found. Do you want to create a new nodepool alfrick/local-runner-compute-cluster/local-runner-nodepool? (y/n): y\n[INFO] 16:02:03.909005 Nodepool with ID \'local-runner-nodepool\' is created:\ncode: SUCCESS\ndescription: "Ok"\nreq_id: "sdk-python-11.8.2-05836a431940495b94b9c3691f6c6d4d"\n |  thread=8800297152 \n[INFO] 16:02:03.921694 Current app_id: local-runner-app |  thread=8800297152 \n[INFO] 16:02:04.203774 Current model_id: local-runner-model |  thread=8800297152 \n[WARNING] 16:02:10.933734 Attempting to patch latest version: 9d38bb9398944de4bdef699835f17ec9 |  thread=8800297152 \n[INFO] 16:02:14.195999 Successfully patched version 9d38bb9398944de4bdef699835f17ec9 |  thread=8800297152 \n[INFO] 16:02:14.197924 Current model version 9d38bb9398944de4bdef699835f17ec9 |  thread=8800297152 \n[WARNING] 16:02:18.679567 Failed to get runner with ID \'f3c46913186449ba99dedd38123d47a3\':\ncode: CONN_DOES_NOT_EXIST\ndescription: "Resource does not exist"\ndetails: "Runner not found. Check your request fields."\nreq_id: "sdk-python-11.8.2-0b3b241c76ef429290c8c54a318f2f21"\n |  thread=8800297152 \n[INFO] 16:02:18.679913 Creating the local runner tying this \'alfrick/local-runner-app/models/local-runner-model\' model (version: 9d38bb9398944de4bdef699835f17ec9) to the \'alfrick/local-runner-compute-cluster/local-runner-nodepool\' nodepool. |  thread=8800297152 \n[INFO] 16:02:19.757117 Runner with ID \'2f84d7194ee8464fad485fd058663fe5\' is created:\ncode: SUCCESS\ndescription: "Ok"\nreq_id: "sdk-python-11.8.2-8ac525dc13ec47629213f0283e89c6a7"\n |  thread=8800297152 \n[INFO] 16:02:19.765198 Current runner_id: 2f84d7194ee8464fad485fd058663fe5 |  thread=8800297152 \n[WARNING] 16:02:20.331980 Failed to get deployment with ID local-runner-deployment:\ncode: CONN_DOES_NOT_EXIST\ndescription: "Resource does not exist"\ndetails: "Deployment with ID \\\'local-runner-deployment\\\' not found. Check your request fields."\nreq_id: "sdk-python-11.8.2-864f55e6bc554614894ee031cc30cdb9"\n |  thread=8800297152 \nDeployment not found. Do you want to create a new deployment alfrick/local-runner-compute-cluster/local-runner-nodepool/local-runner-deployment? (y/n): y\n[INFO] 16:02:25.016935 Deployment with ID \'local-runner-deployment\' is created:\ncode: SUCCESS\ndescription: "Ok"\nreq_id: "sdk-python-11.8.2-b6bf9aad5aa545f8a041113608fc9365"\n |  thread=8800297152 \n[INFO] 16:02:25.024579 Current deployment_id: local-runner-deployment |  thread=8800297152 \n[INFO] 16:02:25.027108 Current model section of config.yaml: {\'app_id\': \'local-dev-runner-app\', \'id\': \'local-dev-model\', \'model_type_id\': \'text-to-text\', \'user_id\': \'clarifai-user-id\'} |  thread=8800297152 \nDo you want to backup config.yaml to config.yaml.bk then update the config.yaml with the new model information? (y/n): y\n[INFO] 16:02:27.407724 Checking 2 dependencies... |  thread=8800297152 \n[INFO] 16:02:27.408555 \u2705 All 2 dependencies are installed! |  thread=8800297152 \n[INFO] 16:02:27.451117 Customizing Ollama model with provided parameters... |  thread=8800297152 \n[INFO] 16:02:27.451785 \u2705 Starting local runner... |  thread=8800297152 \n[INFO] 16:02:27.451852 No secrets path configured, running without secrets |  thread=8800297152 \n[INFO] 16:02:30.020253 Detected OpenAI chat completions for Clarifai model streaming - validating stream_options... |  thread=8800297152 \n[INFO] 16:02:30.027464 Starting Ollama server in the host: 127.0.0.1:23333 |  thread=8800297152 \n[INFO] 16:02:30.040882 Model llama3.2 pulled successfully. |  thread=8800297152 \n[INFO] 16:02:30.041191 Ollama server started successfully on 127.0.0.1:23333 |  thread=8800297152 \n[INFO] 16:02:30.096053 Ollama model loaded successfully: llama3.2 |  thread=8800297152 \n[INFO] 16:02:30.096133 ModelServer initialized successfully |  thread=8800297152 \nException in thread Thread-1 (serve_health):\nTraceback (most recent call last):\n  File "/opt/homebrew/Cellar/python@3.12/3.12.11/Frameworks/Python.framework/Versions/3.12/lib/python3.12/threading.py", line 1075, in _bootstrap_inner\n[INFO] 16:02:30.100802 \u2705 Your model is running locally and is ready for requests from the API...\n |  thread=8800297152 \n[INFO] 16:02:30.100873 > Code Snippet: To call your model via the API, use this code snippet:\n\nimport os\n\nfrom openai import OpenAI\n\nclient = OpenAI(\n    base_url="https://api.clarifai.com/v2/ext/openai/v1",\n    api_key=os.environ[\'CLARIFAI_PAT\'],\n)\n\nresponse = client.chat.completions.create(\n    model="https://clarifai.com/alfrick/local-runner-app/models/local-runner-model",\n    messages=[\n        {"role": "system", "content": "Talk like a pirate."},\n        {\n            "role": "user",\n            "content": "How do I check if a Python object is an instance of a class?",\n        },\n    ],\n    temperature=1.0,\n    stream=False,  # stream=True also works, just iterator over the response\n)\nprint(response)\n |  thread=8800297152 \n[INFO] 16:02:30.100944 > Playground:   To chat with your model, visit: https://clarifai.com/playground?model=local-runner-model__9d38bb9398944de4bdef699835f17ec9&user_id=alfrick&app_id=local-runner-app\n |  thread=8800297152 \n    self.run()\n[INFO] 16:02:30.101006 > API URL:      To call your model via the API, use this model URL: https://clarifai.com/alfrick/local-runner-app/models/local-runner-model\n |  thread=8800297152 \n  File "/opt/homebrew/Cellar/python@3.12/3.12.11/Frameworks/Python.framework/Versions/3.12/lib/python3.12/threading.py", line 1012, in run\n[INFO] 16:02:30.101070 Press CTRL+C to stop the runner.\n |  thread=8800297152 \n[INFO] 16:02:30.101117 Starting 32 threads... |  thread=8800297152 \n',h='from typing import Any, Dict, List, Iterator\nimport os\nimport json\nimport subprocess\nfrom clarifai.runners.models.openai_class import OpenAIModelClass\nfrom clarifai.utils.logging import logger\nfrom clarifai.runners.utils.data_utils import Param\nfrom clarifai.runners.utils.data_types import Image\nfrom clarifai.runners.utils.openai_convertor import build_openai_messages\n\nfrom openai import OpenAI\n\n# Set default host\nif not os.environ.get(\'OLLAMA_HOST\'):\n  PORT = \'23333\'  # Default port for Ollama server\n  os.environ["OLLAMA_HOST"] = f\'127.0.0.1:{PORT}\' # Change host name if you want to run of different host.\nOLLAMA_HOST = os.environ.get(\'OLLAMA_HOST\')\n\nif not os.environ.get(\'OLLAMA_CONTEXT_LENGTH\'):\n    # Set default context length if not set\n    context_length = \'8192\'\n    os.environ["OLLAMA_CONTEXT_LENGTH"] = context_length  # Default context length for Llama 3.2, You can change this for larger context.\nOLLAMA_CONTEXT_LENGTH = os.environ.get(\'OLLAMA_CONTEXT_LENGTH\')\n\nVERBOSE_OLLAMA = False\n\ndef run_ollama_server(model_name: str = \'llama3.2\'):\n    """\n    start the Ollama server.\n    """\n    from clarifai.runners.utils.model_utils import execute_shell_command, terminate_process\n\n    try:\n        logger.info(f"Starting Ollama server in the host: {OLLAMA_HOST}")\n        start_process = execute_shell_command("ollama serve",\n                                                  stdout=None if VERBOSE_OLLAMA else subprocess.DEVNULL,\n                                                  stderr=subprocess.STDOUT if VERBOSE_OLLAMA else subprocess.DEVNULL)\n        if start_process:\n            pull_model=execute_shell_command(f"ollama pull {model_name}",\n                                             stdout=None if VERBOSE_OLLAMA else subprocess.DEVNULL,\n                                             stderr=subprocess.STDOUT if VERBOSE_OLLAMA else subprocess.DEVNULL)\n            logger.info(f"Model {model_name} pulled successfully.")\n            logger.info(f"Ollama server started successfully on {OLLAMA_HOST}")\n\n    except Exception as e:\n        logger.error(f"Error starting Ollama server: {e}")\n        if \'start_process\' in locals():\n            terminate_process(start_process)\n        raise RuntimeError(f"Failed to start Ollama server: {e}")\n\n# Check if Image has content before building messages\ndef has_image_content(image: Image) -> bool:\n    """Check if Image object has either bytes or URL."""\n    return bool(getattr(image, \'url\', None) or getattr(image, \'bytes\', None))\n\n\nclass OllamaModelClass(OpenAIModelClass):\n\n    client =  True\n    model = True\n\n    def load_model(self):\n        """\n        Load the Ollama model.\n        """\n        #set the model name here or via OLLAMA_MODEL_NAME\n        self.model = os.environ.get("OLLAMA_MODEL_NAME", \'llama3.2\') #You can change any model name here which is supported by ollama.\n\n        #start ollama server\n        run_ollama_server(model_name=self.model)\n\n        self.client = OpenAI(\n                api_key="notset",\n                base_url= f"http://{OLLAMA_HOST}/v1")\n\n        logger.info(f"Ollama model loaded successfully: {self.model}")\n\n\n    @OpenAIModelClass.method\n    def predict(self,\n                prompt: str,\n                image: Image = None,\n                images: List[Image] = None,\n                chat_history: List[dict] = None,\n                tools: List[dict] = None,\n                tool_choice: str = None,\n                max_tokens: int = Param(default=2048, description="The maximum number of tokens to generate. Shorter token lengths will provide faster performance.", ),\n                temperature: float = Param(default=0.7, description="A decimal number that determines the degree of randomness in the response", ),\n                top_p: float = Param(default=0.95, description="An alternative to sampling with temperature, where the model considers the results of the tokens with top_p probability mass."),\n                ) -> str:\n      """\n      This method is used to predict the response for the given prompt and chat history using the model and tools.\n      """\n      if tools is not None and tool_choice is None:\n          tool_choice = "auto"\n\n      img_content = image if has_image_content(image) else None\n\n      messages = build_openai_messages(prompt=prompt, image=img_content, images=images, messages=chat_history)\n      response = self.client.chat.completions.create(\n          model=self.model,\n          messages=messages,\n          tools=tools,\n          tool_choice=tool_choice,\n          max_completion_tokens=max_tokens,\n          temperature=temperature,\n          top_p=top_p)\n\n      if response.usage is not None:\n            self.set_output_context(prompt_tokens=response.usage.prompt_tokens,\n                                    completion_tokens=response.usage.completion_tokens)\n            if len(response.choices) == 0:\n                # still need to send the usage back.\n                return ""\n\n      if response.choices[0] and response.choices[0].message.tool_calls:\n        # If the response contains tool calls, return as a string\n        tool_calls = response.choices[0].message.tool_calls\n        tool_calls_json = json.dumps([tc.to_dict() for tc in tool_calls], indent=2)\n        return tool_calls_json\n      else:\n        # Otherwise, return the content of the first choice\n        return response.choices[0].message.content\n\n\n    @OpenAIModelClass.method\n    def generate(self,\n                prompt: str,\n                image: Image = None,\n                images: List[Image] = None,\n                chat_history: List[dict] = None,\n                tools: List[dict] = None,\n                tool_choice: str = None,\n                max_tokens: int = Param(default=2048, description="The maximum number of tokens to generate. Shorter token lengths will provide faster performance.", ),\n                temperature: float = Param(default=0.7, description="A decimal number that determines the degree of randomness in the response", ),\n                top_p: float = Param(default=0.95, description="An alternative to sampling with temperature, where the model considers the results of the tokens with top_p probability mass.")) -> Iterator[str]:\n      """\n      This method is used to stream generated text tokens from a prompt + optional chat history and tools.\n      """\n      if tools is not None and tool_choice is None:\n          tool_choice = "auto"\n\n      img_content = image if has_image_content(image) else None\n\n      messages = build_openai_messages(prompt=prompt, image=img_content, images=images, messages=chat_history)\n      for chunk in self.client.chat.completions.create(\n            model=self.model,\n            messages=messages,\n            tools=tools,\n            tool_choice=tool_choice,\n            max_completion_tokens=max_tokens,\n            temperature=temperature,\n            top_p=top_p,\n            stream=True,\n            stream_options={"include_usage": True}\n            ):\n            if chunk.usage is not None:\n                if chunk.usage.prompt_tokens or chunk.usage.completion_tokens:\n                    self.set_output_context(prompt_tokens=chunk.usage.prompt_tokens, completion_tokens=chunk.usage.completion_tokens)\n                if len(chunk.choices) == 0: # still need to send the usage back.\n                    yield ""\n\n            if chunk.choices:\n                if chunk.choices[0].delta.tool_calls:\n                # If the response contains tool calls, return the first one as a string\n                    import json\n                    tool_calls = chunk.choices[0].delta.tool_calls\n                    tool_calls_json = [tc.to_dict() for tc in tool_calls]\n                    # Convert to JSON string\n                    json_string = json.dumps(tool_calls_json, indent=2)\n                    # Yield the JSON string\n                    yield json_string\n                else:\n                    # Otherwise, return the content of the first choice\n                    text = (chunk.choices[0].delta.content\n                            if (chunk and chunk.choices[0].delta.content) is not None else \'\')\n                    yield text\n',u="clarifai\nopenai\n",m="model:\n  app_id: local-dev-runner-app\n  id: local-dev-model\n  model_type_id: text-to-text\n  user_id: your-user-id\nbuild_info:\n  python_version: '3.12'\ninference_compute_info:\n  cpu_limit: '3'\n  cpu_memory: 14Gi\n  num_accelerators: 0\ntoolkit:\n  provider: ollama\n",p={description:"Download and run Ollama models locally and expose them via a public API",sidebar_position:2},f="Ollama",g={},x=[{value:"Step 1: Perform Prerequisites",id:"step-1-perform-prerequisites",level:2},{value:"Install Ollama",id:"install-ollama",level:3},{value:"Get User ID and PAT",id:"get-user-id-and-pat",level:3},{value:"Install the Clarifai CLI",id:"install-the-clarifai-cli",level:3},{value:"Install OpenAI",id:"install-openai",level:3},{value:"Step 2: Initialize a Model From Ollama",id:"step-2-initialize-a-model-from-ollama",level:2},{value:"<code>model.py</code>",id:"modelpy",level:3},{value:"<code>requirements.txt</code>",id:"requirementstxt",level:3},{value:"<code>config.yaml</code>",id:"configyaml",level:3},{value:"Step 3: Log In to Clarifai",id:"step-3-log-in-to-clarifai",level:2},{value:"Step 4: Start Your Local Runner",id:"step-4-start-your-local-runner",level:2},{value:"Step 5: Test Your Runner",id:"step-5-test-your-runner",level:2},{value:"Additional Examples",id:"additional-examples",level:2}];function y(e){const n={a:"a",admonition:"admonition",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components},{Details:o}=n;return o||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(n.header,{children:(0,l.jsx)(n.h1,{id:"ollama",children:"Ollama"})}),"\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.strong,{children:"Download and run Ollama models locally and expose them via a public API"})}),"\n",(0,l.jsx)("hr",{}),"\n",(0,l.jsx)(n.p,{children:"Ollama is an open-source tool that allows you to download, run, and manage large language models (LLMs) directly on your local machine."}),"\n",(0,l.jsx)(n.p,{children:"When combined with Clarifai\u2019s Local Runners, it enables you to run Ollama models on your machine, expose them securely via a public URL, and tap into Clarifai\u2019s powerful platform \u2014 all while keeping the speed, privacy, and control of local deployment."}),"\n",(0,l.jsxs)(n.blockquote,{children:["\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Note:"})," After initializing a model using the Ollama toolkit, you can ",(0,l.jsx)(n.a,{href:"https://docs.clarifai.com/compute/upload/#step-4-upload-the-model-to-clarifai",children:"upload"})," it to Clarifai to leverage the platform\u2019s capabilities."]}),"\n"]}),"\n","\n","\n",(0,l.jsx)(n.h2,{id:"step-1-perform-prerequisites",children:"Step 1: Perform Prerequisites"}),"\n",(0,l.jsx)(n.h3,{id:"install-ollama",children:"Install Ollama"}),"\n",(0,l.jsxs)(n.p,{children:["Go to the ",(0,l.jsx)(n.a,{href:"https://ollama.com/download",children:"Ollama website"})," and choose the appropriate installer for your system (macOS, Windows, or Linux)."]}),"\n",(0,l.jsxs)(n.blockquote,{children:["\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Note:"})," If you're using Windows, make sure to restart your machine after installing Ollama to ensure that the updated environment variables are properly applied."]}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"get-user-id-and-pat",children:"Get User ID and PAT"}),"\n",(0,l.jsxs)(n.p,{children:["Start by ",(0,l.jsx)(n.a,{href:"https://clarifai.com/login",children:"logging in"})," to your existing Clarifai account or ",(0,l.jsx)(n.a,{href:"https://clarifai.com/signup",children:"signing up"})," for a new one. Once logged in, you'll need the following credentials for setup:"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"User ID"})," \u2013 In the collapsible left sidebar, select ",(0,l.jsx)(n.strong,{children:"Settings"})," and choose ",(0,l.jsx)(n.strong,{children:"Account"})," from the dropdown list. Then, locate your user ID."]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Personal Access Token (PAT)"})," \u2013 From the same ",(0,l.jsx)(n.strong,{children:"Settings"})," option, choose ",(0,l.jsx)(n.strong,{children:"Secrets"})," to generate or copy your ",(0,l.jsx)(n.a,{href:"https://docs.clarifai.com/control/authentication/pat",children:"PAT"}),". This token is used to authenticate your connection with the Clarifai platform."]}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.p,{children:["You can then set the PAT as an environment variable using ",(0,l.jsx)(n.code,{children:"CLARIFAI_PAT"}),"."]}),"\n",(0,l.jsxs)(r.A,{groupId:"code",children:[(0,l.jsx)(s.A,{value:"bash",label:"Unix-Like Systems",children:(0,l.jsx)(i.A,{className:"language-bash",children:"export CLARIFAI_PAT=YOUR_PERSONAL_ACCESS_TOKEN_HERE"})}),(0,l.jsx)(s.A,{value:"bash2",label:"Windows",children:(0,l.jsx)(i.A,{className:"language-bash",children:"set CLARIFAI_PAT=YOUR_PERSONAL_ACCESS_TOKEN_HERE"})})]}),"\n",(0,l.jsx)(n.h3,{id:"install-the-clarifai-cli",children:"Install the Clarifai CLI"}),"\n",(0,l.jsxs)(n.p,{children:["Install the latest version of the ",(0,l.jsx)(n.a,{href:"https://docs.clarifai.com/sdk/cli",children:"Clarifai CLI"}),", which includes built-in support for Local Runners."]}),"\n",(0,l.jsx)(r.A,{groupId:"code",children:(0,l.jsx)(s.A,{value:"bash",label:"Bash",children:(0,l.jsx)(i.A,{className:"language-bash",children:"pip install --upgrade clarifai"})})}),"\n",(0,l.jsxs)(n.blockquote,{children:["\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Note:"})," You must have ",(0,l.jsx)(n.strong,{children:(0,l.jsx)(n.a,{href:"https://docs.clarifai.com/resources/api-overview/python-sdk#python-requirements",children:"Python 3.11 or 3.12"})}),"  installed to use Local Runners."]}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"install-openai",children:"Install OpenAI"}),"\n",(0,l.jsxs)(n.p,{children:["Install the ",(0,l.jsx)(n.code,{children:"openai"})," package, which is required when performing inference with models using the ",(0,l.jsx)(n.a,{href:"https://docs.clarifai.com/compute/inference/#predict-with-openai-compatible-format",children:"OpenAI-compatible format"}),"."]}),"\n",(0,l.jsx)(r.A,{groupId:"code",children:(0,l.jsx)(s.A,{value:"bash",label:"Bash",children:(0,l.jsx)(i.A,{className:"language-bash",children:"pip install openai"})})}),"\n",(0,l.jsx)(n.h2,{id:"step-2-initialize-a-model-from-ollama",children:"Step 2: Initialize a Model From Ollama"}),"\n",(0,l.jsx)(n.p,{children:"You can use the Clarifai CLI to download and initialize any model available in the Ollama library directly into your local environment."}),"\n",(0,l.jsxs)(n.p,{children:["For example, here's how to initialize the ",(0,l.jsx)(n.a,{href:"https://ollama.com/library/llama3.2",children:(0,l.jsx)(n.code,{children:"llama3.2"})})," model in your current directory:"]}),"\n",(0,l.jsx)(r.A,{groupId:"code",children:(0,l.jsx)(s.A,{value:"bash",label:"CLI",children:(0,l.jsx)(i.A,{className:"language-bash",children:"clarifai model init --toolkit ollama"})})}),"\n",(0,l.jsxs)(n.blockquote,{children:["\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Note:"})," You can initialize a model in a specific location by passing a ",(0,l.jsx)(n.a,{href:"https://docs.clarifai.com/resources/api-overview/cli#clarifai-model-init",children:(0,l.jsx)(n.code,{children:"MODEL_PATH"})}),"."]}),"\n"]}),"\n",(0,l.jsxs)(o,{children:[(0,l.jsx)("summary",{children:"Example Output"}),(0,l.jsx)(i.A,{className:"language-text",children:c})]}),"\n",(0,l.jsxs)(n.admonition,{title:"customize initialization",type:"tip",children:[(0,l.jsx)(n.p,{children:"You can customize model initialization from the Ollama library using the Clarifai CLI with the following options:"}),(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"--model-name"})," \u2013 Name of the Ollama model to use (default: ",(0,l.jsx)(n.code,{children:"llama3.2"}),"). This lets you specify any model from the Ollama library. Example: ",(0,l.jsx)(n.code,{children:"clarifai model init --toolkit ollama --model-name gpt-oss:20b"})]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"--port"})," \u2013 Port to run the model on (default: ",(0,l.jsx)(n.code,{children:"23333"}),")"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"--context-length"})," \u2013 Context window size for the model in tokens (default: ",(0,l.jsx)(n.code,{children:"8192"}),")"]}),"\n"]})]}),"\n",(0,l.jsxs)(n.admonition,{title:"tip",type:"note",children:[(0,l.jsxs)(n.p,{children:["You can use Ollama commands such as ",(0,l.jsx)(n.code,{children:"ollama list"})," to list downloaded models and ",(0,l.jsx)(n.code,{children:"ollama rm"})," to remove a model. Run ",(0,l.jsx)(n.code,{children:"ollama --help"})," to see the full list of available commands."]}),(0,l.jsxs)(n.blockquote,{children:["\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Note:"})," Some models are very large and may demand significant memory or GPU resources. Before initializing, make sure your machine has enough compute capacity to load and run the model smoothly."]}),"\n"]})]}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(n.code,{children:"init"})," command will create a new model directory structure that is compatible with the Clarifai platform. You can customize or optimize the generated model by modifying the files as needed."]}),"\n",(0,l.jsx)(n.p,{children:"The generated structure includes:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:"\u251c\u2500\u2500 1/\n\u2502   \u2514\u2500\u2500 model.py\n\u251c\u2500\u2500 requirements.txt\n\u2514\u2500\u2500 config.yaml\n"})}),"\n",(0,l.jsx)(n.h3,{id:"modelpy",children:(0,l.jsx)(n.code,{children:"model.py"})}),"\n",(0,l.jsxs)(o,{children:[(0,l.jsx)("summary",{children:"Example: model.py"}),(0,l.jsx)(i.A,{className:"language-text",children:h})]}),"\n",(0,l.jsxs)(n.p,{children:["This is the ",(0,l.jsx)(n.a,{href:"https://docs.clarifai.com/compute/upload/#prepare-modelpy",children:"main Python file"})," that defines how your Ollama-based model is loaded, served, and used for inference. It extends Clarifai\u2019s ",(0,l.jsx)(n.code,{children:"OpenAIModelClass"}),", which provides a unified interface for OpenAI-compatible models."]}),"\n",(0,l.jsx)(n.p,{children:"It has these key components:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["The environment setup section defines default values, such as the Ollama host (",(0,l.jsx)(n.code,{children:"OLLAMA_HOST"}),") and context length (",(0,l.jsx)(n.code,{children:"OLLAMA_CONTEXT_LENGTH"}),")."]}),"\n",(0,l.jsxs)(n.li,{children:["The ",(0,l.jsx)(n.code,{children:"run_ollama_server()"})," function starts the Ollama server and automatically pulls the specified model (for example, ",(0,l.jsx)(n.code,{children:"llama3.2"}),") if it\u2019s not already available locally."]}),"\n",(0,l.jsxs)(n.li,{children:["The ",(0,l.jsx)(n.code,{children:"OllamaModelClass"})," itself implements the following methods:","\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"load_model()"})," \u2014 Loads and initializes the Ollama model while setting up the local OpenAI-compatible client;"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"predict()"})," \u2014 Generates text completions or tool calls from prompts, optionally handling images or chat history;"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"generate()"})," \u2014 Streams tokens in real time, which is ideal for chat-like or extended responses."]}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["Additionally, helper utilities, such as ",(0,l.jsx)(n.code,{children:"has_image_content()"}),", validate image inputs before constructing OpenAI-compatible messages, and logging utilities track token usage and manage inference context."]}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"requirementstxt",children:(0,l.jsx)(n.code,{children:"requirements.txt"})}),"\n",(0,l.jsxs)(o,{children:[(0,l.jsx)("summary",{children:"Example: requirements.txt"}),(0,l.jsx)(i.A,{className:"language-text",children:u})]}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(n.code,{children:"requirements.txt"})," file lists all the Python packages required for your local runner environment. If you haven\u2019t installed them yet, run the following command to install the dependencies:"]}),"\n",(0,l.jsx)(r.A,{groupId:"code",children:(0,l.jsx)(s.A,{value:"bash",label:"Bash",children:(0,l.jsx)(i.A,{className:"language-bash",children:"pip install -r requirements.txt"})})}),"\n",(0,l.jsx)(n.h3,{id:"configyaml",children:(0,l.jsx)(n.code,{children:"config.yaml"})}),"\n",(0,l.jsxs)(o,{children:[(0,l.jsx)("summary",{children:"Example: config.yaml"}),(0,l.jsx)(i.A,{className:"language-text",children:m})]}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(n.code,{children:"config.yaml"})," file is what tells Clarifai how to run your model \u2014 including where it belongs, which runtime to use, and how much compute it should consume."]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["It identifies where your model will run on the Clarifai platform through parameters like ",(0,l.jsx)(n.code,{children:"app_id"}),", ",(0,l.jsx)(n.code,{children:"id"})," (any model name you choose), ",(0,l.jsx)(n.code,{children:"model_type_id"}),", and ",(0,l.jsx)(n.code,{children:"user_id"})," (set by default from your ",(0,l.jsx)(n.a,{href:"https://docs.clarifai.com/resources/api-overview/cli#clarifai-config",children:"active context"}),")."]}),"\n",(0,l.jsxs)(n.li,{children:["It defines compatibility details under ",(0,l.jsx)(n.code,{children:"build_info"})," \u2014 such as the Python version to use \u2014 and resource allocation details through ",(0,l.jsx)(n.code,{children:"inference_compute_info"}),", which sets CPU, memory, and accelerator requirements."]}),"\n",(0,l.jsxs)(n.li,{children:["The ",(0,l.jsx)(n.code,{children:"toolkit"})," field indicates the type of runtime provider, informing Clarifai which backend framework to use for execution."]}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"step-3-log-in-to-clarifai",children:"Step 3: Log In to Clarifai"}),"\n",(0,l.jsx)(n.p,{children:"Use the following command to log in to the Clarifai platform to create a configuration context and establish a connection:"}),"\n",(0,l.jsx)(r.A,{groupId:"code",children:(0,l.jsx)(s.A,{value:"bash",label:"CLI",children:(0,l.jsx)(i.A,{className:"language-bash",children:"clarifai login"})})}),"\n",(0,l.jsx)(n.p,{children:"After running the command, you'll be prompted to provide a few details for authentication:"}),"\n",(0,l.jsx)(r.A,{groupId:"code",children:(0,l.jsx)(s.A,{value:"bash",label:"CLI",children:(0,l.jsx)(i.A,{className:"language-bash",children:(0,l.jsx)(n.p,{children:'context name (default: "default"):\nuser id:\npersonal access token value (default: "ENVVAR" to get our env var rather than config):'})})})}),"\n",(0,l.jsx)(n.p,{children:"Here\u2019s what each field means:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Context name"})," \u2013 You can assign a custom name to this configuration context, or simply press Enter to use the default name, ",(0,l.jsx)(n.code,{children:'"default"'}),". This is useful if you manage multiple environments or configurations."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"User ID"})," \u2013 Enter your Clarifai user ID."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Personal Access Token (PAT)"})," \u2013 Paste your Clarifai PAT here. If you've already set the ",(0,l.jsx)(n.code,{children:"CLARIFAI_PAT"})," environment variable, you can just press Enter to use it automatically."]}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"step-4-start-your-local-runner",children:"Step 4: Start Your Local Runner"}),"\n",(0,l.jsx)(n.p,{children:"Start a local runner using the following command:"}),"\n",(0,l.jsx)(r.A,{groupId:"code",children:(0,l.jsx)(s.A,{value:"bash",label:"CLI",children:(0,l.jsx)(i.A,{className:"language-bash",children:"clarifai model local-runner"})})}),"\n",(0,l.jsx)(n.p,{children:"If the necessary context configurations aren\u2019t detected, the CLI will guide you through creating them using default values."}),"\n",(0,l.jsxs)(n.p,{children:["This setup ensures all required components \u2014 such as compute clusters, nodepools, and deployments \u2014 are properly included in your configuration context, which are described ",(0,l.jsx)(n.a,{href:"/compute/toolkits/#step-2-create-a-context-optional",children:"here"}),". Simply review each prompt and confirm to proceed."]}),"\n",(0,l.jsxs)(n.blockquote,{children:["\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Note"}),": Use the ",(0,l.jsx)(n.code,{children:"--suppress-toolkit-logs"})," option to show detailed logs from the Ollama server, which is helpful for debugging: ",(0,l.jsx)(n.code,{children:"clarifai model local-runner --suppress-toolkit-logs"}),"."]}),"\n"]}),"\n",(0,l.jsxs)(o,{children:[(0,l.jsx)("summary",{children:"Example Output"}),(0,l.jsx)(i.A,{className:"language-text",children:d})]}),"\n",(0,l.jsx)(n.h2,{id:"step-5-test-your-runner",children:"Step 5: Test Your Runner"}),"\n",(0,l.jsx)(n.p,{children:"When the local runner starts, it displays a public URL where your model is hosted and provides a sample client code snippet for quick testing."}),"\n",(0,l.jsxs)(n.blockquote,{children:["\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Note:"})," Pulling a model from Ollama may take some time depending on your machine\u2019s resources, but once the download finishes, you can run the snippet in a separate terminal within the same directory to get the model\u2019s response."]}),"\n"]}),"\n",(0,l.jsx)(n.p,{children:"Below is an example snippet for running inference using the OpenAI-compatible format:"}),"\n",(0,l.jsx)(r.A,{groupId:"code",children:(0,l.jsx)(s.A,{value:"python",label:"Python",children:(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import os\n\nfrom openai import OpenAI\n\nclient = OpenAI(\n    base_url="https://api.clarifai.com/v2/ext/openai/v1",\n    api_key=os.environ[\'CLARIFAI_PAT\'],\n)\n\nresponse = client.chat.completions.create(\n    model="https://clarifai.com/<user-id>/local-runner-app/models/local-runner-model",\n    messages=[\n        {"role": "system", "content": "Talk like a pirate."},\n        {\n            "role": "user",\n            "content": "How do I check if a Python object is an instance of a class?",\n        },\n    ],\n    temperature=1.0,\n    stream=False,  # stream=True also works, just iterator over the response\n)\nprint(response)\n'})})})}),"\n",(0,l.jsxs)(n.p,{children:["The terminal also shows a link to the ",(0,l.jsx)(n.a,{href:"https://docs.clarifai.com/getting-started/quickstart-playground",children:"AI Playground"}),", which you can copy to interact with the model directly."]}),"\n",(0,l.jsxs)(n.p,{children:["Alternatively, while your runner is active in the terminal, you can open the ",(0,l.jsx)(n.strong,{children:(0,l.jsx)(n.a,{href:"https://clarifai.com/compute/runners",children:"Runners"})})," dashboard on the Clarifai platform, locate your runner in the table, and select ",(0,l.jsx)(n.strong,{children:"Open in Playground"})," from the three-dot menu to start chatting with the model."]}),"\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.img,{src:t(72140).A+"",width:"4200",height:"1700"})}),"\n",(0,l.jsx)(n.p,{children:"When you're done, just close the terminal running the local runner to shut it down."}),"\n",(0,l.jsx)(n.h2,{id:"additional-examples",children:"Additional Examples"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"https://github.com/ollama/ollama-python/tree/main/examples",children:"More examples of calling Ollama models"})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"https://docs.clarifai.com/compute/inference/clarifai/api",children:"Clarifai-specific inference examples with Ollama models"})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"https://github.com/Clarifai/runners-examples/tree/main/local-runners/ollama-model-upload",children:"Example for running Ollama models locally with Clarifai\u2019s Local Runners"})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"https://www.youtube.com/watch?v=TfS2p8LZYBE",children:"YouTube video on running OpenAI\u2019s open-source GPT-OSS-20B model locally with Ollama"})}),"\n"]})]})}function j(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,l.jsx)(n,{...e,children:(0,l.jsx)(y,{...e})}):y(e)}},72140:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/runners-dashboard-ollama-efa680670a0005e6eb41d84ad88b3f2d.png"}}]);
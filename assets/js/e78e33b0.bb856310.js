"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[610],{15680:(e,t,a)=>{a.d(t,{xA:()=>c,yg:()=>h});var o=a(96540);function n(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,o)}return a}function l(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){n(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,o,n=function(e,t){if(null==e)return{};var a,o,n={},i=Object.keys(e);for(o=0;o<i.length;o++)a=i[o],t.indexOf(a)>=0||(n[a]=e[a]);return n}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(o=0;o<i.length;o++)a=i[o],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(n[a]=e[a])}return n}var r=o.createContext({}),g=function(e){var t=o.useContext(r),a=t;return e&&(a="function"==typeof e?e(t):l(l({},t),e)),a},c=function(e){var t=g(e.components);return o.createElement(r.Provider,{value:t},e.children)},u="mdxType",p={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},d=o.forwardRef((function(e,t){var a=e.components,n=e.mdxType,i=e.originalType,r=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),u=g(a),d=n,h=u["".concat(r,".").concat(d)]||u[d]||p[d]||i;return a?o.createElement(h,l(l({ref:t},c),{},{components:a})):o.createElement(h,l({ref:t},c))}));function h(e,t){var a=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var i=a.length,l=new Array(i);l[0]=d;var s={};for(var r in t)hasOwnProperty.call(t,r)&&(s[r]=t[r]);s.originalType=e,s[u]="string"==typeof e?e:n,l[1]=s;for(var g=2;g<i;g++)l[g]=a[g];return o.createElement.apply(null,l)}return o.createElement.apply(null,a)}d.displayName="MDXCreateElement"},47685:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>r,contentTitle:()=>l,default:()=>p,frontMatter:()=>i,metadata:()=>s,toc:()=>g});var o=a(58168),n=(a(96540),a(15680));const i={description:"Learn about the labeling tools available in Scribe",sidebar_position:4},l="Labeling Tasks Tools",s={unversionedId:"portal-guide/annotate/labeling-tools",id:"portal-guide/annotate/labeling-tools",title:"Labeling Tasks Tools",description:"Learn about the labeling tools available in Scribe",source:"@site/docs/portal-guide/annotate/labeling-tools.md",sourceDirName:"portal-guide/annotate",slug:"/portal-guide/annotate/labeling-tools",permalink:"/portal-guide/annotate/labeling-tools",draft:!1,editUrl:"https://github.com/Clarifai/docs/blob/main/docs/portal-guide/annotate/labeling-tools.md",tags:[],version:"current",sidebarPosition:4,frontMatter:{description:"Learn about the labeling tools available in Scribe",sidebar_position:4},sidebar:"tutorialSidebar",previous:{title:"Label Types \u2014 Input-Viewer",permalink:"/portal-guide/annotate/label-types"},next:{title:"AI-Assisted Labeling \u2014 Input-Viewer",permalink:"/portal-guide/annotate/ai-assist"}},r={},g=[{value:"Labeling Tools",id:"labeling-tools",level:2},{value:"View Metadata",id:"view-metadata",level:3},{value:"Brightness",id:"brightness",level:3},{value:"Saturation",id:"saturation",level:3},{value:"Invert Color",id:"invert-color",level:3},{value:"Zoom",id:"zoom",level:3},{value:"Pan",id:"pan",level:3},{value:"Show Hotkeys",id:"show-hotkeys",level:3},{value:"View and Edit Labels",id:"view-and-edit-labels",level:3},{value:"How to Label Image Inputs",id:"how-to-label-image-inputs",level:2},{value:"Classification Labeling",id:"classification-labeling",level:3},{value:"Detection Labeling",id:"detection-labeling",level:3},{value:"Segmentation Labeling Using Polygons",id:"segmentation-labeling-using-polygons",level:3},{value:"How to Label Video Inputs",id:"how-to-label-video-inputs",level:2},{value:"How to Label Text Inputs",id:"how-to-label-text-inputs",level:2}],c={toc:g},u="wrapper";function p(e){let{components:t,...i}=e;return(0,n.yg)(u,(0,o.A)({},c,i,{components:t,mdxType:"MDXLayout"}),(0,n.yg)("h1",{id:"labeling-tasks-tools"},"Labeling Tasks Tools"),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Learn about the labeling tools available in Scribe")),(0,n.yg)("hr",null),(0,n.yg)("div",{style:{position:"relative",width:"100%",overflow:"hidden","padding-top":"56.25%"}},(0,n.yg)("iframe",{width:"900",height:"500",style:{position:"absolute",top:"0",left:"0",bottom:"0",right:"0",width:"100%",height:"100%"},src:"https://www.youtube.com/embed/KLoziNCUKFQ",title:"11 - Detection labeling task",frameborder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share",allowfullscreen:!0})),(0,n.yg)("br",null),(0,n.yg)("br",null),(0,n.yg)("p",null,"The Scribe Labeling Tasks platform provides special tools for working with images, videos, and texts for classification, detection, and segmentation labeling tasks. With the tools, you can annotate your inputs faster and more conveniently. "),(0,n.yg)("p",null,"After successfully ",(0,n.yg)("a",{parentName:"p",href:"https://docs.clarifai.com/portal-guide/annotate/create-a-task"},"creating a labeling task"),", it will appear on the ",(0,n.yg)("strong",{parentName:"p"},"Tasks")," listing page. To begin working on the task, click the blue ",(0,n.yg)("strong",{parentName:"p"},"LABEL")," button. "),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"label button",src:a(81560).A,width:"1889",height:"735"})),(0,n.yg)("p",null,"You\u2019ll be redirected to the ",(0,n.yg)("strong",{parentName:"p"},"Labeling Tasks")," screen, where you can begin annotating your inputs. At the upper-right section of the screen, you'll find a variety of tools you can use to simplify the annotation process."),(0,n.yg)("p",null,"On the right sidebar, you can find a list of the ",(0,n.yg)("a",{parentName:"p",href:"https://docs.clarifai.com/portal-guide/concepts/create-get-update-delete"},"concepts")," you specified when you created a new labeling task. You can use them to label your inputs. "),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"labeling tools",src:a(72403).A,width:"1895",height:"845"})),(0,n.yg)("h2",{id:"labeling-tools"},"Labeling Tools"),(0,n.yg)("p",null,"Let's talk about each of the tools. "),(0,n.yg)("admonition",{title:"make the most of the tools",type:"warning"},(0,n.yg)("ul",{parentName:"admonition"},(0,n.yg)("li",{parentName:"ul"},"After selecting a labeling tool and using the slider that pops up to adjust your input, you can simply click the ",(0,n.yg)("strong",{parentName:"li"},"Reset")," button next to the slider to revert to its initial version."),(0,n.yg)("li",{parentName:"ul"},"You can combine multiple tools as appropriate to make adjustments to your input."),(0,n.yg)("li",{parentName:"ul"},"Clicking a button for the second time will deactivate or remove the highlight effect associated with that button. "))),(0,n.yg)("h3",{id:"view-metadata"},"View Metadata"),(0,n.yg)("p",null,"The metadata button allows you to toggle and view the metadata available in your inputs. "),(0,n.yg)("h3",{id:"brightness"},"Brightness"),(0,n.yg)("p",null,"The brightness button allows you to adjust the visibility of your inputs. "),(0,n.yg)("h3",{id:"saturation"},"Saturation"),(0,n.yg)("p",null,"The saturation button allows you to adjust the hue or saturation intensity of your inputs."),(0,n.yg)("h3",{id:"invert-color"},"Invert Color"),(0,n.yg)("p",null,"The invert color button allows you to apply transformation to your inputs. When clicked, it reverses the colors within the input, turning light areas dark, and vice versa. "),(0,n.yg)("h3",{id:"zoom"},"Zoom"),(0,n.yg)("p",null,"The zoom button allows you to change the level of magnification or the size of the view for your inputs. "),(0,n.yg)("h3",{id:"pan"},"Pan"),(0,n.yg)("p",null,"The pan button allows you to move the visible areas of your inputs so that you can inspect specific regions closely."),(0,n.yg)("h3",{id:"show-hotkeys"},"Show Hotkeys"),(0,n.yg)("p",null,"The show hotkeys button allows you to display the keyboard shortcuts available for each annotation tool. "),(0,n.yg)("p",null,"These are the hotkeys you can use:"),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"display hotkeys",src:a(5290).A,width:"1911",height:"837"})),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"B")," \u2014 brightness"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"S")," \u2014 saturation"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"I")," \u2014 invert color"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"P")," \u2014 pan"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"/")," \u2014 filter concepts"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Enter")," \u2014 submit a label")),(0,n.yg)("h3",{id:"view-and-edit-labels"},"View and Edit Labels"),(0,n.yg)("p",null,"You can view and edit previously submitted labels while working on a task. There is an input carousel tool at the bottom of the labeler screen that allows you to go back and review inputs after you have submitted them."),(0,n.yg)("p",null,"This functionality provides a convenient mechanism to revisit and edit previously submitted labeled inputs. If you realize you made a mistake while labeling, you can easily go back and review your own work. "),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"View and Edit Labels",src:a(72444).A,width:"1920",height:"813"})),(0,n.yg)("h2",{id:"how-to-label-image-inputs"},"How to Label Image Inputs"),(0,n.yg)("h3",{id:"classification-labeling"},"Classification Labeling"),(0,n.yg)("admonition",{type:"note"},(0,n.yg)("p",{parentName:"admonition"},"To perform classification labeling, you'll need to specify ",(0,n.yg)("strong",{parentName:"p"},"Classification")," as the ",(0,n.yg)("a",{parentName:"p",href:"https://docs.clarifai.com/portal-guide/annotate/create-a-task#modeling-objective"},"label type")," when setting up your labeling task. ")),(0,n.yg)("p",null,"With the classification label type, you can provide annotations for an entire image or a single frame of video. On the right sidebar of the Scribe Labeling Tasks platform, you can find a list of the ",(0,n.yg)("a",{parentName:"p",href:"https://docs.clarifai.com/portal-guide/annotate/create-a-task#concepts"},"concepts")," you specified when you created a new labeling task. "),(0,n.yg)("p",null,"To classify an image, just click the tick button next to the concept you want to label the image with. "),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"classify an image",src:a(79489).A,width:"1904",height:"862"})),(0,n.yg)("p",null,"You can also click the cross button to explicitly label the image as not having the concept. "),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"label with cross button",src:a(48906).A,width:"1863",height:"765"})),(0,n.yg)("admonition",{type:"note"},(0,n.yg)("p",{parentName:"admonition"},"Clicking an already-clicked tick or cross button removes the label.")),(0,n.yg)("p",null,"You can also apply multiple labels to the same image. Let's say you have two classes that might be easily confused by your model, such as cats and dogs, you can specifically indicate that a cat is present and a dog is not present. This can improve the performance of your model, but also result in longer labeling times."),(0,n.yg)("p",null,'In the following screenshot, the tick button has been clicked, indicating that the image has been labeled with the "cat" concept. Also, the cross button has been clicked, indicating that the image has been expressly labeled as not having the "dog" concept.'),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"cat and dog label",src:a(58360).A,width:"1893",height:"867"})),(0,n.yg)("p",null,"If you enabled ",(0,n.yg)("a",{parentName:"p",href:"https://docs.clarifai.com/portal-guide/annotate/ai-assist/"},"AI Assist")," when you created a new labeling task, you'll notice the concepts that the Clarifai AI has suggested, which could be present in the image you want to label \u2014 alongside their probabilities."),(0,n.yg)("p",null,"You can accept the highlighted suggestions to help you label your images quickly and fast. You can also adjust the slider to a prediction probability threshold you desire. "),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"label with AI assist",src:a(4115).A,width:"1863",height:"841"})),(0,n.yg)("p",null,"Finally, click the ",(0,n.yg)("strong",{parentName:"p"},"Submit")," button on the lower-right corner of the page to submit the labeled input. "),(0,n.yg)("h3",{id:"detection-labeling"},"Detection Labeling"),(0,n.yg)("admonition",{type:"note"},(0,n.yg)("ul",{parentName:"admonition"},(0,n.yg)("li",{parentName:"ul"},"To perform detection labeling, you'll need to specify ",(0,n.yg)("strong",{parentName:"li"},"Detection")," as the ",(0,n.yg)("a",{parentName:"li",href:"https://docs.clarifai.com/portal-guide/annotate/create-a-task#modeling-objective"},"label type")," when setting up your labeling task. "),(0,n.yg)("li",{parentName:"ul"},"You also need to select a workflow that offers detection capabilities when creating a new labeling task."))),(0,n.yg)("p",null,"With the detection label type, you can provide annotation within a single box-shaped region of an image or a video. "),(0,n.yg)("p",null,"To begin labeling, click the plus (",(0,n.yg)("strong",{parentName:"p"},"+"),") button next to the concept you want to use. This action will highlight the button, enabling you to begin drawing a bounding box around the image. "),(0,n.yg)("p",null,'You also have the option to use keyboard shortcuts provided alongside the concepts. For example, in the image below, pressing the number "2" on the keyboard will highlight the "vehicle" concept.'),(0,n.yg)("p",null,"Once you've highlighted a concept, position your cursor over the input and draw a bounding box around it as closely as possible. Next, release your mouse, and you'll see the annotation appearing under the selected concept."),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"draw bounding box",src:a(89957).A,width:"1908",height:"825"})),(0,n.yg)("admonition",{type:"tip"},(0,n.yg)("p",{parentName:"admonition"},'You can easily resize, move, or relabel existing bounding boxes. Just click inside the bounding box you wish to modify, then drag to move or resize. If you need to change the label, just click the "edit" icon located next to the existing label in the right sidebar.')),(0,n.yg)("p",null,"If you enabled AI Assist when you created a new labeling task, you can accept or modify the detected regions that have been suggested for labeling. "),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"detection for still images",src:a(92468).A,width:"1881",height:"925"})),(0,n.yg)("h3",{id:"segmentation-labeling-using-polygons"},"Segmentation Labeling Using Polygons"),(0,n.yg)("p",null,"Support for segmentation labeling is coming soon."),(0,n.yg)("h2",{id:"how-to-label-video-inputs"},"How to Label Video Inputs"),(0,n.yg)("p",null,"Support for video labeling is coming soon."),(0,n.yg)("h2",{id:"how-to-label-text-inputs"},"How to Label Text Inputs"),(0,n.yg)("p",null,"Scribe makes it easy to classify your text data. You can classify text inputs in the same way that you would classify images."),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"Label text data in Scribe",src:a(82147).A,width:"1000",height:"548"})))}p.isMDXComponent=!0},81560:(e,t,a)=>{a.d(t,{A:()=>o});const o=a.p+"assets/images/labeling_tools_1-ed1857aec42006897c4370141c2589cf.png"},72403:(e,t,a)=>{a.d(t,{A:()=>o});const o=a.p+"assets/images/labeling_tools_2-0df2a2ccec9f9ee036a79f9ea459b4d5.png"},5290:(e,t,a)=>{a.d(t,{A:()=>o});const o=a.p+"assets/images/labeling_tools_3-292f364dc8894949523ea6f02d5a83e1.png"},89957:(e,t,a)=>{a.d(t,{A:()=>o});const o=a.p+"assets/images/labeling_tools_4-1314e3341216bb59a3e6a1b4cc205a76.png"},72444:(e,t,a)=>{a.d(t,{A:()=>o});const o=a.p+"assets/images/labeling_tools_5-0bd68dd1e3d32d24cc1c2f0ab9d85ec7.png"},82147:(e,t,a)=>{a.d(t,{A:()=>o});const o=a.p+"assets/images/label-text-da3dc44e87bf5c5589afc11531546eff.jpg"},58360:(e,t,a)=>{a.d(t,{A:()=>o});const o=a.p+"assets/images/label_types_4-e9ede9f24c2e1150bbb182bd6b9c31b9.png"},79489:(e,t,a)=>{a.d(t,{A:()=>o});const o=a.p+"assets/images/label_types_5-6a87c8474e1a74eb03eea31fa0745b20.png"},48906:(e,t,a)=>{a.d(t,{A:()=>o});const o=a.p+"assets/images/label_types_6-db0acc130cc3c04f699d99019db58878.png"},4115:(e,t,a)=>{a.d(t,{A:()=>o});const o=a.p+"assets/images/label_types_7-0ab25a7c4a806fe6c0f166327eff36c9.png"},92468:(e,t,a)=>{a.d(t,{A:()=>o});const o=a.p+"assets/images/label_types_8-e1f140a4d108a7bdadf873e325357d3b.png"}}]);
"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[1488],{11470:(e,n,t)=>{t.d(n,{A:()=>O});var o=t(96540),r=t(18215),a=t(17559),i=t(23104),l=t(56347),s=t(205),c=t(57485),d=t(31682),p=t(70679);function u(e){return o.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,o.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function h(e){const{values:n,children:t}=e;return(0,o.useMemo)(()=>{const e=n??function(e){return u(e).map(({props:{value:e,label:n,attributes:t,default:o}})=>({value:e,label:n,attributes:t,default:o}))}(t);return function(e){const n=(0,d.XI)(e,(e,n)=>e.value===n.value);if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[n,t])}function m({value:e,tabValues:n}){return n.some(n=>n.value===e)}function f({queryString:e=!1,groupId:n}){const t=(0,l.W6)(),r=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,c.aZ)(r),(0,o.useCallback)(e=>{if(!r)return;const n=new URLSearchParams(t.location.search);n.set(r,e),t.replace({...t.location,search:n.toString()})},[r,t])]}function y(e){const{defaultValue:n,queryString:t=!1,groupId:r}=e,a=h(e),[i,l]=(0,o.useState)(()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!m({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const t=n.find(e=>e.default)??n[0];if(!t)throw new Error("Unexpected error: 0 tabValues");return t.value}({defaultValue:n,tabValues:a})),[c,d]=f({queryString:t,groupId:r}),[u,y]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[t,r]=(0,p.Dv)(n);return[t,(0,o.useCallback)(e=>{n&&r.set(e)},[n,r])]}({groupId:r}),g=(()=>{const e=c??u;return m({value:e,tabValues:a})?e:null})();(0,s.A)(()=>{g&&l(g)},[g]);return{selectedValue:i,selectValue:(0,o.useCallback)(e=>{if(!m({value:e,tabValues:a}))throw new Error(`Can't select invalid tab value=${e}`);l(e),d(e),y(e)},[d,y,a]),tabValues:a}}var g=t(92303);const x={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var I=t(74848);function j({className:e,block:n,selectedValue:t,selectValue:o,tabValues:a}){const l=[],{blockElementScrollPositionUntilNextRender:s}=(0,i.a_)(),c=e=>{const n=e.currentTarget,r=l.indexOf(n),i=a[r].value;i!==t&&(s(n),o(i))},d=e=>{let n=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const t=l.indexOf(e.currentTarget)+1;n=l[t]??l[0];break}case"ArrowLeft":{const t=l.indexOf(e.currentTarget)-1;n=l[t]??l[l.length-1];break}}n?.focus()};return(0,I.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,r.A)("tabs",{"tabs--block":n},e),children:a.map(({value:e,label:n,attributes:o})=>(0,I.jsx)("li",{role:"tab",tabIndex:t===e?0:-1,"aria-selected":t===e,ref:e=>{l.push(e)},onKeyDown:d,onClick:c,...o,className:(0,r.A)("tabs__item",x.tabItem,o?.className,{"tabs__item--active":t===e}),children:n??e},e))})}function b({lazy:e,children:n,selectedValue:t}){const a=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=a.find(e=>e.props.value===t);return e?(0,o.cloneElement)(e,{className:(0,r.A)("margin-top--md",e.props.className)}):null}return(0,I.jsx)("div",{className:"margin-top--md",children:a.map((e,n)=>(0,o.cloneElement)(e,{key:n,hidden:e.props.value!==t}))})}function v(e){const n=y(e);return(0,I.jsxs)("div",{className:(0,r.A)(a.G.tabs.container,"tabs-container",x.tabList),children:[(0,I.jsx)(j,{...n,...e}),(0,I.jsx)(b,{...n,...e})]})}function O(e){const n=(0,g.A)();return(0,I.jsx)(v,{...e,children:u(e.children)},String(n))}},19365:(e,n,t)=>{t.d(n,{A:()=>i});t(96540);var o=t(18215);const r={tabItem:"tabItem_Ymn6"};var a=t(74848);function i({children:e,hidden:n,className:t}){return(0,a.jsx)("div",{role:"tabpanel",className:(0,o.A)(r.tabItem,t),hidden:n,children:e})}},43821:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>I,contentTitle:()=>x,default:()=>v,frontMatter:()=>g,metadata:()=>o,toc:()=>j});const o=JSON.parse('{"id":"compute/toolkits/openai","title":"OpenAI","description":"Run OpenAI-compatible models locally and expose them via a public API","source":"@site/docs/compute/toolkits/openai.md","sourceDirName":"compute/toolkits","slug":"/compute/toolkits/openai","permalink":"/compute/toolkits/openai","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"description":"Run OpenAI-compatible models locally and expose them via a public API","sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Toolkits","permalink":"/compute/toolkits/"},"next":{"title":"Ollama","permalink":"/compute/toolkits/ollama"}}');var r=t(74848),a=t(28453),i=t(11470),l=t(19365),s=t(88149);const c='from typing import List, Iterator\nfrom openai import OpenAI\nfrom clarifai.runners.models.openai_class import OpenAIModelClass\nfrom clarifai.runners.utils.data_utils import Param\nfrom clarifai.runners.utils.openai_convertor import build_openai_messages\n\nclass MyModel(OpenAIModelClass):\n    """A custom model implementation using OpenAIModelClass."""\n\n    # TODO: please fill in\n    # Configure your OpenAI-compatible client for local model\n    client = OpenAI(\n        api_key="local-key",  # TODO: please fill in - use your local API key\n        base_url="http://localhost:8000/v1",  # TODO: please fill in - your local model server endpoint\n    )\n\n    # Automatically get the first available model\n    model = client.models.list().data[0].id\n\n    def load_model(self):\n        """Optional: Add any additional model loading logic here."""\n        # TODO: please fill in (optional)\n        # Add any initialization logic if needed\n        pass\n\n    @OpenAIModelClass.method\n    def predict(\n        self,\n        prompt: str = "",\n        chat_history: List[dict] = None,\n        max_tokens: int = Param(default=256, description="The maximum number of tokens to generate. Shorter token lengths will provide faster performance."),\n        temperature: float = Param(default=1.0, description="A decimal number that determines the degree of randomness in the response"),\n        top_p: float = Param(default=1.0, description="An alternative to sampling with temperature, where the model considers the results of the tokens with top_p probability mass."),\n    ) -> str:\n        """Run a single prompt completion using the OpenAI client."""\n        # TODO: please fill in\n        # Implement your prediction logic here\n        messages = build_openai_messages(prompt, chat_history)\n        response = self.client.chat.completions.create(\n            model=self.model,\n            messages=messages,\n            max_completion_tokens=max_tokens,\n            temperature=temperature,\n            top_p=top_p,\n        )\n        return response.choices[0].message.content\n\n    @OpenAIModelClass.method\n    def generate(\n        self,\n        prompt: str = "",\n        chat_history: List[dict] = None,\n        max_tokens: int = Param(default=256, description="The maximum number of tokens to generate. Shorter token lengths will provide faster performance."),\n        temperature: float = Param(default=1.0, description="A decimal number that determines the degree of randomness in the response"),\n        top_p: float = Param(default=1.0, description="An alternative to sampling with temperature, where the model considers the results of the tokens with top_p probability mass."),\n    ) -> Iterator[str]:\n        """Stream a completion response using the OpenAI client."""\n        # TODO: please fill in\n        # Implement your streaming logic here\n        messages = build_openai_messages(prompt, chat_history)\n        stream = self.client.chat.completions.create(\n            model=self.model,\n            messages=messages,\n            max_completion_tokens=max_tokens,\n            temperature=temperature,\n            top_p=top_p,\n            stream=True,\n        )\n        for chunk in stream:\n            if chunk.choices:\n                text = (chunk.choices[0].delta.content\n                        if (chunk and chunk.choices[0].delta.content) is not None else \'\')\n                yield text\n',d='# Configuration file for your Clarifai model\n\nmodel:\n  id: "my-model"  # TODO: please fill in - replace with your model ID\n  user_id: "user-id"  # TODO: please fill in - replace with your user ID\n  app_id: "app_id"  # TODO: please fill in - replace with your app ID\n  model_type_id: "any-to-any"  # TODO: please fill in - replace if different model type ID\n\nbuild_info:\n  python_version: "3.12"\n\n# TODO: please fill in - adjust compute requirements for your model\ninference_compute_info:\n  cpu_limit: "1"  # TODO: please fill in - Amount of CPUs to use as a limit\n  cpu_memory: "1Gi"  # TODO: please fill in - Amount of CPU memory to use as a limit\n  cpu_requests: "0.5"  # TODO: please fill in - Amount of CPUs to use as a minimum\n  cpu_memory_requests: "512Mi"  # TODO: please fill in - Amount of CPU memory to use as a minimum\n  num_accelerators: 1  # TODO: please fill in - Amount of GPU/TPUs to use\n  accelerator_type: ["NVIDIA-*"]  # TODO: please fill in - type of accelerators requested\n  accelerator_memory: "1Gi"  # TODO: please fill in - Amount of accelerator/GPU memory to use as a minimum\n\n# TODO: please fill in (optional) - add checkpoints section if needed\n# checkpoints:\n#   type: "huggingface"  # supported type\n#   repo_id: "your-model-repo"  # for huggingface like openai/gpt-oss-20b\n#   # hf_token: "your-huggingface-token"  # if private repo\n#   when: "runtime"  # or "build", "upload"\n',p="# Clarifai SDK - required\nclarifai>=11.10.2\nopenai\n\n# TODO: please fill in - add your model's dependencies here\n# Examples:\n# torch>=2.0.0\n# transformers>=4.30.0\n# numpy>=1.21.0\n# pillow>=9.0.0\n",u="clarifai model init --model-type-id openai\n[INFO] 07:19:23.816900 Initializing model with default templates... |  thread=8490328256 \nPress Enter to continue...\n[INFO] 07:19:27.092345 Configuring OpenAI local runner... |  thread=8490328256 \nEnter port (default: 8000): \n[INFO] 07:19:31.983567 Created /Users/macbookpro/Desktop/code3/three/1/model.py |  thread=8490328256 \n[INFO] 07:19:31.984366 Created /Users/macbookpro/Desktop/code3/three/requirements.txt |  thread=8490328256 \n[INFO] 07:19:31.984757 Created /Users/macbookpro/Desktop/code3/three/config.yaml |  thread=8490328256 \n[INFO] 07:19:31.984819 Model initialization complete in /Users/macbookpro/Desktop/code3/three |  thread=8490328256 \n[INFO] 07:19:31.984863 Next steps: |  thread=8490328256 \n[INFO] 07:19:31.984904 1. Search for '# TODO: please fill in' comments in the generated files |  thread=8490328256 \n[INFO] 07:19:31.984946 2. Update the model configuration in config.yaml |  thread=8490328256 \n[INFO] 07:19:31.984985 3. Add your model dependencies to requirements.txt |  thread=8490328256 \n[INFO] 07:19:31.985023 4. Implement your model logic in 1/model.py |  thread=8490328256 ",h='clarifai model local-runner\n[INFO] 11:23:11.406057 Detected OpenAI chat completions for Clarifai model streaming - validating stream_options... |  thread=8821432512 \n[ERROR] 11:23:11.406350 Missing configuration to track usage for OpenAI chat completion calls. Go to your model scripts and make sure to set both: 1) stream_options={\'include_usage\': True}2) set_output_context |  thread=8821432512 \n[INFO] 11:23:11.406705 > Checking local runner requirements... |  thread=8821432512 \n[INFO] 11:23:11.428851 Checking 2 dependencies... |  thread=8821432512 \n[INFO] 11:23:11.429253 \u2705 All 2 dependencies are installed! |  thread=8821432512 \n[INFO] 11:23:11.431322 > Verifying local runner setup... |  thread=8821432512 \n[INFO] 11:23:11.431374 Current context: default |  thread=8821432512 \n[INFO] 11:23:11.431406 Current user_id: alfrick |  thread=8821432512 \n[INFO] 11:23:11.431432 Current PAT: d6974**** |  thread=8821432512 \n[INFO] 11:23:11.433893 Current compute_cluster_id: local-runner-compute-cluster |  thread=8821432512 \n[WARNING] 11:23:14.002018 Failed to get compute cluster with ID \'local-runner-compute-cluster\':\ncode: CONN_DOES_NOT_EXIST\ndescription: "Resource does not exist"\ndetails: "ComputeCluster with ID \\\'local-runner-compute-cluster\\\' not found. Check your request fields."\nreq_id: "sdk-python-11.10.2-34a60189eb514b8b9085ba741a13a7ca"\n |  thread=8821432512 \nCompute cluster not found. Do you want to create a new compute cluster alfrick/local-runner-compute-cluster? (y/n): y\n[INFO] 11:23:26.498698 Compute Cluster with ID \'local-runner-compute-cluster\' is created:\ncode: SUCCESS\ndescription: "Ok"\nreq_id: "sdk-python-11.10.2-a3c367c30482463aa0f23089d8971d7a"\n |  thread=8821432512 \n[INFO] 11:23:26.508120 Current nodepool_id: local-runner-nodepool |  thread=8821432512 \n[WARNING] 11:23:29.251200 Failed to get nodepool with ID \'local-runner-nodepool\':\ncode: CONN_DOES_NOT_EXIST\ndescription: "Resource does not exist"\ndetails: "Nodepool not found. Check your request fields."\nreq_id: "sdk-python-11.10.2-c841763ca1d54452984432a6642ec030"\n |  thread=8821432512 \nNodepool not found. Do you want to create a new nodepool alfrick/local-runner-compute-cluster/local-runner-nodepool? (y/n): y\n[INFO] 11:23:32.994964 Nodepool with ID \'local-runner-nodepool\' is created:\ncode: SUCCESS\ndescription: "Ok"\nreq_id: "sdk-python-11.10.2-0cd9f37bc175431aa57626ac709b6490"\n |  thread=8821432512 \n[INFO] 11:23:33.009030 Current app_id: local-runner-app |  thread=8821432512 \n[WARNING] 11:23:33.330525 Failed to get app with ID \'local-runner-app\':\ncode: CONN_DOES_NOT_EXIST\ndescription: "Resource does not exist"\ndetails: "app identified by path /users/alfrick/apps/local-runner-app not found"\nreq_id: "sdk-python-11.10.2-05940fc72d46478d8309275b3ccf788e"\n |  thread=8821432512 \nApp not found. Do you want to create a new app alfrick/local-runner-app? (y/n): y\n[INFO] 11:23:36.874801 App with ID \'local-runner-app\' is created:\ncode: SUCCESS\ndescription: "Ok"\nreq_id: "sdk-python-11.10.2-da9c70e006774b7a9ab5ae47b339cd6d"\n |  thread=8821432512 \n[INFO] 11:23:36.887817 Current model_id: local-runner-model |  thread=8821432512 \n[WARNING] 11:23:38.066408 Failed to get model with ID \'local-runner-model\':\ncode: MODEL_DOES_NOT_EXIST\ndescription: "Model does not exist"\ndetails: "Model \\\'local-runner-model\\\' does not exist."\nreq_id: "sdk-python-11.10.2-a2157a2599f747559e1f9fcc1d459247"\n |  thread=8821432512 \nModel not found. Do you want to create a new model alfrick/local-runner-app/models/local-runner-model? (y/n): y\n[INFO] 11:23:42.481867 Model with ID \'local-runner-model\' is created:\ncode: SUCCESS\ndescription: "Ok"\nreq_id: "sdk-python-11.10.2-a2949b4d81304deda6cb897a05b89cfd"\n |  thread=8821432512 \n[WARNING] 11:23:44.422210 No model versions found. Creating a new version for local runner. |  thread=8821432512 \n[INFO] 11:23:45.628623 Model Version with ID \'36bde5dcf7c24317a08d1366a8cc5757\' is created:\ncode: SUCCESS\ndescription: "Ok"\nreq_id: "sdk-python-11.10.2-1cf881046e2047b795533c4f08da5c7e"\n |  thread=8821432512 \n[INFO] 11:23:46.723898 Current model version 36bde5dcf7c24317a08d1366a8cc5757 |  thread=8821432512 \n[INFO] 11:23:46.724179 Creating the local runner tying this \'alfrick/local-runner-app/models/local-runner-model\' model (version: 36bde5dcf7c24317a08d1366a8cc5757) to the \'alfrick/local-runner-compute-cluster/local-runner-nodepool\' nodepool. |  thread=8821432512 \n[INFO] 11:23:48.660432 Runner with ID \'805fa3de93d341d7aaac0aed94786236\' is created:\ncode: SUCCESS\ndescription: "Ok"\nreq_id: "sdk-python-11.10.2-a613867263b94219ba9fc558f29c1661"\n |  thread=8821432512 \n[INFO] 11:23:48.670950 Current runner_id: 805fa3de93d341d7aaac0aed94786236 |  thread=8821432512 \n[WARNING] 11:23:48.931127 Failed to get deployment with ID local-runner-deployment:\ncode: CONN_DOES_NOT_EXIST\ndescription: "Resource does not exist"\ndetails: "Deployment with ID \\\'local-runner-deployment\\\' not found. Check your request fields."\nreq_id: "sdk-python-11.10.2-28999059cf6e42b4ab868dcca10b4201"\n |  thread=8821432512 \nDeployment not found. Do you want to create a new deployment alfrick/local-runner-compute-cluster/local-runner-nodepool/local-runner-deployment? (y/n): y\n[INFO] 11:23:53.891169 Deployment with ID \'local-runner-deployment\' is created:\ncode: SUCCESS\ndescription: "Ok"\nreq_id: "sdk-python-11.10.2-8a6701fa26e64acc8109e6affef1df28"\n |  thread=8821432512 \n[INFO] 11:23:53.902286 Current deployment_id: local-runner-deployment |  thread=8821432512 \n[INFO] 11:23:53.902438 Current model section of config.yaml: {\'id\': \'my-model\', \'user_id\': \'alfrick\', \'app_id\': \'app_id\', \'model_type_id\': \'any-to-any\'} |  thread=8821432512 \nDo you want to backup config.yaml to config.yaml.bk then update the config.yaml with the new model information? (y/n): y\n[INFO] 11:23:57.187239 Checking 2 dependencies... |  thread=8821432512 \n[INFO] 11:23:57.188280 \u2705 All 2 dependencies are installed! |  thread=8821432512 \n[INFO] 11:23:57.188387 \u2705 Starting local runner... |  thread=8821432512 \n[INFO] 11:23:57.188475 No secrets path configured, running without secrets |  thread=8821432512 \n[INFO] 11:23:58.334211 Detected OpenAI chat completions for Clarifai model streaming - validating stream_options... |  thread=8821432512 \n[ERROR] 11:23:58.334788 Missing configuration to track usage for OpenAI chat completion calls. Go to your model scripts and make sure to set both: 1) stream_options={\'include_usage\': True}2) set_output_context |  thread=8821432512 \n[INFO] 11:23:58.358199 ModelServer initialized successfully |  thread=8821432512 \n[INFO] 11:23:58.385791 \u2705 Your model is running locally and is ready for requests from the API...\n |  thread=8821432512 \n[INFO] 11:23:58.385872 > Code Snippet: To call your model via the API, use this code snippet:\n\nimport os\n\nfrom openai import OpenAI\n\nclient = OpenAI(\n    base_url="https://api.clarifai.com/v2/ext/openai/v1",\n    api_key=os.environ[\'CLARIFAI_PAT\'],\n)\n\nresponse = client.chat.completions.create(\n    model="https://clarifai.com/alfrick/local-runner-app/models/local-runner-model",\n    messages=[\n        {"role": "system", "content": "Talk like a pirate."},\n        {\n            "role": "user",\n            "content": "How do I check if a Python object is an instance of a class?",\n        },\n    ],\n    temperature=1.0,\n    stream=False,  # stream=True also works, just iterator over the response\n)\nprint(response)\n |  thread=8821432512 \n[INFO] 11:23:58.385916 > Playground:   To chat with your model, visit: https://clarifai.com/playground?model=local-runner-model__36bde5dcf7c24317a08d1366a8cc5757&user_id=alfrick&app_id=local-runner-app\n |  thread=8821432512 \n[INFO] 11:23:58.385946 > API URL:      To call your model via the API, use this model URL: https://clarifai.com/alfrick/local-runner-app/models/local-runner-model\n |  thread=8821432512 \n[INFO] 11:23:58.385966 Press CTRL+C to stop the runner.\n |  thread=8821432512 \n[INFO] 11:23:58.385994 Starting 32 threads... |  thread=8821432512 ',m='import os\n\nfrom openai import OpenAI\n\nclient = OpenAI(\n    base_url="https://api.clarifai.com/v2/ext/openai/v1",\n    api_key=os.environ[\'CLARIFAI_PAT\'],\n)\n\nresponse = client.chat.completions.create(\n    model="https://clarifai.com/user-id/local-runner-app/models/local-runner-model",\n    messages=[\n        {"role": "system", "content": "Talk like a pirate."},\n        {\n            "role": "user",\n            "content": "How do I check if a Python object is an instance of a class?",\n        },\n    ],\n    temperature=1.0,\n    stream=False,  # stream=True also works, just iterator over the response\n)\nprint(response)\n',f="clarifai login\nEnter your Clarifai user ID: user-id\n> To authenticate, you'll need a Personal Access Token (PAT).\n> You can create one from your account settings: https://clarifai.com/alfrick/settings/security\n\nEnter your Personal Access Token (PAT) value (or type \"ENVVAR\" to use an environment variable): ENVVAR\n\n> Verifying token...\n[INFO] 13:59:43.543035 Validating the Context Credentials... |  thread=8490328256 \n[INFO] 13:59:44.940556 \u2705 Context is valid |  thread=8490328256 \n\n> Let's save these credentials to a new context.\n> You can have multiple contexts to easily switch between accounts or projects.\n\nEnter a name for this context [default]: \n\u2705 Success! You are now logged in.\nCredentials saved to the 'default' context.\n\n\ud83d\udca1 To switch contexts later, use `clarifai config use-context <name>`.\n[INFO] 13:59:46.641774 Login successful for user 'alfrick' in context 'default' |  thread=8490328256 \n",y='ChatCompletion(\n    id=\'bf90c9f0a20e44d796780d35360d3951\',\n    choices=[\n        Choice(\n            finish_reason=\'stop\',\n            index=0,\n            logprobs=None,\n            message=ChatCompletionMessage(\n                content=(\n                    "Yer lookin\' fer a way to check if a Python object be an instance "\n                    "o\' a class, eh?\\n\\n"\n                    "In Python, ye can use the `type()` function or `isinstance()` method "\n                    "to determine if a variable be o\' a certain type. Here be some ways "\n                    "to do it:\\n\\n"\n                    "### Method 1: Using `type()`\\n\\n"\n                    "```python\\n"\n                    "x = \\"Hello\\"\\n"\n                    "y = [1, 2, 3]\\n\\n"\n                    "if type(x) == str:\\n"\n                    "    print(\\"x is a string\\")\\n"\n                    "elif type(y) == list:\\n"\n                    "    print(\\"y is a list\\")\\n"\n                    "else:\\n"\n                    "    print(\\"Unknown type\\")\\n"\n                    "```\\n\\n"\n                    "### Method 2: Using `isinstance()`\\n\\n"\n                    "```python\\n"\n                    "class Person:\\n"\n                    "    def __init__(self, name):\\n"\n                    "        self.name = name\\n\\n"\n                    "p = Person(\\"Pirate\\")\\n"\n                    "if isinstance(p, Person):\\n"\n                    "    print(\\"p be an instance o\' the Person class\\")\\n\\n"\n                    "x = \\"Hello\\"\\n"\n                    "y = [1, 2, 3]\\n"\n                    "```\\n\\n"\n                    "### Method 3: Constructor Check with Class Definition (Not Recommended)\\n\\n"\n                    "```python\\n"\n                    "class MyShip:\\n"\n                    "    def __init__(self, speed):\\n"\n                    "        self.speed = speed\\n\\n"\n                    "if obj is MyShip(some_speed):\\n"\n                    "    print(f\\"it\'s a ship made by {some_speed}\\")\\n"\n                    "else:\\n"\n                    "    pass  # object is not of type MyShip.\\n"\n                    "```\\n\\n"\n                    "Note: Python doesn\u2019t use \'object\' in the generic sense; everything is an instance of a class."\n                ),\n                refusal=None,\n                role=\'assistant\',\n                annotations=None,\n                audio=None,\n                function_call=None,\n                tool_calls=None\n            )\n        )\n    ],\n    created=1764578049,\n    model=\'llama3.2:latest\',\n    object=\'chat.completion\',\n    service_tier=None,\n    system_fingerprint=\'fp_ollama\',\n    usage=CompletionUsage(\n        completion_tokens=303,\n        prompt_tokens=45,\n        total_tokens=348,\n        completion_tokens_details=None,\n        prompt_tokens_details=None\n    )\n)\n',g={description:"Run OpenAI-compatible models locally and expose them via a public API",sidebar_position:1},x="OpenAI",I={},j=[{value:"Step 1: Perform Prerequisites",id:"step-1-perform-prerequisites",level:2},{value:"Sign Up or Log In",id:"sign-up-or-log-in",level:3},{value:"Install Clarifai CLI",id:"install-clarifai-cli",level:3},{value:"Install the OpenAI Package",id:"install-the-openai-package",level:3},{value:"Set Up Your OpenAI-Compatible Server",id:"set-up-your-openai-compatible-server",level:3},{value:"Step 2: Initialize a Model",id:"step-2-initialize-a-model",level:2},{value:"<code>model.py</code>",id:"modelpy",level:3},{value:"<code>config.yaml</code>",id:"configyaml",level:3},{value:"<code>requirements.txt</code>",id:"requirementstxt",level:3},{value:"Step 3: Log In to Clarifai",id:"step-3-log-in-to-clarifai",level:2},{value:"Step 4: Start Your Local Runner",id:"step-4-start-your-local-runner",level:2},{value:"Step 5: Test Your Runner",id:"step-5-test-your-runner",level:2}];function b(e){const n={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components},{Details:t}=n;return t||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"openai",children:"OpenAI"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Run OpenAI-compatible models locally and expose them via a public API"})}),"\n",(0,r.jsx)("hr",{}),"\n",(0,r.jsx)(n.p,{children:"OpenAI's API specification has become the industry standard for interacting with large language models."}),"\n",(0,r.jsxs)(n.p,{children:["With Clarifai's ",(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/compute/local-runners/",children:"Local Runners"}),", you can deploy any OpenAI-compatible model locally, whether it's from OpenAI's official services, open-source alternatives, or your own fine-tuned models, and make them available via secure public endpoints."]}),"\n",(0,r.jsx)(n.p,{children:"This approach gives you the flexibility of OpenAI's familiar API interface while maintaining data privacy, reducing latency, and having full control over your deployment environment."}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Note:"})," After setting up your OpenAI-compatible model locally, you can ",(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/compute/upload/#step-4-upload-the-model-to-clarifai",children:"upload"})," it to Clarifai to leverage the platform's capabilities, such as versioning, monitoring, and auto\u2011scaling."]}),"\n"]}),"\n","\n","\n",(0,r.jsx)(n.h2,{id:"step-1-perform-prerequisites",children:"Step 1: Perform Prerequisites"}),"\n",(0,r.jsx)(n.h3,{id:"sign-up-or-log-in",children:"Sign Up or Log In"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"https://clarifai.com/login",children:"Log in"})," to your existing Clarifai account or ",(0,r.jsx)(n.a,{href:"https://clarifai.com/signup",children:"sign up"})," for a new one. Once logged in, you'll need the following credentials for setup:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"App ID"})," \u2013 Navigate to the application you want to use to run the model and select the ",(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/create/applications/manage/#app-overview",children:"Overview"})})," option in the collapsible left sidebar. Get the app ID from there."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"User ID"})," \u2013 In the collapsible left sidebar, select ",(0,r.jsx)(n.strong,{children:"Settings"})," and choose ",(0,r.jsx)(n.strong,{children:"Account"})," from the dropdown list. Then, locate your user ID."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Personal Access Token (PAT)"})," \u2013 From the same ",(0,r.jsx)(n.strong,{children:"Settings"})," option, choose ",(0,r.jsx)(n.strong,{children:"Secrets"})," to generate or copy your ",(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/control/authentication/pat",children:"PAT"}),". This token is used to authenticate your connection with the Clarifai platform."]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["You can then set the PAT as an environment variable using ",(0,r.jsx)(n.code,{children:"CLARIFAI_PAT"}),"."]}),"\n",(0,r.jsxs)(i.A,{groupId:"code",children:[(0,r.jsx)(l.A,{value:"bash",label:"Unix-Like Systems",children:(0,r.jsx)(s.A,{className:"language-bash",children:"export CLARIFAI_PAT=YOUR_PERSONAL_ACCESS_TOKEN_HERE"})}),(0,r.jsx)(l.A,{value:"bash2",label:"Windows",children:(0,r.jsx)(s.A,{className:"language-bash",children:"set CLARIFAI_PAT=YOUR_PERSONAL_ACCESS_TOKEN_HERE"})})]}),"\n",(0,r.jsx)(n.h3,{id:"install-clarifai-cli",children:"Install Clarifai CLI"}),"\n",(0,r.jsxs)(n.p,{children:["Install the latest version of the ",(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/sdk/cli",children:"Clarifai CLI"})," tool. It includes built-in support for Local Runners."]}),"\n",(0,r.jsx)(i.A,{groupId:"code",children:(0,r.jsx)(l.A,{value:"bash",label:"Bash",children:(0,r.jsx)(s.A,{className:"language-bash",children:"pip install --upgrade clarifai"})})}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Note:"})," You'll need ",(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/resources/api-overview/python-sdk#python-requirements",children:"Python 3.11 or 3.12"})})," installed to successfully run the Local Runners."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"install-the-openai-package",children:"Install the OpenAI Package"}),"\n",(0,r.jsxs)(n.p,{children:["Install the ",(0,r.jsx)(n.code,{children:"openai"})," package \u2014 it\u2019s required to perform inference with models that support the ",(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/compute/inference/#predict-with-openai-compatible-format",children:"OpenAI-compatible"})," format."]}),"\n",(0,r.jsx)(i.A,{groupId:"code",children:(0,r.jsx)(l.A,{value:"bash",label:"Bash",children:(0,r.jsx)(s.A,{className:"language-bash",children:"pip install openai"})})}),"\n",(0,r.jsx)(n.h3,{id:"set-up-your-openai-compatible-server",children:"Set Up Your OpenAI-Compatible Server"}),"\n",(0,r.jsx)(n.p,{children:"Before initializing the model, ensure you have an OpenAI-compatible server running locally. Popular options include:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/compute/toolkits/vllm#install-vllm",children:"vLLM"})," \u2014 Usually runs at ",(0,r.jsx)(n.code,{children:"http://localhost:8000/v1"})," (default port: 8000, unless you specify ",(0,r.jsx)(n.code,{children:"--port"}),")."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/compute/toolkits/lmstudio#install-lm-studio",children:"LM Studio"})," \u2014 Usually at ",(0,r.jsx)(n.code,{children:"http://localhost:1234/v1"})," (default port: 1234, shown in the LM Studio UI)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/compute/toolkits/ollama#install-ollama",children:"Ollama"})," (OpenAI mode) \u2014 Usually at ",(0,r.jsx)(n.code,{children:"http://localhost:11434/v1"})," (default port: 11434, unless changed via the config or startup flags)"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["You\u2019ll use this address in the ",(0,r.jsx)(n.a,{href:"#modelpy",children:(0,r.jsx)(n.code,{children:"model.py"})})," file."]}),"\n",(0,r.jsx)(n.h2,{id:"step-2-initialize-a-model",children:"Step 2: Initialize a Model"}),"\n",(0,r.jsx)(n.p,{children:"With the Clarifai CLI, you can set up any OpenAI-compatible model to work with your local server."}),"\n",(0,r.jsx)(n.p,{children:"The command below scaffolds a default OpenAI-compatible model template in your current directory:"}),"\n",(0,r.jsx)(i.A,{groupId:"code",children:(0,r.jsx)(l.A,{value:"bash",label:"Bash",children:(0,r.jsx)(s.A,{className:"language-bash",children:"clarifai model init --model-type-id openai"})})}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Note:"})," You can initialize a model in a specific location by passing a ",(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/resources/api-overview/cli#clarifai-model-init",children:(0,r.jsx)(n.code,{children:"MODEL_PATH"})}),"."]}),"\n"]}),"\n",(0,r.jsxs)(t,{children:[(0,r.jsx)("summary",{children:"Example Output"}),(0,r.jsx)(s.A,{className:"language-text",children:u})]}),"\n",(0,r.jsx)(n.p,{children:"This command generates a model directory structure that's compatible with the Clarifai platform and configured to work with OpenAI-compatible APIs."}),"\n",(0,r.jsx)(n.p,{children:"The generated structure includes:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"\u251c\u2500\u2500 1/\n\u2502   \u2514\u2500\u2500 model.py\n\u251c\u2500\u2500 requirements.txt\n\u2514\u2500\u2500 config.yaml\n"})}),"\n",(0,r.jsx)(n.h3,{id:"modelpy",children:(0,r.jsx)(n.code,{children:"model.py"})}),"\n",(0,r.jsxs)(t,{children:[(0,r.jsx)("summary",{children:"Example: model.py"}),(0,r.jsx)(s.A,{className:"language-text",children:c})]}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/compute/upload/#prepare-modelpy",children:(0,r.jsx)(n.code,{children:"model.py"})})," file, located inside the ",(0,r.jsx)(n.code,{children:"1"})," folder, acts as the bridge between Clarifai\u2019s model execution environment and your local (or remote) OpenAI-compatible server."]}),"\n",(0,r.jsxs)(n.p,{children:["It includes an extension of the Clarifai\u2019s ",(0,r.jsx)(n.code,{children:"OpenAIModelClass"}),". This base class is designed specifically for wrapping OpenAI-compatible model servers and exposing them through Clarifai\u2019s inference infrastructure."]}),"\n",(0,r.jsx)(n.p,{children:"This class implements the following:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["The ",(0,r.jsx)(n.code,{children:"OpenAI"})," client, which connects to your model server (e.g. vLLM, LM Studio, Ollama) via its ",(0,r.jsx)(n.code,{children:"/v1"})," API endpoint."]}),"\n",(0,r.jsxs)(n.li,{children:["The ",(0,r.jsx)(n.code,{children:"predict()"})," method, which handles standard (non-streaming) chat completions."]}),"\n",(0,r.jsxs)(n.li,{children:["The ",(0,r.jsx)(n.code,{children:"generate()"})," method, which supports streaming token generation."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"build_openai_messages()"}),", which automatically converts Clarifai inputs into OpenAI-compatible message format."]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"These are the key components you can configure:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"base_url"}),": Your local model server endpoint address (default: ",(0,r.jsx)(n.code,{children:"http://localhost:8000/v1"}),")."]}),"\n"]}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Note:"})," During model initialization, the CLI will prompt you to choose a port, and this value will be automatically updated in the file to match your selection."]}),"\n"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"api_key"}),": Required only when calling the model through OpenAI\u2019s hosted API. If you\u2019re using a local OpenAI-compatible server, an API key isn\u2019t needed \u2014 you can simply provide any dummy value."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"model"}),": The ID of the model to use. You can leave this as is to let it be automatically detected from your OpenAI-compatible server, or explicitly set it to a specific model ID (for example, ",(0,r.jsx)(n.code,{children:'"gpt-4"'}),")."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"configyaml",children:(0,r.jsx)(n.code,{children:"config.yaml"})}),"\n",(0,r.jsxs)(t,{children:[(0,r.jsx)("summary",{children:"Example: config.yaml"}),(0,r.jsx)(s.A,{className:"language-text",children:d})]}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.code,{children:"config.yaml"})," file tells Clarifai how to run your OpenAI-compatible custom model \u2014 including where it will live on the platform, how it\u2019s served, and what compute resources it needs."]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["It specifies where your model will run using values like ",(0,r.jsx)(n.code,{children:"id"})," (your chosen model name), ",(0,r.jsx)(n.code,{children:"user_id"})," (set by default from your ",(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/resources/api-overview/cli/#clarifai-config",children:"active context"}),"), ",(0,r.jsx)(n.code,{children:"app_id"}),", and ",(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/create/models/#list-of-model-types",children:(0,r.jsx)(n.code,{children:"model_type_id"})}),"."]}),"\n",(0,r.jsxs)(n.li,{children:["In the ",(0,r.jsx)(n.code,{children:"build_info"})," section, specify your configure environment settings, such as the Python version required by your OpenAI model implementation."]}),"\n",(0,r.jsxs)(n.li,{children:["In the ",(0,r.jsx)(n.code,{children:"inference_compute_info"})," section, specify the compute resources your model should use \u2014 including CPU, memory, and optional accelerators (like GPUs) \u2014 ensuring your OpenAI-compatible service has the right performance and scalability characteristics."]}),"\n"]}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"When to use checkpoints:"})," Most OpenAI models are accessed via the API, so you won\u2019t need a ",(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/compute/toolkits/hf#configyaml",children:"checkpoints"})," block. If you are serving a self\u2011hosted Hugging\u202fFace model, you can uncomment the checkpoints section and set the required values."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"requirementstxt",children:(0,r.jsx)(n.code,{children:"requirements.txt"})}),"\n",(0,r.jsxs)(t,{children:[(0,r.jsx)("summary",{children:"Example: requirements.txt"}),(0,r.jsx)(s.A,{className:"language-text",children:p})]}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.code,{children:"requirements.txt"})," file specifies all the Python dependencies your model needs to run. If these packages are not already installed in your environment, install them by running the following command:"]}),"\n",(0,r.jsx)(i.A,{groupId:"code",children:(0,r.jsx)(l.A,{value:"bash",label:"Bash",children:(0,r.jsx)(s.A,{className:"language-bash",children:"pip install -r requirements.txt"})})}),"\n",(0,r.jsx)(n.h2,{id:"step-3-log-in-to-clarifai",children:"Step 3: Log In to Clarifai"}),"\n",(0,r.jsxs)(n.p,{children:["Run the following command to log in to the Clarifai platform, create a configuration ",(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/compute/local-runners/#step-2-create-a-context-optional",children:"context"}),", and establish a connection:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"clarifai login\n"})}),"\n",(0,r.jsx)(n.p,{children:"You'll be prompted to provide:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"User ID"})," \u2013 Enter your Clarifai user ID."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"PAT"})," \u2013 Enter your Clarifai PAT. If you've already set the ",(0,r.jsx)(n.code,{children:"CLARIFAI_PAT"})," environment variable, type ",(0,r.jsx)(n.code,{children:"ENVVAR"})," to use it automatically."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Context name"})," \u2013 Assign a custom name to this configuration context, or press Enter to accept the default name, ",(0,r.jsx)(n.code,{children:'"default"'}),"."]}),"\n"]}),"\n",(0,r.jsxs)(t,{children:[(0,r.jsx)("summary",{children:"Example Output"}),(0,r.jsx)(s.A,{className:"language-text",children:f})]}),"\n",(0,r.jsx)(n.h2,{id:"step-4-start-your-local-runner",children:"Step 4: Start Your Local Runner"}),"\n",(0,r.jsx)(n.p,{children:"Start a local runner with the following command:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"clarifai model local-runner\n"})}),"\n",(0,r.jsx)(n.p,{children:"The CLI will guide you through creating any necessary context configurations with default values, ensuring all components (compute clusters, nodepools, deployments) are properly set up."}),"\n",(0,r.jsxs)(t,{children:[(0,r.jsx)("summary",{children:"Example Output"}),(0,r.jsx)(s.A,{className:"language-text",children:h})]}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Tip:"})," If your underlying model is running on a specific port (like 8000), ensure your ",(0,r.jsx)(n.code,{children:"model.py"})," points to that port, and that the Local Runner does not try to bind to the same port."]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"step-5-test-your-runner",children:"Step 5: Test Your Runner"}),"\n",(0,r.jsx)(n.p,{children:"Once the local runner starts, it provides a sample client code snippet for testing. You can run this in a separate terminal within the same directory."}),"\n",(0,r.jsx)(n.p,{children:"Here's an example test snippet:"}),"\n",(0,r.jsx)(i.A,{groupId:"code",children:(0,r.jsx)(l.A,{value:"python",label:"Python SDK",children:(0,r.jsx)(s.A,{className:"language-python",children:m})})}),"\n",(0,r.jsxs)(t,{children:[(0,r.jsx)("summary",{children:"Example Output"}),(0,r.jsx)(s.A,{className:"language-text",children:y})]}),"\n",(0,r.jsx)(n.p,{children:"That\u2019s it!"}),"\n",(0,r.jsx)(n.p,{children:"When you\u2019re done testing, simply stop the terminal running the local development runner and the process hosting your OpenAI-compatible server."})]})}function v(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(b,{...e})}):b(e)}}}]);
"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[7738],{21669:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>w,contentTitle:()=>D,default:()=>v,frontMatter:()=>O,metadata:()=>a,toc:()=>S});const a=JSON.parse('{"id":"api-guide/predict/audio","title":"Audio","description":"Make predictions on audio inputs","source":"@site/docs/api-guide/predict/audio.md","sourceDirName":"api-guide/predict","slug":"/api-guide/predict/audio","permalink":"/api-guide/predict/audio","draft":false,"unlisted":false,"editUrl":"https://github.com/Clarifai/docs/blob/main/docs/api-guide/predict/audio.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"description":"Make predictions on audio inputs","sidebar_position":5},"sidebar":"tutorialSidebar","previous":{"title":"Large Language Models (LLMs)","permalink":"/api-guide/predict/llms"},"next":{"title":"Multimodal-to-Text","permalink":"/api-guide/predict/multimodal-to-text"}}');var s=t(74848),i=t(28453),o=t(65537),r=t(79329),u=t(58069);const l='#########################################################################################\n# In this section, we set the user authentication, user and app ID, model ID, and\n# audio URL. Change these strings to run your own example.\n########################################################################################\n\n# Your PAT (Personal Access Token) can be found in the Account\'s Security section\nPAT = "YOUR_PAT_HERE"\n# Specify the correct user_id/app_id pairings\n# Since you\'re making inferences outside your app\'s scope\nUSER_ID = "facebook"\nAPP_ID = "asr"\n# Change these to make your own predictions\nMODEL_ID = "asr-wav2vec2-base-960h-english"\nAUDIO_URL = "https://samples.clarifai.com/negative_sentence_1.wav"\n\n##########################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n##########################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (("authorization", "Key " + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(\n    user_id=USER_ID, app_id=APP_ID\n)  # The userDataObject is required when using a PAT\n\npost_model_outputs_response = stub.PostModelOutputs(\n    service_pb2.PostModelOutputsRequest(\n        user_app_id=userDataObject,\n        model_id=MODEL_ID,\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(audio=resources_pb2.Audio(url=AUDIO_URL))\n            )\n        ],\n    ),\n    metadata=metadata,\n)\nif post_model_outputs_response.status.code != status_code_pb2.SUCCESS:\n    print(post_model_outputs_response.status)\n    raise Exception(\n        "Post workflow results failed, status: "\n        + post_model_outputs_response.status.description\n    )\n\n# Since we have one input, one output will exist here\noutput = post_model_outputs_response.outputs[0]\n\n# Print the output\nprint(output.data.text.raw)\n',c='#########################################################################################\n# In this section, we set the user authentication, user and app ID, model ID, and\n# audio file location. Change these strings to run your own example.\n########################################################################################\n\n# Your PAT (Personal Access Token) can be found in the Account\'s Security section\nPAT = "YOUR_PAT_HERE"\n# Specify the correct user_id/app_id pairings\n# Since you\'re making inferences outside your app\'s scope\nUSER_ID = "facebook"\nAPP_ID = "asr"\n# Change these to make your own predictions\nMODEL_ID = "asr-wav2vec2-base-960h-english"\nAUDIO_FILE_LOCATION = "YOUR_AUDIO_FILE_LOCATION_HERE"\n\n##########################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n##########################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (("authorization", "Key " + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(\n    user_id=USER_ID, app_id=APP_ID\n)  # The userDataObject is required when using a PAT\n\nwith open(AUDIO_FILE_LOCATION, "rb") as f:\n    file_bytes = f.read()\n\npost_model_outputs_response = stub.PostModelOutputs(\n    service_pb2.PostModelOutputsRequest(\n        user_app_id=userDataObject,\n        model_id=MODEL_ID,\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(audio=resources_pb2.Audio(base64=file_bytes))\n            )\n        ],\n    ),\n    metadata=metadata,\n)\nif post_model_outputs_response.status.code != status_code_pb2.SUCCESS:\n    print(post_model_outputs_response.status)\n    raise Exception(\n        "Post workflow results failed, status: "\n        + post_model_outputs_response.status.description\n    )\n\n# Since we have one input, one output will exist here\noutput = post_model_outputs_response.outputs[0]\n\n# Print the output\nprint(output.data.text.raw)\n',p='\x3c!--index.html file--\x3e\n\n<script>\n  ////////////////////////////////////////////////////////////////////////////////////////\n  // In this section, we set the user authentication, user and app ID, model ID, and\n  // audio URL. Change these strings to run your own example.\n  ///////////////////////////////////////////////////////////////////////////////////////\n\n  // Your PAT (Personal Access Token) can be found in the Account\'s Security section\n  const PAT = "YOUR_PAT_HERE";\n  // Specify the correct user_id/app_id pairings\n  // Since you\'re making inferences outside your app\'s scope\n  const USER_ID = "facebook";\n  const APP_ID = "asr";\n  // Change these to make your own predictions\n  const MODEL_ID = "asr-wav2vec2-base-960h-english";\n  const AUDIO_URL = "https://samples.clarifai.com/negative_sentence_1.wav";\n\n  ///////////////////////////////////////////////////////////////////////////////////\n  // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n  /////////////////////////////////////////////////////////////////////////////////// \n\n  const raw = JSON.stringify({\n      "user_app_id": {\n          "user_id": USER_ID,\n          "app_id": APP_ID\n      },\n      "inputs": [\n          {\n              "data": {\n                  "audio": {\n                      "url": AUDIO_URL\n                  }\n              }\n          }\n      ]\n  });\n\n  const requestOptions = {\n      method: \'POST\',\n      headers: {\n          \'Accept\': \'application/json\',\n          \'Authorization\': \'Key \' + PAT\n      },\n      body: raw\n  };\n\n  fetch(`https://api.clarifai.com/v2/models/${MODEL_ID}/outputs`, requestOptions)\n      .then(response => response.text())\n      .then(result => console.log(result))\n      .catch(error => console.log(\'error\', error));\n<\/script>',d='\x3c!--index.html file--\x3e\n\n<script>\n  //////////////////////////////////////////////////////////////////////////////////////////////\n  // In this section, we set the user authentication, user and app ID, model ID, and bytes\n  // of the audio we want as an input. Change these strings to run your own example.\n  /////////////////////////////////////////////////////////////////////////////////////////////\n\n  // Your PAT (Personal Access Token) can be found in the Account\'s Security section\n  const PAT = "YOUR_PAT_HERE";\n  // Specify the correct user_id/app_id pairings\n  // Since you\'re making inferences outside your app\'s scope\n  const USER_ID = "facebook";\n  const APP_ID = "asr";\n  // Change these to make your own predictions\n  const MODEL_ID = "asr-wav2vec2-base-960h-english";\n  const AUDIO_BYTES_STRING = "YOUR_BYTES_STRING_HERE";\n\n  ///////////////////////////////////////////////////////////////////////////////////\n  // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n  /////////////////////////////////////////////////////////////////////////////////// \n\n  const raw = JSON.stringify({\n    "user_app_id": {\n      "user_id": USER_ID,\n      "app_id": APP_ID\n    },\n    "inputs": [\n      {\n        "data": {\n          "audio": {\n            "base64": AUDIO_BYTES_STRING\n          }\n        }\n      }\n    ]\n  });\n\n  const requestOptions = {\n    method: \'POST\',\n    headers: {\n      \'Accept\': \'application/json\',\n      \'Authorization\': \'Key \' + PAT\n    },\n    body: raw\n  };\n\n  fetch(`https://api.clarifai.com/v2/models/${MODEL_ID}/outputs`, requestOptions)\n    .then(response => response.text())\n    .then(result => console.log(result))\n    .catch(error => console.log(\'error\', error));\n<\/script>',h='//index.js file\n\n////////////////////////////////////////////////////////////////////////////////////////\n//  In this section, we set the user authentication, user and app ID, model ID, and\n// audio URL. Change these strings to run your own example.\n///////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\nconst PAT = "YOUR_PAT_HERE"\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\nconst USER_ID = "facebook"\nconst APP_ID = "asr"\n// Change these to make your own predictions\nconst MODEL_ID = "asr-wav2vec2-base-960h-english"\nconst AUDIO_URL = "https://samples.clarifai.com/negative_sentence_1.wav"\n\n/////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n/////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\nstub.PostModelOutputs(\n  {\n    user_app_id: {\n      "user_id": USER_ID,\n      "app_id": APP_ID,\n    },\n    model_id: MODEL_ID,\n    inputs: [{ data: { audio: { url: AUDIO_URL } } }],\n  },\n  metadata,\n  (err, response) => {\n    if (err) {\n      throw new Error(err);\n    }\n\n    if (response.status.code !== 10000) {\n      throw new Error(\n        "Post workflow results failed, status: " + response.status.description\n      );\n    }\n\n    // Since we have one input, one output will exist here\n    const output = response.outputs[0];\n\n    // Print the output\n    console.log(output.data.text.raw)\n\n  }\n);\n',_='//index.js file\n\n////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model ID, and\n// audio file location. Change these strings to run your own example.\n///////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\nconst PAT = "YOUR_PAT_HERE"\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\nconst USER_ID = "facebook"\nconst APP_ID = "asr"\n// Change these to make your own predictions\nconst MODEL_ID = "asr-wav2vec2-base-960h-english"\nconst AUDIO_FILE_LOCATION = "YOUR_AUDIO_FILE_LOCATION_HERE"\n\n/////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n/////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\nconst fs = require("fs");\nconst audioBytes = fs.readFileSync(AUDIO_FILE_LOCATION);\n\nstub.PostModelOutputs(\n  {\n    user_app_id: {\n      "user_id": USER_ID,\n      "app_id": APP_ID,\n    },\n    model_id: MODEL_ID,\n    inputs: [{ data: { audio: { base64: audioBytes } } }],\n  },\n  metadata,\n  (err, response) => {\n    if (err) {\n      throw new Error(err);\n    }\n\n    if (response.status.code !== 10000) {\n      throw new Error(\n        "Post workflow results failed, status: " + response.status.description\n      );\n    }\n\n    // Since we have one input, one output will exist here\n    const output = response.outputs[0];\n\n    // Print the output\n    console.log(output.data.text.raw)\n\n  }\n);\n',f='package com.clarifai.example;\n\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.grpc.api.status.StatusCode;\n\npublic class ClarifaiExample {\n\n    //////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model ID, and\n    // audio URL. Change these strings to run your own example.\n    //////////////////////////////////////////////////////////////////////////////////////\n    \n    //Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    static final String USER_ID = "facebook";\n    static final String APP_ID = "asr";\n    // Change these to make your own predictions\n    static final String MODEL_ID = "asr-wav2vec2-base-960h-english";\n    static final String AUDIO_URL = "https://samples.clarifai.com/negative_sentence_1.wav";\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n    public static void main(String[] args) {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n                .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        MultiOutputResponse postModelOutputsResponse = stub.postModelOutputs(\n                PostModelOutputsRequest.newBuilder()\n                        .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n                        .setModelId(MODEL_ID)\n                        .addInputs(\n                                Input.newBuilder().setData(\n                                        Data.newBuilder().setAudio(\n                                                Audio.newBuilder().setUrl(AUDIO_URL)\n                                        )\n                                )\n                        )\n                        .build()\n        );\n\n        if (postModelOutputsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            System.out.println(postModelOutputsResponse.getStatus());\n            throw new RuntimeException("Post workflow results failed, status: " + postModelOutputsResponse.getStatus().getDescription());\n        }\n\n        Output output = postModelOutputsResponse.getOutputs(0);\n\n        // Print the output\n        System.out.println(output.getData().getText().getRaw());\n\n    }\n\n}\n',g='package com.clarifai.example;\n\nimport java.io.File;\nimport java.io.IOException;\nimport java.nio.file.Files;\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.grpc.api.status.StatusCode;\nimport com.google.protobuf.ByteString;\n\npublic class ClarifaiExample {\n\n    ///////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model ID, and\n    // audio file location. Change these strings to run your own example.\n    ///////////////////////////////////////////////////////////////////////////////////////\n\n    //Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    static final String USER_ID = "facebook";\n    static final String APP_ID = "asr";\n    // Change these to make your own predictions\n    static final String MODEL_ID = "asr-wav2vec2-base-960h-english";\n    static final String AUDIO_FILE_LOCATION = "YOUR_AUDIO_FILE_LOCATION_HERE";\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n    public static void main(String[] args) throws IOException {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n                .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        MultiOutputResponse postModelOutputsResponse = stub.postModelOutputs(\n                PostModelOutputsRequest.newBuilder()\n                        .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n                        .setModelId(MODEL_ID)\n                        .addInputs(\n                                Input.newBuilder().setData(\n                                        Data.newBuilder().setAudio(\n                                                Audio.newBuilder().setBase64(ByteString.copyFrom(Files.readAllBytes(\n                                                        new File(AUDIO_FILE_LOCATION).toPath()\n                                                )))\n                                        )\n                                )\n                        )\n                        .build()\n        );\n\n        if (postModelOutputsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            System.out.println(postModelOutputsResponse.getStatus());\n            throw new RuntimeException("Post workflow results failed, status: " + postModelOutputsResponse.getStatus().getDescription());\n        }\n\n        Output output = postModelOutputsResponse.getOutputs(0);\n\n        // Print the output\n        System.out.println(output.getData().getText().getRaw());\n\n    }\n\n}\n',m='<?php\n\nrequire __DIR__ . "/vendor/autoload.php";\n\n///////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model ID, and\n// audio URL. Change these strings to run your own example.\n///////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\n$PAT = "YOUR_PAT_HERE";\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\n$USER_ID = "facebook";\n$APP_ID = "asr";\n// Change these to make your own predictions\n$MODEL_ID = "asr-wav2vec2-base-960h-english";\n$AUDIO_URL = "https://samples.clarifai.com/negative_sentence_1.wav";\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\Api\\Audio;\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\PostModelOutputsRequest;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ["Authorization" => ["Key " . $PAT]];\n\n$userDataObject = new UserAppIDSet([\n    "user_id" => $USER_ID,\n    "app_id" => $APP_ID\n]);\n\n// Let\'s make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client->PostModelOutputs(\n    // The request object carries the request along with the request status and other metadata related to the request itself\n    new PostModelOutputsRequest([\n        "user_app_id" => $userDataObject,\n        "model_id" => $MODEL_ID,\n        "inputs" => [\n            new Input([\n                "data" => new Data([\n                    "audio" => new Audio([\n                        "url" => $AUDIO_URL\n                    ])\n                ])\n            ])\n        ]\n    ]),\n    $metadata\n)->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception("Error: {$status->details}");\n}\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure \n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    print $response->getStatus()->getDetails();\n    throw new Exception("Failure response: " . $response->getStatus()->getDescription());\n}\n\n// We\'ll get one output for each input we used above. Because of one input, we have here one output\n$output = $response->getOutputs()[0];\n\n// Print the output\necho $output->getData()->getText()->getRaw();\n\n?>',A='<?php\n\nrequire __DIR__ . "/vendor/autoload.php";\n\n///////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model ID, and\n// audio file location. Change these strings to run your own example.\n//////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\n$PAT = "YOUR_PAT_HERE";\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\n$USER_ID = "facebook";\n$APP_ID = "asr";\n// Change these to make your own predictions\n$MODEL_ID = "asr-wav2vec2-base-960h-english";\n$AUDIO_FILE_LOCATION = "YOUR_AUDIO_FILE_LOCATION_HERE";\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\Api\\Audio;\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\PostModelOutputsRequest;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ["Authorization" => ["Key " . $PAT]];\n\n$userDataObject = new UserAppIDSet([\n    "user_id" => $USER_ID,\n    "app_id" => $APP_ID\n]);\n\n$audioData = file_get_contents($AUDIO_FILE_LOCATION); // Get the audio bytes data from the location\n\n// Let\'s make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client->PostModelOutputs(\n    // The request object carries the request along with the request status and other metadata related to the request itself\n    new PostModelOutputsRequest([\n        "user_app_id" => $userDataObject,\n        "model_id" => $MODEL_ID,\n        "inputs" => [\n            new Input([\n                "data" => new Data([\n                    "audio" => new Audio([\n                        "base64" => $audioData\n                    ])\n                ])\n            ])\n        ]\n    ]),\n    $metadata\n)->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception("Error: {$status->details}");\n}\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure \n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    print $response->getStatus()->getDetails();\n    throw new Exception("Failure response: " . $response->getStatus()->getDescription());\n}\n\n// We\'ll get one output for each input we used above. Because of one input, we have here one output\n$output = $response->getOutputs()[0];\n\n// Print the output\necho $output->getData()->getText()->getRaw();\n\n?>',I='curl -X POST "https://api.clarifai.com/v2/users/facebook/apps/asr/models/asr-wav2vec2-base-960h-english/outputs" \\\n  -H "authorization: Key YOUR_PAT_HERE" \\\n  -H "content-type: application/json" \\\n  -d \'{\n    "inputs": [\n        {\n          "data": {\n            "audio": {\n              "url": "https://samples.clarifai.com/negative_sentence_1.wav"\n          }\n        }\n      }\n    ]\n}\'',b='curl -X POST "https://api.clarifai.com/v2/users/facebook/apps/asr/models/asr-wav2vec2-base-960h-english/outputs" \\\n  -H "authorization: Key YOUR_PAT_HERE" \\\n  -H "content-type: application/json" \\\n  -d \'{\n    "inputs": [\n        {\n          "data": {\n            "audio": {\n              "base64": "YOUR_BYTES_STRING_HERE"\n          }\n        }\n      }\n    ]\n}\'',E="I AM NOT FLYING TO ENGLAND",O={description:"Make predictions on audio inputs",sidebar_position:5},D="Audio",w={},S=[{value:"Predict via URL",id:"predict-via-url",level:2},{value:"Predict via Bytes",id:"predict-via-bytes",level:2}];function T(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",header:"header",p:"p",strong:"strong",...(0,i.R)(),...e.components},{Details:t}=n;return t||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"audio",children:"Audio"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Make predictions on audio inputs"})}),"\n",(0,s.jsx)("hr",{}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Input"}),": Audio"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Output"}),": Text"]}),"\n",(0,s.jsx)(n.p,{children:"To get predictions for a given audio input, you need to supply the audio along with the specific model from which you wish to receive predictions. You can supply the input via a publicly accessible URL or by directly sending bytes."}),"\n",(0,s.jsxs)(n.p,{children:["You need to specify your choice of ",(0,s.jsx)(n.a,{href:"https://clarifai.com/explore/models?filterData=%5B%7B%22field%22%3A%22input_fields%22%2C%22value%22%3A%5B%22audio%22%5D%7D%5D&page=1&perPage=24",children:"model"})," for prediction by utilizing the ",(0,s.jsx)(n.code,{children:"MODEL_ID"})," parameter."]}),"\n",(0,s.jsx)(n.p,{children:"The file size of each audio input should be under 20MB. This is typically suitable for a 48kHz audio file lasting up to 60 seconds, recorded with 16-bit audio quality. If your file exceeds this limit, you will need to split it into smaller chunks."}),"\n",(0,s.jsx)(n.p,{children:"You can send up to 128 audio files in one API call."}),"\n","\n","\n","\n","\n","\n","\n","\n","\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsxs)(n.p,{children:["The initialization code used in the following examples is outlined in detail on the ",(0,s.jsx)(n.a,{href:"https://docs.clarifai.com/api-guide/api-overview/api-clients/#client-installation-instructions",children:"client installation page."})]})}),"\n",(0,s.jsx)(n.h2,{id:"predict-via-url",children:"Predict via URL"}),"\n",(0,s.jsxs)(n.p,{children:["Below is an example of how you would use the ",(0,s.jsx)(n.a,{href:"https://clarifai.com/facebook/asr/models/asr-wav2vec2-base-960h-english",children:"asr-wav2vec2-base-960h-english"})," audio transcription model to convert English speech audio, sent via a URL, into English text."]}),"\n",(0,s.jsxs)(o.A,{children:[(0,s.jsx)(r.A,{value:"python",label:"Python",children:(0,s.jsx)(u.A,{className:"language-python",children:l})}),(0,s.jsx)(r.A,{value:"js_rest",label:"JavaScript (REST)",children:(0,s.jsx)(u.A,{className:"language-javascript",children:p})}),(0,s.jsx)(r.A,{value:"nodejs",label:"NodeJS",children:(0,s.jsx)(u.A,{className:"language-javascript",children:h})}),(0,s.jsx)(r.A,{value:"java",label:"Java",children:(0,s.jsx)(u.A,{className:"language-java",children:f})}),(0,s.jsx)(r.A,{value:"php",label:"PHP",children:(0,s.jsx)(u.A,{className:"language-php",children:m})}),(0,s.jsx)(r.A,{value:"curl",label:"cURL",children:(0,s.jsx)(u.A,{className:"language-bash",children:I})})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:"Text Output Example"}),(0,s.jsx)(u.A,{className:"language-text",children:E})]}),"\n",(0,s.jsx)(n.h2,{id:"predict-via-bytes",children:"Predict via Bytes"}),"\n",(0,s.jsxs)(n.p,{children:["Below is an example of how you would use the ",(0,s.jsx)(n.a,{href:"https://clarifai.com/facebook/asr/models/asr-wav2vec2-base-960h-english",children:"asr-wav2vec2-base-960h-english"})," audio transcription model to convert English speech audio, sent as bytes, into English text."]}),"\n",(0,s.jsxs)(o.A,{children:[(0,s.jsx)(r.A,{value:"python",label:"Python",children:(0,s.jsx)(u.A,{className:"language-python",children:c})}),(0,s.jsx)(r.A,{value:"js_rest",label:"JavaScript (REST)",children:(0,s.jsx)(u.A,{className:"language-javascript",children:d})}),(0,s.jsx)(r.A,{value:"nodejs",label:"NodeJS",children:(0,s.jsx)(u.A,{className:"language-javascript",children:_})}),(0,s.jsx)(r.A,{value:"java",label:"Java",children:(0,s.jsx)(u.A,{className:"language-java",children:g})}),(0,s.jsx)(r.A,{value:"php",label:"PHP",children:(0,s.jsx)(u.A,{className:"language-php",children:A})}),(0,s.jsx)(r.A,{value:"curl",label:"cURL",children:(0,s.jsx)(u.A,{className:"language-bash",children:b})})]})]})}function v(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(T,{...e})}):T(e)}},65537:(e,n,t)=>{t.d(n,{A:()=>O});var a=t(96540),s=t(18215),i=t(65627),o=t(56347),r=t(50372),u=t(30604),l=t(11861),c=t(78749);function p(e){return a.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,a.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function d(e){const{values:n,children:t}=e;return(0,a.useMemo)((()=>{const e=n??function(e){return p(e).map((e=>{let{props:{value:n,label:t,attributes:a,default:s}}=e;return{value:n,label:t,attributes:a,default:s}}))}(t);return function(e){const n=(0,l.XI)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,t])}function h(e){let{value:n,tabValues:t}=e;return t.some((e=>e.value===n))}function _(e){let{queryString:n=!1,groupId:t}=e;const s=(0,o.W6)(),i=function(e){let{queryString:n=!1,groupId:t}=e;if("string"==typeof n)return n;if(!1===n)return null;if(!0===n&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:n,groupId:t});return[(0,u.aZ)(i),(0,a.useCallback)((e=>{if(!i)return;const n=new URLSearchParams(s.location.search);n.set(i,e),s.replace({...s.location,search:n.toString()})}),[i,s])]}function f(e){const{defaultValue:n,queryString:t=!1,groupId:s}=e,i=d(e),[o,u]=(0,a.useState)((()=>function(e){let{defaultValue:n,tabValues:t}=e;if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(n){if(!h({value:n,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${n}" but none of its children has the corresponding value. Available values are: ${t.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return n}const a=t.find((e=>e.default))??t[0];if(!a)throw new Error("Unexpected error: 0 tabValues");return a.value}({defaultValue:n,tabValues:i}))),[l,p]=_({queryString:t,groupId:s}),[f,g]=function(e){let{groupId:n}=e;const t=function(e){return e?`docusaurus.tab.${e}`:null}(n),[s,i]=(0,c.Dv)(t);return[s,(0,a.useCallback)((e=>{t&&i.set(e)}),[t,i])]}({groupId:s}),m=(()=>{const e=l??f;return h({value:e,tabValues:i})?e:null})();(0,r.A)((()=>{m&&u(m)}),[m]);return{selectedValue:o,selectValue:(0,a.useCallback)((e=>{if(!h({value:e,tabValues:i}))throw new Error(`Can't select invalid tab value=${e}`);u(e),p(e),g(e)}),[p,g,i]),tabValues:i}}var g=t(9136);const m={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var A=t(74848);function I(e){let{className:n,block:t,selectedValue:a,selectValue:o,tabValues:r}=e;const u=[],{blockElementScrollPositionUntilNextRender:l}=(0,i.a_)(),c=e=>{const n=e.currentTarget,t=u.indexOf(n),s=r[t].value;s!==a&&(l(n),o(s))},p=e=>{let n=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const t=u.indexOf(e.currentTarget)+1;n=u[t]??u[0];break}case"ArrowLeft":{const t=u.indexOf(e.currentTarget)-1;n=u[t]??u[u.length-1];break}}n?.focus()};return(0,A.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,s.A)("tabs",{"tabs--block":t},n),children:r.map((e=>{let{value:n,label:t,attributes:i}=e;return(0,A.jsx)("li",{role:"tab",tabIndex:a===n?0:-1,"aria-selected":a===n,ref:e=>{u.push(e)},onKeyDown:p,onClick:c,...i,className:(0,s.A)("tabs__item",m.tabItem,i?.className,{"tabs__item--active":a===n}),children:t??n},n)}))})}function b(e){let{lazy:n,children:t,selectedValue:i}=e;const o=(Array.isArray(t)?t:[t]).filter(Boolean);if(n){const e=o.find((e=>e.props.value===i));return e?(0,a.cloneElement)(e,{className:(0,s.A)("margin-top--md",e.props.className)}):null}return(0,A.jsx)("div",{className:"margin-top--md",children:o.map(((e,n)=>(0,a.cloneElement)(e,{key:n,hidden:e.props.value!==i})))})}function E(e){const n=f(e);return(0,A.jsxs)("div",{className:(0,s.A)("tabs-container",m.tabList),children:[(0,A.jsx)(I,{...n,...e}),(0,A.jsx)(b,{...n,...e})]})}function O(e){const n=(0,g.A)();return(0,A.jsx)(E,{...e,children:p(e.children)},String(n))}},79329:(e,n,t)=>{t.d(n,{A:()=>o});t(96540);var a=t(18215);const s={tabItem:"tabItem_Ymn6"};var i=t(74848);function o(e){let{children:n,hidden:t,className:o}=e;return(0,i.jsx)("div",{role:"tabpanel",className:(0,a.A)(s.tabItem,o),hidden:t,children:n})}}}]);
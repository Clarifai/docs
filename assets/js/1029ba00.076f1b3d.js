"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[2922],{3905:(e,t,i)=>{i.d(t,{Zo:()=>d,kt:()=>m});var n=i(67294);function a(e,t,i){return t in e?Object.defineProperty(e,t,{value:i,enumerable:!0,configurable:!0,writable:!0}):e[t]=i,e}function o(e,t){var i=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),i.push.apply(i,n)}return i}function r(e){for(var t=1;t<arguments.length;t++){var i=null!=arguments[t]?arguments[t]:{};t%2?o(Object(i),!0).forEach((function(t){a(e,t,i[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(i)):o(Object(i)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(i,t))}))}return e}function s(e,t){if(null==e)return{};var i,n,a=function(e,t){if(null==e)return{};var i,n,a={},o=Object.keys(e);for(n=0;n<o.length;n++)i=o[n],t.indexOf(i)>=0||(a[i]=e[i]);return a}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)i=o[n],t.indexOf(i)>=0||Object.prototype.propertyIsEnumerable.call(e,i)&&(a[i]=e[i])}return a}var l=n.createContext({}),c=function(e){var t=n.useContext(l),i=t;return e&&(i="function"==typeof e?e(t):r(r({},t),e)),i},d=function(e){var t=c(e.components);return n.createElement(l.Provider,{value:t},e.children)},p="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},f=n.forwardRef((function(e,t){var i=e.components,a=e.mdxType,o=e.originalType,l=e.parentName,d=s(e,["components","mdxType","originalType","parentName"]),p=c(i),f=a,m=p["".concat(l,".").concat(f)]||p[f]||u[f]||o;return i?n.createElement(m,r(r({ref:t},d),{},{components:i})):n.createElement(m,r({ref:t},d))}));function m(e,t){var i=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var o=i.length,r=new Array(o);r[0]=f;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[p]="string"==typeof e?e:a,r[1]=s;for(var c=2;c<o;c++)r[c]=i[c];return n.createElement.apply(null,r)}return n.createElement.apply(null,i)}f.displayName="MDXCreateElement"},16899:(e,t,i)=>{i.r(t),i.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>u,frontMatter:()=>o,metadata:()=>s,toc:()=>c});var n=i(87462),a=(i(67294),i(3905));const o={description:"Learn about our visual detection templates",sidebar_position:2},r="Visual Detection Templates",s={unversionedId:"portal-guide/model/deep-training/visual-detection-templates",id:"portal-guide/model/deep-training/visual-detection-templates",title:"Visual Detection Templates",description:"Learn about our visual detection templates",source:"@site/docs/portal-guide/model/deep-training/visual-detection-templates.md",sourceDirName:"portal-guide/model/deep-training",slug:"/portal-guide/model/deep-training/visual-detection-templates",permalink:"/portal-guide/model/deep-training/visual-detection-templates",draft:!1,tags:[],version:"current",sidebarPosition:2,frontMatter:{description:"Learn about our visual detection templates",sidebar_position:2},sidebar:"tutorialSidebar",previous:{title:"Visual Classification Templates",permalink:"/portal-guide/model/deep-training/visual-classification-templates"},next:{title:"Visual Embedding Templates",permalink:"/portal-guide/model/deep-training/visual-embedding-templates"}},l={},c=[{value:"MMDetection_YoloF",id:"mmdetection_yolof",level:2},{value:"MMDetection_SSD",id:"mmdetection_ssd",level:2},{value:"MMDetection_FasterRCNN",id:"mmdetection_fasterrcnn",level:2},{value:"Clarifai_InceptionV4",id:"clarifai_inceptionv4",level:2},{value:"Clarifai_InceptionV2",id:"clarifai_inceptionv2",level:2}],d={toc:c},p="wrapper";function u(e){let{components:t,...i}=e;return(0,a.kt)(p,(0,n.Z)({},d,i,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"visual-detection-templates"},"Visual Detection Templates"),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"Learn about our visual detection templates")),(0,a.kt)("hr",null),(0,a.kt)("p",null,"Detection templates make it easy to build models that can identify objects within a region of your images or videos. Detection models return concepts and bounding boxes."),(0,a.kt)("h2",{id:"mmdetection_yolof"},"MMDetection_YoloF"),(0,a.kt)("p",null,"This is a deep learning template model from MMDetection that focuses on object detection using the YOLO (You Only Look Once) framework."),(0,a.kt)("p",null,'MMDetection, short for "OpenMMLab Detection Toolbox and Benchmark," is an open-source software framework developed by OpenMMLab. It is designed to facilitate research and development in the field of object detection and instance segmentation. MMDetection provides a comprehensive collection of state-of-the-art models, datasets, and evaluation metrics, making it a valuable resource for both academic and industrial applications.'),(0,a.kt)("p",null,"The MMDetection_YoloF template leverages the power of convolutional neural networks (CNNs) and advanced techniques like anchor-based prediction and feature pyramid networks to accurately detect and localize objects in images or videos."),(0,a.kt)("p",null,"With its robust architecture and pretrained weights, MMDetection_YoloF provides a strong foundation for developers and researchers to build custom object detection solutions for various use cases."),(0,a.kt)("h2",{id:"mmdetection_ssd"},"MMDetection_SSD"),(0,a.kt)("p",null,"This template is an implementation of the SSD object detection algorithm within the MMDetection framework, offering a convenient and powerful tool for object detection tasks. The Single Shot MultiBox Detector (SSD) architecture is a popular object detection algorithm that offers a good trade-off between accuracy and speed."),(0,a.kt)("p",null,"The SSD model in MMDetection is designed to detect and localize objects in images using a single deep neural network. It achieves this by dividing the input image into a grid of cells and predicting object bounding boxes and class probabilities within each cell. SSD incorporates multiple convolutional layers of different scales to capture objects of various sizes and aspect ratios, allowing it to detect objects at different scales in a single pass."),(0,a.kt)("p",null,"MMDetection_SSD provides a pre-configured implementation of the SSD architecture along with trained weights on standard benchmark datasets such as COCO and VOC. This allows users to utilize the model out of the box for various object detection tasks or as a starting point for further customization and fine-tuning."),(0,a.kt)("p",null,"By leveraging the MMDetection framework, users can take advantage of its data pre-processing, model training, and evaluation capabilities to train and evaluate the MMDetection_SSD model on their own datasets. The framework also provides tools for visualizing and analyzing the detection results."),(0,a.kt)("h2",{id:"mmdetection_fasterrcnn"},"MMDetection_FasterRCNN"),(0,a.kt)("p",null,"MMDetection_FasterRCNN refers to a specific model implemented in the MMDetection framework that is based on the Faster R-CNN (Region-based Convolutional Neural Networks) architecture. Faster R-CNN is a widely used and highly effective object detection algorithm."),(0,a.kt)("p",null,"The Faster R-CNN algorithm consists of two main components: a region proposal network (RPN) and a region-based CNN for detection. The RPN generates potential object bounding box proposals, and the region-based CNN classifies and refines these proposals to produce the final detection results."),(0,a.kt)("p",null,"In MMDetection, the MMDetection_FasterRCNN model provides a pre-configured implementation of the Faster R-CNN architecture along with pre-trained weights on standard benchmark datasets like COCO and VOC. It allows users to utilize the model out of the box for object detection tasks or as a starting point for further customization and fine-tuning."),(0,a.kt)("p",null,"MMDetection_FasterRCNN leverages the MMDetection framework's capabilities for data preprocessing, model training, and evaluation. Users can train the model on their own datasets, adjust hyperparameters, and analyze the detection results using the provided tools."),(0,a.kt)("p",null,"The Faster R-CNN algorithm has been proven to achieve excellent performance in terms of accuracy, making MMDetection_FasterRCNN a valuable tool for a wide range of object detection applications."),(0,a.kt)("h2",{id:"clarifai_inceptionv4"},"Clarifai_InceptionV4"),(0,a.kt)("p",null,"This is a visual detector template based on RetinaNet, a popular object detection framework, that utilizes the InceptionV4 architecture as its backbone."),(0,a.kt)("p",null,"InceptionV4 is a variant of the Inception architecture, which was originally introduced by Google for image classification tasks. The InceptionV4 model is a convolutional neural network (CNN) that is designed to extract high-level features from images for tasks such as object recognition, classification, and detection. It incorporates various innovative techniques, including inception modules with multiple parallel branches, factorized convolutions, and residual connections, to enhance its performance and efficiency."),(0,a.kt)("p",null,"Clarifai_InceptionV4 template leverages the strengths of InceptionV4 by applying it at multiple image scales, allowing for robust detection across a range of object sizes."),(0,a.kt)("p",null,"Compared to InceptionV2, Clarifai_InceptionV4 sacrifices speed for increased accuracy. While InceptionV2 is faster, Clarifai_InceptionV4 is slower but offers improved precision in object detection tasks. This makes it well-suited for applications that prioritize accuracy over real-time inference."),(0,a.kt)("p",null,"Clarifai_InceptionV4 is pretrained on either the COCO (Common Objects in Context) dataset or the OpenImages dataset. COCO is a widely used benchmark dataset for object detection, while OpenImages is a large-scale dataset with a diverse range of object categories. Pretraining on these datasets enables the model to learn general representations of objects, improving its ability to detect and classify objects accurately."),(0,a.kt)("p",null,"By combining the strengths of the RetinaNet framework, the powerful InceptionV4 backbone, and pretrained weights on COCO or OpenImages, Clarifai_InceptionV4 provides a robust and accurate solution for object detection tasks, making it a valuable tool for various computer vision applications."),(0,a.kt)("h2",{id:"clarifai_inceptionv2"},"Clarifai_InceptionV2"),(0,a.kt)("p",null,"This is a visual detector template based on RetinaNet using the Inception V2 backbone architecture, which is applied at multiple image scales. It offers a balance between speed and accuracy. Compared to InceptionV4, InceptionV2 is faster but provides slightly lower accuracy."),(0,a.kt)("p",null,"The model can be pre-trained on either the COCO (Common Objects in Context) dataset or the OpenImages dataset. These datasets contain a wide range of labeled images, enabling the model to learn to detect various objects and entities in images. The choice of dataset for pretraining depends on the specific application and the types of objects or entities you want the model to detect."),(0,a.kt)("p",null,"Clarifai_InceptionV2 serves as an efficient deep learning template that leverages the Inception V2 backbone architecture, providing a good trade-off between speed and accuracy for object detection tasks."))}u.isMDXComponent=!0}}]);
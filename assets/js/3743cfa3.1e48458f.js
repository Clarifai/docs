"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[4126],{85162:(a,e,t)=>{t.d(e,{Z:()=>i});var n=t(67294),r=t(86010);const o={tabItem:"tabItem_Ymn6"};function i(a){let{children:e,hidden:t,className:i}=a;return n.createElement("div",{role:"tabpanel",className:(0,r.Z)(o.tabItem,i),hidden:t},e)}},74866:(a,e,t)=>{t.d(e,{Z:()=>E});var n=t(87462),r=t(67294),o=t(86010),i=t(12466),l=t(16550),s=t(91980),u=t(67392),p=t(50012);function d(a){return function(a){return r.Children.map(a,(a=>{if(!a||(0,r.isValidElement)(a)&&function(a){const{props:e}=a;return!!e&&"object"==typeof e&&"value"in e}(a))return a;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof a.type?a.type:a.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}(a).map((a=>{let{props:{value:e,label:t,attributes:n,default:r}}=a;return{value:e,label:t,attributes:n,default:r}}))}function c(a){const{values:e,children:t}=a;return(0,r.useMemo)((()=>{const a=e??d(t);return function(a){const e=(0,u.l)(a,((a,e)=>a.value===e.value));if(e.length>0)throw new Error(`Docusaurus error: Duplicate values "${e.map((a=>a.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(a),a}),[e,t])}function m(a){let{value:e,tabValues:t}=a;return t.some((a=>a.value===e))}function f(a){let{queryString:e=!1,groupId:t}=a;const n=(0,l.k6)(),o=function(a){let{queryString:e=!1,groupId:t}=a;if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:e,groupId:t});return[(0,s._X)(o),(0,r.useCallback)((a=>{if(!o)return;const e=new URLSearchParams(n.location.search);e.set(o,a),n.replace({...n.location,search:e.toString()})}),[o,n])]}function h(a){const{defaultValue:e,queryString:t=!1,groupId:n}=a,o=c(a),[i,l]=(0,r.useState)((()=>function(a){let{defaultValue:e,tabValues:t}=a;if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!m({value:e,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${t.map((a=>a.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const n=t.find((a=>a.default))??t[0];if(!n)throw new Error("Unexpected error: 0 tabValues");return n.value}({defaultValue:e,tabValues:o}))),[s,u]=f({queryString:t,groupId:n}),[d,h]=function(a){let{groupId:e}=a;const t=function(a){return a?`docusaurus.tab.${a}`:null}(e),[n,o]=(0,p.Nk)(t);return[n,(0,r.useCallback)((a=>{t&&o.set(a)}),[t,o])]}({groupId:n}),k=(()=>{const a=s??d;return m({value:a,tabValues:o})?a:null})();(0,r.useLayoutEffect)((()=>{k&&l(k)}),[k]);return{selectedValue:i,selectValue:(0,r.useCallback)((a=>{if(!m({value:a,tabValues:o}))throw new Error(`Can't select invalid tab value=${a}`);l(a),u(a),h(a)}),[u,h,o]),tabValues:o}}var k=t(72389);const _={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};function b(a){let{className:e,block:t,selectedValue:l,selectValue:s,tabValues:u}=a;const p=[],{blockElementScrollPositionUntilNextRender:d}=(0,i.o5)(),c=a=>{const e=a.currentTarget,t=p.indexOf(e),n=u[t].value;n!==l&&(d(e),s(n))},m=a=>{let e=null;switch(a.key){case"Enter":c(a);break;case"ArrowRight":{const t=p.indexOf(a.currentTarget)+1;e=p[t]??p[0];break}case"ArrowLeft":{const t=p.indexOf(a.currentTarget)-1;e=p[t]??p[p.length-1];break}}e?.focus()};return r.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,o.Z)("tabs",{"tabs--block":t},e)},u.map((a=>{let{value:e,label:t,attributes:i}=a;return r.createElement("li",(0,n.Z)({role:"tab",tabIndex:l===e?0:-1,"aria-selected":l===e,key:e,ref:a=>p.push(a),onKeyDown:m,onClick:c},i,{className:(0,o.Z)("tabs__item",_.tabItem,i?.className,{"tabs__item--active":l===e})}),t??e)})))}function y(a){let{lazy:e,children:t,selectedValue:n}=a;const o=(Array.isArray(t)?t:[t]).filter(Boolean);if(e){const a=o.find((a=>a.props.value===n));return a?(0,r.cloneElement)(a,{className:"margin-top--md"}):null}return r.createElement("div",{className:"margin-top--md"},o.map(((a,e)=>(0,r.cloneElement)(a,{key:e,hidden:a.props.value!==n}))))}function T(a){const e=h(a);return r.createElement("div",{className:(0,o.Z)("tabs-container",_.tabList)},r.createElement(b,(0,n.Z)({},a,e)),r.createElement(y,(0,n.Z)({},a,e)))}function E(a){const e=(0,k.Z)();return r.createElement(T,(0,n.Z)({key:String(e)},a))}},1064:(a,e,t)=>{t.r(e),t.d(e,{assets:()=>k,contentTitle:()=>f,default:()=>T,frontMatter:()=>m,metadata:()=>h,toc:()=>_});var n=t(87462),r=(t(67294),t(3905)),o=t(74866),i=t(85162),l=t(90814);const s='###################################################################################\n# In this section, we set the user authentication, user and app ID, dataset ID,\n# and Databricks folder path. Change these strings to run your own example.\n##################################################################################\n\n# Your PAT (Personal Access Token) can be found in the Account\'s Security section\nPAT = "YOUR_PAT_HERE"\nUSER_ID = "YOUR_USER_ID_HERE"\nAPP_ID = "YOUR_APP_ID_HERE"\nDATASET_ID = "YOUR_DATASET_ID_HERE"\n# URL path of your Databricks folder; Example: "/Volumes/test1/default/volume1/folder1"\nFOLDER_PATH = "YOUR_DATABRICKS_FOLDER_PATH_HERE"\n\n############################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n############################################################################\n\n# Import the required packages\nimport os\nfrom clarifaipyspark.client import ClarifaiPySpark\n\n# Set Clarifai PAT as environment variable\nos.environ["CLARIFAI_PAT"] = PAT\n# Create a Clarifai-PySpark client object to connect to your app on Clarifai\ncspark_obj = ClarifaiPySpark(user_id=USER_ID, app_id=APP_ID)\n# This creates a new dataset in the app if it doesn\'t already exist\ndataset_obj = cspark_obj.dataset(dataset_id=DATASET_ID)\n\n#  Upload from a volume folder\ndataset_obj.upload_dataset_from_folder(\n    folder_path=FOLDER_PATH,\n    input_type="image",\n    # input_type="text", # Or, specify to upload text data\n    labels=False,  # Set to True if the folder name serves as the label for all the images within it\n)\n',u='###################################################################################\n# In this section, we set the user authentication, user and app ID, dataset ID,\n# and Databricks CSV path. Change these strings to run your own example.\n##################################################################################\n\n# Your PAT (Personal Access Token) can be found in the Account\'s Security section\nPAT = "YOUR_PAT_HERE"\nUSER_ID = "YOUR_USER_ID_HERE"\nAPP_ID = "YOUR_APP_ID_HERE"\nDATASET_ID = "YOUR_DATASET_ID_HERE"\n# URL path of your Databricks CSV file; Example: "/Volumes/test1/default/volume1/SMS_train_1.csv"\nCSV_PATH = "YOUR_DATABRICKS_CSV_PATH_HERE"\n\n############################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n############################################################################\n\n# Import the required packages\nimport os\nfrom clarifaipyspark.client import ClarifaiPySpark\n\n# Set Clarifai PAT as environment variable\nos.environ["CLARIFAI_PAT"] = PAT\n# Create a Clarifai-PySpark client object to connect to your app on Clarifai\ncspark_obj = ClarifaiPySpark(user_id=USER_ID, app_id=APP_ID)\n# This creates a new dataset in the app if it doesn\'t already exist\ndataset_obj = cspark_obj.dataset(dataset_id=DATASET_ID)\n\n#  Upload from CSV\ndataset_obj.upload_dataset_from_csv(\n    csv_path=CSV_PATH,\n    input_type="text",\n    # input_type="image", # Or, specify to upload image data\n    labels=False,  # Set to True if "concepts" column exists\n    csv_type="raw",  # Or, specify as "url" or "filepath"\n    source="volume",  # Specify as "s3" to use a CSV file directly from your AWS S3 bucket\n)\n',p='################################################################################### \n# In this section, we set the user authentication, user and app ID, dataset ID,\n# and Databricks delta table path. Change these strings to run your own example.\n##################################################################################\n\n# Your PAT (Personal Access Token) can be found in the Account\'s Security section\nPAT = "YOUR_PAT_HERE"\nUSER_ID = "YOUR_USER_ID_HERE"\nAPP_ID = "YOUR_APP_ID_HERE"\nDATASET_ID = "YOUR_DATASET_ID_HERE"\n# URL path of your Databricks delta table \nTABLE_PATH = "YOUR_DATABRICKS_TABLE_PATH_HERE"\n\n############################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n############################################################################\n\n# Import the required packages\nimport os\nfrom clarifaipyspark.client import ClarifaiPySpark\n\n# Set Clarifai PAT as environment variable\nos.environ["CLARIFAI_PAT"] = PAT\n# Create a Clarifai-PySpark client object to connect to your app on Clarifai\ncspark_obj = ClarifaiPySpark(user_id=USER_ID, app_id=APP_ID)\n# This creates a new dataset in the app if it doesn\'t already exist\ndataset_obj = cspark_obj.dataset(dataset_id=DATASET_ID)\n\n#  Upload from delta table\ndataset_obj.upload_dataset_from_table(\n    table_path=TABLE_PATH,\n    input_type="text",\n    # input_type="image", # Or, specify to upload image data\n    labels=False,  # Set to True if "concepts" column exists\n    table_type="raw", # Or, specify as "url" or "filepath"     \n)\n',d='######################################################################################\n# In this section, we set the user authentication, user and app ID, and dataset ID. \n# Change these strings to run your own example.\n#####################################################################################\n\n# Your PAT (Personal Access Token) can be found in the Account\'s Security section\nPAT = "YOUR_PAT_HERE"\nUSER_ID = "YOUR_USER_ID_HERE"\nAPP_ID = "YOUR_APP_ID_HERE"\nDATASET_ID = "YOUR_DATASET_ID_HERE"\n\n############################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n############################################################################\n\n# Import the required packages\nimport os\nfrom clarifaipyspark.client import ClarifaiPySpark\n\n# Set Clarifai PAT as environment variable\nos.environ["CLARIFAI_PAT"] = PAT\n# Create a Clarifai-PySpark client object to connect to your app on Clarifai\ncspark_obj = ClarifaiPySpark(user_id=USER_ID, app_id=APP_ID)\n# This creates a new dataset in the app if it doesn\'t already exist\ndataset_obj = cspark_obj.dataset(dataset_id=DATASET_ID)\n\n#  Upload from dataframe\ndataset_obj.upload_dataset_from_dataframe(\n    dataframe=spark_dataframe,\n    input_type="text",\n    # input_type="image", # Or, specify to upload image data\n    labels=False,  # Set to True if "concepts" column exists\n    csv_type="raw", # Or, specify as "url" or "filepath"     \n)\n',c='################################################################################### \n# In this section, we set the user authentication, user and app ID, dataset ID, \n# and volume module path. Change these strings to run your own example.\n##################################################################################\n\n# Your PAT (Personal Access Token) can be found in the Account\'s Security section\nPAT = "YOUR_PAT_HERE"\nUSER_ID = "YOUR_USER_ID_HERE"\nAPP_ID = "YOUR_APP_ID_HERE"\nDATASET_ID = "YOUR_DATASET_ID_HERE"\n# URL path of your Databricks volume\nVOLUME_MODULE_PATH = "YOUR_VOLUME_MODULE_PATH_HERE"\n\n############################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n############################################################################\n\n# Import the required packages\nimport os\nfrom clarifaipyspark.client import ClarifaiPySpark\n\n# Set Clarifai PAT as environment variable\nos.environ["CLARIFAI_PAT"] = PAT\n# Create a Clarifai-PySpark client object to connect to your app on Clarifai\ncspark_obj = ClarifaiPySpark(user_id=USER_ID, app_id=APP_ID)\n# This creates a new dataset in the app if it doesn\'t already exist\ndataset_obj = cspark_obj.dataset(dataset_id=DATASET_ID)\n\n#  Upload with custom dataloader\ndataset_obj.upload_dataset_from_dataloader(\n    task="visual-classification",\n    split ="train",\n    module_dir=VOLUME_MODULE_PATH\n  )\n',m={description:"Seamlessly upload your data from Databricks into Clarifai",sidebar_position:1},f="Upload Data",h={unversionedId:"integrations/databricks/upload-data",id:"integrations/databricks/upload-data",title:"Upload Data",description:"Seamlessly upload your data from Databricks into Clarifai",source:"@site/docs/integrations/databricks/upload-data.md",sourceDirName:"integrations/databricks",slug:"/integrations/databricks/upload-data",permalink:"/integrations/databricks/upload-data",draft:!1,editUrl:"https://github.com/Clarifai/docs/blob/main/docs/integrations/databricks/upload-data.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{description:"Seamlessly upload your data from Databricks into Clarifai",sidebar_position:1},sidebar:"tutorialSidebar",previous:{title:"Integrating Clarifai With Databricks",permalink:"/integrations/databricks/"},next:{title:"Fetch Data",permalink:"/integrations/databricks/fetch-data"}},k={},_=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Upload Data From a Volume Folder",id:"upload-data-from-a-volume-folder",level:2},{value:"Upload Data From CSV",id:"upload-data-from-csv",level:2},{value:"Upload From Delta Table",id:"upload-from-delta-table",level:2},{value:"Upload From Dataframe",id:"upload-from-dataframe",level:2},{value:"Upload With Custom Dataloader",id:"upload-with-custom-dataloader",level:2}],b={toc:_},y="wrapper";function T(a){let{components:e,...t}=a;return(0,r.kt)(y,(0,n.Z)({},b,t,{components:e,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"upload-data"},"Upload Data"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Seamlessly upload your data from Databricks into Clarifai")),(0,r.kt)("hr",null),(0,r.kt)("p",null,"You can ingest datasets from Databricks into your Clarifai environment. This allows you to easily take advantage of Clarifai's AI capabilities to analyze and extract insights from your data, without having to manually move it between the two platforms. "),(0,r.kt)("p",null,"Once your data has been uploaded to Clarifai, you can leverage Clarifai\u2019s AI capabilities to get the most out of your data. For example, you can use Clarifai to identify and classify ",(0,r.kt)("a",{parentName:"p",href:"https://docs.clarifai.com/portal-guide/annotate/create-get-update-delete/"},"objects and scenes")," in images. "),(0,r.kt)("p",null,"Let\u2019s illustrate how you can seamlessly transfer data from a Databricks ",(0,r.kt)("a",{parentName:"p",href:"https://docs.databricks.com/en/sql/language-manual/sql-ref-volumes.html"},"volume")," to Clarifai."),(0,r.kt)("h2",{id:"prerequisites"},"Prerequisites"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Databricks notebook development environment. Also, ensure your Databricks workspace is enabled to work with ",(0,r.kt)("a",{parentName:"li",href:"https://docs.databricks.com/en/data-governance/unity-catalog/get-started.html"},"Unity Catalog")),(0,r.kt)("li",{parentName:"ul"},"Get the path URL of the Databricks location having your data "),(0,r.kt)("li",{parentName:"ul"},"Get your PAT (Personal Access Token) from the Clarifai\u2019s portal under the ",(0,r.kt)("a",{parentName:"li",href:"https://clarifai.com/settings/security"},"Settings/Security")," section"),(0,r.kt)("li",{parentName:"ul"},"Get your Clarifai user ID "),(0,r.kt)("li",{parentName:"ul"},"Get the ID of the Clarifai app where you want to upload the data"),(0,r.kt)("li",{parentName:"ul"},"Get the ID of a dataset within your app"),(0,r.kt)("li",{parentName:"ul"},"Install the ",(0,r.kt)("a",{parentName:"li",href:"https://github.com/Clarifai/clarifai-pyspark"},"Clarifai PySpark")," package by running ",(0,r.kt)("inlineCode",{parentName:"li"},"pip install clarifai-pyspark ")),(0,r.kt)("li",{parentName:"ul"},"Install Protocol Buffers by running ",(0,r.kt)("inlineCode",{parentName:"li"},"pip install protobuf==4.24.2 "),". It\u2019s a cross-platform, serialization protocol that describes the structure of the data to be sent ")),(0,r.kt)("admonition",{type:"info"},(0,r.kt)("p",{parentName:"admonition"},"You can learn how to authenticate with the Clarifai platform ",(0,r.kt)("a",{parentName:"p",href:"https://docs.clarifai.com/clarifai-basics/authentication/personal-access-tokens"},"here"),".")),(0,r.kt)("h2",{id:"upload-data-from-a-volume-folder"},"Upload Data From a Volume Folder"),(0,r.kt)("p",null,"You can upload images or text files stored in a Databricks volume to your Clarifai application. It\u2019s important to ensure that the folder exclusively contains either images or text files. "),(0,r.kt)(o.Z,{mdxType:"Tabs"},(0,r.kt)(i.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,r.kt)(l.Z,{className:"language-python",mdxType:"CodeBlock"},s))),(0,r.kt)("h2",{id:"upload-data-from-csv"},"Upload Data From CSV"),(0,r.kt)("p",null,"You can upload data from a CSV file stored in a Databricks volume to your Clarifai application. The CSV file must include two essential columns: ",(0,r.kt)("inlineCode",{parentName:"p"},"inputid")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"input"),". "),(0,r.kt)("p",null,"You can also include additional supported columns such as ",(0,r.kt)("inlineCode",{parentName:"p"},"concepts"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"metadata"),", and ",(0,r.kt)("inlineCode",{parentName:"p"},"geopoints"),". The ",(0,r.kt)("inlineCode",{parentName:"p"},"input")," column within the CSV is a versatile field, capable of accommodating either a file URL or path, or it can contain raw text. "),(0,r.kt)(o.Z,{mdxType:"Tabs"},(0,r.kt)(i.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,r.kt)(l.Z,{className:"language-python",mdxType:"CodeBlock"},u))),(0,r.kt)("h2",{id:"upload-from-delta-table"},"Upload From Delta Table"),(0,r.kt)("p",null,"You can upload data from a delta table in a Databricks volume to your Clarifai application. The table must include two essential columns: ",(0,r.kt)("inlineCode",{parentName:"p"},"inputid")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"input"),". "),(0,r.kt)("p",null,"You can also include additional supported columns such as ",(0,r.kt)("inlineCode",{parentName:"p"},"concepts"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"metadata"),", and ",(0,r.kt)("inlineCode",{parentName:"p"},"geopoints"),". The ",(0,r.kt)("inlineCode",{parentName:"p"},"input")," column within the table is a versatile field, capable of accommodating either a file URL or path, or it can contain raw text. "),(0,r.kt)(o.Z,{mdxType:"Tabs"},(0,r.kt)(i.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,r.kt)(l.Z,{className:"language-python",mdxType:"CodeBlock"},p))),(0,r.kt)("h2",{id:"upload-from-dataframe"},"Upload From Dataframe"),(0,r.kt)("p",null,"You can upload data from a PySpark dataframe in a Databricks volume to your Clarifai application. The dataframe must include two essential columns: ",(0,r.kt)("inlineCode",{parentName:"p"},"inputid")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"input"),"."),(0,r.kt)("p",null,"You can also include additional supported columns such as ",(0,r.kt)("inlineCode",{parentName:"p"},"concepts"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"metadata"),", and ",(0,r.kt)("inlineCode",{parentName:"p"},"geopoints"),". The ",(0,r.kt)("inlineCode",{parentName:"p"},"input")," column within the table is a versatile field, capable of accommodating either a file URL or path, or it can contain raw text. "),(0,r.kt)(o.Z,{mdxType:"Tabs"},(0,r.kt)(i.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,r.kt)(l.Z,{className:"language-python",mdxType:"CodeBlock"},d))),(0,r.kt)("h2",{id:"upload-with-custom-dataloader"},"Upload With Custom Dataloader"),(0,r.kt)("p",null,"You can utilize the custom data loader option if your dataset is stored in an alternative format or necessitates preprocessing. This grants you the flexibility to provide a specialized data loader tailored to your specific requirements."),(0,r.kt)("p",null,"For reference, you can explore a variety of data loader examples ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/Clarifai/examples/tree/main/datasets/upload"},"here"),". "),(0,r.kt)("p",null,"Ensure that the necessary files and folders for the dataloader are stored in Databricks volume storage to facilitate seamless integration and accessibility."),(0,r.kt)(o.Z,{mdxType:"Tabs"},(0,r.kt)(i.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,r.kt)(l.Z,{className:"language-python",mdxType:"CodeBlock"},c))),(0,r.kt)("admonition",{type:"info"},(0,r.kt)("p",{parentName:"admonition"},"You can get examples for integrating Clarifai with Databricks ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/Clarifai/clarifai-pyspark/tree/main/examples"},"here"),".")))}T.isMDXComponent=!0}}]);
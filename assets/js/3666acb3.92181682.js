"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[3953],{85162:(e,n,t)=>{t.d(n,{Z:()=>r});var a=t(67294),i=t(86010);const o={tabItem:"tabItem_Ymn6"};function r(e){let{children:n,hidden:t,className:r}=e;return a.createElement("div",{role:"tabpanel",className:(0,i.Z)(o.tabItem,r),hidden:t},n)}},74866:(e,n,t)=>{t.d(n,{Z:()=>I});var a=t(87462),i=t(67294),o=t(86010),r=t(12466),l=t(16550),s=t(91980),d=t(67392),c=t(50012);function p(e){return function(e){return i.Children.map(e,(e=>{if(!e||(0,i.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}(e).map((e=>{let{props:{value:n,label:t,attributes:a,default:i}}=e;return{value:n,label:t,attributes:a,default:i}}))}function u(e){const{values:n,children:t}=e;return(0,i.useMemo)((()=>{const e=n??p(t);return function(e){const n=(0,d.l)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,t])}function m(e){let{value:n,tabValues:t}=e;return t.some((e=>e.value===n))}function g(e){let{queryString:n=!1,groupId:t}=e;const a=(0,l.k6)(),o=function(e){let{queryString:n=!1,groupId:t}=e;if("string"==typeof n)return n;if(!1===n)return null;if(!0===n&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:n,groupId:t});return[(0,s._X)(o),(0,i.useCallback)((e=>{if(!o)return;const n=new URLSearchParams(a.location.search);n.set(o,e),a.replace({...a.location,search:n.toString()})}),[o,a])]}function f(e){const{defaultValue:n,queryString:t=!1,groupId:a}=e,o=u(e),[r,l]=(0,i.useState)((()=>function(e){let{defaultValue:n,tabValues:t}=e;if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(n){if(!m({value:n,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${n}" but none of its children has the corresponding value. Available values are: ${t.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return n}const a=t.find((e=>e.default))??t[0];if(!a)throw new Error("Unexpected error: 0 tabValues");return a.value}({defaultValue:n,tabValues:o}))),[s,d]=g({queryString:t,groupId:a}),[p,f]=function(e){let{groupId:n}=e;const t=function(e){return e?`docusaurus.tab.${e}`:null}(n),[a,o]=(0,c.Nk)(t);return[a,(0,i.useCallback)((e=>{t&&o.set(e)}),[t,o])]}({groupId:a}),_=(()=>{const e=s??p;return m({value:e,tabValues:o})?e:null})();(0,i.useLayoutEffect)((()=>{_&&l(_)}),[_]);return{selectedValue:r,selectValue:(0,i.useCallback)((e=>{if(!m({value:e,tabValues:o}))throw new Error(`Can't select invalid tab value=${e}`);l(e),d(e),f(e)}),[d,f,o]),tabValues:o}}var _=t(72389);const h={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};function y(e){let{className:n,block:t,selectedValue:l,selectValue:s,tabValues:d}=e;const c=[],{blockElementScrollPositionUntilNextRender:p}=(0,r.o5)(),u=e=>{const n=e.currentTarget,t=c.indexOf(n),a=d[t].value;a!==l&&(p(n),s(a))},m=e=>{let n=null;switch(e.key){case"Enter":u(e);break;case"ArrowRight":{const t=c.indexOf(e.currentTarget)+1;n=c[t]??c[0];break}case"ArrowLeft":{const t=c.indexOf(e.currentTarget)-1;n=c[t]??c[c.length-1];break}}n?.focus()};return i.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,o.Z)("tabs",{"tabs--block":t},n)},d.map((e=>{let{value:n,label:t,attributes:r}=e;return i.createElement("li",(0,a.Z)({role:"tab",tabIndex:l===n?0:-1,"aria-selected":l===n,key:n,ref:e=>c.push(e),onKeyDown:m,onClick:u},r,{className:(0,o.Z)("tabs__item",h.tabItem,r?.className,{"tabs__item--active":l===n})}),t??n)})))}function b(e){let{lazy:n,children:t,selectedValue:a}=e;const o=(Array.isArray(t)?t:[t]).filter(Boolean);if(n){const e=o.find((e=>e.props.value===a));return e?(0,i.cloneElement)(e,{className:"margin-top--md"}):null}return i.createElement("div",{className:"margin-top--md"},o.map(((e,n)=>(0,i.cloneElement)(e,{key:n,hidden:e.props.value!==a}))))}function v(e){const n=f(e);return i.createElement("div",{className:(0,o.Z)("tabs-container",h.tabList)},i.createElement(y,(0,a.Z)({},e,n)),i.createElement(b,(0,a.Z)({},e,n)))}function I(e){const n=(0,_.Z)();return i.createElement(v,(0,a.Z)({key:String(n)},e))}},77846:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>x,contentTitle:()=>k,default:()=>E,frontMatter:()=>I,metadata:()=>D,toc:()=>T});var a=t(87462),i=(t(67294),t(3905)),o=t(74866),r=t(85162),l=t(90814);const s='from clarifai.client.model import Model\n\n# Your PAT (Personal Access Token) can be found in the Account\'s Security section\n# Specify the correct user_id/app_id pairings\n# Since you\'re making inferences outside your app\'s scope\n#USER_ID = "clarifai"\n#APP_ID = "main"\n\n# You can set the model using model URL or model ID.\n# Change these to whatever model you want to use\n# eg : MODEL_ID = "general-image-recognition"\n# You can also set a particular model version by specifying the  version ID\n# eg: MODEL_VERSION_ID = "aa7f35c01e0642fda5cf400f543e7c40"\n#  Model class objects can be inititalised by providing its URL or also by defining respective user_id, app_id and model_id\n\n# eg : model = Model(user_id="clarifai", app_id="main", model_id=MODEL_ID)\n\nmodel_url = "https://clarifai.com/clarifai/main/models/general-image-recognition"\nimage_url = "https://samples.clarifai.com/metro-north.jpg"\n\n# The predict API gives flexibility to generate predictions for data provided through URL,Filepath and bytes format.\n\n\n# Example for prediction through Bytes:\n# model_prediction = model.predict_by_bytes(input_bytes, input_type="image")\n\n\n# Example for prediction through Filepath:\n# model_prediction = Model(model_url).predict_by_filepath(filepath, input_type="image")\n\nmodel_prediction = Model(url=model_url, pat="YOUR_PAT").predict_by_url(\n    image_url, input_type="image"\n)\n\n# Get the output\nprint(model_prediction.outputs[0].data)\n',d='from clarifai.client.model import Model\n\n# Your PAT (Personal Access Token) can be found in the Account\'s Security section\n# Specify the correct user_id/app_id pairings\n# Since you\'re making inferences outside your app\'s scope\n#USER_ID = "clarifai"\n#APP_ID = "main"\n\n# You can set the model using model URL or model ID.\n# Change these to whatever model you want to use\n# eg : MODEL_ID = \'general-image-detection\'\n# You can also set a particular model version by specifying the  version ID\n# eg: MODEL_VERSION_ID = \'1580bb1932594c93b7e2e04456af7c6f\'\n\n#  Model class objects can be inititalised by providing its URL or also by defining respective user_id, app_id and model_id\n# eg : model = Model(user_id="clarifai", app_id="main", model_id=MODEL_ID)\n\n\nDETECTION_IMAGE_URL = "https://s3.amazonaws.com/samples.clarifai.com/people_walking2.jpeg"\nmodel_url = "https://clarifai.com/clarifai/main/models/general-image-detection"\ndetector_model = Model(\n    url=model_url,\n    pat="YOUR_PAT",\n)\n\n\n\n# The predict API gives flexibility to generate predictions for data provided through URL,Filepath and bytes format.\n\n\n# Example for prediction through Bytes:\n# model_prediction = model.predict_by_bytes(input_bytes, input_type="image")\n\n\n# Example for prediction through Filepath:\n# model_prediction = Model(model_url).predict_by_filepath(filepath, input_type="image")\n\nprediction_response = detector_model.predict_by_url(\n    DETECTION_IMAGE_URL, input_type="image"\n)\n\n# Since we have one input, one output will exist here\nregions = prediction_response.outputs[0].data.regions\n\nfor region in regions:\n    # Accessing and rounding the bounding box values\n    top_row = round(region.region_info.bounding_box.top_row, 3)\n    left_col = round(region.region_info.bounding_box.left_col, 3)\n    bottom_row = round(region.region_info.bounding_box.bottom_row, 3)\n    right_col = round(region.region_info.bounding_box.right_col, 3)\n\n    for concept in region.data.concepts:\n        # Accessing and rounding the concept value\n        name = concept.name\n        value = round(concept.value, 4)\n\n        print(\n            (f"{name}: {value} BBox: {top_row}, {left_col}, {bottom_row}, {right_col}")\n        )\n',c='from clarifai.client.model import Model\n\n# Your PAT (Personal Access Token) can be found in the portal under # Authentification\n# Specify the correct user_id/app_id pairings\n# Since you\'re making inferences outside your app\'s scope\nUSER_ID = "clarifai"\nAPP_ID = "main"\n# Change these to whatever model and video URL you want to use\nMODEL_ID = "general-image-recognition"\n# You can also set a particular model version by specifying the  version ID\n# eg: MODEL_VERSION_ID = \'aa7f35c01e0642fda5cf400f543e7c40\'\n\nVIDEO_URL = "https://samples.clarifai.com/beer.mp4"\n# Change this to configure the FPS rate (If it\'s not configured, it defaults to 1 FPS)\n# The number must range betweeen 100 and 60000.\n# FPS = 1000/sample_ms\n\nSAMPLE_MS = 2000\n\n# Model class objects can be inititalised by providing its URL or also by defining respective user_id, app_id and model_id\n# eg: model = Model("https://clarifai.com/clarifai/main/models/general-image-recognition")\n\n\nmodel = Model(user_id="clarifai", app_id="main", model_id=MODEL_ID, pat="YOUR_PAT")\noutput_config = {"sample_ms": SAMPLE_MS}  # Run inference every 2 seconds\nmodel_prediction = model.predict_by_url(\n    BEER_VIDEO_URL, input_type="video", output_config=output_config\n)\n\n# The predict API gives flexibility to generate predictions for data provided through filepath, URL and bytes format.\n\n# Example for prediction through Filepath:\n# model_prediction = model.predict_by_filepath(video_file_path, input_type="video", output_config=output_config)\n\n# Example for prediction through Bytes:\n# model_prediction = model.predict_by_bytes(input_video_bytes, input_type="video", output_config=output_config)\n\n\n# Print the frame info and the first concept name in each frame\nfor frame in model_prediction.outputs[0].data.frames:\n    print(f"Frame Info: {frame.frame_info} Concept: {frame.data.concepts[0].name}\\n")\n',p='from clarifai.client.model import Model\n\n# Your PAT (Personal Access Token) can be found in the Account\'s Security section\n# Specify the correct user_id/app_id pairings\n# Since you\'re making inferences outside your app\'s scope\n#USER_ID = "clarifai"\n#APP_ID = "main"\n\n# You can set the model using model URL or model ID.\n# Change these to whatever model you want to use\n# eg : MODEL_ID = \'image-general-segmentation\'\n# You can also set a particular model version by specifying the  version ID\n# eg: MODEL_VERSION_ID = \'1581820110264581908ce024b12b4bfb\'\n#  Model class objects can be inititalised by providing its URL or also by defining respective user_id, app_id and model_id\n\n# eg : model = Model(user_id="clarifai", app_id="main", model_id=MODEL_ID)\n\nSEGMENT_IMAGE_URL = "https://s3.amazonaws.com/samples.clarifai.com/people_walking2.jpeg"\n\n# The predict API gives flexibility to generate predictions for data provided through URL,Filepath and bytes format.\n\n# Example for prediction through Bytes:\n# model_prediction = model.predict_by_bytes(input_bytes, input_type="image")\n\n\n# Example for prediction through Filepath:\n# model_prediction = Model(model_url).predict_by_filepath(filepath, input_type="image")\n\nmodel_url = "https://clarifai.com/clarifai/main/models/image-general-segmentation"\nsegmentor_model = Model(\n    url=model_url,\n    pat="YOUR_PAT",\n)\n\nprediction_response = segmentor_model.predict_by_url(\n    SEGMENT_IMAGE_URL, input_type="image"\n)\n\nregions = prediction_response.outputs[0].data.regions\n\nfor region in regions:\n    for concept in region.data.concepts:\n        # Accessing and rounding the concept\'s percentage of image covered\n        name = concept.name\n        value = round(concept.value, 4)\n        print((f"{name}: {value}"))\n',u='from clarifai.client.model import Model\n\n# Your PAT (Personal Access Token) can be found in the Account\'s Security section\n# Specify the correct user_id/app_id pairings\n# Since you\'re making inferences outside your app\'s scope\n#USER_ID = "salesforce"\n#APP_ID = "blip"\n\n# You can set the model using model URL or model ID.\n# Change these to whatever model you want to use\n# eg : MODEL_ID = "general-english-image-caption-blip"\n# You can also set a particular model version by specifying the  version ID\n# eg: MODEL_VERSION_ID = "cdb690f13e62470ea6723642044f95e4"\n#  Model class objects can be inititalised by providing its URL or also by defining respective user_id, app_id and model_id\n\n# eg : model = Model(user_id="clarifai", app_id="main", model_id=MODEL_ID)\n\nmodel_url = (\n    "https://clarifai.com/salesforce/blip/models/general-english-image-caption-blip"\n)\nimage_url = "https://s3.amazonaws.com/samples.clarifai.com/featured-models/image-captioning-statue-of-liberty.jpeg"\n\n# The Predict API also accepts data through URL, Filepath & Bytes.\n# Example for predict by filepath:\n# model_prediction = Model(model_url).predict_by_filepath(filepath, input_type="text")\n\n# Example for predict by bytes:\n# model_prediction = Model(model_url).predict_by_bytes(image_bytes, input_type="text")\n\nmodel_prediction = Model(url=model_url, pat="YOUR_PAT").predict_by_url(\n    image_url, input_type="image"\n)\n\n# Get the output\nprint(model_prediction.outputs[0].data.text.raw)\n',m='from clarifai.client.model import Model\n\n# Your PAT (Personal Access Token) can be found in the Account\'s Security section\n# Specify the correct user_id/app_id pairings\n# Since you\'re making inferences outside your app\'s scope\n#USER_ID = "stability-ai"\n#APP_ID = "Upscale"\n\n# You can set the model using model URL or model ID.\n# Change these to whatever model you want to use\n# eg : MODEL_ID = \'stabilityai-upscale\'\n# You can also set a particular model version by specifying the  version ID\n# eg: MODEL_VERSION_ID = \'model_version\'\n#  Model class objects can be inititalised by providing its URL or also by defining respective user_id, app_id and model_id\n\n# eg : model = Model(user_id="clarifai", app_id="main", model_id=MODEL_ID)\n\n\ninference_params = dict(width=1024)\n\n# The predict API gives the flexibility to generate predictions for data provided through URL, Filepath and bytes format.\n\n# Example for prediction through Bytes:\n# model_prediction = model.predict_by_bytes(image_bytes, input_type="image")\n\n\n# Example for prediction through Filepath:\n# model_prediction = Model(model_url).predict_by_filepath(image_filepath, input_type="image")\n\nmodel_url = "https://clarifai.com/stability-ai/Upscale/models/stabilityai-upscale"\n\n\nimage_url = "https://s3.amazonaws.com/samples.clarifai.com/featured-models/image-captioning-statue-of-liberty.jpeg"\nmodel_prediction = Model(url=model_url, pat="YOUR_PAT").predict_by_url(\n    image_url, input_type="image", inference_params=inference_params\n)\n\n# Get the output\noutput_base64 = model_prediction.outputs[0].data.image.base64\n\nimage_info = model_prediction.outputs[0].data.image.image_info\n\nwith open("image.png", "wb") as f:\n    f.write(output_base64)\n',g='from clarifai.client.model import Model\n\n# Your PAT (Personal Access Token) can be found in the Account\'s Security section\n# Specify the correct user_id/app_id pairings\n# Since you\'re making inferences outside your app\'s scope\n#USER_ID = "clarifai"\n#APP_ID = "main"\n\n# You can set the model using model URL or model ID.\n# Change these to whatever model you want to use\n# eg : MODEL_ID = \'image-embedder-clip\'\n# You can also set a particular model version by specifying the  version ID\n# eg: MODEL_VERSION_ID = \'model_version\'\n#  Model class objects can be inititalised by providing its URL or also by defining respective user_id, app_id and model_id\n\n# eg : model = Model(user_id="clarifai", app_id="main", model_id=MODEL_ID)\n\nimage_url = "https://s3.amazonaws.com/samples.clarifai.com/featured-models/general-elephants.jpg"\n\n\n# The predict API gives the flexibility to generate predictions for data provided through URL, Filepath and bytes format.\n\n# Example for prediction through Bytes:\n# model_prediction = model.predict_by_url(input_bytes ,input_type="image")\n\n\n# Example for prediction through Filepath:\n# model_prediction = Model(model_url).predict_by_filepath(image_filepath, input_type="image")\n\n\nmodel_url = "https://clarifai.com/clarifai/main/models/image-embedder-clip"\n\n# Model Predict\nmodel_prediction = Model(url=model_url, pat="YOUR_PAT").predict_by_url(\n    image_url, "image"\n)\n# print(model_prediction.outputs[0].data.text.raw)\n\nembeddings = model_prediction.outputs[0].data.embeddings[0].vector\n\nnum_dimensions = model_prediction.outputs[0].data.embeddings[0].num_dimensions\n\nprint(embeddings[:10])\n',f='concepts {\n\n  id: "ai_HLmqFqBf"\n\n  name: "train"\n\n  value: 0.999604881\n\n  app_id: "main"\n\n}\n\nconcepts {\n\n  id: "ai_fvlBqXZR"\n\n  name: "railway"\n\n  value: 0.999297619\n\n  app_id: "main"\n\n}\n\nconcepts {\n\n  id: "ai_SHNDcmJ3"\n\n  name: "subway system"\n\n  value: 0.99825567\n\n  app_id: "main"\n\n}\n\nconcepts {\n\n  id: "ai_6kTjGfF6"\n\n  name: "station"\n\n  value: 0.998010933\n\n  app_id: "main"\n\n}\n\nconcepts {\n\n  id: "ai_RRXLczch"\n\n  name: "locomotive"\n\n  value: 0.997254908\n\n  app_id: "main"\n\n}\n\nconcepts {\n\n  id: "ai_Xxjc3MhT"\n\n  name: "transportation system"\n\n  value: 0.996976852\n\n  app_id: "main"\n\n}\n\nconcepts {\n\n  id: "ai_VRmbGVWh"\n\n  name: "travel"\n\n  value: 0.988967717\n\n  app_id: "main"\n\n}\n\nconcepts {\n\n  id: "ai_jlb9q33b"\n\n  name: "commuter"\n\n  value: 0.98089534\n\n  app_id: "main"\n\n}\n\nconcepts {\n\n  id: "ai_2gkfMDsM"\n\n  name: "platform"\n\n  value: 0.980635285\n\n  app_id: "main"\n\n}\n\nconcepts {\n\n  id: "ai_n9vjC1jB"\n\n  name: "light"\n\n  value: 0.974186838\n\n  app_id: "main"\n\n}\n\nconcepts {\n\n  id: "ai_sQQj52KZ"\n\n  name: "train station"\n\n  value: 0.96878773\n\n  app_id: "main"\n\n}\n\nconcepts {\n\n  id: "ai_l4WckcJN"\n\n  name: "blur"\n\n  value: 0.967302203\n\n  app_id: "main"\n\n}\n\nconcepts {\n\n  id: "ai_WBQfVV0p"\n\n  name: "city"\n\n  value: 0.96151042\n\n  app_id: "main"\n\n}\n\nconcepts {\n\n  id: "ai_TZ3C79C6"\n\n  name: "road"\n\n  value: 0.961382031\n\n  app_id: "main"\n\n}\n\nconcepts {\n\n  id: "ai_CpFBRWzD"\n\n  name: "urban"\n\n  value: 0.960375667\n\n  app_id: "main"\n\n}\n\nconcepts {\n\n  id: "ai_tr0MBp64"\n\n  name: "traffic"\n\n  value: 0.959969819\n\n  app_id: "main"\n\n}\n\nconcepts {\n\n  id: "ai_GjVpxXrs"\n\n  name: "street"\n\n  value: 0.947492182\n\n  app_id: "main"\n\n}\n\nconcepts {\n\n  id: "ai_mcSHVRfS"\n\n  name: "public"\n\n  value: 0.934322\n\n  app_id: "main"\n\n}\n\nconcepts {\n\n  id: "ai_J6d1kV8t"\n\n  name: "tramway"\n\n  value: 0.931958199\n\n  app_id: "main"\n\n}\n\nconcepts {\n\n  id: "ai_6lhccv44"\n\n  name: "business"\n\n  value: 0.929547548\n\n  app_id: "main"\n\n}',_="Footwear: 0.9618 BBox: 0.879, 0.305, 0.925, 0.327\n\nFootwear: 0.9593 BBox: 0.882, 0.284, 0.922, 0.305\n\nFootwear: 0.9571 BBox: 0.874, 0.401, 0.923, 0.418\n\nFootwear: 0.9546 BBox: 0.87, 0.712, 0.916, 0.732\n\nFootwear: 0.9518 BBox: 0.882, 0.605, 0.918, 0.623\n\nFootwear: 0.95 BBox: 0.847, 0.587, 0.907, 0.604\n\nFootwear: 0.9349 BBox: 0.878, 0.475, 0.917, 0.492\n\nTree: 0.9145 BBox: 0.009, 0.019, 0.451, 0.542\n\nFootwear: 0.9127 BBox: 0.858, 0.393, 0.909, 0.407\n\nFootwear: 0.8969 BBox: 0.812, 0.433, 0.844, 0.445\n\nFootwear: 0.8747 BBox: 0.852, 0.49, 0.912, 0.506\n\nJeans: 0.8699 BBox: 0.511, 0.255, 0.917, 0.336\n\nFootwear: 0.8203 BBox: 0.808, 0.453, 0.833, 0.465\n\nFootwear: 0.8186 BBox: 0.8, 0.378, 0.834, 0.391\n\nJeans: 0.7921 BBox: 0.715, 0.273, 0.895, 0.326\n\nTree: 0.7851 BBox: 0.0, 0.512, 0.635, 0.998\n\nWoman: 0.7693 BBox: 0.466, 0.36, 0.915, 0.449\n\nJeans: 0.7614 BBox: 0.567, 0.567, 0.901, 0.647\n\nFootwear: 0.7287 BBox: 0.847, 0.494, 0.884, 0.51\n\nTree: 0.7216 BBox: 0.002, 0.005, 0.474, 0.14\n\nJeans: 0.7098 BBox: 0.493, 0.447, 0.914, 0.528\n\nFootwear: 0.6929 BBox: 0.808, 0.424, 0.839, 0.437\n\nJeans: 0.6734 BBox: 0.728, 0.464, 0.887, 0.515\n\nWoman: 0.6141 BBox: 0.464, 0.674, 0.922, 0.782\n\nHuman leg: 0.6032 BBox: 0.681, 0.577, 0.897, 0.634\n\n...\n\nFootwear: 0.3527 BBox: 0.844, 0.5, 0.875, 0.515\n\nFootwear: 0.3395 BBox: 0.863, 0.396, 0.914, 0.413\n\nHuman hair: 0.3358 BBox: 0.443, 0.586, 0.505, 0.622\n\nTree: 0.3306 BBox: 0.6, 0.759, 0.805, 0.929",h="Frame Info: time: 1000\n\n Concept: beer\n\nFrame Info: index: 1\n\ntime: 3000\n\n Concept: beer\n\nFrame Info: index: 2\n\ntime: 5000\n\n Concept: beer\n\nFrame Info: index: 3\n\ntime: 7000\n\n Concept: beer\n\nFrame Info: index: 4\n\ntime: 9000\n\n Concept: beer",y="tree: 0.4965\n\nperson: 0.151\n\nhouse: 0.0872\n\npavement: 0.0694\n\nbush: 0.0588\n\nroad: 0.0519\n\nsky-other: 0.0401\n\ngrass: 0.0296\n\nbuilding-other: 0.0096\n\nunlabeled: 0.0035\n\nroof: 0.0017\n\nteddy bear: 0.0006",b="a photograph of a statue of liberty in front of a blue sky",v="[-0.016209319233894348,\n\n -0.03517452999949455,\n\n 0.0031261674594134092,\n\n 0.03941042721271515,\n\n 0.01166260801255703,\n\n -0.02489173412322998,\n\n 0.04667072370648384,\n\n 0.006998186931014061,\n\n 0.05729646235704422,\n\n 0.0077746850438416]",I={sidebar_position:1},k="Image as Input",D={unversionedId:"python-sdk/Inference-from-AI-Models/Image-as-Input",id:"python-sdk/Inference-from-AI-Models/Image-as-Input",title:"Image as Input",description:"Learn how to perform inference with image as input using Clarifai Python SDK",source:"@site/docs/python-sdk/Inference-from-AI-Models/Image-as-Input.md",sourceDirName:"python-sdk/Inference-from-AI-Models",slug:"/python-sdk/Inference-from-AI-Models/Image-as-Input",permalink:"/python-sdk/Inference-from-AI-Models/Image-as-Input",draft:!1,editUrl:"https://github.com/Clarifai/docs/blob/main/docs/python-sdk/Inference-from-AI-Models/Image-as-Input.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"tutorialSidebar",previous:{title:"Inference from AI Models",permalink:"/python-sdk/Inference-from-AI-Models/"},next:{title:"Text as Input",permalink:"/python-sdk/Inference-from-AI-Models/Text-as-Input"}},x={},T=[{value:"Visual Classifier",id:"visual-classifier",level:2},{value:"Visual Detector - Image",id:"visual-detector---image",level:2},{value:"Visual Detector - Video",id:"visual-detector---video",level:2},{value:"Visual Segmenter",id:"visual-segmenter",level:2},{value:"Image To Text",id:"image-to-text",level:2},{value:"Image To Image",id:"image-to-image",level:2},{value:"Visual Embedder",id:"visual-embedder",level:2}],B={toc:T},w="wrapper";function E(e){let{components:n,...t}=e;return(0,i.kt)(w,(0,a.Z)({},B,t,{components:n,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"image-as-input"},"Image as Input"),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Learn how to perform inference with image as input using Clarifai Python SDK")),(0,i.kt)("hr",null),(0,i.kt)("p",null,"Clarifai's Python SDK empowers you to seamlessly integrate advanced image recognition functionalities into your applications, using the potential of artificial intelligence. The Clarifai Python SDK utilises different model types that takes the image as inputs for various tasks.. Whether you're building applications for content moderation, object detection, or image classification, our SDK offers a robust foundation to turn images into actionable information. "),(0,i.kt)("h2",{id:"visual-classifier"},"Visual Classifier"),(0,i.kt)("p",null,"Harnessing the power of Clarifai's Visual Classifier ",(0,i.kt)("a",{parentName:"p",href:"https://clarifai.com/explore/models?page=1&perPage=24&filterData=%5B%7B%22field%22%3A%22model_type_id%22%2C%22value%22%3A%5B%22visual-classifier%22%5D%7D%5D"},"models"),", you can seamlessly categorize images through the intuitive Predict API for images. This capability enables you to submit input images to a classification model of your choice, providing a straightforward mechanism for obtaining accurate and meaningful predictions. You have the option to supply image data through either URLs or files, enhancing the adaptability of the platform for diverse image classification scenarios."),(0,i.kt)("admonition",{type:"note"},(0,i.kt)("p",{parentName:"admonition"},"You can send up to 128 images in one API call. The file size of each image input should be less than 20MB.")),(0,i.kt)(o.Z,{mdxType:"Tabs"},(0,i.kt)(r.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,i.kt)(l.Z,{className:"language-python",mdxType:"CodeBlock"},s))),(0,i.kt)("details",null,(0,i.kt)("summary",null,"Output"),(0,i.kt)(l.Z,{className:"language-text",mdxType:"CodeBlock"},f)),(0,i.kt)("h2",{id:"visual-detector---image"},"Visual Detector - Image"),(0,i.kt)("p",null,"Dive into a richer understanding of image content with Clarifai's Predict API for Visual Detector ",(0,i.kt)("a",{parentName:"p",href:"https://clarifai.com/explore/models?page=1&perPage=24&filterData=%5B%7B%22field%22%3A%22model_type_id%22%2C%22value%22%3A%5B%22visual-detector%22%5D%7D%5D"},"models"),". Unlike image classification, where a single label is assigned to the entire image, Visual Detector goes beyond, detecting and outlining multiple objects or regions within an image, associating them with specific classes or labels. Similar to image classification, the Predict API for visual detection accommodates input images through URLs or files."),(0,i.kt)(o.Z,{mdxType:"Tabs"},(0,i.kt)(r.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,i.kt)(l.Z,{className:"language-python",mdxType:"CodeBlock"},d))),(0,i.kt)("details",null,(0,i.kt)("summary",null,"Output"),(0,i.kt)(l.Z,{className:"language-text",mdxType:"CodeBlock"},_)),(0,i.kt)("h2",{id:"visual-detector---video"},"Visual Detector - Video"),(0,i.kt)("p",null,"Enhance your capabilities with Clarifai's Predict API, which provides predictions for every frame when processing a video as input. The video Predict API is highly configurable, allowing users to fine-tune requests, including the number of frames processed per second for more control over analysis speed. Choose the most suitable ",(0,i.kt)("a",{parentName:"p",href:"https://clarifai.com/explore/models?filterData=%5B%7B%22field%22%3A%22model_type_id%22%2C%22value%22%3A%5B%22visual-detector%22%5D%7D%5D&page=2&perPage=24"},"model")," for your visual detection task."),(0,i.kt)("admonition",{type:"note"},(0,i.kt)("p",{parentName:"admonition"},"Video length should be at most 10mins in length or 100 MB in size when uploaded through URL.")),(0,i.kt)(o.Z,{mdxType:"Tabs"},(0,i.kt)(r.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,i.kt)(l.Z,{className:"language-python",mdxType:"CodeBlock"},c))),(0,i.kt)("details",null,(0,i.kt)("summary",null,"Output"),(0,i.kt)(l.Z,{className:"language-text",mdxType:"CodeBlock"},h)),(0,i.kt)("h2",{id:"visual-segmenter"},"Visual Segmenter"),(0,i.kt)("p",null,"The Clarifai Predict API offers a powerful capability to generate segmentation masks by providing an image as input to a segmentation ",(0,i.kt)("a",{parentName:"p",href:"https://clarifai.com/explore/models?page=1&perPage=24&filterData=%5B%7B%22field%22%3A%22model_type_id%22%2C%22value%22%3A%5B%22visual-segmenter%22%5D%7D%5D"},"model"),". This functionality allows for the detailed analysis of images, where distinct regions are identified and associated with specific concepts."),(0,i.kt)(o.Z,{mdxType:"Tabs"},(0,i.kt)(r.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,i.kt)(l.Z,{className:"language-python",mdxType:"CodeBlock"},p))),(0,i.kt)("details",null,(0,i.kt)("summary",null,"Output"),(0,i.kt)(l.Z,{className:"language-text",mdxType:"CodeBlock"},y)),(0,i.kt)("h2",{id:"image-to-text"},"Image To Text"),(0,i.kt)("p",null,"Enhance your application by producing descriptive captions for images using the Clarifai Predict API. By providing an image as input to a state-of-the-art image-to-text ",(0,i.kt)("a",{parentName:"p",href:"https://clarifai.com/explore/models?page=1&perPage=24&filterData=%5B%7B%22field%22%3A%22model_type_id%22%2C%22value%22%3A%5B%22image-to-text%22%5D%7D%5D"},"model"),", you can extract meaningful textual descriptions effortlessly."),(0,i.kt)(o.Z,{mdxType:"Tabs"},(0,i.kt)(r.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,i.kt)(l.Z,{className:"language-python",mdxType:"CodeBlock"},u))),(0,i.kt)("details",null,(0,i.kt)("summary",null,"Output"),(0,i.kt)(l.Z,{className:"language-text",mdxType:"CodeBlock"},b)),(0,i.kt)("h2",{id:"image-to-image"},"Image To Image"),(0,i.kt)("p",null,"Elevate the resolution of your images using the Clarifai Predict API, specifically designed for image upscaling. This functionality allows you to enhance the quality of an image using an upscaling ",(0,i.kt)("a",{parentName:"p",href:"https://clarifai.com/explore/models?page=1&perPage=24&filterData=%5B%7B%22field%22%3A%22model_type_id%22%2C%22value%22%3A%5B%22image-to-image%22%5D%7D%5D"},"model"),"."),(0,i.kt)(o.Z,{mdxType:"Tabs"},(0,i.kt)(r.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,i.kt)(l.Z,{className:"language-python",mdxType:"CodeBlock"},m))),(0,i.kt)("h2",{id:"visual-embedder"},"Visual Embedder"),(0,i.kt)("p",null,"The Predict API empowers you to leverage image embeddings through an embedding ",(0,i.kt)("a",{parentName:"p",href:"https://clarifai.com/explore/models?page=1&perPage=24&filterData=%5B%7B%22field%22%3A%22model_type_id%22%2C%22value%22%3A%5B%22visual-embedder%22%5D%7D%5D"},"model"),". Image embeddings are vector representations that encapsulate the semantic content of an image, offering a powerful tool for various applications such as similarity search, recommendation systems, and more."),(0,i.kt)(o.Z,{mdxType:"Tabs"},(0,i.kt)(r.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,i.kt)(l.Z,{className:"language-python",mdxType:"CodeBlock"},g))),(0,i.kt)("details",null,(0,i.kt)("summary",null,"Output"),(0,i.kt)(l.Z,{className:"language-text",mdxType:"CodeBlock"},v)))}E.isMDXComponent=!0}}]);
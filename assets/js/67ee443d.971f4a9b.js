"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[8293],{6936:(e,t,n)=>{n.d(t,{A:()=>i});const i=n.p+"assets/images/generative-ai-2-eaf4340de28e21686cfa40689080b085.png"},28453:(e,t,n)=>{n.d(t,{R:()=>o,x:()=>a});var i=n(96540);const s={},r=i.createContext(s);function o(e){const t=i.useContext(r);return i.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function a(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),i.createElement(r.Provider,{value:t},e.children)}},60518:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>a,default:()=>c,frontMatter:()=>o,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"portal-guide/ppredict/generative-ai","title":"Generative AI","description":"Learn how to make predictions using our generative AI models, including LLMs","source":"@site/docs/portal-guide/ppredict/generative-ai.md","sourceDirName":"portal-guide/ppredict","slug":"/portal-guide/ppredict/generative-ai","permalink":"/portal-guide/ppredict/generative-ai","draft":false,"unlisted":false,"editUrl":"https://github.com/Clarifai/docs/blob/main/docs/portal-guide/ppredict/generative-ai.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"description":"Learn how to make predictions using our generative AI models, including LLMs","pagination_next":null,"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Making Predictions","permalink":"/portal-guide/ppredict/"}}');var s=n(74848),r=n(28453);const o={description:"Learn how to make predictions using our generative AI models, including LLMs",pagination_next:null,sidebar_position:2},a="Generative AI",l={},d=[{value:"Prompt",id:"prompt",level:2},{value:"Question Answering",id:"question-answering",level:4},{value:"Grammar Correction",id:"grammar-correction",level:4},{value:"Summarize",id:"summarize",level:4},{value:"Translation",id:"translation",level:4},{value:"Inference Parameters",id:"inference-parameters",level:2},{value:"Max Tokens (or Max Length)",id:"max-tokens-or-max-length",level:3},{value:"Temperature",id:"temperature",level:3},{value:"Top-p (Nucleus)",id:"top-p-nucleus",level:3},{value:"Top-k",id:"top-k",level:3},{value:"Number of Beams",id:"number-of-beams",level:3},{value:"Do Sample",id:"do-sample",level:3},{value:"Return Full Text",id:"return-full-text",level:3},{value:"System Prompt",id:"system-prompt",level:3},{value:"Prompt Template",id:"prompt-template",level:3}];function h(e){const t={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.header,{children:(0,s.jsx)(t.h1,{id:"generative-ai",children:"Generative AI"})}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.strong,{children:"Make predictions using our generative AI models, including LLMs"})}),"\n",(0,s.jsx)("hr",{}),"\n",(0,s.jsx)(t.p,{children:"Generative AI models are a type of artificial intelligence system that are designed to create new content, such as text, images, audio, or even videos, based on patterns learned from existing data."}),"\n",(0,s.jsx)(t.p,{children:"Unlike traditional AI models, which typically classify or predict data, generative models can produce novel and creative outputs that are similar to, but not exactly the same as, the data they were trained on."}),"\n",(0,s.jsx)(t.p,{children:"Large Language Models (LLMs) are a specific type of generative AI models designed primarily for natural language processing (NLP) tasks. LLMs are able to understand and generate text based on the instructions they receive."}),"\n",(0,s.jsx)(t.h2,{id:"prompt",children:"Prompt"}),"\n",(0,s.jsx)(t.p,{children:"A prompt is a piece of text or set of instructions that you provide to an LLM to generate a specific response or action. The clarity and specificity of the provided input prompt will greatly determine the quality and relevance of the generated response."}),"\n",(0,s.jsx)(t.p,{children:"So, it's important to design and refine prompts to effectively interact with and generate desired responses. Prompt engineering involves crafting specific questions, statements, or input formats that guide the model to generate useful, relevant, and accurate predictions."}),"\n",(0,s.jsx)(t.p,{children:"There are several prompting techniques you can use to communicate with an LLM. For example, zero-shot prompting leverages a model\u2019s inherent language understanding capabilities to generate responses without any specific preparation or examples."}),"\n",(0,s.jsxs)(t.p,{children:["You can learn about other prompting techniques ",(0,s.jsx)(t.a,{href:"https://docs.clarifai.com/portal-guide/agent-system-operators/prompter/",children:"here"}),"."]}),"\n",(0,s.jsx)(t.p,{children:"Here are some examples of prompts."}),"\n",(0,s.jsx)(t.h4,{id:"question-answering",children:"Question Answering"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-text",children:"Prompt: \nWho was president of the United States in 1955?\n"})}),"\n",(0,s.jsxs)(t.p,{children:["Here is how you can provide it to a model, such as the ",(0,s.jsx)(t.a,{href:"https://clarifai.com/meta/Llama-3/models/llama-3_1-8b-instruct",children:"Llama 3.1-8b-Instruct"})," model."]}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{src:n(97779).A+"",width:"1798",height:"775"})}),"\n",(0,s.jsxs)(t.p,{children:["To get a response, click the ",(0,s.jsx)(t.strong,{children:"Generate"})," button."]}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{src:n(6936).A+"",width:"1616",height:"847"})}),"\n",(0,s.jsx)(t.p,{children:"After processing the prompt, the model will output the response."}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{src:n(91553).A+"",width:"1797",height:"795"})}),"\n",(0,s.jsx)(t.h4,{id:"grammar-correction",children:"Grammar Correction"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-text",children:"Prompt: \nCorrect this to standard English: She no went to the market.\n\nSample Response: \nShe did not go to the market.\n"})}),"\n",(0,s.jsx)(t.h4,{id:"summarize",children:"Summarize"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-text",children:"Prompt: \nSummarize this: Jupiter is the fifth planet from the Sun and the largest in the Solar System. It is a \ngas giant with a mass one-thousandth that of the Sun, but two-and-a-half times\nthat of all the other planets in the Solar System combined. Jupiter is one of the \nbrightest objects visible to the naked eye in the night sky, and has been known to\nancient civilizations since before recorded history. It is named after the Roman \ngod Jupiter.[19] When viewed from Earth, Jupiter can be bright enough for its \nreflected light to cast visible shadows,[20] and is on average the third-brightest \nnatural object in the night sky after the Moon and Venus.\n\nSample Response: \nJupiter is the fifth planet from the Sun and is very big and bright. It can be seen\nwith our eyes in the night sky and it has been known since ancient times. Its \nname comes from the Roman god Jupiter. It is usually the third brightest object in \nthe night sky after the Moon and Venus.\n"})}),"\n",(0,s.jsx)(t.h4,{id:"translation",children:"Translation"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-text",children:"Prompt: \nTranslate this into 1. French, 2. Spanish, and 3. Japanese: What rooms do you have available?`\n\nSample Response: \nQuels sont les chambres que vous avez disponibles?\n2. \xbfQu\xe9 habitaciones tienes disponibles?\n3. \u3069\u306e\u90e8\u5c4b\u304c\u5229\u7528\u53ef\u80fd\u3067\u3059\u304b\uff1f\n"})}),"\n",(0,s.jsx)(t.admonition,{type:"tip",children:(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.a,{href:"https://platform.openai.com/docs/examples",children:"Click here"})," for more prompting examples."]})}),"\n",(0,s.jsx)(t.h2,{id:"inference-parameters",children:"Inference Parameters"}),"\n",(0,s.jsx)(t.p,{children:"When making predictions using LLMs on our platform, some models offer the ability to specify various inference parameters to influence their output. These parameters control the behavior of the model during the generation process, affecting aspects like creativity, coherence, and the diversity of the generated text."}),"\n",(0,s.jsx)(t.p,{children:"Let\u2019s talk about them."}),"\n",(0,s.jsx)(t.h3,{id:"max-tokens-or-max-length",children:"Max Tokens (or Max Length)"}),"\n",(0,s.jsx)(t.p,{children:"Max Tokens specifies the maximum number of tokens (words or characters) the model is allowed to generate in a single response. It limits the length of the output, preventing the model from generating overly long responses. As such, shorter token lengths will provide faster performance."}),"\n",(0,s.jsx)(t.p,{children:"This inference parameter helps in controlling the verbosity of the output, especially in applications where concise responses are required."}),"\n",(0,s.jsx)(t.h3,{id:"temperature",children:"Temperature"}),"\n",(0,s.jsx)(t.p,{children:"Temperature is a decimal number that controls the degree of randomness in the response."}),"\n",(0,s.jsx)(t.p,{children:"A low temperature (e.g., 0.2) makes the model more deterministic, leading to a more conservative and predictable output. A high temperature (e.g., 0.8) increases the randomness, allowing for more creative and varied responses."}),"\n",(0,s.jsx)(t.p,{children:"Adjusting temperature is useful when you want to balance between creative responses and focused, precise answers."}),"\n",(0,s.jsx)(t.h3,{id:"top-p-nucleus",children:"Top-p (Nucleus)"}),"\n",(0,s.jsx)(t.p,{children:"Top-p sampling is an alternative to temperature sampling that controls output diversity by considering the smallest set of tokens whose cumulative probability is greater than or equal to a specified threshold p (e.g., 0.9)."}),"\n",(0,s.jsx)(t.p,{children:"Rather than restricting the model to a fixed number of top tokens, this method dynamically adjusts the selection based on token probabilities, ensuring that the most likely tokens are always included while maintaining flexibility in the number of tokens considered."}),"\n",(0,s.jsx)(t.p,{children:"It\u2019s useful when you want to dynamically control the diversity of the generated output without setting a fixed limit on the number of tokens."}),"\n",(0,s.jsx)(t.h3,{id:"top-k",children:"Top-k"}),"\n",(0,s.jsx)(t.p,{children:"Top-k sampling limits the model to only consider the top k most probable tokens when generating the next word, ignoring all others."}),"\n",(0,s.jsx)(t.p,{children:"A low k (e.g., 10) reduces diversity by restricting the choice of tokens, leading to more focused outputs. A high k increases diversity by allowing a broader range of possible tokens, leading to more varied outputs."}),"\n",(0,s.jsx)(t.p,{children:"It\u2019s useful when you want to prevent the model from choosing rare, less likely words, but still allow for some diversity."}),"\n",(0,s.jsx)(t.h3,{id:"number-of-beams",children:"Number of Beams"}),"\n",(0,s.jsx)(t.p,{children:"The number of beams inference parameter is integral to a method called beam search. Beam search is a search algorithm that keeps track of the top n (beam width) sequences at each step of generation, considering multiple possibilities before selecting the best one."}),"\n",(0,s.jsx)(t.p,{children:"It helps produce more coherent and optimized outputs by exploring multiple potential sequences. This parameter is particularly useful in tasks where the quality and diversity of the entire sequence is crucial, such as translation or summarization."}),"\n",(0,s.jsx)(t.h3,{id:"do-sample",children:"Do Sample"}),"\n",(0,s.jsx)(t.p,{children:"This parameter determines whether the model should sample from the probability distribution of the next token or select the token with the highest probability."}),"\n",(0,s.jsx)(t.p,{children:"If set to true, the model samples from the probability distribution, introducing randomness and allowing for more creative and diverse outputs. If set to false, the model selects the token with the highest probability, leading to more deterministic and predictable responses."}),"\n",(0,s.jsx)(t.p,{children:"Sampling is typically enabled (set to true) when you want the model to generate varied and creative text. When precision and consistency are more important, sampling may be disabled (set to false)."}),"\n",(0,s.jsx)(t.h3,{id:"return-full-text",children:"Return Full Text"}),"\n",(0,s.jsx)(t.p,{children:"This parameter determines whether the entire generated text should be returned or just a portion of it."}),"\n",(0,s.jsx)(t.p,{children:"If set to true, the model returns the full text, including both the prompt (if provided) and the generated continuation. If set to false, the model returns only the newly generated text, excluding the prompt."}),"\n",(0,s.jsx)(t.p,{children:"It\u2019s useful when you need the complete context, including the prompt, in the output. This can be important for understanding the generated response in the context of the input."}),"\n",(0,s.jsx)(t.h3,{id:"system-prompt",children:"System Prompt"}),"\n",(0,s.jsx)(t.p,{children:"A system prompt is a special input prompt provided to guide the model's behavior throughout the conversation or task. It sets the tone, style, or context for the model\u2019s responses."}),"\n",(0,s.jsx)(t.p,{children:"It influences how the model generates responses by setting expectations or providing instructions that the model follows."}),"\n",(0,s.jsx)(t.p,{children:"It\u2019s often used in conversational AI to define the role the model should play (e.g., a helpful assistant, a friendly chatbot) or in specialized tasks where specific behavior or output style is desired."}),"\n",(0,s.jsx)(t.p,{children:"It helps steer the model's responses in a consistent and contextually appropriate direction."}),"\n",(0,s.jsx)(t.h3,{id:"prompt-template",children:"Prompt Template"}),"\n",(0,s.jsx)(t.p,{children:"A prompt template serves as a pre-configured piece of text used to instruct an LLM. It acts as a structured query or input that guides the model in generating the desired response. You can use a template to tailor your prompts for different use cases."}),"\n",(0,s.jsxs)(t.p,{children:["Many LLMs require prompts to follow a specific template format. To streamline this process, we provide the ",(0,s.jsx)(t.code,{children:"prompt_template"})," inference parameter, which automatically applies the correct template format for the LLM. This means that you do not need to manually format your prompts when using an LLM through our UI or ",(0,s.jsx)(t.a,{href:"https://docs.clarifai.com/sdk/Inference-from-AI-Models/Text-as-Input#text-generation-using-llm#set-inference-parameters",children:"SDK"}),"."]}),"\n",(0,s.jsxs)(t.p,{children:["By default, the ",(0,s.jsx)(t.code,{children:"prompt_template"})," is set to the LLM's standard template, allowing you to simply enter your prompts without worrying about formatting. The prompts will be automatically adjusted to fit the required template."]}),"\n",(0,s.jsxs)(t.p,{children:["If you need more flexibility, you can customize the ",(0,s.jsx)(t.code,{children:"prompt_template"})," parameter. When modifying this variable, make sure it includes the placeholder ",(0,s.jsx)(t.code,{children:"{prompt}"}),", which will be replaced with the user's prompt input."]}),"\n",(0,s.jsxs)(t.p,{children:["For example, the ",(0,s.jsx)(t.a,{href:"https://clarifai.com/openchat/openchat/models/openchat-3_6-8b-20240522",children:"Openchat-3.6-8b"})," model supports the following chat template format:"]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-text",children:"GPT4 Correct User: {prompt}<|end_of_turn|>GPT4 Correct Assistant:\n"})}),"\n",(0,s.jsx)(t.p,{children:"Let\u2019s break down the meaning of the template:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"GPT4 Correct User"}),":  \u2014 This delimiter indicates the start of a user's input."]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"{prompt}"}),": \u2014 This substring will be replaced by the actual input or question from the user. It must be included in the prompt template. It works just like the ",(0,s.jsx)(t.a,{href:"https://docs.clarifai.com/portal-guide/agent-system-operators/prompter#zero-shot-prompting",children:"prompter node"})," in a workflow builder, which must contain the ",(0,s.jsx)(t.code,{children:"{data.raw.text}"})," substring. When your text data is inputted at inference time, all occurrences of the ",(0,s.jsx)(t.code,{children:"{prompt}"})," variable within the template will be replaced with the prompt text."]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"<|end_of_turn|>"}),":\u2014 This delimiter indicates the end of a user's input."]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"GPT4 Correct Assistant:"})," \u2014 This indicates the start of the assistant's (or the language model's) response, which should be a corrected or refined version of the user's input or an appropriate answer to the user's question."]}),"\n"]}),"\n",(0,s.jsxs)(t.p,{children:["You can also add the ",(0,s.jsx)(t.code,{children:"<|start_of_turn|>"})," delimiter, which specifically indicates the start of a turn; in this case, a user\u2019s input."]}),"\n",(0,s.jsx)(t.p,{children:"Here is an example:"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-text",children:"GPT4 Correct User: <|start_of_turn|> {prompt}<|end_of_turn|>GPT4 Correct Assistant:\n"})}),"\n",(0,s.jsxs)(t.p,{children:["Another example is the ",(0,s.jsx)(t.a,{href:"https://clarifai.com/meta/Llama-3/models/llama-3_1-8b-instruct",children:"Llama 3.1-8b-Instruct"})," model, which supports the following chat template format:"]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-text",children:"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\n{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n"})}),"\n",(0,s.jsx)(t.p,{children:"The main purpose of this format is to clearly delineate the roles and contributions of different participants in the conversation: system, user, and assistant."}),"\n",(0,s.jsx)(t.p,{children:"Let\u2019s break down its meaning:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"<|begin_of_text|>"})," \u2014 This delimiter marks the beginning of the text content."]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"<|start_header_id|>system<|end_header_id|>"})," \u2014 This indicates the beginning of a system-level instruction or context."]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"{system_prompt}"})," \u2014 This placeholder is for the actual system-level instruction or context."]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"<|eot_id|>"})," \u2014 This indicates the end of a text unit; in this case, the system prompt."]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"<|start_header_id|>user<|end_header_id|>"})," \u2014 This marks the beginning of a user's input."]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"{prompt}"})," \u2014 As earlier described, this placeholder represents the actual prompt or query from the user."]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"<|eot_id|>"})," \u2014 This marks the end of a text unit; in this case, the user's input."]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"<|start_header_id|>assistant<|end_header_id|>"})," \u2014  This indicates the beginning of the assistant's response."]}),"\n"]})]})}function c(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}},91553:(e,t,n)=>{n.d(t,{A:()=>i});const i=n.p+"assets/images/generative-ai-3-a7b8ee00ca8ddbe441ac2af080eeefd9.png"},97779:(e,t,n)=>{n.d(t,{A:()=>i});const i=n.p+"assets/images/generative-ai-1-90b2c33980af88e7e4c3d57321064bff.png"}}]);
"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[2164],{65537:(e,n,t)=>{t.d(n,{A:()=>b});var a=t(96540),i=t(18215),o=t(65627),r=t(56347),s=t(50372),l=t(30604),d=t(11861),c=t(78749);function p(e){return a.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,a.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function m(e){const{values:n,children:t}=e;return(0,a.useMemo)((()=>{const e=n??function(e){return p(e).map((e=>{let{props:{value:n,label:t,attributes:a,default:i}}=e;return{value:n,label:t,attributes:a,default:i}}))}(t);return function(e){const n=(0,d.XI)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,t])}function h(e){let{value:n,tabValues:t}=e;return t.some((e=>e.value===n))}function u(e){let{queryString:n=!1,groupId:t}=e;const i=(0,r.W6)(),o=function(e){let{queryString:n=!1,groupId:t}=e;if("string"==typeof n)return n;if(!1===n)return null;if(!0===n&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:n,groupId:t});return[(0,l.aZ)(o),(0,a.useCallback)((e=>{if(!o)return;const n=new URLSearchParams(i.location.search);n.set(o,e),i.replace({...i.location,search:n.toString()})}),[o,i])]}function g(e){const{defaultValue:n,queryString:t=!1,groupId:i}=e,o=m(e),[r,l]=(0,a.useState)((()=>function(e){let{defaultValue:n,tabValues:t}=e;if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(n){if(!h({value:n,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${n}" but none of its children has the corresponding value. Available values are: ${t.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return n}const a=t.find((e=>e.default))??t[0];if(!a)throw new Error("Unexpected error: 0 tabValues");return a.value}({defaultValue:n,tabValues:o}))),[d,p]=u({queryString:t,groupId:i}),[g,f]=function(e){let{groupId:n}=e;const t=function(e){return e?`docusaurus.tab.${e}`:null}(n),[i,o]=(0,c.Dv)(t);return[i,(0,a.useCallback)((e=>{t&&o.set(e)}),[t,o])]}({groupId:i}),x=(()=>{const e=d??g;return h({value:e,tabValues:o})?e:null})();(0,s.A)((()=>{x&&l(x)}),[x]);return{selectedValue:r,selectValue:(0,a.useCallback)((e=>{if(!h({value:e,tabValues:o}))throw new Error(`Can't select invalid tab value=${e}`);l(e),p(e),f(e)}),[p,f,o]),tabValues:o}}var f=t(9136);const x={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var y=t(74848);function _(e){let{className:n,block:t,selectedValue:a,selectValue:r,tabValues:s}=e;const l=[],{blockElementScrollPositionUntilNextRender:d}=(0,o.a_)(),c=e=>{const n=e.currentTarget,t=l.indexOf(n),i=s[t].value;i!==a&&(d(n),r(i))},p=e=>{let n=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const t=l.indexOf(e.currentTarget)+1;n=l[t]??l[0];break}case"ArrowLeft":{const t=l.indexOf(e.currentTarget)-1;n=l[t]??l[l.length-1];break}}n?.focus()};return(0,y.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,i.A)("tabs",{"tabs--block":t},n),children:s.map((e=>{let{value:n,label:t,attributes:o}=e;return(0,y.jsx)("li",{role:"tab",tabIndex:a===n?0:-1,"aria-selected":a===n,ref:e=>{l.push(e)},onKeyDown:p,onClick:c,...o,className:(0,i.A)("tabs__item",x.tabItem,o?.className,{"tabs__item--active":a===n}),children:t??n},n)}))})}function j(e){let{lazy:n,children:t,selectedValue:o}=e;const r=(Array.isArray(t)?t:[t]).filter(Boolean);if(n){const e=r.find((e=>e.props.value===o));return e?(0,a.cloneElement)(e,{className:(0,i.A)("margin-top--md",e.props.className)}):null}return(0,y.jsx)("div",{className:"margin-top--md",children:r.map(((e,n)=>(0,a.cloneElement)(e,{key:n,hidden:e.props.value!==o})))})}function v(e){const n=g(e);return(0,y.jsxs)("div",{className:(0,i.A)("tabs-container",x.tabList),children:[(0,y.jsx)(_,{...n,...e}),(0,y.jsx)(j,{...n,...e})]})}function b(e){const n=(0,f.A)();return(0,y.jsx)(v,{...e,children:p(e.children)},String(n))}},79329:(e,n,t)=>{t.d(n,{A:()=>r});t(96540);var a=t(18215);const i={tabItem:"tabItem_Ymn6"};var o=t(74848);function r(e){let{children:n,hidden:t,className:r}=e;return(0,o.jsx)("div",{role:"tabpanel",className:(0,a.A)(i.tabItem,r),hidden:t,children:n})}},79406:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>N,contentTitle:()=>C,default:()=>L,frontMatter:()=>R,metadata:()=>a,toc:()=>M});const a=JSON.parse('{"id":"compute/inference/clarifai/api","title":"Inference via API","description":"Generate predictions with models","source":"@site/docs/compute/inference/clarifai/api.md","sourceDirName":"compute/inference/clarifai","slug":"/compute/inference/clarifai/api","permalink":"/compute/inference/clarifai/api","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"description":"Generate predictions with models","sidebar_position":2,"toc_max_heading_level":4},"sidebar":"tutorialSidebar","previous":{"title":"Model Inference","permalink":"/compute/inference/clarifai/"},"next":{"title":"Legacy Inference via API","permalink":"/compute/inference/clarifai/api-legacy"}}');var i=t(74848),o=t(28453),r=t(65537),s=t(79329),l=t(58069);const d='import os\nfrom clarifai.client import Model\nfrom clarifai.runners.utils.data_types import Image\n\n# Set your Personal Access Token (PAT)\nos.environ["CLARIFAI_PAT"] = "YOUR_PAT_HERE"\n\n# Initialize with model URL\nmodel = Model(\n    url="https://clarifai.com/openai/chat-completion/models/o4-mini",\n    # deployment_id="DEPLOYMENT_ID_HERE"\n)\n\nresponse = model.predict(\n    prompt="Describe the image",\n    image=Image(url="https://samples.clarifai.com/cat1.jpeg") \n)\n\nprint(response)\n\n"""\n# --- Predict using an image uploaded from a local machine ---\n\n# 1. Specify the path to your local image file\nlocal_image_path = "path/to/your/image.jpg"  # Replace with the actual path to your image\n\n# 2. Read the image file into bytes\nwith open(local_image_path, "rb") as f:\n    image_bytes = f.read()\n\nresponse = model.predict(\n    prompt="Describe the image",\n    # Provide Image as bytes\n    image=Image(bytes=image_bytes)\n)\n\nprint(response)\n\n# You can also convert a Pillow (PIL) Image object into a Clarifai Image data type \n# image=Image.from_pil(pil_image)\n\n"""',c='import os\nfrom clarifai.client import Model\n\n# Set your Personal Access Token (PAT)\nos.environ["CLARIFAI_PAT"] = "YOUR_PAT_HERE"\n\n# Initialize with model URL\nmodel = Model(\n    url="https://clarifai.com/openai/chat-completion/models/o4-mini",\n    # deployment_id="DEPLOYMENT_ID_HERE"\n)\n\nresponse = model.predict("What is photosynthesis?")\n# Or\n# response = model.predict(prompt="What is photosynthesis?")\n\nprint(response)\n',p='import os\n\nfrom clarifai.client import Model\n\n# Set your Personal Access Token (PAT)\nos.environ["CLARIFAI_PAT"] = "YOUR_PAT_HERE"\n\n# Initialize with model URL\nmodel = Model(\n    url="https://clarifai.com/openai/chat-completion/models/o4-mini",\n    # deployment_id="DEPLOYMENT_ID_HERE"\n)\n\nresponse_stream = model.generate(\n    prompt="Explain quantum computing in simple terms"  \n)\n\nfor text_chunk in response_stream:\n    print(text_chunk, end="", flush=True)\n\n"""\n# --- Load prompt text from URL ---\n\nprompt_from_url = requests.get("https://samples.clarifai.com/featured-models/redpajama-economic.txt") # Remember to import requests\nprompt_text = prompt_from_url.text.strip()\n\nresponse_stream = model.generate(\n    prompt=prompt_text\n)\n\nfor text_chunk in response_stream:\n    print(text_chunk, end="", flush=True)\n\n"""',m='import os\nfrom clarifai.client import Model\nfrom clarifai.runners.utils.data_types import Audio\n\n# Set your Personal Access Token (PAT)\nos.environ["CLARIFAI_PAT"] = "YOUR_PAT_HERE"\n\n# Initialize with model URL\nmodel = Model(\n    url="MODEL_URL_HERE",\n    # deployment_id="DEPLOYMENT_ID_HERE"\n)\n\n# client-side streaming\nresponse_stream = model.transcribe_audio(\n    audio=iter(Audio(bytes=b\'\'))\n    # Or, provide audio as URL\n    # audio=Audio(url="https://example.com/audio.mp3")\n)\n\nfor text_chunk in response_stream:\n    print(text_chunk.text, end="", flush=True)',h='import os\nfrom clarifai.client import Model\nfrom clarifai.runners.utils.data_types import Text\n\n# Set your Personal Access Token (PAT)\nos.environ["CLARIFAI_PAT"] = "YOUR_PAT_HERE"\n\n# Initialize with model URL\nmodel = Model(\n    url="MODEL_URL_HERE", \n    # deployment_id="YOUR_DEPLOYMENT_ID_HERE"\n)\n\n# Create a list of input Texts to simulate a stream\ninput_texts = iter([\n    Text(text="First input."),\n    Text(text="Second input."),\n    Text(text="Third input.")\n])\n\n# Call the stream method and process outputs\nresponse_iterator = model.stream(input_texts)\n\n# Print streamed results\nprint("Streaming output:\\n")\nfor response in response_iterator:\n    print(response.text)\n',u='import os\nfrom clarifai.client import Model\nfrom clarifai.runners.utils.data_types import Image\n\n# Set your Personal Access Token (PAT)\nos.environ["CLARIFAI_PAT"] = "YOUR_PAT_HERE"\n\n# Initialize with model URL\nmodel = Model(\n    url="MODEL_URL_HERE",\n    # deployment_id="DEPLOYMENT_ID_HERE"\n)\n\n# Batch processing (automatically handled)\nbatch_results = model.predict_image([\n    {"image": Image(url="https://samples.clarifai.com/cat1.jpeg")},\n    {"image": Image(url="https://samples.clarifai.com/cat2.jpeg")},\n])\n\nfor i, pred in enumerate(batch_results):\n    print(f"Image {i+1} cat confidence: {pred[\'cat\']:.2%}")',g='import os\nfrom clarifai.client import Model\nfrom clarifai.runners.utils.data_types import Text\n\n# Set your Personal Access Token (PAT)\nos.environ["CLARIFAI_PAT"] = "YOUR_PAT_HERE"\n\n# Initialize with model URL\nmodel = Model(\n    url="MODEL_URL_HERE",\n    # deployment_id="DEPLOYMENT_ID_HERE"\n)\n\n# Batch prediction\nbatch_results = model.predict([\n    {"text": Text("Positive review")},\n    {"text": Text("Positive review")},\n    {"text": Text("Positive review")},\n])',f='import os\nfrom clarifai.client import Model\nfrom clarifai.runners.utils.data_types import Image\n\n# Set your Personal Access Token (PAT)\nos.environ["CLARIFAI_PAT"] = "YOUR_PAT_HERE"\n\n# Initialize model\nmodel = Model(\n    url="https://clarifai.com/openai/chat-completion/models/o4-mini",\n    # deployment_id="DEPLOYMENT_ID_HERE"\n)\n\n# Perform prediction with prompt and image\nresult = model.predict(\n    prompt="Describe this image",\n    image=Image(url="https://samples.clarifai.com/metro-north.jpg"),\n    max_tokens=1024    \n)\n\n# Print the prediction result\nprint(result)',x='import { Model } from "clarifai-nodejs";\n\nconst model = new Model({\n  url: "https://clarifai.com/openai/chat-completion/models/o4-mini",\n  authConfig: {\n    pat: process.env.CLARIFAI_PAT,\n  },\n});\n\nconst response = await model.predict({\n  // see available methodNames using model.availableMethods()\n  methodName: "predict",\n  prompt: "What is photosynthesis?",\n});\n\nconsole.log(JSON.stringify(response));\n\n// get response data from the response object\nModel.getOutputDataFromModelResponse(response);',y='import { Model } from "clarifai-nodejs";\n\nconst model = new Model({\n  url: "https://clarifai.com/openai/chat-completion/models/o4-mini",\n  authConfig: {\n    pat: process.env.CLARIFAI_PAT,\n  },\n});\n\nconst response = await model.predict({\n  // see available methodNames using model.availableMethods()\n  methodName: "predict",\n  prompt: "Describe the image",\n  image: {\n    url: "https://samples.clarifai.com/cat1.jpeg",\n  },\n});\n\nconsole.log(JSON.stringify(response));\n\n// get response data from the response object\nModel.getOutputDataFromModelResponse(response);',_='import { Model } from "clarifai-nodejs";\n\nconst model = new Model({\n  url: "https://clarifai.com/openai/chat-completion/models/o4-mini",\n  authConfig: {\n    pat: process.env.CLARIFAI_PAT,\n  },\n});\n\nconst responseStream = model.generate({\n  // see available methodNames using model.availableMethods()\n  methodName: "generate",\n  prompt: "what is photosynthesis?",\n});\n\nfor await (const response of responseStream) {\n  console.log(JSON.stringify(response));\n\n  // get response data from the response object\n  Model.getOutputDataFromModelResponse(response);\n}',j="Photosynthesis is the process by which certain organisms\u2014primarily plants, algae, and some bacteria\u2014convert light energy (usually from the sun) into chemical energy stored in sugars. In essence, these organisms capture carbon dioxide (CO\u2082) from the air and water (H\u2082O) from the soil, then use sunlight to drive a series of reactions that produce oxygen (O\u2082) as a by-product and synthesize glucose (C\u2086H\u2081\u2082O\u2086) or related carbohydrates.\n\nKey points:\n\n1. Light absorption\n   \u2022 Chlorophyll and other pigments in chloroplasts (in plants and algae) absorb photons, elevating electrons to higher energy states.\n\n2. Light-dependent reactions (in thylakoid membranes)\n   \u2022 Convert light energy into chemical energy in the form of ATP and NADPH.\n   \u2022 Split water molecules, releasing O\u2082.\n\n3. Calvin cycle (light-independent reactions, in the stroma)\n   \u2022 Use ATP and NADPH to fix CO\u2082 into organic molecules.\n   \u2022 Produce glyceraldehyde-3-phosphate (G3P), which can be converted into glucose and other carbs.\n\nOverall simplified equation:\n6 CO\u2082 + 6 H\u2082O + light energy \u2192 C\u2086H\u2081\u2082O\u2086 + 6 O\u2082\n\nImportance:\n\u2022 Generates the oxygen we breathe.\n\u2022 Forms the base of most food chains by producing organic matter.\n\u2022 Plays a critical role in the global carbon cycle and helps mitigate CO\u2082 in the atmosphere.",v="The image shows a young ginger tabby cat lying on its side against what looks like a rough, earth-toned wall. Its coat is a warm orange with classic darker orange stripe markings. The cat\u2019s front paw is tucked in, and its head rests on the surface below, with its large amber eyes gazing directly toward the viewer. The lighting is soft, highlighting the cat\u2019s whiskers, ear fur, and the texture of its velvety coat. Overall, the scene feels calm and slightly curious, as if the cat has paused mid-nap to watch something interesting.",b='import os\nfrom clarifai.client import Model\nfrom clarifai.runners.utils.data_types import Image\n\n# Set your Personal Access Token (PAT)\nos.environ["CLARIFAI_PAT"] = "YOUR_PAT_HERE"\n\n# Initialize with model URL\nmodel = Model(\n    url="https://clarifai.com/openai/chat-completion/models/o4-mini",\n    # deployment_id="DEPLOYMENT_ID_HERE"\n)\n\nresponse_stream = model.generate(\n    prompt="Describe the image",\n    image=Image(url="https://samples.clarifai.com/cat1.jpeg") \n)\n\nfor text_chunk in response_stream:\n    print(text_chunk, end="", flush=True)\n\n\n"""\n# --- Predict using an image uploaded from a local machine ---\n\n# 1. Specify the path to your local image file\nlocal_image_path = "path/to/your/image.jpg"  # Replace with the actual path to your image\n\n# 2. Read the image file into bytes\nwith open(local_image_path, "rb") as f:\n    image_bytes = f.read()\n\nresponse_stream = model.generate(\n    prompt="Describe the image",\n    # Provide Image as bytes\n    image=Image(bytes=image_bytes)\n)\n\nfor text_chunk in response_stream:\n    print(text_chunk, end="", flush=True)\n\n# You can also convert a Pillow (PIL) Image object into a Clarifai Image data type \n# image=Image.from_pil(pil_image)\n\n"""',A='import { Model } from "clarifai-nodejs";\n\nconst model = new Model({\n  url: "https://clarifai.com/openai/chat-completion/models/o4-mini",\n  authConfig: {\n    pat: process.env.CLARIFAI_PAT,\n  },\n});\n\nconst responseStream = await model.generate({\n  // see available methodNames using model.availableMethods()\n  methodName: "generate",\n  prompt: "Describe the image",\n  image: {\n    url: "https://samples.clarifai.com/cat1.jpeg",\n  },\n});\n\nfor await (const response of responseStream) {\n    console.log(JSON.stringify(response));\n  \n    // get response data from the response object\n    Model.getOutputDataFromModelResponse(response);\n  }',I='from clarifai.client import Model\nimport os\n\n# Set your Personal Access Token (PAT)\nos.environ["CLARIFAI_PAT"] = "YOUR_PAT_HERE"\n\n# Initialize with model URL\nmodel = Model(url="https://clarifai.com/anthropic/completion/models/claude-sonnet-4")\n\n# Define tools \ntools = [\n    {\n        "type": "function",\n        "function": {\n            "name": "get_weather",\n            "description": "Get current temperature for a given location.",\n            "parameters": {\n                "type": "object",\n                "properties": {\n                    "location": {\n                        "type": "string",\n                        "description": "City and country e.g. Bogot\xe1, Colombia"\n                    },\n                    "units": {\n                        "type": "string",\n                        "description": "Temperature units, e.g. Celsius or Fahrenheit",\n                        "enum": ["Celsius", "Fahrenheit"]\n                    }\n                },\n                "required": ["location"],\n                "additionalProperties": False\n            },\n            "strict": True\n        }\n    }\n]\n\nresponse = model.generate(\n    prompt="What is the temperature in Tokyo in Celsius?",\n    tools=tools,\n    tool_choice=\'auto\',\n    max_tokens=1024,\n    temperature=0.5,\n)\n\n# Print response summary\nprint("Iterate or print response as needed:\\n", response)\n',P='import os\nimport matplotlib.pyplot as plt\nfrom clarifai.runners.utils.data_types import Image\nfrom clarifai.client import Model\nfrom PIL import Image as PILImage\n\n# Set your PAT\nos.environ["CLARIFAI_PAT"] = "YOUR_PAT_HERE"\n\n# Initialize the model \nmodel = Model(\n    url="https://clarifai.com/meta/segment-anything/models/sam2_1-hiera-base-plus",\n    #deployment_id="DEPLOYMENT_ID_HERE"\n)\n\n# Load input image from URL\ninput_image = Image(url="https://samples.clarifai.com/cat1.jpeg")\n#Or, load from local file: input_image = Image.from_pil(PILImage.open("path/to/image.png"))\n\n# Run segmentation: returns a list of Region objects\nregions = model.segment_anything(image=input_image)\n\n# Loop through each Region, extract its mask, and display it\nfor idx, region in enumerate(regions):\n    mask = region.mask\n    #mask = Image.from_proto(region.mask.proto)  # Alternative low-level access\n    # region.mask is a Clarifai Image object; convert it to a NumPy array for visualization\n    mask_array = mask.to_numpy()\n\n    # Plot the mask with matplotlib\n    plt.figure(figsize=(5, 5))\n    plt.imshow(mask_array, cmap=\'gray\')\n    plt.title(f"Mask {idx + 1}")\n    plt.axis(\'off\')\n    plt.show()\n\n    # Print progress\n    print(f"Processed mask {idx + 1} out of {len(regions)}")\n\n    # Optional: do anything else with mask_array here\n    # (e.g., save to disk, overlay on the original image, etc.)\n',T='import os\nimport matplotlib.pyplot as plt\nfrom PIL import Image as PILImage\nfrom clarifai.client import Model\nfrom clarifai.runners.utils.data_types import Image, Region, Concept\n\n# Set your PAT\nos.environ["CLARIFAI_PAT"] = "YOUR_PAT_HERE"\n\n# Initialize the model \nmodel = Model(\n    url="https://clarifai.com/meta/segment-anything/models/sam2_1-hiera-base-plus",\n     #deployment_id="DEPLOYMENT_ID_HERE"\n)\n\ninput_image = Image(url="https://samples.clarifai.com/cat1.jpeg")\n# Load input image from local file\n#input_image = Image.from_pil(PILImage.open("path/to/image.png"))\n\n# Point-based input (Region form)\npoint_prompt_regions = [\n    Region(point=[0.1, 0.2, 0.0], concepts=[Concept(name="1", value=1.0)]),\n    Region(point=[0.2, 0.3, 0.0], concepts=[Concept(name="0", value=0.0)])\n]\n\nregions = model.predict(image=input_image, regions=point_prompt_regions)\n\n# # Optional: use dict format instead\n# point_prompt_dict = dict(points=[[0.1, 0.2], [0.2, 0.3]], labels=[1, 0])\n# regions = model.predict(image=input_image, dict_inputs=point_prompt_dict)\n\n# Box-based input (Region form)\n# box_prompt_regions = [Region(box=[0.1, 0.2, 0.3, 0.4])]\n# regions = model.predict(image=input_image, regions=box_prompt_regions)\n\n# # Optional: use dict format instead\n# box_prompt_dict = dict(box=[0.1, 0.2, 0.3, 0.4])\n# regions = model.predict(image=input_image, dict_inputs=box_prompt_dict)\n\n# Visualize each predicted mask\nfor idx, region in enumerate(regions):\n    mask = region.mask\n    #mask = Image.from_proto(region.mask.proto)  # Alternative low-level access\n    # region.mask is a Clarifai Image object; convert it to a NumPy array for visualization\n    mask_array = mask.to_numpy()\n\n    # Plot the mask with matplotlib\n    plt.figure(figsize=(5, 5))\n    plt.imshow(mask_array, cmap=\'gray\')\n    plt.title(f"Mask {idx + 1}")\n    plt.axis(\'off\')\n    plt.show()\n\n     # Print progress\n    print(f"Processed mask {idx + 1} out of {len(regions)}")\n\n    # Optional: do anything else with mask_array here\n    # (e.g., save to disk, overlay on the original image, etc.)\n',k='import os\nimport matplotlib.pyplot as plt\nfrom clarifai.client import Model\nfrom clarifai.runners.utils.data_types import Video, Region, Concept, Frame\n\n# Set your PAT\nos.environ["CLARIFAI_PAT"] = "YOUR_PAT_HERE"\n\n# Initialize the model \nmodel = Model(\n    url="https://clarifai.com/meta/segment-anything/models/sam2_1-hiera-base-plus",\n    #deployment_id="DEPLOYMENT_ID_HERE"\n)\n\n# Load video from a URL\nvideo = Video(url="https://samples.clarifai.com/beer.mp4")\n\n# Or: load from local file \n# video_path = "path/to/video.mp4"\n# with open(video_path, "rb") as f:\n#     video = Video(bytes=f.read())\n\n# Define frame-level prompts using Regions with track IDs\nframe0 = Frame(\n    regions=[\n        Region(point=[0.1, 0.2, 0.0], concepts=[Concept(name="1", value=1.0)], track_id="1"),\n        Region(point=[0.2, 0.3, 0.0], concepts=[Concept(name="0", value=0.0)], track_id="1"),\n    ]\n)\nframe0.proto.frame_info.index = 0\n\nframe1 = Frame(\n    regions=[\n        Region(point=[0.11, 0.22, 0.0], concepts=[Concept(name="1", value=1.0)], track_id="2"),\n        Region(point=[0.22, 0.33, 0.0], concepts=[Concept(name="0", value=0.0)], track_id="2"),\n    ]\n)\nframe1.proto.frame_info.index = 1\n\n# Generate masks using the frame-based approach\noutput_frames = model.generate(video=video, frames=[frame0, frame1])\n\n# Alternatively,  use `list_dict_inputs` instead\n# frame_objs = [\n#     dict(\n#         points=[[0.1, 0.2], [0.2, 0.3]],\n#         box=None,\n#         obj_id=0,\n#         labels=[1, 0],\n#         frame_idx=0\n#     ),\n#     dict(\n#         points=[[0.11, 0.22], [0.22, 0.33]],\n#         box=None,\n#         obj_id=1,\n#         labels=[1, 0],\n#         frame_idx=1\n#     ),\n# ]\n# output_frames = model.generate(video=video, list_dict_inputs=frame_objs)\n\n# Visualize the output masks\nfor frame_idx, frame in enumerate(output_frames):\n    for region_idx, region in enumerate(frame.regions):\n        mask = region.mask  # Clarifai Image object\n        track_id = region.track_id\n\n        # Convert mask to NumPy array\n        mask_array = mask.to_numpy()\n\n        # Show the mask\n        plt.figure(figsize=(5, 5))\n        plt.imshow(mask_array, cmap=\'gray\')\n        plt.title(f"Frame {frame_idx}, Region {region_idx}, Track ID: {track_id}")\n        plt.axis(\'off\')\n        plt.show()\n\n        print(f"Displayed mask for Frame {frame_idx}, Track ID: {track_id}")\n',w='from clarifai.client import Model\n\n# Initialize with explicit IDs\nmodel = Model(user_id="model_user_id", app_id="model_app_id", model_id="model_id")\n\n# Or initialize with model URL\nmodel = Model(url="https://clarifai.com/model_user_id/model_app_id/models/model_id")\n  ',E='import { Model } from "clarifai-nodejs";\n\nconst model = new Model({\n    modelId: "model_id",\n    modelUserAppId: {\n      userId: "model_user_id",\n      appId: "model_app_id",\n    },\n    authConfig: {\n      pat: "pat",\n    },\n  });',R={description:"Generate predictions with models",sidebar_position:2,toc_max_heading_level:4},C="Inference via API",N={},M=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Install Clarifai Packages",id:"install-clarifai-packages",level:3},{value:"Get a PAT Key",id:"get-a-pat-key",level:3},{value:"Prediction Tips",id:"prediction-tips",level:2},{value:"Set up a Deployment",id:"set-up-a-deployment",level:4},{value:"Specify a Model Version",id:"specify-a-model-version",level:4},{value:"Initialize the Model Client",id:"initialize-the-model-client",level:4},{value:"Unary-Unary Predict Call",id:"unary-unary-predict-call",level:2},{value:"Text Inputs",id:"text-inputs",level:3},{value:"Image Inputs",id:"image-inputs",level:3},{value:"Image-to-Text",id:"image-to-text",level:4},{value:"Visual Segmentation",id:"visual-segmentation",level:4},{value:"Example 1",id:"example-1",level:5},{value:"Example 2",id:"example-2",level:5},{value:"Unary-Stream Predict Call",id:"unary-stream-predict-call",level:2},{value:"Text Inputs",id:"text-inputs-1",level:3},{value:"Image Inputs",id:"image-inputs-1",level:3},{value:"Video Inputs",id:"video-inputs",level:3},{value:"Stream-Stream Predict Call",id:"stream-stream-predict-call",level:2},{value:"Text Inputs",id:"text-inputs-2",level:3},{value:"Audio Inputs",id:"audio-inputs",level:3},{value:"Dynamic Batch Prediction Handling",id:"dynamic-batch-prediction-handling",level:2},{value:"Image Inputs",id:"image-inputs-2",level:3},{value:"Text Inputs",id:"text-inputs-3",level:3},{value:"Multimodal Predictions",id:"multimodal-predictions",level:2},{value:"Tool Calling",id:"tool-calling",level:2}];function D(e){const n={a:"a",admonition:"admonition",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",h5:"h5",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components},{Details:t}=n;return t||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"inference-via-api",children:"Inference via API"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Generate predictions with models"})}),"\n",(0,i.jsx)("hr",{}),"\n",(0,i.jsx)(n.admonition,{type:"tip",children:(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.a,{href:"https://github.com/Clarifai/examples/tree/main/models/model_predict",children:"Click here"})," for additional examples on how to perform model predictions using various SDKs \u2014 such as the Clarifai SDK, OpenAI client, and LiteLLM. The examples demonstrate various model types and include both streaming and non-streaming modes, as well as tool calling capabilities."]})}),"\n","\n","\n","\n","\n","\n","\n","\n",(0,i.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsx)(n.h3,{id:"install-clarifai-packages",children:"Install Clarifai Packages"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Install the latest version of the Clarifai ",(0,i.jsx)(n.a,{href:"https://github.com/Clarifai/clarifai-python/",children:"Python"})," SDK package:"]}),"\n"]}),"\n",(0,i.jsx)(r.A,{groupId:"code",children:(0,i.jsx)(s.A,{value:"bash",label:"Bash",children:(0,i.jsx)(l.A,{className:"language-bash",children:" pip install --upgrade clarifai "})})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Install the latest version of the Clarifai ",(0,i.jsx)(n.a,{href:"https://github.com/Clarifai/clarifai-nodejs",children:"Node.js"})," SDK package:"]}),"\n"]}),"\n",(0,i.jsx)(r.A,{groupId:"code",children:(0,i.jsx)(s.A,{value:"bash",label:"Bash",children:(0,i.jsx)(l.A,{className:"language-bash",children:" npm install clarifai-nodejs "})})}),"\n",(0,i.jsx)(n.h3,{id:"get-a-pat-key",children:"Get a PAT Key"}),"\n",(0,i.jsxs)(n.p,{children:["You need a PAT (Personal Access Token) key to authenticate your connection to the Clarifai platform. You can generate the PAT key in your personal settings page by navigating to the ",(0,i.jsx)(n.strong,{children:"Security"})," section."]}),"\n",(0,i.jsxs)(n.p,{children:["You can then set the PAT as an environment variable using ",(0,i.jsx)(n.code,{children:"CLARIFAI_PAT"}),", in which case you don\u2019t need to define it explicitly in your code."]}),"\n",(0,i.jsxs)(r.A,{groupId:"code",children:[(0,i.jsx)(s.A,{value:"bash",label:"Unix-Like Systems",children:(0,i.jsx)(l.A,{className:"language-bash",children:" export CLARIFAI_PAT=YOUR_PERSONAL_ACCESS_TOKEN_HERE "})}),(0,i.jsx)(s.A,{value:"bash2",label:"Windows",children:(0,i.jsx)(l.A,{className:"language-bash",children:" set CLARIFAI_PAT=YOUR_PERSONAL_ACCESS_TOKEN_HERE "})})]}),"\n",(0,i.jsx)(n.h2,{id:"prediction-tips",children:"Prediction Tips"}),"\n",(0,i.jsx)(n.h4,{id:"set-up-a-deployment",children:"Set up a Deployment"}),"\n",(0,i.jsxs)(n.p,{children:["To use our Compute Orchestration capabilities, ensure your model is ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/compute/deployments/deploy-model",children:"deployed"}),". Then, specify the ",(0,i.jsx)(n.code,{children:"deployment_id"})," parameter \u2014 alternatively, you can specify both ",(0,i.jsx)(n.code,{children:"compute_cluster_id"})," and ",(0,i.jsx)(n.code,{children:"nodepool_id"}),", as explained ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/compute/inference/#predict-with-compute-orchestration",children:"here"}),"."]}),"\n",(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsxs)(n.p,{children:["For deployments owned by an organization, also provide the organization id as the ",(0,i.jsx)(n.code,{children:"Model"}),"'s ",(0,i.jsx)(n.code,{children:"deployment_user_id"}),"."]})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",children:'model = Model(\n    url="MODEL_URL_HERE",  \n    deployment_id="DEPLOYMENT_ID_HERE",\n    # if you are targeting a specific deployment owned by an organization:\n    # deployment_user_id="ORGANIZATION_ID_HERE", \n    # Or, set cluster and nodepool \n    # compute_cluster_id = "COMPUTE_CLUSTER_ID_HERE",\n    # nodepool_id = "NODEPOOL_ID_HERE"\n)\n'})}),"\n",(0,i.jsx)(n.h4,{id:"specify-a-model-version",children:"Specify a Model Version"}),"\n",(0,i.jsx)(n.p,{children:"By default, the latest version of the model is used for inference. However, you can specify a different version in either of the following two ways:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'model = Model(url="https://clarifai.com/model_user_id/model_app_id/models/model_id/model_version/model_version_id")\n'})}),"\n",(0,i.jsx)(n.p,{children:"Or:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'model = Model(url="https://clarifai.com/model_user_id/model_app_id/models/model_id", model_version = {"id": "model_version_id"})\n'})}),"\n",(0,i.jsx)(n.h4,{id:"initialize-the-model-client",children:"Initialize the Model Client"}),"\n",(0,i.jsx)(n.p,{children:"You can initialize the model client using either explicit IDs or the full model URL."}),"\n",(0,i.jsxs)(r.A,{groupId:"code",children:[(0,i.jsx)(s.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:w})}),(0,i.jsx)(s.A,{value:"node.js",label:"Node.js SDK",children:(0,i.jsx)(l.A,{className:"language-javascript",children:E})})]}),"\n",(0,i.jsx)(n.h2,{id:"unary-unary-predict-call",children:"Unary-Unary Predict Call"}),"\n",(0,i.jsx)(n.p,{children:"This is the simplest form of prediction: a single input is sent to the model, and a single response is returned. It\u2019s ideal for quick, non-streaming tasks, such as classifying an image or analyzing a short piece of text."}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"NOTE"}),": Streaming means that the response is streamed back token by token, rather than waiting for the entire completion to be generated before returning. This is useful for building interactive applications where you want to display the response as it's being generated."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"text-inputs",children:"Text Inputs"}),"\n",(0,i.jsx)(n.p,{children:"Here is an example of a model signature configured on the server side for handling text inputs:"}),"\n",(0,i.jsx)(r.A,{groupId:"code",children:(0,i.jsx)(s.A,{value:"python",label:"Python",children:(0,i.jsx)(l.A,{className:"language-python",children:'@ModelClass.method\ndef predict(self, prompt: str = "") -> str:'})})}),"\n",(0,i.jsx)(n.p,{children:"Here\u2019s how you can make a corresponding unary-unary predict call from the client side:"}),"\n",(0,i.jsxs)(r.A,{groupId:"code",children:[(0,i.jsx)(s.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:c})}),(0,i.jsx)(s.A,{value:"node.js",label:"Node.js SDK",children:(0,i.jsx)(l.A,{className:"language-javascript",children:x})})]}),"\n",(0,i.jsxs)(t,{children:[(0,i.jsx)("summary",{children:"Example Output"}),(0,i.jsx)(l.A,{className:"language-text",children:j})]}),"\n",(0,i.jsx)(n.h3,{id:"image-inputs",children:"Image Inputs"}),"\n",(0,i.jsx)(n.h4,{id:"image-to-text",children:"Image-to-Text"}),"\n",(0,i.jsx)(n.p,{children:"Here is an example of a model signature configured on the server side for handling image inputs:"}),"\n",(0,i.jsx)(r.A,{groupId:"code",children:(0,i.jsx)(s.A,{value:"python",label:"Python",children:(0,i.jsx)(l.A,{className:"language-python",children:"@ModelClass.method\ndef predict(self, image: Image) -> str:"})})}),"\n",(0,i.jsx)(n.p,{children:"Here\u2019s how you can make a corresponding unary-unary predict call from the client side:"}),"\n",(0,i.jsxs)(r.A,{groupId:"code",children:[(0,i.jsx)(s.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:d})}),(0,i.jsx)(s.A,{value:"node.js",label:"Node.js SDK",children:(0,i.jsx)(l.A,{className:"language-javascript",children:y})})]}),"\n",(0,i.jsxs)(t,{children:[(0,i.jsx)("summary",{children:"Example Output"}),(0,i.jsx)(l.A,{className:"language-text",children:v})]}),"\n",(0,i.jsx)(n.h4,{id:"visual-segmentation",children:"Visual Segmentation"}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Note:"})," The following visual segmentation examples use Matplotlib, Pillow, and NumPy. You can install them by running: ",(0,i.jsx)(n.code,{children:"pip install matplotlib Pillow numpy"}),"."]}),"\n"]}),"\n",(0,i.jsx)(n.h5,{id:"example-1",children:"Example 1"}),"\n",(0,i.jsxs)(n.p,{children:["Here is an example of a ",(0,i.jsx)(n.a,{href:"https://clarifai.com/meta/segment-anything/models/sam2_1-hiera-base-plus?tab=overview",children:"model signature"})," configured on the server side for automatic mask generation:"]}),"\n",(0,i.jsx)(r.A,{groupId:"code",children:(0,i.jsx)(s.A,{value:"python",label:"Python",children:(0,i.jsx)(l.A,{className:"language-python",children:"@ModelClass.method\ndef segment_anything(image: data_types.Image) -> List[data_types.Region]:"})})}),"\n",(0,i.jsx)(n.p,{children:"Here\u2019s how to make a corresponding unary-unary predict call from the client side to generate masks for all objects in a given image."}),"\n",(0,i.jsx)(r.A,{groupId:"code",children:(0,i.jsx)(s.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:P})})}),"\n",(0,i.jsx)(n.h5,{id:"example-2",children:"Example 2"}),"\n",(0,i.jsx)(n.p,{children:"Here is an example of a model signature configured on the server side for creating masks in a given image:"}),"\n",(0,i.jsx)(r.A,{groupId:"code",children:(0,i.jsx)(s.A,{value:"python",label:"Python",children:(0,i.jsx)(l.A,{className:"language-python",children:"@ModelClass.method\ndef predict(image: data_types.Image, regions: List[data_types.Region], dict_inputs: data_types.JSON, round_mask: bool = False, multimask_output: bool = False, denormalize_coord: bool = True) -> List[data_types.Region]:"})})}),"\n",(0,i.jsx)(n.p,{children:"Here\u2019s how to make a corresponding unary-unary predict call from the client side to generate masks using a points or boxes prompt."}),"\n",(0,i.jsx)(r.A,{groupId:"code",children:(0,i.jsx)(s.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:T})})}),"\n",(0,i.jsx)(n.admonition,{type:"tip",children:(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/compute/models/model-upload/data-types/",children:"Click here"})," to explore how to make predictions with other data types."]})}),"\n",(0,i.jsx)(n.h2,{id:"unary-stream-predict-call",children:"Unary-Stream Predict Call"}),"\n",(0,i.jsx)(n.p,{children:"This call sends a single input to the model but returns a stream of responses. This is especially useful for tasks that produce multiple outputs from one input, such as generating text completions or progressive predictions from a prompt."}),"\n",(0,i.jsx)(n.h3,{id:"text-inputs-1",children:"Text Inputs"}),"\n",(0,i.jsx)(n.p,{children:"Here is an example of a model signature configured on the server side for handling text inputs:"}),"\n",(0,i.jsx)(r.A,{groupId:"code",children:(0,i.jsx)(s.A,{value:"python",label:"Python",children:(0,i.jsx)(l.A,{className:"language-python",children:"@ModelClass.method\ndef generate(self, prompt: str) -> Iterator[str]:"})})}),"\n",(0,i.jsx)(n.p,{children:"Here\u2019s how you can make a corresponding unary-stream predict call from the client side:"}),"\n",(0,i.jsxs)(r.A,{groupId:"code",children:[(0,i.jsx)(s.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:p})}),(0,i.jsx)(s.A,{value:"node.js",label:"Node.js SDK",children:(0,i.jsx)(l.A,{className:"language-javascript",children:_})})]}),"\n",(0,i.jsx)(n.h3,{id:"image-inputs-1",children:"Image Inputs"}),"\n",(0,i.jsx)(n.p,{children:"Here is an example of a model signature configured on the server side for handling image inputs:"}),"\n",(0,i.jsx)(r.A,{groupId:"code",children:(0,i.jsx)(s.A,{value:"python",label:"Python",children:(0,i.jsx)(l.A,{className:"language-python",children:"@ModelClass.method\ndef generate(self, image: Image) -> Iterator[str]:"})})}),"\n",(0,i.jsx)(n.p,{children:"Here\u2019s how you can make a corresponding unary-stream predict call from the client side:"}),"\n",(0,i.jsxs)(r.A,{groupId:"code",children:[(0,i.jsx)(s.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:b})}),(0,i.jsx)(s.A,{value:"node.js",label:"Node.js SDK",children:(0,i.jsx)(l.A,{className:"language-javascript",children:A})})]}),"\n",(0,i.jsx)(n.h3,{id:"video-inputs",children:"Video Inputs"}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Note:"})," The following video tracking example uses Matplotlib and NumPy. You can install them by running: ",(0,i.jsx)(n.code,{children:"pip install matplotlib numpy"}),"."]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Here is an example of a ",(0,i.jsx)(n.a,{href:"https://clarifai.com/meta/segment-anything/models/sam2_1-hiera-base-plus?tab=overview",children:"model signature"})," configured on the server side for handling video inputs:"]}),"\n",(0,i.jsx)(r.A,{groupId:"code",children:(0,i.jsx)(s.A,{value:"python",label:"Python",children:(0,i.jsx)(l.A,{className:"language-python",children:"@ModelClass.method\ndef generate(video: data_types.Video, frames: List[data_types.Frame], list_dict_inputs: List[data_types.JSON], denormalize_coord: bool = True) -> Iterator[data_types.Frame]:"})})}),"\n",(0,i.jsx)(n.p,{children:"Here\u2019s how to make a corresponding unary-stream predict call from the client side to track objects in a video:"}),"\n",(0,i.jsx)(r.A,{groupId:"code",children:(0,i.jsx)(s.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:k})})}),"\n",(0,i.jsx)(n.h2,{id:"stream-stream-predict-call",children:"Stream-Stream Predict Call"}),"\n",(0,i.jsx)(n.p,{children:"This call enables bidirectional streaming of both inputs and outputs, making it ideal for real-time applications and processing large datasets."}),"\n",(0,i.jsx)(n.p,{children:"In this setup, multiple inputs can be continuously streamed to the model, while predictions are returned in real time. It\u2019s especially useful for use cases like live video analysis or streaming sensor data."}),"\n",(0,i.jsx)(n.h3,{id:"text-inputs-2",children:"Text Inputs"}),"\n",(0,i.jsxs)(n.p,{children:["Here is an example of a ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/compute/models/upload/#step-1-prepare-the-modelpy-file",children:"model signature"})," configured on the server side for handling text inputs:"]}),"\n",(0,i.jsx)(r.A,{groupId:"code",children:(0,i.jsx)(s.A,{value:"python",label:"Python",children:(0,i.jsx)(l.A,{className:"language-python",children:"@ModelClass.method\ndef stream(self, input_iterator: Iterator[str]) -> Iterator[str]:"})})}),"\n",(0,i.jsx)(n.p,{children:"Here\u2019s how you can make a corresponding stream-stream predict call from the client side:"}),"\n",(0,i.jsx)(r.A,{groupId:"code",children:(0,i.jsx)(s.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:h})})}),"\n",(0,i.jsx)(n.h3,{id:"audio-inputs",children:"Audio Inputs"}),"\n",(0,i.jsx)(n.p,{children:"Here is an example of a model signature configured on the server side for handling audio inputs:"}),"\n",(0,i.jsx)(r.A,{groupId:"code",children:(0,i.jsx)(s.A,{value:"python",label:"Python",children:(0,i.jsx)(l.A,{className:"language-python",children:"@ModelClass.method\ndef transcribe_audio(self, audio: Iterator[Audio]) -> Iterator[Text]:"})})}),"\n",(0,i.jsx)(n.p,{children:"Here\u2019s how you can make a corresponding stream-stream predict call from the client side:"}),"\n",(0,i.jsx)(r.A,{groupId:"code",children:(0,i.jsx)(s.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:m})})}),"\n",(0,i.jsx)(n.h2,{id:"dynamic-batch-prediction-handling",children:"Dynamic Batch Prediction Handling"}),"\n",(0,i.jsx)(n.p,{children:"Clarifai\u2019s model framework seamlessly supports both single and batch predictions through a unified interface. It dynamically adapts to the input format, so no code changes are needed."}),"\n",(0,i.jsx)(n.p,{children:"The system automatically detects the type of input provided:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"If you pass a single input, it\u2019s treated as a singleton batch;"}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"If you pass multiple inputs as a list, they are handled as a parallel batch."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"This means you can pass either a single input or a list of inputs, and the system will automatically process them appropriately \u2014 making your code cleaner and more flexible."}),"\n",(0,i.jsx)(n.h3,{id:"image-inputs-2",children:"Image Inputs"}),"\n",(0,i.jsx)(n.p,{children:"Here is an example of a model signature configured on the server side for handling image inputs:"}),"\n",(0,i.jsx)(r.A,{groupId:"code",children:(0,i.jsx)(s.A,{value:"python",label:"Python",children:(0,i.jsx)(l.A,{className:"language-python",children:"@ModelClass.method\ndef predict_image(self, image: Image) -> Dict[str, float]:"})})}),"\n",(0,i.jsx)(n.p,{children:"Here\u2019s how you can perform batch predictions with image inputs from the client side:"}),"\n",(0,i.jsx)(r.A,{groupId:"code",children:(0,i.jsx)(s.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:u})})}),"\n",(0,i.jsx)(n.h3,{id:"text-inputs-3",children:"Text Inputs"}),"\n",(0,i.jsx)(n.p,{children:"Here is an example of a model signature configured on the server side for handling text inputs:"}),"\n",(0,i.jsx)(r.A,{groupId:"code",children:(0,i.jsx)(s.A,{value:"python",label:"Python",children:(0,i.jsx)(l.A,{className:"language-python",children:'class TextClassifier(ModelClass):\n@ModelClass.method\ndef predict(self, text: Text) -> float:\n"""Single text classification (automatically batched)"""\nreturn self.model(text.text)'})})}),"\n",(0,i.jsx)(n.p,{children:"Here\u2019s how you can perform batch predictions with text inputs from the client side:"}),"\n",(0,i.jsx)(r.A,{groupId:"code",children:(0,i.jsx)(s.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:g})})}),"\n",(0,i.jsx)(n.h2,{id:"multimodal-predictions",children:"Multimodal Predictions"}),"\n",(0,i.jsx)(n.p,{children:"You can make predictions using models that support multimodal inputs, such as a combination of images and text."}),"\n",(0,i.jsxs)(n.p,{children:["Additionally, you can configure various ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/sdk/Inference-from-AI-Models/Advance-Inference-Options/#prediction-paramaters",children:"inference parameters"})," to customize your prediction requests to better suit your use case."]}),"\n",(0,i.jsx)(n.p,{children:"Here is an example:"}),"\n",(0,i.jsx)(r.A,{groupId:"code",children:(0,i.jsx)(s.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:f})})}),"\n",(0,i.jsx)(n.h2,{id:"tool-calling",children:"Tool Calling"}),"\n",(0,i.jsx)(n.p,{children:"Tool calling in LLMs is a capability that allows models to autonomously decide when and how to call external tools, functions, or APIs during a conversation \u2014 based on the user\u2019s input and the context."}),"\n",(0,i.jsxs)(n.p,{children:["You can learn more about it ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/compute/models/inference/open-ai#tool-calling",children:"here"}),"."]}),"\n",(0,i.jsx)(r.A,{groupId:"code",children:(0,i.jsx)(s.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:I})})})]})}function L(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(D,{...e})}):D(e)}}}]);
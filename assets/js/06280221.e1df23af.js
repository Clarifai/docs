"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[2057],{85162:(e,n,t)=>{t.d(n,{Z:()=>o});var a=t(67294),s=t(86010);const i={tabItem:"tabItem_Ymn6"};function o(e){let{children:n,hidden:t,className:o}=e;return a.createElement("div",{role:"tabpanel",className:(0,s.Z)(i.tabItem,o),hidden:t},n)}},74866:(e,n,t)=>{t.d(n,{Z:()=>T});var a=t(87462),s=t(67294),i=t(86010),o=t(12466),r=t(16550),u=t(91980),p=t(67392),l=t(50012);function c(e){return function(e){return s.Children.map(e,(e=>{if(!e||(0,s.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}(e).map((e=>{let{props:{value:n,label:t,attributes:a,default:s}}=e;return{value:n,label:t,attributes:a,default:s}}))}function d(e){const{values:n,children:t}=e;return(0,s.useMemo)((()=>{const e=n??c(t);return function(e){const n=(0,p.l)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,t])}function h(e){let{value:n,tabValues:t}=e;return t.some((e=>e.value===n))}function m(e){let{queryString:n=!1,groupId:t}=e;const a=(0,r.k6)(),i=function(e){let{queryString:n=!1,groupId:t}=e;if("string"==typeof n)return n;if(!1===n)return null;if(!0===n&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:n,groupId:t});return[(0,u._X)(i),(0,s.useCallback)((e=>{if(!i)return;const n=new URLSearchParams(a.location.search);n.set(i,e),a.replace({...a.location,search:n.toString()})}),[i,a])]}function _(e){const{defaultValue:n,queryString:t=!1,groupId:a}=e,i=d(e),[o,r]=(0,s.useState)((()=>function(e){let{defaultValue:n,tabValues:t}=e;if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(n){if(!h({value:n,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${n}" but none of its children has the corresponding value. Available values are: ${t.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return n}const a=t.find((e=>e.default))??t[0];if(!a)throw new Error("Unexpected error: 0 tabValues");return a.value}({defaultValue:n,tabValues:i}))),[u,p]=m({queryString:t,groupId:a}),[c,_]=function(e){let{groupId:n}=e;const t=function(e){return e?`docusaurus.tab.${e}`:null}(n),[a,i]=(0,l.Nk)(t);return[a,(0,s.useCallback)((e=>{t&&i.set(e)}),[t,i])]}({groupId:a}),f=(()=>{const e=u??c;return h({value:e,tabValues:i})?e:null})();(0,s.useLayoutEffect)((()=>{f&&r(f)}),[f]);return{selectedValue:o,selectValue:(0,s.useCallback)((e=>{if(!h({value:e,tabValues:i}))throw new Error(`Can't select invalid tab value=${e}`);r(e),p(e),_(e)}),[p,_,i]),tabValues:i}}var f=t(72389);const g={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};function I(e){let{className:n,block:t,selectedValue:r,selectValue:u,tabValues:p}=e;const l=[],{blockElementScrollPositionUntilNextRender:c}=(0,o.o5)(),d=e=>{const n=e.currentTarget,t=l.indexOf(n),a=p[t].value;a!==r&&(c(n),u(a))},h=e=>{let n=null;switch(e.key){case"Enter":d(e);break;case"ArrowRight":{const t=l.indexOf(e.currentTarget)+1;n=l[t]??l[0];break}case"ArrowLeft":{const t=l.indexOf(e.currentTarget)-1;n=l[t]??l[l.length-1];break}}n?.focus()};return s.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,i.Z)("tabs",{"tabs--block":t},n)},p.map((e=>{let{value:n,label:t,attributes:o}=e;return s.createElement("li",(0,a.Z)({role:"tab",tabIndex:r===n?0:-1,"aria-selected":r===n,key:n,ref:e=>l.push(e),onKeyDown:h,onClick:d},o,{className:(0,i.Z)("tabs__item",g.tabItem,o?.className,{"tabs__item--active":r===n})}),t??n)})))}function b(e){let{lazy:n,children:t,selectedValue:a}=e;const i=(Array.isArray(t)?t:[t]).filter(Boolean);if(n){const e=i.find((e=>e.props.value===a));return e?(0,s.cloneElement)(e,{className:"margin-top--md"}):null}return s.createElement("div",{className:"margin-top--md"},i.map(((e,n)=>(0,s.cloneElement)(e,{key:n,hidden:e.props.value!==a}))))}function E(e){const n=_(e);return s.createElement("div",{className:(0,i.Z)("tabs-container",g.tabList)},s.createElement(I,(0,a.Z)({},e,n)),s.createElement(b,(0,a.Z)({},e,n)))}function T(e){const n=(0,f.Z)();return s.createElement(E,(0,a.Z)({key:String(n)},e))}},40257:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>D,contentTitle:()=>A,default:()=>y,frontMatter:()=>T,metadata:()=>O,toc:()=>w});var a=t(87462),s=(t(67294),t(3905)),i=t(74866),o=t(85162),r=t(90814);const u='#########################################################################################\n# In this section, we set the user authentication, user and app ID, model ID, and\n# audio URL. Change these strings to run your own example.\n########################################################################################\n\n# Your PAT (Personal Access Token) can be found in the portal under Authentification\nPAT = "YOUR_PAT_HERE"\n# Specify the correct user_id/app_id pairings\n# Since you\'re making inferences outside your app\'s scope\nUSER_ID = "facebook"\nAPP_ID = "asr"\n# Change these to make your own predictions\nMODEL_ID = "asr-wav2vec2-base-960h-english"\nAUDIO_URL = "https://samples.clarifai.com/negative_sentence_1.wav"\n\n##########################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n##########################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (("authorization", "Key " + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(\n    user_id=USER_ID, app_id=APP_ID\n)  # The userDataObject is required when using a PAT\n\npost_model_outputs_response = stub.PostModelOutputs(\n    service_pb2.PostModelOutputsRequest(\n        user_app_id=userDataObject,\n        model_id=MODEL_ID,\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(audio=resources_pb2.Audio(url=AUDIO_URL))\n            )\n        ],\n    ),\n    metadata=metadata,\n)\nif post_model_outputs_response.status.code != status_code_pb2.SUCCESS:\n    print(post_model_outputs_response.status)\n    raise Exception(\n        "Post workflow results failed, status: "\n        + post_model_outputs_response.status.description\n    )\n\n# Since we have one input, one output will exist here\noutput = post_model_outputs_response.outputs[0]\n\n# Print the output\nprint(output.data.text.raw)\n',p='#########################################################################################\n# In this section, we set the user authentication, user and app ID, model ID, and\n# audio file location. Change these strings to run your own example.\n########################################################################################\n\n# Your PAT (Personal Access Token) can be found in the portal under Authentification\nPAT = "YOUR_PAT_HERE"\n# Specify the correct user_id/app_id pairings\n# Since you\'re making inferences outside your app\'s scope\nUSER_ID = "facebook"\nAPP_ID = "asr"\n# Change these to make your own predictions\nMODEL_ID = "asr-wav2vec2-base-960h-english"\nAUDIO_FILE_LOCATION = "YOUR_AUDIO_FILE_LOCATION_HERE"\n\n##########################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n##########################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (("authorization", "Key " + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(\n    user_id=USER_ID, app_id=APP_ID\n)  # The userDataObject is required when using a PAT\n\nwith open(AUDIO_FILE_LOCATION, "rb") as f:\n    file_bytes = f.read()\n\npost_model_outputs_response = stub.PostModelOutputs(\n    service_pb2.PostModelOutputsRequest(\n        user_app_id=userDataObject,\n        model_id=MODEL_ID,\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(audio=resources_pb2.Audio(base64=file_bytes))\n            )\n        ],\n    ),\n    metadata=metadata,\n)\nif post_model_outputs_response.status.code != status_code_pb2.SUCCESS:\n    print(post_model_outputs_response.status)\n    raise Exception(\n        "Post workflow results failed, status: "\n        + post_model_outputs_response.status.description\n    )\n\n# Since we have one input, one output will exist here\noutput = post_model_outputs_response.outputs[0]\n\n# Print the output\nprint(output.data.text.raw)\n',l='\x3c!--index.html file--\x3e\n\n<script>\n  ////////////////////////////////////////////////////////////////////////////////////////\n  // In this section, we set the user authentication, user and app ID, model ID, and\n  // audio URL. Change these strings to run your own example.\n  ///////////////////////////////////////////////////////////////////////////////////////\n\n  // Your PAT (Personal Access Token) can be found in the portal under Authentification\n  const PAT = "YOUR_PAT_HERE";\n  // Specify the correct user_id/app_id pairings\n  // Since you\'re making inferences outside your app\'s scope\n  const USER_ID = "facebook";\n  const APP_ID = "asr";\n  // Change these to make your own predictions\n  const MODEL_ID = "asr-wav2vec2-base-960h-english";\n  const AUDIO_URL = "https://samples.clarifai.com/negative_sentence_1.wav";\n\n  ///////////////////////////////////////////////////////////////////////////////////\n  // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n  /////////////////////////////////////////////////////////////////////////////////// \n\n  const raw = JSON.stringify({\n      "user_app_id": {\n          "user_id": USER_ID,\n          "app_id": APP_ID\n      },\n      "inputs": [\n          {\n              "data": {\n                  "audio": {\n                      "url": AUDIO_URL\n                  }\n              }\n          }\n      ]\n  });\n\n  const requestOptions = {\n      method: \'POST\',\n      headers: {\n          \'Accept\': \'application/json\',\n          \'Authorization\': \'Key \' + PAT\n      },\n      body: raw\n  };\n\n  fetch(`https://api.clarifai.com/v2/models/${MODEL_ID}/outputs`, requestOptions)\n      .then(response => response.text())\n      .then(result => console.log(result))\n      .catch(error => console.log(\'error\', error));\n<\/script>',c='\x3c!--index.html file--\x3e\n\n<script>\n  //////////////////////////////////////////////////////////////////////////////////////////////\n  // In this section, we set the user authentication, user and app ID, model ID, and bytes\n  // of the audio we want as an input. Change these strings to run your own example.\n  /////////////////////////////////////////////////////////////////////////////////////////////\n\n  // Your PAT (Personal Access Token) can be found in the portal under Authentification\n  const PAT = "YOUR_PAT_HERE";\n  // Specify the correct user_id/app_id pairings\n  // Since you\'re making inferences outside your app\'s scope\n  const USER_ID = "facebook";\n  const APP_ID = "asr";\n  // Change these to make your own predictions\n  const MODEL_ID = "asr-wav2vec2-base-960h-english";\n  const AUDIO_BYTES_STRING = "YOUR_BYTES_STRING_HERE";\n\n  ///////////////////////////////////////////////////////////////////////////////////\n  // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n  /////////////////////////////////////////////////////////////////////////////////// \n\n  const raw = JSON.stringify({\n    "user_app_id": {\n      "user_id": USER_ID,\n      "app_id": APP_ID\n    },\n    "inputs": [\n      {\n        "data": {\n          "audio": {\n            "base64": AUDIO_BYTES_STRING\n          }\n        }\n      }\n    ]\n  });\n\n  const requestOptions = {\n    method: \'POST\',\n    headers: {\n      \'Accept\': \'application/json\',\n      \'Authorization\': \'Key \' + PAT\n    },\n    body: raw\n  };\n\n  fetch(`https://api.clarifai.com/v2/models/${MODEL_ID}/outputs`, requestOptions)\n    .then(response => response.text())\n    .then(result => console.log(result))\n    .catch(error => console.log(\'error\', error));\n<\/script>',d='//index.js file\n\n////////////////////////////////////////////////////////////////////////////////////////\n//  In this section, we set the user authentication, user and app ID, model ID, and\n// audio URL. Change these strings to run your own example.\n///////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the portal under Authentification\nconst PAT = "YOUR_PAT_HERE"\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\nconst USER_ID = "facebook"\nconst APP_ID = "asr"\n// Change these to make your own predictions\nconst MODEL_ID = "asr-wav2vec2-base-960h-english"\nconst AUDIO_URL = "https://samples.clarifai.com/negative_sentence_1.wav"\n\n/////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n/////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\nstub.PostModelOutputs(\n  {\n    user_app_id: {\n      "user_id": USER_ID,\n      "app_id": APP_ID,\n    },\n    model_id: MODEL_ID,\n    inputs: [{ data: { audio: { url: AUDIO_URL } } }],\n  },\n  metadata,\n  (err, response) => {\n    if (err) {\n      throw new Error(err);\n    }\n\n    if (response.status.code !== 10000) {\n      throw new Error(\n        "Post workflow results failed, status: " + response.status.description\n      );\n    }\n\n    // Since we have one input, one output will exist here\n    const output = response.outputs[0];\n\n    // Print the output\n    console.log(output.data.text.raw)\n\n  }\n);\n',h='//index.js file\n\n////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model ID, and\n// audio file location. Change these strings to run your own example.\n///////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the portal under Authentification\nconst PAT = "YOUR_PAT_HERE"\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\nconst USER_ID = "facebook"\nconst APP_ID = "asr"\n// Change these to make your own predictions\nconst MODEL_ID = "asr-wav2vec2-base-960h-english"\nconst AUDIO_FILE_LOCATION = "YOUR_AUDIO_FILE_LOCATION_HERE"\n\n/////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n/////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\nconst fs = require("fs");\nconst audioBytes = fs.readFileSync(AUDIO_FILE_LOCATION);\n\nstub.PostModelOutputs(\n  {\n    user_app_id: {\n      "user_id": USER_ID,\n      "app_id": APP_ID,\n    },\n    model_id: MODEL_ID,\n    inputs: [{ data: { audio: { base64: audioBytes } } }],\n  },\n  metadata,\n  (err, response) => {\n    if (err) {\n      throw new Error(err);\n    }\n\n    if (response.status.code !== 10000) {\n      throw new Error(\n        "Post workflow results failed, status: " + response.status.description\n      );\n    }\n\n    // Since we have one input, one output will exist here\n    const output = response.outputs[0];\n\n    // Print the output\n    console.log(output.data.text.raw)\n\n  }\n);\n',m='package com.clarifai.example;\n\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.grpc.api.status.StatusCode;\n\npublic class ClarifaiExample {\n\n    //////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model ID, and\n    // audio URL. Change these strings to run your own example.\n    //////////////////////////////////////////////////////////////////////////////////////\n    \n    //Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    static final String USER_ID = "facebook";\n    static final String APP_ID = "asr";\n    // Change these to make your own predictions\n    static final String MODEL_ID = "asr-wav2vec2-base-960h-english";\n    static final String AUDIO_URL = "https://samples.clarifai.com/negative_sentence_1.wav";\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n    public static void main(String[] args) {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n                .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        MultiOutputResponse postModelOutputsResponse = stub.postModelOutputs(\n                PostModelOutputsRequest.newBuilder()\n                        .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n                        .setModelId(MODEL_ID)\n                        .addInputs(\n                                Input.newBuilder().setData(\n                                        Data.newBuilder().setAudio(\n                                                Audio.newBuilder().setUrl(AUDIO_URL)\n                                        )\n                                )\n                        )\n                        .build()\n        );\n\n        if (postModelOutputsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            System.out.println(postModelOutputsResponse.getStatus());\n            throw new RuntimeException("Post workflow results failed, status: " + postModelOutputsResponse.getStatus().getDescription());\n        }\n\n        Output output = postModelOutputsResponse.getOutputs(0);\n\n        // Print the output\n        System.out.println(output.getData().getText().getRaw());\n\n    }\n\n}\n',_='package com.clarifai.example;\n\nimport java.io.File;\nimport java.io.IOException;\nimport java.nio.file.Files;\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.grpc.api.status.StatusCode;\nimport com.google.protobuf.ByteString;\n\npublic class ClarifaiExample {\n\n    ///////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model ID, and\n    // audio file location. Change these strings to run your own example.\n    ///////////////////////////////////////////////////////////////////////////////////////\n\n    //Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    static final String USER_ID = "facebook";\n    static final String APP_ID = "asr";\n    // Change these to make your own predictions\n    static final String MODEL_ID = "asr-wav2vec2-base-960h-english";\n    static final String AUDIO_FILE_LOCATION = "YOUR_AUDIO_FILE_LOCATION_HERE";\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n    public static void main(String[] args) throws IOException {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n                .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        MultiOutputResponse postModelOutputsResponse = stub.postModelOutputs(\n                PostModelOutputsRequest.newBuilder()\n                        .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n                        .setModelId(MODEL_ID)\n                        .addInputs(\n                                Input.newBuilder().setData(\n                                        Data.newBuilder().setAudio(\n                                                Audio.newBuilder().setBase64(ByteString.copyFrom(Files.readAllBytes(\n                                                        new File(AUDIO_FILE_LOCATION).toPath()\n                                                )))\n                                        )\n                                )\n                        )\n                        .build()\n        );\n\n        if (postModelOutputsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            System.out.println(postModelOutputsResponse.getStatus());\n            throw new RuntimeException("Post workflow results failed, status: " + postModelOutputsResponse.getStatus().getDescription());\n        }\n\n        Output output = postModelOutputsResponse.getOutputs(0);\n\n        // Print the output\n        System.out.println(output.getData().getText().getRaw());\n\n    }\n\n}\n',f='<?php\n\nrequire __DIR__ . "/vendor/autoload.php";\n\n///////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model ID, and\n// audio URL. Change these strings to run your own example.\n///////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the portal under Authentification\n$PAT = "YOUR_PAT_HERE";\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\n$USER_ID = "facebook";\n$APP_ID = "asr";\n// Change these to make your own predictions\n$MODEL_ID = "asr-wav2vec2-base-960h-english";\n$AUDIO_URL = "https://samples.clarifai.com/negative_sentence_1.wav";\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\Api\\Audio;\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\PostModelOutputsRequest;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ["Authorization" => ["Key " . $PAT]];\n\n$userDataObject = new UserAppIDSet([\n    "user_id" => $USER_ID,\n    "app_id" => $APP_ID\n]);\n\n// Let\'s make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client->PostModelOutputs(\n    // The request object carries the request along with the request status and other metadata related to the request itself\n    new PostModelOutputsRequest([\n        "user_app_id" => $userDataObject,\n        "model_id" => $MODEL_ID,\n        "inputs" => [\n            new Input([\n                "data" => new Data([\n                    "audio" => new Audio([\n                        "url" => $AUDIO_URL\n                    ])\n                ])\n            ])\n        ]\n    ]),\n    $metadata\n)->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception("Error: {$status->details}");\n}\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure \n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    print $response->getStatus()->getDetails();\n    throw new Exception("Failure response: " . $response->getStatus()->getDescription());\n}\n\n// We\'ll get one output for each input we used above. Because of one input, we have here one output\n$output = $response->getOutputs()[0];\n\n// Print the output\necho $output->getData()->getText()->getRaw();\n\n?>',g='<?php\n\nrequire __DIR__ . "/vendor/autoload.php";\n\n///////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model ID, and\n// audio file location. Change these strings to run your own example.\n//////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the portal under Authentification\n$PAT = "YOUR_PAT_HERE";\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\n$USER_ID = "facebook";\n$APP_ID = "asr";\n// Change these to make your own predictions\n$MODEL_ID = "asr-wav2vec2-base-960h-english";\n$AUDIO_FILE_LOCATION = "YOUR_AUDIO_FILE_LOCATION_HERE";\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\Api\\Audio;\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\PostModelOutputsRequest;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ["Authorization" => ["Key " . $PAT]];\n\n$userDataObject = new UserAppIDSet([\n    "user_id" => $USER_ID,\n    "app_id" => $APP_ID\n]);\n\n$audioData = file_get_contents($AUDIO_FILE_LOCATION); // Get the audio bytes data from the location\n\n// Let\'s make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client->PostModelOutputs(\n    // The request object carries the request along with the request status and other metadata related to the request itself\n    new PostModelOutputsRequest([\n        "user_app_id" => $userDataObject,\n        "model_id" => $MODEL_ID,\n        "inputs" => [\n            new Input([\n                "data" => new Data([\n                    "audio" => new Audio([\n                        "base64" => $audioData\n                    ])\n                ])\n            ])\n        ]\n    ]),\n    $metadata\n)->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception("Error: {$status->details}");\n}\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure \n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    print $response->getStatus()->getDetails();\n    throw new Exception("Failure response: " . $response->getStatus()->getDescription());\n}\n\n// We\'ll get one output for each input we used above. Because of one input, we have here one output\n$output = $response->getOutputs()[0];\n\n// Print the output\necho $output->getData()->getText()->getRaw();\n\n?>',I='curl -X POST "https://api.clarifai.com/v2/users/facebook/apps/asr/models/asr-wav2vec2-base-960h-english/outputs" \\\n  -H "authorization: Key YOUR_PAT_HERE" \\\n  -H "content-type: application/json" \\\n  -d \'{\n    "inputs": [\n        {\n          "data": {\n            "audio": {\n              "url": "https://samples.clarifai.com/negative_sentence_1.wav"\n          }\n        }\n      }\n    ]\n}\'',b='curl -X POST "https://api.clarifai.com/v2/users/facebook/apps/asr/models/asr-wav2vec2-base-960h-english/outputs" \\\n  -H "authorization: Key YOUR_PAT_HERE" \\\n  -H "content-type: application/json" \\\n  -d \'{\n    "inputs": [\n        {\n          "data": {\n            "audio": {\n              "base64": "YOUR_BYTES_STRING_HERE"\n          }\n        }\n      }\n    ]\n}\'',E="I AM NOT FLYING TO ENGLAND",T={description:"Make predictions on audio inputs",sidebar_position:5},A="Audio",O={unversionedId:"api-guide/predict/audio",id:"api-guide/predict/audio",title:"Audio",description:"Make predictions on audio inputs",source:"@site/docs/api-guide/predict/audio.md",sourceDirName:"api-guide/predict",slug:"/api-guide/predict/audio",permalink:"/api-guide/predict/audio",draft:!1,editUrl:"https://github.com/Clarifai/docs/blob/main/docs/api-guide/predict/audio.md",tags:[],version:"current",sidebarPosition:5,frontMatter:{description:"Make predictions on audio inputs",sidebar_position:5},sidebar:"tutorialSidebar",previous:{title:"Large Language Models (LLMs)",permalink:"/api-guide/predict/llms"},next:{title:"Prediction Parameters",permalink:"/api-guide/predict/prediction-parameters"}},D={},w=[{value:"Predict via URL",id:"predict-via-url",level:2},{value:"Predict via Bytes",id:"predict-via-bytes",level:2}],S={toc:w},C="wrapper";function y(e){let{components:n,...t}=e;return(0,s.kt)(C,(0,a.Z)({},S,t,{components:n,mdxType:"MDXLayout"}),(0,s.kt)("h1",{id:"audio"},"Audio"),(0,s.kt)("p",null,(0,s.kt)("strong",{parentName:"p"},"Make predictions on audio inputs")),(0,s.kt)("hr",null),(0,s.kt)("p",null,"To get predictions for a given audio input, you need to supply the audio along with the specific model from which you wish to receive predictions. You can supply the input via a publicly accessible URL or by directly sending bytes."),(0,s.kt)("p",null,"You need to specify your choice of ",(0,s.kt)("a",{parentName:"p",href:"https://clarifai.com/explore/models?filterData=%5B%7B%22field%22%3A%22input_fields%22%2C%22value%22%3A%5B%22audio%22%5D%7D%5D&page=1&perPage=24"},"model")," for prediction by utilizing the ",(0,s.kt)("inlineCode",{parentName:"p"},"MODEL_ID")," parameter."),(0,s.kt)("p",null,"The file size of each audio input should be less than 20MB."),(0,s.kt)("admonition",{type:"info"},(0,s.kt)("p",{parentName:"admonition"},"The initialization code used in the following examples is outlined in detail on the ",(0,s.kt)("a",{parentName:"p",href:"https://docs.clarifai.com/api-guide/api-overview/api-clients/#client-installation-instructions"},"client installation page."))),(0,s.kt)("h2",{id:"predict-via-url"},"Predict via URL"),(0,s.kt)("p",null,"Below is an example of how you would use the ",(0,s.kt)("a",{parentName:"p",href:"https://clarifai.com/facebook/asr/models/asr-wav2vec2-base-960h-english"},"asr-wav2vec2-base-960h-english")," audio transcription model to convert English speech audio, sent via a URL, into English text."),(0,s.kt)(i.Z,{mdxType:"Tabs"},(0,s.kt)(o.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,s.kt)(r.Z,{className:"language-python",mdxType:"CodeBlock"},u)),(0,s.kt)(o.Z,{value:"js_rest",label:"JavaScript (REST)",mdxType:"TabItem"},(0,s.kt)(r.Z,{className:"language-javascript",mdxType:"CodeBlock"},l)),(0,s.kt)(o.Z,{value:"nodejs",label:"NodeJS",mdxType:"TabItem"},(0,s.kt)(r.Z,{className:"language-javascript",mdxType:"CodeBlock"},d)),(0,s.kt)(o.Z,{value:"java",label:"Java",mdxType:"TabItem"},(0,s.kt)(r.Z,{className:"language-java",mdxType:"CodeBlock"},m)),(0,s.kt)(o.Z,{value:"php",label:"PHP",mdxType:"TabItem"},(0,s.kt)(r.Z,{className:"language-php",mdxType:"CodeBlock"},f)),(0,s.kt)(o.Z,{value:"curl",label:"cURL",mdxType:"TabItem"},(0,s.kt)(r.Z,{className:"language-bash",mdxType:"CodeBlock"},I))),(0,s.kt)("details",null,(0,s.kt)("summary",null,"Text Output Example"),(0,s.kt)(r.Z,{className:"language-text",mdxType:"CodeBlock"},E)),(0,s.kt)("h2",{id:"predict-via-bytes"},"Predict via Bytes"),(0,s.kt)("p",null,"Below is an example of how you would use the ",(0,s.kt)("a",{parentName:"p",href:"https://clarifai.com/facebook/asr/models/asr-wav2vec2-base-960h-english"},"asr-wav2vec2-base-960h-english")," audio transcription model to convert English speech audio, sent as bytes, into English text."),(0,s.kt)(i.Z,{mdxType:"Tabs"},(0,s.kt)(o.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,s.kt)(r.Z,{className:"language-python",mdxType:"CodeBlock"},p)),(0,s.kt)(o.Z,{value:"js_rest",label:"JavaScript (REST)",mdxType:"TabItem"},(0,s.kt)(r.Z,{className:"language-javascript",mdxType:"CodeBlock"},c)),(0,s.kt)(o.Z,{value:"nodejs",label:"NodeJS",mdxType:"TabItem"},(0,s.kt)(r.Z,{className:"language-javascript",mdxType:"CodeBlock"},h)),(0,s.kt)(o.Z,{value:"java",label:"Java",mdxType:"TabItem"},(0,s.kt)(r.Z,{className:"language-java",mdxType:"CodeBlock"},_)),(0,s.kt)(o.Z,{value:"php",label:"PHP",mdxType:"TabItem"},(0,s.kt)(r.Z,{className:"language-php",mdxType:"CodeBlock"},g)),(0,s.kt)(o.Z,{value:"curl",label:"cURL",mdxType:"TabItem"},(0,s.kt)(r.Z,{className:"language-bash",mdxType:"CodeBlock"},b))))}y.isMDXComponent=!0}}]);
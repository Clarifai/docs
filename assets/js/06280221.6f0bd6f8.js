"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[2057],{85162:(e,t,n)=>{n.d(t,{Z:()=>i});var a=n(67294),s=n(86010);const o={tabItem:"tabItem_Ymn6"};function i(e){let{children:t,hidden:n,className:i}=e;return a.createElement("div",{role:"tabpanel",className:(0,s.Z)(o.tabItem,i),hidden:n},t)}},74866:(e,t,n)=>{n.d(t,{Z:()=>T});var a=n(87462),s=n(67294),o=n(86010),i=n(12466),r=n(16550),u=n(91980),p=n(67392),l=n(50012);function c(e){return function(e){return s.Children.map(e,(e=>{if(!e||(0,s.isValidElement)(e)&&function(e){const{props:t}=e;return!!t&&"object"==typeof t&&"value"in t}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}(e).map((e=>{let{props:{value:t,label:n,attributes:a,default:s}}=e;return{value:t,label:n,attributes:a,default:s}}))}function d(e){const{values:t,children:n}=e;return(0,s.useMemo)((()=>{const e=t??c(n);return function(e){const t=(0,p.l)(e,((e,t)=>e.value===t.value));if(t.length>0)throw new Error(`Docusaurus error: Duplicate values "${t.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[t,n])}function h(e){let{value:t,tabValues:n}=e;return n.some((e=>e.value===t))}function m(e){let{queryString:t=!1,groupId:n}=e;const a=(0,r.k6)(),o=function(e){let{queryString:t=!1,groupId:n}=e;if("string"==typeof t)return t;if(!1===t)return null;if(!0===t&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:t,groupId:n});return[(0,u._X)(o),(0,s.useCallback)((e=>{if(!o)return;const t=new URLSearchParams(a.location.search);t.set(o,e),a.replace({...a.location,search:t.toString()})}),[o,a])]}function _(e){const{defaultValue:t,queryString:n=!1,groupId:a}=e,o=d(e),[i,r]=(0,s.useState)((()=>function(e){let{defaultValue:t,tabValues:n}=e;if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(t){if(!h({value:t,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${t}" but none of its children has the corresponding value. Available values are: ${n.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return t}const a=n.find((e=>e.default))??n[0];if(!a)throw new Error("Unexpected error: 0 tabValues");return a.value}({defaultValue:t,tabValues:o}))),[u,p]=m({queryString:n,groupId:a}),[c,_]=function(e){let{groupId:t}=e;const n=function(e){return e?`docusaurus.tab.${e}`:null}(t),[a,o]=(0,l.Nk)(n);return[a,(0,s.useCallback)((e=>{n&&o.set(e)}),[n,o])]}({groupId:a}),f=(()=>{const e=u??c;return h({value:e,tabValues:o})?e:null})();(0,s.useLayoutEffect)((()=>{f&&r(f)}),[f]);return{selectedValue:i,selectValue:(0,s.useCallback)((e=>{if(!h({value:e,tabValues:o}))throw new Error(`Can't select invalid tab value=${e}`);r(e),p(e),_(e)}),[p,_,o]),tabValues:o}}var f=n(72389);const g={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};function I(e){let{className:t,block:n,selectedValue:r,selectValue:u,tabValues:p}=e;const l=[],{blockElementScrollPositionUntilNextRender:c}=(0,i.o5)(),d=e=>{const t=e.currentTarget,n=l.indexOf(t),a=p[n].value;a!==r&&(c(t),u(a))},h=e=>{let t=null;switch(e.key){case"Enter":d(e);break;case"ArrowRight":{const n=l.indexOf(e.currentTarget)+1;t=l[n]??l[0];break}case"ArrowLeft":{const n=l.indexOf(e.currentTarget)-1;t=l[n]??l[l.length-1];break}}t?.focus()};return s.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,o.Z)("tabs",{"tabs--block":n},t)},p.map((e=>{let{value:t,label:n,attributes:i}=e;return s.createElement("li",(0,a.Z)({role:"tab",tabIndex:r===t?0:-1,"aria-selected":r===t,key:t,ref:e=>l.push(e),onKeyDown:h,onClick:d},i,{className:(0,o.Z)("tabs__item",g.tabItem,i?.className,{"tabs__item--active":r===t})}),n??t)})))}function b(e){let{lazy:t,children:n,selectedValue:a}=e;const o=(Array.isArray(n)?n:[n]).filter(Boolean);if(t){const e=o.find((e=>e.props.value===a));return e?(0,s.cloneElement)(e,{className:"margin-top--md"}):null}return s.createElement("div",{className:"margin-top--md"},o.map(((e,t)=>(0,s.cloneElement)(e,{key:t,hidden:e.props.value!==a}))))}function E(e){const t=_(e);return s.createElement("div",{className:(0,o.Z)("tabs-container",g.tabList)},s.createElement(I,(0,a.Z)({},e,t)),s.createElement(b,(0,a.Z)({},e,t)))}function T(e){const t=(0,f.Z)();return s.createElement(E,(0,a.Z)({key:String(t)},e))}},40257:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>D,contentTitle:()=>A,default:()=>C,frontMatter:()=>T,metadata:()=>O,toc:()=>w});var a=n(87462),s=(n(67294),n(3905)),o=n(74866),i=n(85162),r=n(90814);const u='#########################################################################################\n# In this section, we set the user authentication, user and app ID, model ID, and\n# audio URL. Change these strings to run your own example.\n########################################################################################\n\n# Your PAT (Personal Access Token) can be found in the Account\'s Security section\nPAT = "YOUR_PAT_HERE"\n# Specify the correct user_id/app_id pairings\n# Since you\'re making inferences outside your app\'s scope\nUSER_ID = "facebook"\nAPP_ID = "asr"\n# Change these to make your own predictions\nMODEL_ID = "asr-wav2vec2-base-960h-english"\nAUDIO_URL = "https://samples.clarifai.com/negative_sentence_1.wav"\n\n##########################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n##########################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (("authorization", "Key " + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(\n    user_id=USER_ID, app_id=APP_ID\n)  # The userDataObject is required when using a PAT\n\npost_model_outputs_response = stub.PostModelOutputs(\n    service_pb2.PostModelOutputsRequest(\n        user_app_id=userDataObject,\n        model_id=MODEL_ID,\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(audio=resources_pb2.Audio(url=AUDIO_URL))\n            )\n        ],\n    ),\n    metadata=metadata,\n)\nif post_model_outputs_response.status.code != status_code_pb2.SUCCESS:\n    print(post_model_outputs_response.status)\n    raise Exception(\n        "Post workflow results failed, status: "\n        + post_model_outputs_response.status.description\n    )\n\n# Since we have one input, one output will exist here\noutput = post_model_outputs_response.outputs[0]\n\n# Print the output\nprint(output.data.text.raw)\n',p='#########################################################################################\n# In this section, we set the user authentication, user and app ID, model ID, and\n# audio file location. Change these strings to run your own example.\n########################################################################################\n\n# Your PAT (Personal Access Token) can be found in the Account\'s Security section\nPAT = "YOUR_PAT_HERE"\n# Specify the correct user_id/app_id pairings\n# Since you\'re making inferences outside your app\'s scope\nUSER_ID = "facebook"\nAPP_ID = "asr"\n# Change these to make your own predictions\nMODEL_ID = "asr-wav2vec2-base-960h-english"\nAUDIO_FILE_LOCATION = "YOUR_AUDIO_FILE_LOCATION_HERE"\n\n##########################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n##########################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (("authorization", "Key " + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(\n    user_id=USER_ID, app_id=APP_ID\n)  # The userDataObject is required when using a PAT\n\nwith open(AUDIO_FILE_LOCATION, "rb") as f:\n    file_bytes = f.read()\n\npost_model_outputs_response = stub.PostModelOutputs(\n    service_pb2.PostModelOutputsRequest(\n        user_app_id=userDataObject,\n        model_id=MODEL_ID,\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(audio=resources_pb2.Audio(base64=file_bytes))\n            )\n        ],\n    ),\n    metadata=metadata,\n)\nif post_model_outputs_response.status.code != status_code_pb2.SUCCESS:\n    print(post_model_outputs_response.status)\n    raise Exception(\n        "Post workflow results failed, status: "\n        + post_model_outputs_response.status.description\n    )\n\n# Since we have one input, one output will exist here\noutput = post_model_outputs_response.outputs[0]\n\n# Print the output\nprint(output.data.text.raw)\n',l='\x3c!--index.html file--\x3e\n\n<script>\n  ////////////////////////////////////////////////////////////////////////////////////////\n  // In this section, we set the user authentication, user and app ID, model ID, and\n  // audio URL. Change these strings to run your own example.\n  ///////////////////////////////////////////////////////////////////////////////////////\n\n  // Your PAT (Personal Access Token) can be found in the Account\'s Security section\n  const PAT = "YOUR_PAT_HERE";\n  // Specify the correct user_id/app_id pairings\n  // Since you\'re making inferences outside your app\'s scope\n  const USER_ID = "facebook";\n  const APP_ID = "asr";\n  // Change these to make your own predictions\n  const MODEL_ID = "asr-wav2vec2-base-960h-english";\n  const AUDIO_URL = "https://samples.clarifai.com/negative_sentence_1.wav";\n\n  ///////////////////////////////////////////////////////////////////////////////////\n  // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n  /////////////////////////////////////////////////////////////////////////////////// \n\n  const raw = JSON.stringify({\n      "user_app_id": {\n          "user_id": USER_ID,\n          "app_id": APP_ID\n      },\n      "inputs": [\n          {\n              "data": {\n                  "audio": {\n                      "url": AUDIO_URL\n                  }\n              }\n          }\n      ]\n  });\n\n  const requestOptions = {\n      method: \'POST\',\n      headers: {\n          \'Accept\': \'application/json\',\n          \'Authorization\': \'Key \' + PAT\n      },\n      body: raw\n  };\n\n  fetch(`https://api.clarifai.com/v2/models/${MODEL_ID}/outputs`, requestOptions)\n      .then(response => response.text())\n      .then(result => console.log(result))\n      .catch(error => console.log(\'error\', error));\n<\/script>',c='\x3c!--index.html file--\x3e\n\n<script>\n  //////////////////////////////////////////////////////////////////////////////////////////////\n  // In this section, we set the user authentication, user and app ID, model ID, and bytes\n  // of the audio we want as an input. Change these strings to run your own example.\n  /////////////////////////////////////////////////////////////////////////////////////////////\n\n  // Your PAT (Personal Access Token) can be found in the Account\'s Security section\n  const PAT = "YOUR_PAT_HERE";\n  // Specify the correct user_id/app_id pairings\n  // Since you\'re making inferences outside your app\'s scope\n  const USER_ID = "facebook";\n  const APP_ID = "asr";\n  // Change these to make your own predictions\n  const MODEL_ID = "asr-wav2vec2-base-960h-english";\n  const AUDIO_BYTES_STRING = "YOUR_BYTES_STRING_HERE";\n\n  ///////////////////////////////////////////////////////////////////////////////////\n  // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n  /////////////////////////////////////////////////////////////////////////////////// \n\n  const raw = JSON.stringify({\n    "user_app_id": {\n      "user_id": USER_ID,\n      "app_id": APP_ID\n    },\n    "inputs": [\n      {\n        "data": {\n          "audio": {\n            "base64": AUDIO_BYTES_STRING\n          }\n        }\n      }\n    ]\n  });\n\n  const requestOptions = {\n    method: \'POST\',\n    headers: {\n      \'Accept\': \'application/json\',\n      \'Authorization\': \'Key \' + PAT\n    },\n    body: raw\n  };\n\n  fetch(`https://api.clarifai.com/v2/models/${MODEL_ID}/outputs`, requestOptions)\n    .then(response => response.text())\n    .then(result => console.log(result))\n    .catch(error => console.log(\'error\', error));\n<\/script>',d='//index.js file\n\n////////////////////////////////////////////////////////////////////////////////////////\n//  In this section, we set the user authentication, user and app ID, model ID, and\n// audio URL. Change these strings to run your own example.\n///////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\nconst PAT = "YOUR_PAT_HERE"\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\nconst USER_ID = "facebook"\nconst APP_ID = "asr"\n// Change these to make your own predictions\nconst MODEL_ID = "asr-wav2vec2-base-960h-english"\nconst AUDIO_URL = "https://samples.clarifai.com/negative_sentence_1.wav"\n\n/////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n/////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\nstub.PostModelOutputs(\n  {\n    user_app_id: {\n      "user_id": USER_ID,\n      "app_id": APP_ID,\n    },\n    model_id: MODEL_ID,\n    inputs: [{ data: { audio: { url: AUDIO_URL } } }],\n  },\n  metadata,\n  (err, response) => {\n    if (err) {\n      throw new Error(err);\n    }\n\n    if (response.status.code !== 10000) {\n      throw new Error(\n        "Post workflow results failed, status: " + response.status.description\n      );\n    }\n\n    // Since we have one input, one output will exist here\n    const output = response.outputs[0];\n\n    // Print the output\n    console.log(output.data.text.raw)\n\n  }\n);\n',h='//index.js file\n\n////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model ID, and\n// audio file location. Change these strings to run your own example.\n///////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\nconst PAT = "YOUR_PAT_HERE"\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\nconst USER_ID = "facebook"\nconst APP_ID = "asr"\n// Change these to make your own predictions\nconst MODEL_ID = "asr-wav2vec2-base-960h-english"\nconst AUDIO_FILE_LOCATION = "YOUR_AUDIO_FILE_LOCATION_HERE"\n\n/////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n/////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\nconst fs = require("fs");\nconst audioBytes = fs.readFileSync(AUDIO_FILE_LOCATION);\n\nstub.PostModelOutputs(\n  {\n    user_app_id: {\n      "user_id": USER_ID,\n      "app_id": APP_ID,\n    },\n    model_id: MODEL_ID,\n    inputs: [{ data: { audio: { base64: audioBytes } } }],\n  },\n  metadata,\n  (err, response) => {\n    if (err) {\n      throw new Error(err);\n    }\n\n    if (response.status.code !== 10000) {\n      throw new Error(\n        "Post workflow results failed, status: " + response.status.description\n      );\n    }\n\n    // Since we have one input, one output will exist here\n    const output = response.outputs[0];\n\n    // Print the output\n    console.log(output.data.text.raw)\n\n  }\n);\n',m='package com.clarifai.example;\n\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.grpc.api.status.StatusCode;\n\npublic class ClarifaiExample {\n\n    //////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model ID, and\n    // audio URL. Change these strings to run your own example.\n    //////////////////////////////////////////////////////////////////////////////////////\n    \n    //Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    static final String USER_ID = "facebook";\n    static final String APP_ID = "asr";\n    // Change these to make your own predictions\n    static final String MODEL_ID = "asr-wav2vec2-base-960h-english";\n    static final String AUDIO_URL = "https://samples.clarifai.com/negative_sentence_1.wav";\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n    public static void main(String[] args) {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n                .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        MultiOutputResponse postModelOutputsResponse = stub.postModelOutputs(\n                PostModelOutputsRequest.newBuilder()\n                        .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n                        .setModelId(MODEL_ID)\n                        .addInputs(\n                                Input.newBuilder().setData(\n                                        Data.newBuilder().setAudio(\n                                                Audio.newBuilder().setUrl(AUDIO_URL)\n                                        )\n                                )\n                        )\n                        .build()\n        );\n\n        if (postModelOutputsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            System.out.println(postModelOutputsResponse.getStatus());\n            throw new RuntimeException("Post workflow results failed, status: " + postModelOutputsResponse.getStatus().getDescription());\n        }\n\n        Output output = postModelOutputsResponse.getOutputs(0);\n\n        // Print the output\n        System.out.println(output.getData().getText().getRaw());\n\n    }\n\n}\n',_='package com.clarifai.example;\n\nimport java.io.File;\nimport java.io.IOException;\nimport java.nio.file.Files;\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.grpc.api.status.StatusCode;\nimport com.google.protobuf.ByteString;\n\npublic class ClarifaiExample {\n\n    ///////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model ID, and\n    // audio file location. Change these strings to run your own example.\n    ///////////////////////////////////////////////////////////////////////////////////////\n\n    //Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    static final String USER_ID = "facebook";\n    static final String APP_ID = "asr";\n    // Change these to make your own predictions\n    static final String MODEL_ID = "asr-wav2vec2-base-960h-english";\n    static final String AUDIO_FILE_LOCATION = "YOUR_AUDIO_FILE_LOCATION_HERE";\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n    public static void main(String[] args) throws IOException {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n                .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        MultiOutputResponse postModelOutputsResponse = stub.postModelOutputs(\n                PostModelOutputsRequest.newBuilder()\n                        .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n                        .setModelId(MODEL_ID)\n                        .addInputs(\n                                Input.newBuilder().setData(\n                                        Data.newBuilder().setAudio(\n                                                Audio.newBuilder().setBase64(ByteString.copyFrom(Files.readAllBytes(\n                                                        new File(AUDIO_FILE_LOCATION).toPath()\n                                                )))\n                                        )\n                                )\n                        )\n                        .build()\n        );\n\n        if (postModelOutputsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            System.out.println(postModelOutputsResponse.getStatus());\n            throw new RuntimeException("Post workflow results failed, status: " + postModelOutputsResponse.getStatus().getDescription());\n        }\n\n        Output output = postModelOutputsResponse.getOutputs(0);\n\n        // Print the output\n        System.out.println(output.getData().getText().getRaw());\n\n    }\n\n}\n',f='<?php\n\nrequire __DIR__ . "/vendor/autoload.php";\n\n///////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model ID, and\n// audio URL. Change these strings to run your own example.\n///////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\n$PAT = "YOUR_PAT_HERE";\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\n$USER_ID = "facebook";\n$APP_ID = "asr";\n// Change these to make your own predictions\n$MODEL_ID = "asr-wav2vec2-base-960h-english";\n$AUDIO_URL = "https://samples.clarifai.com/negative_sentence_1.wav";\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\Api\\Audio;\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\PostModelOutputsRequest;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ["Authorization" => ["Key " . $PAT]];\n\n$userDataObject = new UserAppIDSet([\n    "user_id" => $USER_ID,\n    "app_id" => $APP_ID\n]);\n\n// Let\'s make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client->PostModelOutputs(\n    // The request object carries the request along with the request status and other metadata related to the request itself\n    new PostModelOutputsRequest([\n        "user_app_id" => $userDataObject,\n        "model_id" => $MODEL_ID,\n        "inputs" => [\n            new Input([\n                "data" => new Data([\n                    "audio" => new Audio([\n                        "url" => $AUDIO_URL\n                    ])\n                ])\n            ])\n        ]\n    ]),\n    $metadata\n)->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception("Error: {$status->details}");\n}\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure \n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    print $response->getStatus()->getDetails();\n    throw new Exception("Failure response: " . $response->getStatus()->getDescription());\n}\n\n// We\'ll get one output for each input we used above. Because of one input, we have here one output\n$output = $response->getOutputs()[0];\n\n// Print the output\necho $output->getData()->getText()->getRaw();\n\n?>',g='<?php\n\nrequire __DIR__ . "/vendor/autoload.php";\n\n///////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model ID, and\n// audio file location. Change these strings to run your own example.\n//////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\n$PAT = "YOUR_PAT_HERE";\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\n$USER_ID = "facebook";\n$APP_ID = "asr";\n// Change these to make your own predictions\n$MODEL_ID = "asr-wav2vec2-base-960h-english";\n$AUDIO_FILE_LOCATION = "YOUR_AUDIO_FILE_LOCATION_HERE";\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\Api\\Audio;\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\PostModelOutputsRequest;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ["Authorization" => ["Key " . $PAT]];\n\n$userDataObject = new UserAppIDSet([\n    "user_id" => $USER_ID,\n    "app_id" => $APP_ID\n]);\n\n$audioData = file_get_contents($AUDIO_FILE_LOCATION); // Get the audio bytes data from the location\n\n// Let\'s make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client->PostModelOutputs(\n    // The request object carries the request along with the request status and other metadata related to the request itself\n    new PostModelOutputsRequest([\n        "user_app_id" => $userDataObject,\n        "model_id" => $MODEL_ID,\n        "inputs" => [\n            new Input([\n                "data" => new Data([\n                    "audio" => new Audio([\n                        "base64" => $audioData\n                    ])\n                ])\n            ])\n        ]\n    ]),\n    $metadata\n)->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception("Error: {$status->details}");\n}\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure \n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    print $response->getStatus()->getDetails();\n    throw new Exception("Failure response: " . $response->getStatus()->getDescription());\n}\n\n// We\'ll get one output for each input we used above. Because of one input, we have here one output\n$output = $response->getOutputs()[0];\n\n// Print the output\necho $output->getData()->getText()->getRaw();\n\n?>',I='curl -X POST "https://api.clarifai.com/v2/users/facebook/apps/asr/models/asr-wav2vec2-base-960h-english/outputs" \\\n  -H "authorization: Key YOUR_PAT_HERE" \\\n  -H "content-type: application/json" \\\n  -d \'{\n    "inputs": [\n        {\n          "data": {\n            "audio": {\n              "url": "https://samples.clarifai.com/negative_sentence_1.wav"\n          }\n        }\n      }\n    ]\n}\'',b='curl -X POST "https://api.clarifai.com/v2/users/facebook/apps/asr/models/asr-wav2vec2-base-960h-english/outputs" \\\n  -H "authorization: Key YOUR_PAT_HERE" \\\n  -H "content-type: application/json" \\\n  -d \'{\n    "inputs": [\n        {\n          "data": {\n            "audio": {\n              "base64": "YOUR_BYTES_STRING_HERE"\n          }\n        }\n      }\n    ]\n}\'',E="I AM NOT FLYING TO ENGLAND",T={description:"Make predictions on audio inputs",sidebar_position:5},A="Audio",O={unversionedId:"api-guide/predict/audio",id:"api-guide/predict/audio",title:"Audio",description:"Make predictions on audio inputs",source:"@site/docs/api-guide/predict/audio.md",sourceDirName:"api-guide/predict",slug:"/api-guide/predict/audio",permalink:"/api-guide/predict/audio",draft:!1,editUrl:"https://github.com/Clarifai/docs/blob/main/docs/api-guide/predict/audio.md",tags:[],version:"current",sidebarPosition:5,frontMatter:{description:"Make predictions on audio inputs",sidebar_position:5},sidebar:"tutorialSidebar",previous:{title:"Large Language Models (LLMs)",permalink:"/api-guide/predict/llms"},next:{title:"Multimodal-to-Text",permalink:"/api-guide/predict/multimodal-to-text"}},D={},w=[{value:"Predict via URL",id:"predict-via-url",level:2},{value:"Predict via Bytes",id:"predict-via-bytes",level:2}],S={toc:w},y="wrapper";function C(e){let{components:t,...n}=e;return(0,s.kt)(y,(0,a.Z)({},S,n,{components:t,mdxType:"MDXLayout"}),(0,s.kt)("h1",{id:"audio"},"Audio"),(0,s.kt)("p",null,(0,s.kt)("strong",{parentName:"p"},"Make predictions on audio inputs")),(0,s.kt)("hr",null),(0,s.kt)("p",null,(0,s.kt)("strong",{parentName:"p"},"Input"),": Audio"),(0,s.kt)("p",null,(0,s.kt)("strong",{parentName:"p"},"Output"),": Text"),(0,s.kt)("p",null,"To get predictions for a given audio input, you need to supply the audio along with the specific model from which you wish to receive predictions. You can supply the input via a publicly accessible URL or by directly sending bytes."),(0,s.kt)("p",null,"You need to specify your choice of ",(0,s.kt)("a",{parentName:"p",href:"https://clarifai.com/explore/models?filterData=%5B%7B%22field%22%3A%22input_fields%22%2C%22value%22%3A%5B%22audio%22%5D%7D%5D&page=1&perPage=24"},"model")," for prediction by utilizing the ",(0,s.kt)("inlineCode",{parentName:"p"},"MODEL_ID")," parameter."),(0,s.kt)("p",null,"The file size of each audio input should be under 5MB. This is typically suitable for a 48kHz audio file lasting up to 60 seconds, recorded with 16-bit audio quality. If your file exceeds this limit, you will need to split it into smaller chunks."),(0,s.kt)("admonition",{type:"info"},(0,s.kt)("p",{parentName:"admonition"},"The initialization code used in the following examples is outlined in detail on the ",(0,s.kt)("a",{parentName:"p",href:"https://docs.clarifai.com/api-guide/api-overview/api-clients/#client-installation-instructions"},"client installation page."))),(0,s.kt)("h2",{id:"predict-via-url"},"Predict via URL"),(0,s.kt)("p",null,"Below is an example of how you would use the ",(0,s.kt)("a",{parentName:"p",href:"https://clarifai.com/facebook/asr/models/asr-wav2vec2-base-960h-english"},"asr-wav2vec2-base-960h-english")," audio transcription model to convert English speech audio, sent via a URL, into English text."),(0,s.kt)(o.Z,{mdxType:"Tabs"},(0,s.kt)(i.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,s.kt)(r.Z,{className:"language-python",mdxType:"CodeBlock"},u)),(0,s.kt)(i.Z,{value:"js_rest",label:"JavaScript (REST)",mdxType:"TabItem"},(0,s.kt)(r.Z,{className:"language-javascript",mdxType:"CodeBlock"},l)),(0,s.kt)(i.Z,{value:"nodejs",label:"NodeJS",mdxType:"TabItem"},(0,s.kt)(r.Z,{className:"language-javascript",mdxType:"CodeBlock"},d)),(0,s.kt)(i.Z,{value:"java",label:"Java",mdxType:"TabItem"},(0,s.kt)(r.Z,{className:"language-java",mdxType:"CodeBlock"},m)),(0,s.kt)(i.Z,{value:"php",label:"PHP",mdxType:"TabItem"},(0,s.kt)(r.Z,{className:"language-php",mdxType:"CodeBlock"},f)),(0,s.kt)(i.Z,{value:"curl",label:"cURL",mdxType:"TabItem"},(0,s.kt)(r.Z,{className:"language-bash",mdxType:"CodeBlock"},I))),(0,s.kt)("details",null,(0,s.kt)("summary",null,"Text Output Example"),(0,s.kt)(r.Z,{className:"language-text",mdxType:"CodeBlock"},E)),(0,s.kt)("h2",{id:"predict-via-bytes"},"Predict via Bytes"),(0,s.kt)("p",null,"Below is an example of how you would use the ",(0,s.kt)("a",{parentName:"p",href:"https://clarifai.com/facebook/asr/models/asr-wav2vec2-base-960h-english"},"asr-wav2vec2-base-960h-english")," audio transcription model to convert English speech audio, sent as bytes, into English text."),(0,s.kt)(o.Z,{mdxType:"Tabs"},(0,s.kt)(i.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,s.kt)(r.Z,{className:"language-python",mdxType:"CodeBlock"},p)),(0,s.kt)(i.Z,{value:"js_rest",label:"JavaScript (REST)",mdxType:"TabItem"},(0,s.kt)(r.Z,{className:"language-javascript",mdxType:"CodeBlock"},c)),(0,s.kt)(i.Z,{value:"nodejs",label:"NodeJS",mdxType:"TabItem"},(0,s.kt)(r.Z,{className:"language-javascript",mdxType:"CodeBlock"},h)),(0,s.kt)(i.Z,{value:"java",label:"Java",mdxType:"TabItem"},(0,s.kt)(r.Z,{className:"language-java",mdxType:"CodeBlock"},_)),(0,s.kt)(i.Z,{value:"php",label:"PHP",mdxType:"TabItem"},(0,s.kt)(r.Z,{className:"language-php",mdxType:"CodeBlock"},g)),(0,s.kt)(i.Z,{value:"curl",label:"cURL",mdxType:"TabItem"},(0,s.kt)(r.Z,{className:"language-bash",mdxType:"CodeBlock"},b))))}C.isMDXComponent=!0}}]);
"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[3226],{11470:(e,n,t)=>{t.d(n,{A:()=>w});var a=t(96540),i=t(18215),s=t(17559),r=t(23104),o=t(56347),l=t(205),d=t(57485),c=t(31682),p=t(70679);function u(e){return a.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,a.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function h(e){const{values:n,children:t}=e;return(0,a.useMemo)(()=>{const e=n??function(e){return u(e).map(({props:{value:e,label:n,attributes:t,default:a}})=>({value:e,label:n,attributes:t,default:a}))}(t);return function(e){const n=(0,c.XI)(e,(e,n)=>e.value===n.value);if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[n,t])}function m({value:e,tabValues:n}){return n.some(n=>n.value===e)}function g({queryString:e=!1,groupId:n}){const t=(0,o.W6)(),i=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,d.aZ)(i),(0,a.useCallback)(e=>{if(!i)return;const n=new URLSearchParams(t.location.search);n.set(i,e),t.replace({...t.location,search:n.toString()})},[i,t])]}function f(e){const{defaultValue:n,queryString:t=!1,groupId:i}=e,s=h(e),[r,o]=(0,a.useState)(()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!m({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const t=n.find(e=>e.default)??n[0];if(!t)throw new Error("Unexpected error: 0 tabValues");return t.value}({defaultValue:n,tabValues:s})),[d,c]=g({queryString:t,groupId:i}),[u,f]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[t,i]=(0,p.Dv)(n);return[t,(0,a.useCallback)(e=>{n&&i.set(e)},[n,i])]}({groupId:i}),v=(()=>{const e=d??u;return m({value:e,tabValues:s})?e:null})();(0,l.A)(()=>{v&&o(v)},[v]);return{selectedValue:r,selectValue:(0,a.useCallback)(e=>{if(!m({value:e,tabValues:s}))throw new Error(`Can't select invalid tab value=${e}`);o(e),c(e),f(e)},[c,f,s]),tabValues:s}}var v=t(92303);const x={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var b=t(74848);function y({className:e,block:n,selectedValue:t,selectValue:a,tabValues:s}){const o=[],{blockElementScrollPositionUntilNextRender:l}=(0,r.a_)(),d=e=>{const n=e.currentTarget,i=o.indexOf(n),r=s[i].value;r!==t&&(l(n),a(r))},c=e=>{let n=null;switch(e.key){case"Enter":d(e);break;case"ArrowRight":{const t=o.indexOf(e.currentTarget)+1;n=o[t]??o[0];break}case"ArrowLeft":{const t=o.indexOf(e.currentTarget)-1;n=o[t]??o[o.length-1];break}}n?.focus()};return(0,b.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,i.A)("tabs",{"tabs--block":n},e),children:s.map(({value:e,label:n,attributes:a})=>(0,b.jsx)("li",{role:"tab",tabIndex:t===e?0:-1,"aria-selected":t===e,ref:e=>{o.push(e)},onKeyDown:c,onClick:d,...a,className:(0,i.A)("tabs__item",x.tabItem,a?.className,{"tabs__item--active":t===e}),children:n??e},e))})}function j({lazy:e,children:n,selectedValue:t}){const s=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=s.find(e=>e.props.value===t);return e?(0,a.cloneElement)(e,{className:(0,i.A)("margin-top--md",e.props.className)}):null}return(0,b.jsx)("div",{className:"margin-top--md",children:s.map((e,n)=>(0,a.cloneElement)(e,{key:n,hidden:e.props.value!==t}))})}function _(e){const n=f(e);return(0,b.jsxs)("div",{className:(0,i.A)(s.G.tabs.container,"tabs-container",x.tabList),children:[(0,b.jsx)(y,{...n,...e}),(0,b.jsx)(j,{...n,...e})]})}function w(e){const n=(0,v.A)();return(0,b.jsx)(_,{...e,children:u(e.children)},String(n))}},19365:(e,n,t)=>{t.d(n,{A:()=>r});t(96540);var a=t(18215);const i={tabItem:"tabItem_Ymn6"};var s=t(74848);function r({children:e,hidden:n,className:t}){return(0,s.jsx)("div",{role:"tabpanel",className:(0,a.A)(i.tabItem,t),hidden:n,children:e})}},25604:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>I,contentTitle:()=>S,default:()=>T,frontMatter:()=>A,metadata:()=>a,toc:()=>k});const a=JSON.parse('{"id":"create/models/deep-fine-tuning/visual-segmenter","title":"Visual Segmenter","description":"Learn about our visual segmenter model type","source":"@site/docs/create/models/deep-fine-tuning/visual-segmenter.md","sourceDirName":"create/models/deep-fine-tuning","slug":"/create/models/deep-fine-tuning/visual-segmenter","permalink":"/create/models/deep-fine-tuning/visual-segmenter","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"description":"Learn about our visual segmenter model type","sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Visual Detector","permalink":"/create/models/deep-fine-tuning/visual-detector"},"next":{"title":"Visual Anomaly","permalink":"/create/models/deep-fine-tuning/visual-anomaly"}}');var i=t(74848),s=t(28453),r=t(11470),o=t(19365),l=t(88149);const d='from clarifai.client.user import User\n#replace your "user_id"\nclient = User(user_id="user_id")\napp = client.create_app(app_id="demo_train", base_workflow="Universal")',c="#importing load_module_dataloader for calling the dataloader object in dataset.py in the local data folder\nfrom clarifai.datasets.upload.utils import load_module_dataloader\n\n\n# Construct the path to the dataset folder\nmodule_path = os.path.join(os.getcwd().split('/models/model_train')[0],'datasets/upload/image_segmentation/coco')\n\n\n# Load the dataloader module using the provided function from your module\ncoco_dataloader = load_module_dataloader(module_path)\n\n# Create a Clarifai dataset with the specified dataset_id (\"image_dataset\")\ndataset = app.create_dataset(dataset_id=\"segmentation_dataset\")\n\n# Upload the dataset using the provided dataloader and get the upload status\ndataset.upload_dataset(dataloader=coco_dataloader)\n",p="print(app.list_trainable_model_types())",u='MODEL_ID = "segmenter"\nMODEL_TYPE_ID = "visual-segmenter"\n\n# Create a model by passing the model name and model type as parameter\nmodel = app.create_model(model_id=MODEL_ID, model_type_id=MODEL_TYPE_ID)\n',h="print(model.list_training_templates())",m="import yaml\nYAML_FILE = 'model_params.yaml'\nmodel_params = model.get_params(template='MMSegmentation_SegFormer',save_to=YAML_FILE)\n# Preview YAML content\nfile = open(YAML_FILE)\ndata = yaml.safe_load(file)\nprint(data)\n",g="# List the concept\nconcepts = [concept.id for concept in app.list_concepts()]\nprint(concepts)",f="#creating dataset version\ndataset_version = dataset.create_version()\ndataset_version_id = dataset_version.version.id\n\n#update params\nmodel.update_params(dataset_id = 'segmentation_dataset', dataset_version_id=dataset_version_id,concepts = concepts, num_epochs = 5)",v='import time\n#Starting the training\nmodel_version_id = model.train()\n\n#Checking the status of training\nwhile True:\n    status = model.training_status(version_id=model_version_id,training_logs=False)\n    if status.code == 21106: #MODEL_TRAINING_FAILED\n        print(status)\n        break\n    elif status.code == 21100: #MODEL_TRAINED\n        print(status)\n        break\n    else:\n        print("Current Status:",status)\n        print("Waiting---")\n        time.sleep(120)\n',x="# Display the predicted masks\nimport cv2\nfrom urllib.request import urlopen\nimport numpy as np\nfrom PIL import Image as PILImage\nfrom io import BytesIO\nimport random\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom google.colab.patches import cv2_imshow\n\nIMAGE_PATH = os.path.join(os.getcwd().split('/models')[0],'datasets/upload/image_segmentation/coco/images/000000424349.jpg')\n\nprediction_response = model.predict_by_filepath(IMAGE_PATH, input_type=\"image\")\n\n# Get the output\nregions = prediction_response.outputs[0].data.regions\n\nimg = cv2.imread(IMAGE_PATH)\nimg=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\nmasks = []\nconcepts = []\nfor region in regions:\n    if region.data.concepts[0].value > 0.05:\n        masks.append(np.array(PILImage.open(BytesIO(region.region_info.mask.image.base64))))\n        concepts.append(region.data.concepts[0].name)\n\n\n# Generate random colors\ncolors = []\nfor i in range(len(masks)):\n    r = random.randint(0,255)\n    g = random.randint(0,255)\n    b = random.randint(0,255)\n    colors.append((b,g,r))\n\n# Map masks to overlays\noverlays = []\nfor i in range(len(masks)):\n    mask = masks[i]\n    color = colors[i]\n\n    overlay = np.zeros_like(img)\n    overlay[mask > 0] = color\n    overlays.append(overlay)\n\n# Overlay masks on original image\noverlayed = np.copy(img)\n\nfor overlay in overlays:\n  # Apply alpha blending\n  cv2.addWeighted(overlay, 0.15, overlayed, 0.85, 0, overlayed)\n\noverlayed = cv2.convertScaleAbs(overlayed, alpha=1.5, beta=50)\n\n\n# Display overlayed image\nimg = overlayed\n# for displaying in google colab or else use cv2.imshow()\ncv2_imshow(img) \n\n# Create legend with colors and concepts\nlegend_items = []\nfor i in range(len(overlays)):\n    color = [c/255 for c in colors[i]]\n    concept = concepts[i]\n    legend_items.append(mpatches.Patch(color=color, label=concept))\n\nplt.legend(handles=legend_items, loc='lower left', bbox_to_anchor=(1.05, 0))\nplt.axis('off')\nplt.show()\n",b="['visual-classifier',\n 'visual-detector',\n 'visual-segmenter',\n 'visual-embedder',\n 'clusterer',\n 'text-classifier',\n 'embedding-classifier',\n 'text-to-text']",y="['MMSegmentation', 'MMSegmentation_SegFormer']",j="{'dataset_id': '',\n 'dataset_version_id': '',\n 'concepts': [],\n 'train_params': {'invalid_data_tolerance_percent': 5.0,\n  'template': 'MMSegmentation_SegFormer',\n  'image_size': [520.0],\n  'batch_size': 2.0,\n  'num_epochs': 1.0,\n  'per_item_lrate': 7.5e-06,\n  'pretrained_weights': 'ade20k'}}\n",_="['id-chair',\n 'id-cup',\n 'id-couch',\n 'id-baseballbat',\n 'id-kite',\n 'id-person',\n 'id-elephant',\n 'id-cellphone',\n 'id-handbag',\n 'id-cat',\n 'id-toilet',\n 'id-laptop',\n 'id-diningtable',\n 'id-keyboard',\n 'id-mouse',\n 'id-oven',\n 'id-pizza',\n 'id-clock']\n",w="{'dataset_id': 'segmentation_dataset',\n 'dataset_version_id': '43cdc090797c41f19bb420ab6e4baf0c',\n 'concepts': ['id-chair',\n  'id-cup',\n  'id-couch',\n  'id-baseballbat',\n  'id-kite',\n  'id-person',\n  'id-elephant',\n  'id-cellphone',\n  'id-handbag',\n  'id-cat',\n  'id-toilet',\n  'id-laptop',\n  'id-diningtable',\n  'id-keyboard',\n  'id-mouse',\n  'id-oven',\n  'id-pizza',\n  'id-clock'],\n 'train_params': {'invalid_data_tolerance_percent': 5.0,\n  'template': 'MMSegmentation_SegFormer',\n  'image_size': [520.0],\n  'batch_size': 2.0,\n  'num_epochs': 5,\n  'per_item_lrate': 7.5e-06,\n  'pretrained_weights': 'ade20k'}}",A={description:"Learn about our visual segmenter model type",sidebar_position:4},S="Visual Segmenter",I={},k=[{value:"Example Use Case",id:"example-use-case",level:2},{value:"Create and Train a Visual Segmenter",id:"create-and-train-a-visual-segmenter",level:2},{value:"Step 1: App Creation",id:"step-1-app-creation",level:3},{value:"Step 2: Dataset Upload",id:"step-2-dataset-upload",level:3},{value:"Step 3: Model Creation",id:"step-3-model-creation",level:3},{value:"Step 4: Template Selection",id:"step-4-template-selection",level:3},{value:"Step 5: Set Up Model Parameters",id:"step-5-set-up-model-parameters",level:3},{value:"Step 6: Initiate Model Training",id:"step-6-initiate-model-training",level:3},{value:"Step 7: Model Prediction",id:"step-7-model-prediction",level:3}];function D(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components},{Details:t}=n;return t||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"visual-segmenter",children:"Visual Segmenter"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Learn about our visual segmenter model type"})}),"\n",(0,i.jsx)("hr",{}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Input"}),": Images and videos"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Output"}),": Regions"]}),"\n",(0,i.jsx)(n.p,{children:"Visual segmenter, also known as semantic segmentation, is a type of deep fine-tuned model used in image analysis and understanding tasks."}),"\n",(0,i.jsx)(n.p,{children:"It aims to achieve a fine-grained understanding of the content within an image by associating each pixel with a particular class label. This is more detailed than traditional object detection, which typically identifies bounding boxes around objects."}),"\n",(0,i.jsx)(n.p,{children:"The primary task of a visual segmenter model is twofold:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Semantic segmentation"}),": The model segments an input image into per-pixel masks, where each mask corresponds to a particular object or region of interest. Each pixel in the image is assigned a label that indicates the class of the object it belongs to. This process effectively divides the image into segments based on the objects or regions present in it."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Object classification or labeling"}),": Once the semantic segmentation is done, the model can then classify the segmented objects or regions into specific categories, descriptive words, or topics. This classification step involves assigning labels or tags to the segmented areas, indicating what each segment represents."]}),"\n"]}),"\n",(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsxs)(n.p,{children:["The visual segmenter model type also comes with various ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/portal-guide/model/deep-training/visual-segmenter-templates",children:"templates"})," that give you the control to choose the specific architecture used by your neural network, as well as define a set of hyperparameters you can use to fine-tune the way your model learns."]})}),"\n",(0,i.jsx)(n.p,{children:"Visual Segmenter models are used in a wide variety of applications, including:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Self-driving cars"}),": Visual Segmenter models can be used to identify objects in the road and surroundings, such as other cars, pedestrians, and traffic signs. This information can be used to help self-driving cars navigate safely."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Robotics"}),": Visual Segmenter models can be used to help robots interact with the physical world. For example, a robot could use a Visual Segmenter model to identify objects in its environment and then plan a path to avoid those objects."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Image editing"}),": Visual segmenter models can assist in automatic background removal, object manipulation, and other image editing tasks."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Augmented reality"}),": In AR applications, semantic segmentation helps in understanding the scene and integrating virtual objects more realistically."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"You may choose a visual segmenter model type in cases where:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Your application requires high accuracy, and you're willing to sacrifice speed and ease of use. These models tend to be computationally intensive due to their per-pixel processing."}),"\n",(0,i.jsxs)(n.li,{children:['You need a segmentation model to learn new features not recognized by the existing Clarifai models, especially if your application requires a detailed understanding of the content within an image at a per-pixel level. In that case, you may need to "deep fine-tune" your custom segmenter model and integrate it directly within your ',(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/portal-guide/workflows/",children:"workflows"}),"."]}),"\n",(0,i.jsx)(n.li,{children:"You have a custom-tailored dataset, accurate labels, and the expertise and time to fine-tune models."}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"example-use-case",children:"Example Use Case"}),"\n",(0,i.jsx)(n.p,{children:'Given an image of a street scene, a visual segmenter model could segment the image into per-pixel masks representing cars, pedestrians, buildings, roads, and other objects. Then, for each segmented area, the model could classify the objects into categories like "sedan," "person," "skyscraper," and "asphalt road.\u201d'}),"\n",(0,i.jsx)(n.h2,{id:"create-and-train-a-visual-segmenter",children:"Create and Train a Visual Segmenter"}),"\n",(0,i.jsx)(n.p,{children:"Let's demonstrate how to create and train a visual segmenter model using our API."}),"\n",(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsxs)(n.p,{children:["Before using the ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/additional-resources/api-overview/python-sdk",children:"Python SDK"}),", ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/additional-resources/api-overview/nodejs-sdk",children:"Node.js SDK"}),", or any of our ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/additional-resources/api-overview/grpc-clients",children:"gRPC clients"}),", ensure they are properly installed on your machine. Refer to their respective installation guides for instructions on how to install and initialize them."]})}),"\n","\n","\n","\n",(0,i.jsx)(n.h3,{id:"step-1-app-creation",children:"Step 1: App Creation"}),"\n",(0,i.jsxs)(n.p,{children:["Let's start by creating an ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/create-manage/applications/create",children:"app"}),"."]}),"\n",(0,i.jsx)(r.A,{groupId:"code",children:(0,i.jsx)(o.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:d})})}),"\n",(0,i.jsx)(n.h3,{id:"step-2-dataset-upload",children:"Step 2: Dataset Upload"}),"\n",(0,i.jsxs)(n.p,{children:["Next, let\u2019s upload the ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/create-manage/datasets/upload",children:"dataset"})," that will be used to train the model to the app."]}),"\n",(0,i.jsxs)(n.p,{children:["You can find the dataset we used ",(0,i.jsx)(n.a,{href:"https://github.com/Clarifai/examples/tree/main/datasets/upload/image_segmentation",children:"here"}),"."]}),"\n",(0,i.jsx)(r.A,{groupId:"code",children:(0,i.jsx)(o.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:c})})}),"\n",(0,i.jsx)(n.h3,{id:"step-3-model-creation",children:"Step 3: Model Creation"}),"\n",(0,i.jsx)(n.p,{children:"Let's list all the available trainable model types in the Clarifai platform."}),"\n",(0,i.jsx)(r.A,{groupId:"code",children:(0,i.jsx)(o.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:p})})}),"\n",(0,i.jsxs)(t,{children:[(0,i.jsx)("summary",{children:"Output"}),(0,i.jsx)(l.A,{className:"language-text",children:b})]}),"\n",(0,i.jsxs)(n.p,{children:["Next, let's select the ",(0,i.jsx)(n.code,{children:"visual-segmenter"})," model type and use it to create a model."]}),"\n",(0,i.jsx)(r.A,{groupId:"code",children:(0,i.jsx)(o.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:u})})}),"\n",(0,i.jsx)(n.h3,{id:"step-4-template-selection",children:"Step 4: Template Selection"}),"\n",(0,i.jsx)(n.p,{children:"Let's list all the available training templates in the Clarifai platform."}),"\n",(0,i.jsx)(r.A,{groupId:"code",children:(0,i.jsx)(o.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:h})})}),"\n",(0,i.jsxs)(t,{children:[(0,i.jsx)("summary",{children:"Output"}),(0,i.jsx)(l.A,{className:"language-text",children:y})]}),"\n",(0,i.jsxs)(n.p,{children:["Next, let's choose the ",(0,i.jsx)(n.code,{children:"'MMSegmentation_SegFormer' "})," template to use for training our model, as demonstrated below."]}),"\n",(0,i.jsx)(n.h3,{id:"step-5-set-up-model-parameters",children:"Step 5: Set Up Model Parameters"}),"\n",(0,i.jsx)(n.p,{children:"You can save the model parameters in a YAML file, which can then be passed to the model when initiating training."}),"\n",(0,i.jsx)(r.A,{groupId:"code",children:(0,i.jsx)(o.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:m})})}),"\n",(0,i.jsxs)(t,{children:[(0,i.jsx)("summary",{children:"Output"}),(0,i.jsx)(l.A,{className:"language-text",children:j})]}),"\n",(0,i.jsx)(n.p,{children:"You can modify the YAML file to suit your specific needs and reload it for model training."}),"\n",(0,i.jsx)(n.p,{children:"Before making changes, let\u2019s first list the available concepts in the app. After that, we\u2019ll show an example of the edited YAML configuration."}),"\n",(0,i.jsx)(r.A,{groupId:"code",children:(0,i.jsx)(o.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:g})})}),"\n",(0,i.jsxs)(t,{children:[(0,i.jsx)("summary",{children:"Output"}),(0,i.jsx)(l.A,{className:"language-text",children:_})]}),"\n",(0,i.jsxs)(n.p,{children:["Next, we\u2019ll create a dataset version and then use the ",(0,i.jsx)(n.code,{children:"model.update_params()"})," method to update the model parameters accordingly."]}),"\n",(0,i.jsx)(r.A,{groupId:"code",children:(0,i.jsx)(o.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:f})})}),"\n",(0,i.jsxs)(t,{children:[(0,i.jsx)("summary",{children:"Output"}),(0,i.jsx)(l.A,{className:"language-text",children:w})]}),"\n",(0,i.jsx)(n.h3,{id:"step-6-initiate-model-training",children:"Step 6: Initiate Model Training"}),"\n",(0,i.jsxs)(n.p,{children:["To initiate the model training process, call the ",(0,i.jsx)(n.code,{children:"model.train()"})," method. The Clarifai API also provides features for monitoring training status and saving training logs to a local file."]}),"\n",(0,i.jsx)(n.admonition,{type:"note",children:(0,i.jsxs)(n.p,{children:["If the status code is ",(0,i.jsx)(n.code,{children:"MODEL-TRAINED"}),", it indicates that the model has been successfully trained and is ready for use."]})}),"\n",(0,i.jsx)(r.A,{groupId:"code",children:(0,i.jsx)(o.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:v})})}),"\n",(0,i.jsxs)(t,{children:[(0,i.jsx)("summary",{children:"Output"}),(0,i.jsx)("img",{src:"/img/python-sdk/vs_imt.png"})]}),"\n",(0,i.jsx)(n.h3,{id:"step-7-model-prediction",children:"Step 7: Model Prediction"}),"\n",(0,i.jsx)(n.p,{children:"After the model is trained and ready to use, you can run some predictions with it."}),"\n",(0,i.jsx)(r.A,{groupId:"code",children:(0,i.jsx)(o.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:x})})}),"\n",(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsxs)(n.p,{children:["The model\u2019s performance can be further improved by increasing the number of training ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/additional-resources/glossary/#epoch",children:"epochs"}),"."]})}),"\n",(0,i.jsxs)(t,{children:[(0,i.jsx)("summary",{children:"Image Output"}),(0,i.jsx)("img",{src:"/img/python-sdk/vs_mp.png"})]})]})}function T(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(D,{...e})}):D(e)}}}]);
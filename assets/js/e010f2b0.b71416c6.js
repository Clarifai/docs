"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[9861],{49104:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>O,contentTitle:()=>S,default:()=>H,frontMatter:()=>N,metadata:()=>a,toc:()=>D});const a=JSON.parse('{"id":"compute/models/inference/api","title":"Inference via API","description":"Generate predictions using your deployed models","source":"@site/docs/compute/models/inference/api.md","sourceDirName":"compute/models/inference","slug":"/compute/models/inference/api","permalink":"/compute/models/inference/api","draft":false,"unlisted":false,"editUrl":"https://github.com/Clarifai/docs/blob/main/docs/compute/models/inference/api.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"description":"Generate predictions using your deployed models","sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Model Inference","permalink":"/compute/models/inference/"},"next":{"title":"OpenAI Inferences","permalink":"/compute/models/inference/open-ai"}}');var i=t(74848),o=t(28453),r=t(65537),s=t(79329),l=t(58069);const d='import os\nfrom clarifai.client import Model\n\n# Set your Personal Access Token (PAT)\nos.environ["CLARIFAI_PAT"] = "YOUR_PAT_HERE"\n\n# Initialize with model URL\nmodel = Model(url="https://clarifai.com/openai/chat-completion/models/o4-mini")\n\nmodel_methods = model.available_methods()\n\nprint(model_methods)',c='import os\nfrom clarifai.client import Model\n\n# Set your Personal Access Token (PAT)\nos.environ["CLARIFAI_PAT"] = "YOUR_PAT_HERE"\n\n# Initialize with model URL\nmodel = Model(url="https://clarifai.com/openai/chat-completion/models/o4-mini")\n\nmethod_name = "predict" # Or, "generate", "chat", etc\n\nmethod_signature = model.method_signature(method_name= method_name)\n\nprint(method_signature)',p='import os\nfrom clarifai.client import Model\n\n# Set your Personal Access Token (PAT)\nos.environ["CLARIFAI_PAT"] = "YOUR_PAT_HERE"\n\n# Initialize with model URL\nmodel = Model(url="https://clarifai.com/openai/chat-completion/models/o4-mini")\n\nmodel_script = model.generate_client_script()\n\nprint(model_script)',m="# Clarifai Model Client Script\n# Set the environment variables `CLARIFAI_DEPLOYMENT_ID` and `CLARIFAI_PAT` to run this script.\n# Example usage:\nfrom clarifai.runners.utils import data_types\nimport os\nfrom clarifai.client import Model\n\nmodel = Model(\"www.clarifai.com/openai/chat-completion/o4-mini\",\n    deployment_id = os.environ['CLARIFAI_DEPLOYMENT_ID'], # Only needed for dedicated deployed models\n)\n\n# Example model prediction from different model methods:\n\nresponse = model.predict(prompt='What is the future of AI?', image=Image(url='https://samples.clarifai.com/metro-north.jpg'), images=None, chat_history=None, max_tokens=512.0, temperature=1.0, top_p=0.8, reasoning_effort='\"low\"')\nprint(response)\n\nresponse = model.generate(prompt='What is the future of AI?', image=Image(url='https://samples.clarifai.com/metro-north.jpg'), images=None, chat_history=None, max_tokens=512.0, temperature=0.7, top_p=0.8, reasoning_effort='\"low\"')\nfor res in response:\n    print(res)\n\nresponse = model.chat(messages=None, max_tokens=750.0, temperature=0.7, top_p=0.8, reasoning_effort='\"low\"')\nfor res in response:\n    print(res)",h='import os\nfrom clarifai.client import Model\nfrom clarifai.runners.utils.data_types import Image\n\n# Set your Personal Access Token (PAT)\nos.environ["CLARIFAI_PAT"] = "YOUR_PAT_HERE"\n\n# Initialize with model URL\nmodel = Model(\n    url="https://clarifai.com/openai/chat-completion/models/o4-mini",\n    # deployment_id="DEPLOYMENT_ID_HERE"\n)\n\nresponse = model.predict(\n    prompt="Describe the image",\n    image=Image(url="https://samples.clarifai.com/cat1.jpeg") \n)\n\nprint(response)\n\n"""\n# --- Predict using an image uploaded from a local machine ---\n\n# 1. Specify the path to your local image file\nlocal_image_path = "path/to/your/image.jpg"  # Replace with the actual path to your image\n\n# 2. Read the image file into bytes\nwith open(local_image_path, "rb") as f:\n    image_bytes = f.read()\n\nresponse = model.predict(\n    prompt="Describe the image",\n    # Provide Image as bytes\n    image=Image(bytes=image_bytes)\n)\n\nprint(response)\n\n# You can also convert a Pillow (PIL) Image object into a Clarifai Image data type \n# image=Image.from_pil(pil_image)\n\n"""',u='import os\nfrom clarifai.client import Model\n\n# Set your Personal Access Token (PAT)\nos.environ["CLARIFAI_PAT"] = "YOUR_PAT_HERE"\n\n# Initialize with model URL\nmodel = Model(\n    url="https://clarifai.com/openai/chat-completion/models/o4-mini",\n    # deployment_id="DEPLOYMENT_ID_HERE"\n)\n\nresponse = model.predict("What is photosynthesis?")\n# Or\n# response = model.predict(prompt="What is photosynthesis?")\n\nprint(response)\n',g='import os\n\nfrom clarifai.client import Model\n\n# Set your Personal Access Token (PAT)\nos.environ["CLARIFAI_PAT"] = "YOUR_PAT_HERE"\n\n# Initialize with model URL\nmodel = Model(\n    url="https://clarifai.com/openai/chat-completion/models/o4-mini",\n    # deployment_id="DEPLOYMENT_ID_HERE"\n)\n\nresponse_stream = model.generate(\n    prompt="Explain quantum computing in simple terms"  \n)\n\nfor text_chunk in response_stream:\n    print(text_chunk, end="", flush=True)\n\n"""\n# --- Load prompt text from URL ---\n\nprompt_from_url = requests.get("https://samples.clarifai.com/featured-models/redpajama-economic.txt") # Remember to import requests\nprompt_text = prompt_from_url.text.strip()\n\nresponse_stream = model.generate(\n    prompt=prompt_text\n)\n\nfor text_chunk in response_stream:\n    print(text_chunk, end="", flush=True)\n\n"""',f='import os\nfrom clarifai.client import Model\nfrom clarifai.runners.utils.data_types import Audio\n\n# Set your Personal Access Token (PAT)\nos.environ["CLARIFAI_PAT"] = "YOUR_PAT_HERE"\n\n# Initialize with model URL\nmodel = Model(\n    url="MODEL_URL_HERE",\n    # deployment_id="DEPLOYMENT_ID_HERE"\n)\n\n# client-side streaming\nresponse_stream = model.transcribe_audio(\n    audio=iter(Audio(bytes=b\'\'))\n    # Or, provide audio as URL\n    # audio=Audio(url="https://example.com/audio.mp3")\n)\n\nfor text_chunk in response_stream:\n    print(text_chunk.text, end="", flush=True)',x='import os\nfrom clarifai.client import Model\nfrom clarifai.runners.utils.data_types import Text\n\n# Set your Personal Access Token (PAT)\nos.environ["CLARIFAI_PAT"] = "YOUR_PAT_HERE"\n\n# Initialize with model URL\nmodel = Model(\n    url="MODEL_URL_HERE", \n    # deployment_id="YOUR_DEPLOYMENT_ID_HERE"\n)\n\n# Create a list of input Texts to simulate a stream\ninput_texts = iter([\n    Text(text="First input."),\n    Text(text="Second input."),\n    Text(text="Third input.")\n])\n\n# Call the stream method and process outputs\nresponse_iterator = model.stream(input_texts)\n\n# Print streamed results\nprint("Streaming output:\\n")\nfor response in response_iterator:\n    print(response.text)\n',y='import os\nfrom clarifai.client import Model\nfrom clarifai.runners.utils.data_types import Image\n\n# Set your Personal Access Token (PAT)\nos.environ["CLARIFAI_PAT"] = "YOUR_PAT_HERE"\n\n# Initialize with model URL\nmodel = Model(\n    url="MODEL_URL_HERE",\n    # deployment_id="DEPLOYMENT_ID_HERE"\n)\n\n# Batch processing (automatically handled)\nbatch_results = model.predict_image([\n    {"image": Image(url="https://samples.clarifai.com/cat1.jpeg")},\n    {"image": Image(url="https://samples.clarifai.com/cat2.jpeg")},\n])\n\nfor i, pred in enumerate(batch_results):\n    print(f"Image {i+1} cat confidence: {pred[\'cat\']:.2%}")',j='import os\nfrom clarifai.client import Model\nfrom clarifai.runners.utils.data_types import Text\n\n# Set your Personal Access Token (PAT)\nos.environ["CLARIFAI_PAT"] = "YOUR_PAT_HERE"\n\n# Initialize with model URL\nmodel = Model(\n    url="MODEL_URL_HERE",\n    # deployment_id="DEPLOYMENT_ID_HERE"\n)\n\n# Batch prediction\nbatch_results = model.predict([\n    {"text": Text("Positive review")},\n    {"text": Text("Positive review")},\n    {"text": Text("Positive review")},\n])',A='import os\nfrom clarifai.client import Model\nfrom clarifai.runners.utils.data_types import Image\n\n# Set your Personal Access Token (PAT)\nos.environ["CLARIFAI_PAT"] = "YOUR_PAT_HERE"\n\n# Initialize model\nmodel = Model(\n    url="MODEL_URL_HERE",\n    # deployment_id="DEPLOYMENT_ID_HERE"\n)\n\n# Perform prediction with prompt and image\nresult = model.predict(\n    prompt="What is the future of AI?",\n    image=Image(url="https://samples.clarifai.com/metro-north.jpg"),\n    max_tokens=512,\n    temperature=0.7,\n    top_p=0.8\n)\n\n# Print the prediction result\nprint(result)',_='import { Model } from "clarifai-nodejs";\n\nconst model = new Model({\n  url: "https://clarifai.com/openai/chat-completion/models/o4-mini",\n  authConfig: {\n    pat: process.env.CLARIFAI_PAT,\n  },\n});\n\nconst modelMethods = await model.availableMethods();\n\nconsole.log(modelMethods);',b='import { Model } from "clarifai-nodejs";\n\nconst model = new Model({\n  url: "https://clarifai.com/openai/chat-completion/models/o4-mini",\n  authConfig: {\n    pat: process.env.CLARIFAI_PAT,\n  },\n});\n\nconst methodSignatures = await model.methodSignatures();\n\nconsole.log(methodSignatures);',v='import { Model } from "clarifai-nodejs";\n\nconst model = new Model({\n  url: "https://clarifai.com/openai/chat-completion/models/o4-mini",\n  authConfig: {\n    pat: process.env.CLARIFAI_PAT,\n  },\n});\n\nconst response = await model.predict({\n  // see available methodNames using model.availableMethods()\n  methodName: "predict",\n  prompt: "What is photosynthesis?",\n});\n\nconsole.log(JSON.stringify(response));\n\n// get response data from the response object\nModel.getOutputDataFromModelResponse(response);',I='import { Model } from "clarifai-nodejs";\n\nconst model = new Model({\n  url: "https://clarifai.com/openai/chat-completion/models/o4-mini",\n  authConfig: {\n    pat: process.env.CLARIFAI_PAT,\n  },\n});\n\nconst response = await model.predict({\n  // see available methodNames using model.availableMethods()\n  methodName: "predict",\n  prompt: "Describe the image",\n  image: {\n    url: "https://samples.clarifai.com/cat1.jpeg",\n  },\n});\n\nconsole.log(JSON.stringify(response));\n\n// get response data from the response object\nModel.getOutputDataFromModelResponse(response);',P='import { Model } from "clarifai-nodejs";\n\nconst model = new Model({\n  url: "https://clarifai.com/openai/chat-completion/models/o4-mini",\n  authConfig: {\n    pat: process.env.CLARIFAI_PAT,\n  },\n});\n\nconst responseStream = model.generate({\n  // see available methodNames using model.availableMethods()\n  methodName: "generate",\n  prompt: "what is photosynthesis?",\n});\n\nfor await (const response of responseStream) {\n  console.log(JSON.stringify(response));\n\n  // get response data from the response object\n  Model.getOutputDataFromModelResponse(response);\n}',T="Photosynthesis is the process by which certain organisms\u2014primarily plants, algae, and some bacteria\u2014convert light energy (usually from the sun) into chemical energy stored in sugars. In essence, these organisms capture carbon dioxide (CO\u2082) from the air and water (H\u2082O) from the soil, then use sunlight to drive a series of reactions that produce oxygen (O\u2082) as a by-product and synthesize glucose (C\u2086H\u2081\u2082O\u2086) or related carbohydrates.\n\nKey points:\n\n1. Light absorption\n   \u2022 Chlorophyll and other pigments in chloroplasts (in plants and algae) absorb photons, elevating electrons to higher energy states.\n\n2. Light-dependent reactions (in thylakoid membranes)\n   \u2022 Convert light energy into chemical energy in the form of ATP and NADPH.\n   \u2022 Split water molecules, releasing O\u2082.\n\n3. Calvin cycle (light-independent reactions, in the stroma)\n   \u2022 Use ATP and NADPH to fix CO\u2082 into organic molecules.\n   \u2022 Produce glyceraldehyde-3-phosphate (G3P), which can be converted into glucose and other carbs.\n\nOverall simplified equation:\n6 CO\u2082 + 6 H\u2082O + light energy \u2192 C\u2086H\u2081\u2082O\u2086 + 6 O\u2082\n\nImportance:\n\u2022 Generates the oxygen we breathe.\n\u2022 Forms the base of most food chains by producing organic matter.\n\u2022 Plays a critical role in the global carbon cycle and helps mitigate CO\u2082 in the atmosphere.",E="The image shows a young ginger tabby cat lying on its side against what looks like a rough, earth-toned wall. Its coat is a warm orange with classic darker orange stripe markings. The cat\u2019s front paw is tucked in, and its head rests on the surface below, with its large amber eyes gazing directly toward the viewer. The lighting is soft, highlighting the cat\u2019s whiskers, ear fur, and the texture of its velvety coat. Overall, the scene feels calm and slightly curious, as if the cat has paused mid-nap to watch something interesting.",w='import os\nfrom clarifai.client import Model\nfrom clarifai.runners.utils.data_types import Image\n\n# Set your Personal Access Token (PAT)\nos.environ["CLARIFAI_PAT"] = "YOUR_PAT_HERE"\n\n# Initialize with model URL\nmodel = Model(\n    url="https://clarifai.com/openai/chat-completion/models/o4-mini",\n    # deployment_id="DEPLOYMENT_ID_HERE"\n)\n\nresponse_stream = model.generate(\n    prompt="Describe the image",\n    image=Image(url="https://samples.clarifai.com/cat1.jpeg") \n)\n\nfor text_chunk in response_stream:\n    print(text_chunk, end="", flush=True)\n\n\n"""\n# --- Predict using an image uploaded from a local machine ---\n\n# 1. Specify the path to your local image file\nlocal_image_path = "path/to/your/image.jpg"  # Replace with the actual path to your image\n\n# 2. Read the image file into bytes\nwith open(local_image_path, "rb") as f:\n    image_bytes = f.read()\n\nresponse_stream = model.generate(\n    prompt="Describe the image",\n    # Provide Image as bytes\n    image=Image(bytes=image_bytes)\n)\n\nfor text_chunk in response_stream:\n    print(text_chunk, end="", flush=True)\n\n# You can also convert a Pillow (PIL) Image object into a Clarifai Image data type \n# image=Image.from_pil(pil_image)\n\n"""',R='import { Model } from "clarifai-nodejs";\n\nconst model = new Model({\n  url: "https://clarifai.com/openai/chat-completion/models/o4-mini",\n  authConfig: {\n    pat: process.env.CLARIFAI_PAT,\n  },\n});\n\nconst responseStream = await model.generate({\n  // see available methodNames using model.availableMethods()\n  methodName: "generate",\n  prompt: "Describe the image",\n  image: {\n    url: "https://samples.clarifai.com/cat1.jpeg",\n  },\n});\n\nfor await (const response of responseStream) {\n    console.log(JSON.stringify(response));\n  \n    // get response data from the response object\n    Model.getOutputDataFromModelResponse(response);\n  }',M='from clarifai.client import Model\n\n# Initialize with explicit IDs\nmodel = Model(user_id="model_user_id", app_id="model_app_id", model_id="model_id")\n\n# Or initialize with model URL\nmodel = Model(url="https://clarifai.com/model_user_id/model_app_id/models/model_id")\n  ',C='import { Model } from "clarifai-nodejs";\n\nconst model = new Model({\n    modelId: "model_id",\n    modelUserAppId: {\n      userId: "model_user_id",\n      appId: "model_app_id",\n    },\n    authConfig: {\n      pat: "pat",\n    },\n  });',k='from clarifai.client import Model\nimport os\n\n# Set your Personal Access Token (PAT)\nos.environ["CLARIFAI_PAT"] = "YOUR_PAT_HERE"\n\n# Initialize with model URL\nmodel = Model(url="https://clarifai.com/anthropic/completion/models/claude-sonnet-4")\n\n# Define tools \ntools = [\n    {\n        "type": "function",\n        "function": {\n            "name": "get_weather",\n            "description": "Get current temperature for a given location.",\n            "parameters": {\n                "type": "object",\n                "properties": {\n                    "location": {\n                        "type": "string",\n                        "description": "City and country e.g. Bogot\xe1, Colombia"\n                    },\n                    "units": {\n                        "type": "string",\n                        "description": "Temperature units, e.g. Celsius or Fahrenheit",\n                        "enum": ["Celsius", "Fahrenheit"]\n                    }\n                },\n                "required": ["location"],\n                "additionalProperties": False\n            },\n            "strict": True\n        }\n    }\n]\n\nresponse = model.generate(\n    prompt="What is the temperature in Tokyo in Celsius?",\n    tools=tools,\n    tool_choice=\'auto\',\n    max_tokens=1024,\n    temperature=0.5,\n)\n\n# Print response summary\nprint("Iterate or print response as needed:\\n", response)\n',N={description:"Generate predictions using your deployed models",sidebar_position:1},S="Inference via API",O={},D=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Install Clarifai Packages",id:"install-clarifai-packages",level:3},{value:"Get a PAT Key",id:"get-a-pat-key",level:3},{value:"Structure of Prediction Methods",id:"structure-of-prediction-methods",level:2},{value:"Get Available Methods",id:"get-available-methods",level:3},{value:"Get Method Signature",id:"get-method-signature",level:3},{value:"Generate Example Code",id:"generate-example-code",level:3},{value:"Unary-Unary Predict Call",id:"unary-unary-predict-call",level:2},{value:"Text Inputs",id:"text-inputs",level:3},{value:"Image Inputs",id:"image-inputs",level:3},{value:"Unary-Stream Predict Call",id:"unary-stream-predict-call",level:2},{value:"Text Inputs",id:"text-inputs-1",level:3},{value:"Image Inputs",id:"image-inputs-1",level:3},{value:"Stream-Stream Predict Call",id:"stream-stream-predict-call",level:2},{value:"Text Inputs",id:"text-inputs-2",level:3},{value:"Audio Inputs",id:"audio-inputs",level:3},{value:"Dynamic Batch Prediction Handling",id:"dynamic-batch-prediction-handling",level:2},{value:"Image Inputs",id:"image-inputs-2",level:3},{value:"Text Inputs",id:"text-inputs-3",level:3},{value:"Multimodal Predictions",id:"multimodal-predictions",level:2},{value:"Tool Calling",id:"tool-calling",level:2}];function L(e){const n={a:"a",admonition:"admonition",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components},{Details:t}=n;return t||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"inference-via-api",children:"Inference via API"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Generate predictions using your deployed models"})}),"\n",(0,i.jsx)("hr",{}),"\n",(0,i.jsx)(n.p,{children:"Our new inference technique provides an efficient, scalable, and streamlined way to perform predictions with models."}),"\n",(0,i.jsx)(n.p,{children:"Built with a Python-first, user-centric design, this flexible approach simplifies the process of working with models \u2014 enabling users to focus more on building and iterating, and less on navigating API mechanics."}),"\n","\n","\n","\n","\n","\n","\n",(0,i.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsx)(n.h3,{id:"install-clarifai-packages",children:"Install Clarifai Packages"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Install the latest version of the Clarifai ",(0,i.jsx)(n.a,{href:"https://github.com/Clarifai/clarifai-python/",children:"Python"})," SDK package:"]}),"\n"]}),"\n",(0,i.jsx)(r.A,{groupId:"code",children:(0,i.jsx)(s.A,{value:"bash",label:"Bash",children:(0,i.jsx)(l.A,{className:"language-bash",children:" pip install --upgrade clarifai "})})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Install the latest version of the Clarifai ",(0,i.jsx)(n.a,{href:"https://github.com/Clarifai/clarifai-nodejs",children:"Node.js"})," SDK package:"]}),"\n"]}),"\n",(0,i.jsx)(r.A,{groupId:"code",children:(0,i.jsx)(s.A,{value:"bash",label:"Bash",children:(0,i.jsx)(l.A,{className:"language-bash",children:" npm install clarifai-nodejs "})})}),"\n",(0,i.jsx)(n.h3,{id:"get-a-pat-key",children:"Get a PAT Key"}),"\n",(0,i.jsxs)(n.p,{children:["You need a PAT (Personal Access Token) key to authenticate your connection to the Clarifai platform. You can generate the PAT key in your personal settings page by navigating to the ",(0,i.jsx)(n.a,{href:"https://clarifai.com/settings/security",children:"Security section"}),"."]}),"\n",(0,i.jsxs)(n.p,{children:["You can then set the PAT as an environment variable using ",(0,i.jsx)(n.code,{children:"CLARIFAI_PAT"}),":"]}),"\n",(0,i.jsxs)(r.A,{groupId:"code",children:[(0,i.jsx)(s.A,{value:"bash",label:"Unix-Like Systems",children:(0,i.jsx)(l.A,{className:"language-bash",children:" export CLARIFAI_PAT=YOUR_PERSONAL_ACCESS_TOKEN_HERE "})}),(0,i.jsx)(s.A,{value:"bash2",label:"Windows",children:(0,i.jsx)(l.A,{className:"language-bash",children:" set CLARIFAI_PAT=YOUR_PERSONAL_ACCESS_TOKEN_HERE "})})]}),"\n",(0,i.jsx)(n.admonition,{title:"tip",type:"note",children:(0,i.jsxs)(n.p,{children:["On Windows, the Clarifai Python SDK expects a ",(0,i.jsx)(n.code,{children:"HOME"})," environment variable, which isn\u2019t set by default. To ensure compatibility with file paths used by the SDK, set ",(0,i.jsx)(n.code,{children:"HOME"})," to the value of your ",(0,i.jsx)(n.code,{children:"USERPROFILE"}),". You can set it in your Command Prompt this way: ",(0,i.jsx)(n.code,{children:"set HOME=%USERPROFILE%"}),"."]})}),"\n",(0,i.jsx)(n.h2,{id:"structure-of-prediction-methods",children:"Structure of Prediction Methods"}),"\n",(0,i.jsx)(n.p,{children:"Before making a prediction with a model, it\u2019s important to understand how its prediction methods are structured."}),"\n",(0,i.jsxs)(n.p,{children:["You can learn more about the structure of model prediction methods ",(0,i.jsx)(n.a,{href:"/compute/models/inference/#structure-of-prediction-methods",children:"here"}),"."]}),"\n",(0,i.jsx)(n.admonition,{title:"Initializing the Model Client",type:"note",children:(0,i.jsxs)(r.A,{groupId:"code",children:[(0,i.jsx)(s.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:M})}),(0,i.jsx)(s.A,{value:"node.js",label:"Node.js SDK",children:(0,i.jsx)(l.A,{className:"language-javascript",children:C})})]})}),"\n",(0,i.jsx)(n.h3,{id:"get-available-methods",children:"Get Available Methods"}),"\n",(0,i.jsx)(n.p,{children:"You can list all the methods implemented in the model's configuration that are available for performing model inference."}),"\n",(0,i.jsxs)(r.A,{groupId:"code",children:[(0,i.jsx)(s.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:d})}),(0,i.jsx)(s.A,{value:"node.js",label:"Node.js SDK",children:(0,i.jsx)(l.A,{className:"language-javascript",children:_})})]}),"\n",(0,i.jsxs)(t,{children:[(0,i.jsx)("summary",{children:"Example Output"}),(0,i.jsx)(l.A,{className:"language-text",children:"dict_keys(['predict', 'generate', 'chat'])"})]}),"\n",(0,i.jsx)(n.h3,{id:"get-method-signature",children:"Get Method Signature"}),"\n",(0,i.jsx)(n.p,{children:"You can retrieve the method signature of a specified model's method to identify all its arguments and their type annotations, which are essential for performing model inference."}),"\n",(0,i.jsx)(n.p,{children:"A method signature defines the method's name, its input parameters (with types and default values), and the return type, helping you understand how to properly call the method."}),"\n",(0,i.jsxs)(r.A,{groupId:"code",children:[(0,i.jsx)(s.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:c})}),(0,i.jsx)(s.A,{value:"node.js",label:"Node.js SDK",children:(0,i.jsx)(l.A,{className:"language-javascript",children:b})})]}),"\n",(0,i.jsxs)(t,{children:[(0,i.jsx)("summary",{children:"Example Output"}),(0,i.jsx)(l.A,{className:"language-text",children:"def predict(prompt: str, image: data_types.Image, images: Any, chat_history: Any, max_tokens: float = 512.0, temperature: float = 1.0, top_p: float = 0.8, reasoning_effort: str = '\"low\"') -> str:"})]}),"\n",(0,i.jsx)(n.h3,{id:"generate-example-code",children:"Generate Example Code"}),"\n",(0,i.jsx)(n.p,{children:"You can generate a sample code snippet to better understand how to perform inference using a model."}),"\n",(0,i.jsx)(r.A,{groupId:"code",children:(0,i.jsx)(s.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:p})})}),"\n",(0,i.jsxs)(t,{children:[(0,i.jsx)("summary",{children:"Example Output"}),(0,i.jsx)(l.A,{className:"language-text",children:m})]}),"\n",(0,i.jsxs)(n.admonition,{title:"Set up a deployment",type:"warning",children:[(0,i.jsxs)(n.p,{children:["To use Clarifai\u2019s Compute Orchestration capabilities, ensure your model is deployed, ",(0,i.jsx)(n.a,{href:"/compute/models/inference/",children:"as described previously"}),".\nThen, configure the ",(0,i.jsx)(n.code,{children:"deployment_id"})," parameter \u2014 alternatively, you can specify ",(0,i.jsx)(n.code,{children:"compute_cluster_id"})," and ",(0,i.jsx)(n.code,{children:"nodepool_id"}),". If none of these are set, the prediction will default to the ",(0,i.jsx)(n.code,{children:"Clarifai Shared"})," deployment type."]}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",children:'model = Model(\n    url="MODEL_URL_HERE",  \n    deployment_id="DEPLOYMENT_ID_HERE",\n    # Or, set cluster and nodepool \n    # compute_cluster_id = "COMPUTE_CLUSTER_ID_HERE",\n    # nodepool_id = "NODEPOOL_ID_HERE"\n)\n'})})]}),"\n",(0,i.jsx)(n.h2,{id:"unary-unary-predict-call",children:"Unary-Unary Predict Call"}),"\n",(0,i.jsx)(n.p,{children:"This is the simplest form of prediction: a single input is sent to the model, and a single response is returned. It\u2019s ideal for quick, non-streaming tasks, such as classifying an image or analyzing a short piece of text."}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"NOTE"}),": Streaming means that the response is streamed back token by token, rather than waiting for the entire completion to be generated before returning. This is useful for building interactive applications where you want to display the response as it's being generated."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"text-inputs",children:"Text Inputs"}),"\n",(0,i.jsxs)(n.p,{children:["Here is an example of a ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/compute/models/upload/#step-1-prepare-the-modelpy-file",children:"model signature"})," configured on the server side for handling text inputs:"]}),"\n",(0,i.jsx)(r.A,{groupId:"code",children:(0,i.jsx)(s.A,{value:"python",label:"Python",children:(0,i.jsx)(l.A,{className:"language-python",children:'@ModelClass.method\ndef predict(self, prompt: str = "") -> str:'})})}),"\n",(0,i.jsx)(n.p,{children:"Here\u2019s how you can make a corresponding unary-unary predict call from the client side:"}),"\n",(0,i.jsxs)(r.A,{groupId:"code",children:[(0,i.jsx)(s.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:u})}),(0,i.jsx)(s.A,{value:"node.js",label:"Node.js SDK",children:(0,i.jsx)(l.A,{className:"language-javascript",children:v})})]}),"\n",(0,i.jsxs)(t,{children:[(0,i.jsx)("summary",{children:"Example Output"}),(0,i.jsx)(l.A,{className:"language-text",children:T})]}),"\n",(0,i.jsx)(n.h3,{id:"image-inputs",children:"Image Inputs"}),"\n",(0,i.jsx)(n.p,{children:"Here is an example of a model signature configured on the server side for handling image inputs:"}),"\n",(0,i.jsx)(r.A,{groupId:"code",children:(0,i.jsx)(s.A,{value:"python",label:"Python",children:(0,i.jsx)(l.A,{className:"language-python",children:"@ModelClass.method\ndef predict(self, image: Image) -> str:"})})}),"\n",(0,i.jsx)(n.p,{children:"Here\u2019s how you can make a corresponding unary-unary predict call from the client side:"}),"\n",(0,i.jsxs)(r.A,{groupId:"code",children:[(0,i.jsx)(s.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:h})}),(0,i.jsx)(s.A,{value:"node.js",label:"Node.js SDK",children:(0,i.jsx)(l.A,{className:"language-javascript",children:I})})]}),"\n",(0,i.jsxs)(t,{children:[(0,i.jsx)("summary",{children:"Example Output"}),(0,i.jsx)(l.A,{className:"language-text",children:E})]}),"\n",(0,i.jsx)(n.admonition,{type:"tip",children:(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/compute/models/model-upload/data-types/",children:"Click here"})," to explore how to make predictions with other data types."]})}),"\n",(0,i.jsx)(n.h2,{id:"unary-stream-predict-call",children:"Unary-Stream Predict Call"}),"\n",(0,i.jsx)(n.p,{children:"This call sends a single input to the model but returns a stream of responses. This is especially useful for tasks that produce multiple outputs from one input, such as generating text completions or progressive predictions from a prompt."}),"\n",(0,i.jsx)(n.h3,{id:"text-inputs-1",children:"Text Inputs"}),"\n",(0,i.jsx)(n.p,{children:"Here is an example of a model signature configured on the server side for handling text inputs:"}),"\n",(0,i.jsx)(r.A,{groupId:"code",children:(0,i.jsx)(s.A,{value:"python",label:"Python",children:(0,i.jsx)(l.A,{className:"language-python",children:"@ModelClass.method\ndef generate(self, prompt: str) -> Iterator[str]:"})})}),"\n",(0,i.jsx)(n.p,{children:"Here\u2019s how you can make a corresponding unary-stream predict call from the client side:"}),"\n",(0,i.jsxs)(r.A,{groupId:"code",children:[(0,i.jsx)(s.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:g})}),(0,i.jsx)(s.A,{value:"node.js",label:"Node.js SDK",children:(0,i.jsx)(l.A,{className:"language-javascript",children:P})})]}),"\n",(0,i.jsx)(n.h3,{id:"image-inputs-1",children:"Image Inputs"}),"\n",(0,i.jsx)(n.p,{children:"Here is an example of a model signature configured on the server side for handling image inputs:"}),"\n",(0,i.jsx)(r.A,{groupId:"code",children:(0,i.jsx)(s.A,{value:"python",label:"Python",children:(0,i.jsx)(l.A,{className:"language-python",children:"@ModelClass.method\ndef generate(self, image: Image) -> Iterator[str]:"})})}),"\n",(0,i.jsx)(n.p,{children:"Here\u2019s how you can make a corresponding unary-stream predict call from the client side:"}),"\n",(0,i.jsxs)(r.A,{groupId:"code",children:[(0,i.jsx)(s.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:w})}),(0,i.jsx)(s.A,{value:"node.js",label:"Node.js SDK",children:(0,i.jsx)(l.A,{className:"language-javascript",children:R})})]}),"\n",(0,i.jsx)(n.h2,{id:"stream-stream-predict-call",children:"Stream-Stream Predict Call"}),"\n",(0,i.jsx)(n.p,{children:"This call enables bidirectional streaming of both inputs and outputs, making it ideal for real-time applications and processing large datasets."}),"\n",(0,i.jsx)(n.p,{children:"In this setup, multiple inputs can be continuously streamed to the model, while predictions are returned in real time. It\u2019s especially useful for use cases like live video analysis or streaming sensor data."}),"\n",(0,i.jsx)(n.h3,{id:"text-inputs-2",children:"Text Inputs"}),"\n",(0,i.jsxs)(n.p,{children:["Here is an example of a ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/compute/models/upload/#step-1-prepare-the-modelpy-file",children:"model signature"})," configured on the server side for handling text inputs:"]}),"\n",(0,i.jsx)(r.A,{groupId:"code",children:(0,i.jsx)(s.A,{value:"python",label:"Python",children:(0,i.jsx)(l.A,{className:"language-python",children:"@ModelClass.method\ndef stream(self, input_iterator: Iterator[str]) -> Iterator[str]:"})})}),"\n",(0,i.jsx)(n.p,{children:"Here\u2019s how you can make a corresponding stream-stream predict call from the client side:"}),"\n",(0,i.jsx)(r.A,{groupId:"code",children:(0,i.jsx)(s.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:x})})}),"\n",(0,i.jsx)(n.h3,{id:"audio-inputs",children:"Audio Inputs"}),"\n",(0,i.jsx)(n.p,{children:"Here is an example of a model signature configured on the server side for handling audio inputs:"}),"\n",(0,i.jsx)(r.A,{groupId:"code",children:(0,i.jsx)(s.A,{value:"python",label:"Python",children:(0,i.jsx)(l.A,{className:"language-python",children:"@ModelClass.method\ndef transcribe_audio(self, audio: Iterator[Audio]) -> Iterator[Text]:"})})}),"\n",(0,i.jsx)(n.p,{children:"Here\u2019s how you can make a corresponding stream-stream predict call from the client side:"}),"\n",(0,i.jsx)(r.A,{groupId:"code",children:(0,i.jsx)(s.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:f})})}),"\n",(0,i.jsx)(n.h2,{id:"dynamic-batch-prediction-handling",children:"Dynamic Batch Prediction Handling"}),"\n",(0,i.jsx)(n.p,{children:"Clarifai\u2019s model framework seamlessly supports both single and batch predictions through a unified interface. It dynamically adapts to the input format, so no code changes are needed."}),"\n",(0,i.jsx)(n.p,{children:"The system automatically detects the type of input provided:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"If you pass a single input, it\u2019s treated as a singleton batch;"}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"If you pass multiple inputs as a list, they are handled as a parallel batch."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"This means you can pass either a single input or a list of inputs, and the system will automatically process them appropriately \u2014 making your code cleaner and more flexible."}),"\n",(0,i.jsx)(n.h3,{id:"image-inputs-2",children:"Image Inputs"}),"\n",(0,i.jsx)(n.p,{children:"Here is an example of a model signature configured on the server side for handling image inputs:"}),"\n",(0,i.jsx)(r.A,{groupId:"code",children:(0,i.jsx)(s.A,{value:"python",label:"Python",children:(0,i.jsx)(l.A,{className:"language-python",children:"@ModelClass.method\ndef predict_image(self, image: Image) -> Dict[str, float]:"})})}),"\n",(0,i.jsx)(n.p,{children:"Here\u2019s how you can perform batch predictions with image inputs from the client side:"}),"\n",(0,i.jsx)(r.A,{groupId:"code",children:(0,i.jsx)(s.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:y})})}),"\n",(0,i.jsx)(n.h3,{id:"text-inputs-3",children:"Text Inputs"}),"\n",(0,i.jsx)(n.p,{children:"Here is an example of a model signature configured on the server side for handling text inputs:"}),"\n",(0,i.jsx)(r.A,{groupId:"code",children:(0,i.jsx)(s.A,{value:"python",label:"Python",children:(0,i.jsx)(l.A,{className:"language-python",children:'class TextClassifier(ModelClass):\n@ModelClass.method\ndef predict(self, text: Text) -> float:\n"""Single text classification (automatically batched)"""\nreturn self.model(text.text)'})})}),"\n",(0,i.jsx)(n.p,{children:"Here\u2019s how you can perform batch predictions with text inputs from the client side:"}),"\n",(0,i.jsx)(r.A,{groupId:"code",children:(0,i.jsx)(s.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:j})})}),"\n",(0,i.jsx)(n.h2,{id:"multimodal-predictions",children:"Multimodal Predictions"}),"\n",(0,i.jsx)(n.p,{children:"You can make predictions using models that support multimodal inputs, such as a combination of images and text."}),"\n",(0,i.jsxs)(n.p,{children:["Additionally, you can configure various ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/sdk/Inference-from-AI-Models/Advance-Inference-Options/#prediction-paramaters",children:"inference parameters"})," to customize your prediction requests to better suit your use case."]}),"\n",(0,i.jsx)(n.admonition,{type:"tip",children:(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.a,{href:"https://github.com/Clarifai/runners-examples/tree/main/multimodal-models/openai_server_vlm",children:"Click here"})," to learn more how to make multimodal predictions, including how to use parameters like ",(0,i.jsx)(n.code,{children:"chat_history"})," and ",(0,i.jsx)(n.code,{children:"messages"}),"."]})}),"\n",(0,i.jsx)(n.p,{children:"Here is an example:"}),"\n",(0,i.jsx)(r.A,{groupId:"code",children:(0,i.jsx)(s.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:A})})}),"\n",(0,i.jsx)(n.h2,{id:"tool-calling",children:"Tool Calling"}),"\n",(0,i.jsx)(n.p,{children:"Tool calling in LLMs is a capability that allows models to autonomously decide when and how to call external tools, functions, or APIs during a conversation \u2014 based on the user\u2019s input and the context."}),"\n",(0,i.jsxs)(n.p,{children:["You can learn more about it ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/compute/models/inference/open-ai#tool-calling",children:"here"}),"."]}),"\n",(0,i.jsx)(r.A,{groupId:"code",children:(0,i.jsx)(s.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(l.A,{className:"language-python",children:k})})})]})}function H(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(L,{...e})}):L(e)}},65537:(e,n,t)=>{t.d(n,{A:()=>b});var a=t(96540),i=t(18215),o=t(65627),r=t(56347),s=t(50372),l=t(30604),d=t(11861),c=t(78749);function p(e){return a.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,a.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function m(e){const{values:n,children:t}=e;return(0,a.useMemo)((()=>{const e=n??function(e){return p(e).map((e=>{let{props:{value:n,label:t,attributes:a,default:i}}=e;return{value:n,label:t,attributes:a,default:i}}))}(t);return function(e){const n=(0,d.XI)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,t])}function h(e){let{value:n,tabValues:t}=e;return t.some((e=>e.value===n))}function u(e){let{queryString:n=!1,groupId:t}=e;const i=(0,r.W6)(),o=function(e){let{queryString:n=!1,groupId:t}=e;if("string"==typeof n)return n;if(!1===n)return null;if(!0===n&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:n,groupId:t});return[(0,l.aZ)(o),(0,a.useCallback)((e=>{if(!o)return;const n=new URLSearchParams(i.location.search);n.set(o,e),i.replace({...i.location,search:n.toString()})}),[o,i])]}function g(e){const{defaultValue:n,queryString:t=!1,groupId:i}=e,o=m(e),[r,l]=(0,a.useState)((()=>function(e){let{defaultValue:n,tabValues:t}=e;if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(n){if(!h({value:n,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${n}" but none of its children has the corresponding value. Available values are: ${t.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return n}const a=t.find((e=>e.default))??t[0];if(!a)throw new Error("Unexpected error: 0 tabValues");return a.value}({defaultValue:n,tabValues:o}))),[d,p]=u({queryString:t,groupId:i}),[g,f]=function(e){let{groupId:n}=e;const t=function(e){return e?`docusaurus.tab.${e}`:null}(n),[i,o]=(0,c.Dv)(t);return[i,(0,a.useCallback)((e=>{t&&o.set(e)}),[t,o])]}({groupId:i}),x=(()=>{const e=d??g;return h({value:e,tabValues:o})?e:null})();(0,s.A)((()=>{x&&l(x)}),[x]);return{selectedValue:r,selectValue:(0,a.useCallback)((e=>{if(!h({value:e,tabValues:o}))throw new Error(`Can't select invalid tab value=${e}`);l(e),p(e),f(e)}),[p,f,o]),tabValues:o}}var f=t(9136);const x={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var y=t(74848);function j(e){let{className:n,block:t,selectedValue:a,selectValue:r,tabValues:s}=e;const l=[],{blockElementScrollPositionUntilNextRender:d}=(0,o.a_)(),c=e=>{const n=e.currentTarget,t=l.indexOf(n),i=s[t].value;i!==a&&(d(n),r(i))},p=e=>{let n=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const t=l.indexOf(e.currentTarget)+1;n=l[t]??l[0];break}case"ArrowLeft":{const t=l.indexOf(e.currentTarget)-1;n=l[t]??l[l.length-1];break}}n?.focus()};return(0,y.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,i.A)("tabs",{"tabs--block":t},n),children:s.map((e=>{let{value:n,label:t,attributes:o}=e;return(0,y.jsx)("li",{role:"tab",tabIndex:a===n?0:-1,"aria-selected":a===n,ref:e=>{l.push(e)},onKeyDown:p,onClick:c,...o,className:(0,i.A)("tabs__item",x.tabItem,o?.className,{"tabs__item--active":a===n}),children:t??n},n)}))})}function A(e){let{lazy:n,children:t,selectedValue:o}=e;const r=(Array.isArray(t)?t:[t]).filter(Boolean);if(n){const e=r.find((e=>e.props.value===o));return e?(0,a.cloneElement)(e,{className:(0,i.A)("margin-top--md",e.props.className)}):null}return(0,y.jsx)("div",{className:"margin-top--md",children:r.map(((e,n)=>(0,a.cloneElement)(e,{key:n,hidden:e.props.value!==o})))})}function _(e){const n=g(e);return(0,y.jsxs)("div",{className:(0,i.A)("tabs-container",x.tabList),children:[(0,y.jsx)(j,{...n,...e}),(0,y.jsx)(A,{...n,...e})]})}function b(e){const n=(0,f.A)();return(0,y.jsx)(_,{...e,children:p(e.children)},String(n))}},79329:(e,n,t)=>{t.d(n,{A:()=>r});t(96540);var a=t(18215);const i={tabItem:"tabItem_Ymn6"};var o=t(74848);function r(e){let{children:n,hidden:t,className:r}=e;return(0,o.jsx)("div",{role:"tabpanel",className:(0,a.A)(i.tabItem,r),hidden:t,children:n})}}}]);
"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[1565],{11470:(e,n,l)=>{l.d(n,{A:()=>O});var a=l(96540),t=l(18215),o=l(23104),r=l(56347),i=l(205),s=l(57485),c=l(31682),d=l(70679);function u(e){return a.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,a.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function h(e){const{values:n,children:l}=e;return(0,a.useMemo)(()=>{const e=n??function(e){return u(e).map(({props:{value:e,label:n,attributes:l,default:a}})=>({value:e,label:n,attributes:l,default:a}))}(l);return function(e){const n=(0,c.XI)(e,(e,n)=>e.value===n.value);if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[n,l])}function p({value:e,tabValues:n}){return n.some(n=>n.value===e)}function m({queryString:e=!1,groupId:n}){const l=(0,r.W6)(),t=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,s.aZ)(t),(0,a.useCallback)(e=>{if(!t)return;const n=new URLSearchParams(l.location.search);n.set(t,e),l.replace({...l.location,search:n.toString()})},[t,l])]}function f(e){const{defaultValue:n,queryString:l=!1,groupId:t}=e,o=h(e),[r,s]=(0,a.useState)(()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!p({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const l=n.find(e=>e.default)??n[0];if(!l)throw new Error("Unexpected error: 0 tabValues");return l.value}({defaultValue:n,tabValues:o})),[c,u]=m({queryString:l,groupId:t}),[f,g]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[l,t]=(0,d.Dv)(n);return[l,(0,a.useCallback)(e=>{n&&t.set(e)},[n,t])]}({groupId:t}),y=(()=>{const e=c??f;return p({value:e,tabValues:o})?e:null})();(0,i.A)(()=>{y&&s(y)},[y]);return{selectedValue:r,selectValue:(0,a.useCallback)(e=>{if(!p({value:e,tabValues:o}))throw new Error(`Can't select invalid tab value=${e}`);s(e),u(e),g(e)},[u,g,o]),tabValues:o}}var g=l(92303);const y={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var b=l(74848);function x({className:e,block:n,selectedValue:l,selectValue:a,tabValues:r}){const i=[],{blockElementScrollPositionUntilNextRender:s}=(0,o.a_)(),c=e=>{const n=e.currentTarget,t=i.indexOf(n),o=r[t].value;o!==l&&(s(n),a(o))},d=e=>{let n=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const l=i.indexOf(e.currentTarget)+1;n=i[l]??i[0];break}case"ArrowLeft":{const l=i.indexOf(e.currentTarget)-1;n=i[l]??i[i.length-1];break}}n?.focus()};return(0,b.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,t.A)("tabs",{"tabs--block":n},e),children:r.map(({value:e,label:n,attributes:a})=>(0,b.jsx)("li",{role:"tab",tabIndex:l===e?0:-1,"aria-selected":l===e,ref:e=>{i.push(e)},onKeyDown:d,onClick:c,...a,className:(0,t.A)("tabs__item",y.tabItem,a?.className,{"tabs__item--active":l===e}),children:n??e},e))})}function I({lazy:e,children:n,selectedValue:l}){const o=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=o.find(e=>e.props.value===l);return e?(0,a.cloneElement)(e,{className:(0,t.A)("margin-top--md",e.props.className)}):null}return(0,b.jsx)("div",{className:"margin-top--md",children:o.map((e,n)=>(0,a.cloneElement)(e,{key:n,hidden:e.props.value!==l}))})}function j(e){const n=f(e);return(0,b.jsxs)("div",{className:(0,t.A)("tabs-container",y.tabList),children:[(0,b.jsx)(x,{...n,...e}),(0,b.jsx)(I,{...n,...e})]})}function O(e){const n=(0,g.A)();return(0,b.jsx)(j,{...e,children:u(e.children)},String(n))}},19365:(e,n,l)=>{l.d(n,{A:()=>r});l(96540);var a=l(18215);const t={tabItem:"tabItem_Ymn6"};var o=l(74848);function r({children:e,hidden:n,className:l}){return(0,o.jsx)("div",{role:"tabpanel",className:(0,a.A)(t.tabItem,l),hidden:n,children:e})}},31779:(e,n,l)=>{l.r(n),l.d(n,{assets:()=>m,contentTitle:()=>p,default:()=>y,frontMatter:()=>h,metadata:()=>a,toc:()=>f});const a=JSON.parse('{"id":"compute/local-runners/ollama","title":"Run Ollama Models Locally","description":"Run Ollama models locally and make them available via a public API","source":"@site/docs/compute/local-runners/ollama.md","sourceDirName":"compute/local-runners","slug":"/compute/local-runners/ollama","permalink":"/compute/local-runners/ollama","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"description":"Run Ollama models locally and make them available via a public API","sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Local Runners","permalink":"/compute/local-runners/"},"next":{"title":"Agents","permalink":"/compute/agents/"}}');var t=l(74848),o=l(28453),r=l(11470),i=l(19365),s=l(73748);const c="[INFO] 11:29:45.137222 Initializing model from GitHub repository: https://github.com/Clarifai/runners-examples |  thread=8589516992 \n[INFO] 11:29:46.985596 Successfully cloned repository from https://github.com/Clarifai/runners-examples (branch: ollama) |  thread=8589516992 \n[INFO] 11:29:46.991283 Model initialization complete with GitHub repository |  thread=8589516992 \n[INFO] 11:29:46.991330 Next steps: |  thread=8589516992 \n[INFO] 11:29:46.991358 1. Review the model configuration |  thread=8589516992 \n[INFO] 11:29:46.991380 2. Install any required dependencies manually |  thread=8589516992 \n[INFO] 11:29:46.991403 3. Test the model locally using 'clarifai model local-test' |  thread=8589516992 ",d='clarifai model local-runner --verbose\n[INFO] 11:39:05.038034 > Checking local runner requirements... |  thread=8589516992 \n[INFO] 11:39:05.063469 Checking 2 dependencies... |  thread=8589516992 \n[INFO] 11:39:05.063902 \u2705 All 2 dependencies are installed! |  thread=8589516992 \n[INFO] 11:39:05.064096 Verifying Ollama installation... |  thread=8589516992 \n[INFO] 11:39:05.099099 > Verifying local runner setup... |  thread=8589516992 \n[INFO] 11:39:05.099300 Current context: default |  thread=8589516992 \n[INFO] 11:39:05.099342 Current user_id: alfrick |  thread=8589516992 \n[INFO] 11:39:05.099370 Current PAT: d6570**** |  thread=8589516992 \n[INFO] 11:39:05.101001 Current compute_cluster_id: local-runner-compute-cluster |  thread=8589516992 \n[WARNING] 11:39:06.191904 Failed to get compute cluster with ID \'local-runner-compute-cluster\':\ncode: CONN_DOES_NOT_EXIST\ndescription: "Resource does not exist"\ndetails: "ComputeCluster with ID \\\'local-runner-compute-cluster\\\' not found. Check your request fields."\nreq_id: "sdk-python-11.7.2-56dfdebebd3d4f42bdff435838d050e1"\n |  thread=8589516992 \nCompute cluster not found. Do you want to create a new compute cluster alfrick/local-runner-compute-cluster? (y/n): y\n[INFO] 11:39:12.426673 Compute Cluster with ID \'local-runner-compute-cluster\' is created:\ncode: SUCCESS\ndescription: "Ok"\nreq_id: "sdk-python-11.7.2-f972b78a4731420ea7018a5591854fe6"\n |  thread=8589516992 \n[INFO] 11:39:12.431533 Current nodepool_id: local-runner-nodepool |  thread=8589516992 \n[WARNING] 11:39:13.391660 Failed to get nodepool with ID \'local-runner-nodepool\':\ncode: CONN_DOES_NOT_EXIST\ndescription: "Resource does not exist"\ndetails: "Nodepool not found. Check your request fields."\nreq_id: "sdk-python-11.7.2-8660a719c7354675bc350e84ab6702c3"\n |  thread=8589516992 \nNodepool not found. Do you want to create a new nodepool alfrick/local-runner-compute-cluster/local-runner-nodepool? (y/n): y\n[INFO] 11:39:18.440918 Nodepool with ID \'local-runner-nodepool\' is created:\ncode: SUCCESS\ndescription: "Ok"\nreq_id: "sdk-python-11.7.2-35eae829232746da8e3c50e743bb90a0"\n |  thread=8589516992 \n[INFO] 11:39:18.457288 Current app_id: local-runner-app |  thread=8589516992 \n[INFO] 11:39:18.790072 Current model_id: local-runner-model |  thread=8589516992 \n[INFO] 11:39:23.772397 Current model version 9d38bb9398944de4bdef699835f17ec9 |  thread=8589516992 \n[INFO] 11:39:23.772643 Creating the local runner tying this \'alfrick/local-runner-app/models/local-runner-model\' model (version: 9d38bb9398944de4bdef699835f17ec9) to the \'alfrick/local-runner-compute-cluster/local-runner-nodepool\' nodepool. |  thread=8589516992 \n[INFO] 11:39:24.885380 Runner with ID \'db1794a7d250406badcad63cf4ce695c\' is created:\ncode: SUCCESS\ndescription: "Ok"\nreq_id: "sdk-python-11.7.2-8f319a34bd134cf7828c41fc9eb7d27d"\n |  thread=8589516992 \n[INFO] 11:39:24.893927 Current runner_id: db1794a7d250406badcad63cf4ce695c |  thread=8589516992 \n[WARNING] 11:39:25.252771 Failed to get deployment with ID local-runner-deployment:\ncode: CONN_DOES_NOT_EXIST\ndescription: "Resource does not exist"\ndetails: "Deployment with ID \\\'local-runner-deployment\\\' not found. Check your request fields."\nreq_id: "sdk-python-11.7.2-4a9e10638c9f461688312571e0a2ceb8"\n |  thread=8589516992 \nDeployment not found. Do you want to create a new deployment alfrick/local-runner-compute-cluster/local-runner-nodepool/local-runner-deployment? (y/n): y\n[INFO] 11:39:28.762471 Deployment with ID \'local-runner-deployment\' is created:\ncode: SUCCESS\ndescription: "Ok"\nreq_id: "sdk-python-11.7.2-afda1f4e53e14055a604c43e85ff6eba"\n |  thread=8589516992 \n[INFO] 11:39:28.769409 Current deployment_id: local-runner-deployment |  thread=8589516992 \n[INFO] 11:39:28.771852 Current model section of config.yaml: {\'app_id\': \'local-dev-runner-app\', \'id\': \'local-dev-model\', \'model_type_id\': \'text-to-text\', \'user_id\': \'clarifai-user-id\'} |  thread=8589516992 \nDo you want to backup config.yaml to config.yaml.bk then update the config.yaml with the new model information? (y/n): y\n[INFO] 11:39:33.362901 Checking 2 dependencies... |  thread=8589516992 \n[INFO] 11:39:33.363602 \u2705 All 2 dependencies are installed! |  thread=8589516992 \n[INFO] 11:39:33.405524 Customizing Ollama model with provided parameters... |  thread=8589516992 \n[INFO] 11:39:33.406114 \u2705 Starting local runner... |  thread=8589516992 \n[INFO] 11:39:34.440960 Detected OpenAI chat completions for Clarifai model streaming - validating stream_options... |  thread=8589516992 \n[INFO] 11:39:34.448319 Starting Ollama server in the host: 127.0.0.1:23333 |  thread=8589516992 \n[INFO] 11:39:34.461759 Model llama3.2 pulled successfully. |  thread=8589516992 \n[INFO] 11:39:34.462122 Ollama server started successfully on 127.0.0.1:23333 |  thread=8589516992 \ntime=2025-08-22T11:39:34.473+03:00 level=INFO source=routes.go:1318 msg="server config" env="map[HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:8192 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:23333 OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/Users/macbookpro/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NEW_ESTIMATES:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false http_proxy: https_proxy: no_proxy:]"\ntime=2025-08-22T11:39:34.473+03:00 level=INFO source=images.go:477 msg="total blobs: 0"\ntime=2025-08-22T11:39:34.473+03:00 level=INFO source=images.go:484 msg="total unused blobs removed: 0"\ntime=2025-08-22T11:39:34.474+03:00 level=INFO source=routes.go:1371 msg="Listening on 127.0.0.1:23333 (version 0.11.6)"\ntime=2025-08-22T11:39:34.507+03:00 level=INFO source=types.go:130 msg="inference compute" id=0 library=metal variant="" compute="" driver=0.0 name="" total="10.7 GiB" available="10.7 GiB"\ntime=2025-08-22T11:39:34.507+03:00 level=INFO source=routes.go:1412 msg="entering low vram mode" "total vram"="10.7 GiB" threshold="20.0 GiB"\n[GIN] 2025/08/22 - 11:39:34 | 200 |        68.5\xb5s |       127.0.0.1 | HEAD     "/"\n[INFO] 11:39:34.521195 Ollama model loaded successfully: llama3.2 |  thread=8589516992 \n[INFO] 11:39:34.525089 \u2705 Your model is running locally and is ready for requests from the API...\n |  thread=8589516992 \n[INFO] 11:39:34.525126 > Code Snippet: To call your model via the API, use this code snippet:\n\nimport os\n\nfrom openai import OpenAI\n\nclient = OpenAI(\n    base_url="https://api.clarifai.com/v2/ext/openai/v1",\n    api_key=os.environ[\'CLARIFAI_PAT\'],\n)\n\nresponse = client.chat.completions.create(\n    model="https://clarifai.com/alfrick/local-runner-app/models/local-runner-model",\n    messages=[\n        {"role": "system", "content": "Talk like a pirate."},\n        {\n            "role": "user",\n            "content": "How do I check if a Python object is an instance of a class?",\n        },\n    ],\n    temperature=0.7,\n    stream=False,  # stream=True also works, just iterator over the response\n)\nprint(response)\n |  thread=8589516992 \n[INFO] 11:39:34.525153 > Playground:   To chat with your model, visit: https://clarifai.com/playground?model=local-runner-model__9d38bb9398944de4bdef699835f17ec9&user_id=alfrick&app_id=local-runner-app\n |  thread=8589516992 \n[INFO] 11:39:34.525172 > API URL:      To call your model via the API, use this model URL: https://clarifai.com/users/alfrick/apps/local-runner-app/models/local-runner-model\n |  thread=8589516992 \n[INFO] 11:39:34.525185 Press CTRL+C to stop the runner.\n |  thread=8589516992 \n[INFO] 11:39:34.525202 Starting 32 threads... |  thread=8589516992 \npulling manifest \u280f time=2025-08-22T11:39:36.435+03:00 level=INFO source=download.go:177 msg="downloading dde5aa3fc5ff in 16 126 MB part(s)"\npulling manifest \npulling dde5aa3fc5ff:   1% \u2595\u2588                                                                                                                                   pulling manifest \npulling dde5aa3fc5ff:   1% \u2595\u2588                                                                                                                                   pulling manifest \npulling manifest \npulling manifest \npulling manifest \npulling dde5aa3fc5ff:  31% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                           pulling manifest \npulling dde5aa3fc5ff:  35% \u2595\u2588\u2588\u2588\u2588\u2588\u2588            \u258f 712 MB/2.0 GB  2.9 MB/s   7m23stime=2025-08-22T11:45:20.033+03:00 level=INFO source=download.go:295 msg="dde5aa3pulling manifest \npulling dde5aa3fc5ff:  93% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u258f 1.9 GB/2.0 GB  3.1 MB/s     48stime=2025-08-22T11:53:56.586+03:00 level=INFO source=download.go:295 msg="dde5aa3pulling manifest \npulling dde5aa3fc5ff: 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 2.0 GB                         time=2025-08-22T11:54:47.224+03:00 level=INFO source=download.go:177 msg="downloapulling manifest \npulling dde5aa3fc5ff: 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 2.0 GB                         \npulling 966de95ca8a6: 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 1.4 KB                         tpulling manifest \npulling dde5aa3fc5ff: 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 2.0 GB                         \npulling 966de95ca8a6: 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 1.4 KB                         \npulling manifest \npulling dde5aa3fc5ff: 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 2.0 GB                         \npulling 966de95ca8a6: 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 1.4 KB                         \npulling manifest \npulling dde5aa3fc5ff: 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 2.0 GB                         \npulling 966de95ca8a6: 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 1.4 KB                         \npulling manifest \npulling dde5aa3fc5ff: 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 2.0 GB                         \npulling manifest \npulling dde5aa3fc5ff: 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 2.0 GB                         \npulling 966de95ca8a6: 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 1.4 KB                         \npulling fcc5a6bec9da: 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 7.7 KB                         \npulling a70ff7e570d9: 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 6.0 KB                         \npulling 56bb8bd477a5: 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f   96 B                         \npulling 34bb5ab01051: 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  561 B                         \nverifying sha256 digest \nwriting manifest \nsuccess ',u="ChatCompletion(id='chatcmpl-79', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Ye be wantin\\' to know how to check if a Python object be an instance o\\' a class, eh?\\n\\nWell, matey, ye can use the `type()` function and the `isinstance()` function. Here be the ways:\\n\\n**Method 1: Using `type()`**\\n```python\\nmy_object = \"Hello, world!\"\\nif type(my_object) == str:\\n    \nprint(\"Aye, it\\'s a string!\")\\n```\\nIn this example, we use `type()` to get the type o\\' `my_object`, which is indeed `str`. So, we can check if it\\'s equal to `str` using the `==` operator.\\n\\n**Method 2: Using `isinstance()`**\\n```python\\nmy_object = \"Hello, world!\"\\nif isinstance(my_object, str):\\n    print(\"Aye, it\\'s a string!\")\\n```\\nIn this example, we use `isinstance()` to check if `my_object` be an instance o\\' the `str` class. \nThis method is more readable and Pythonic, matey!\\n\\n**Method 3: Using f-strings**\\n```python\\nmy_object = \"Hello, world!\"\\nif type(my_object) == str:\\n    print(f\"{my_object} be a string!\")\\n```\\nOr,\\n```python\\nmy_object = \"Hello, world!\"\\nif isinstance(my_object, str):\\n    \nprint(f\"{my_object} be an instance o\\' the {type(my_object).__name__} class!\")\\n```\\nIn these examples, we use f-strings to format the output.\\n\\nSo, hoist the sails and set course for type checking, me hearty!', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None))], created=1755853613, model='llama3.2', object='chat.completion', service_tier=None, system_fingerprint='fp_ollama', usage=CompletionUsage(completion_tokens=338, prompt_tokens=45, total_tokens=383, completion_tokens_details=None, prompt_tokens_details=None))",h={description:"Run Ollama models locally and make them available via a public API",sidebar_position:1},p="Run Ollama Models Locally",m={},f=[{value:"Step 1: Perform Prerequisites",id:"step-1-perform-prerequisites",level:2},{value:"Install Ollama",id:"install-ollama",level:3},{value:"Sign Up or Log In",id:"sign-up-or-log-in",level:3},{value:"Install the Clarifai CLI",id:"install-the-clarifai-cli",level:3},{value:"Install OpenAI Package",id:"install-openai-package",level:3},{value:"Step 2: Initialize a Model From Ollama",id:"step-2-initialize-a-model-from-ollama",level:2},{value:"Step 3: Log In to Clarifai",id:"step-3-log-in-to-clarifai",level:2},{value:"Step 4: Start Your Local Runner",id:"step-4-start-your-local-runner",level:2},{value:"Step 5: Run Inference",id:"step-5-run-inference",level:2},{value:"Additional Examples",id:"additional-examples",level:2}];function g(e){const n={a:"a",admonition:"admonition",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components},{Details:l}=n;return l||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"run-ollama-models-locally",children:"Run Ollama Models Locally"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Run Ollama models locally and make them available via a public API"})}),"\n",(0,t.jsx)("hr",{}),"\n",(0,t.jsx)(n.p,{children:"Ollama is an open-source tool that allows you to download, run, and manage large language models (LLMs) directly on your local machine."}),"\n",(0,t.jsx)(n.p,{children:"When combined with Clarifai\u2019s Local Runners, it enables you to run Ollama models on your machine, expose them securely via a public URL, and tap into Clarifai\u2019s powerful platform \u2014 all while keeping the speed, privacy, and control of local deployment."}),"\n","\n","\n",(0,t.jsx)(n.h2,{id:"step-1-perform-prerequisites",children:"Step 1: Perform Prerequisites"}),"\n",(0,t.jsx)(n.h3,{id:"install-ollama",children:"Install Ollama"}),"\n",(0,t.jsxs)(n.p,{children:["Go to the ",(0,t.jsx)(n.a,{href:"https://ollama.com/download",children:"Ollama website"})," and choose the appropriate installer for your system (macOS, Windows, or Linux)."]}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Note:"})," If you're using Windows, make sure to restart your machine after installing Ollama to ensure that the updated environment variables are properly applied."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"sign-up-or-log-in",children:"Sign Up or Log In"}),"\n",(0,t.jsxs)(n.p,{children:["Start by ",(0,t.jsx)(n.a,{href:"https://clarifai.com/login",children:"logging in"})," to your existing Clarifai account or ",(0,t.jsx)(n.a,{href:"https://clarifai.com/signup",children:"signing up"})," for a new one. Once logged in, you'll need the following credentials for setup:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"User ID"})," \u2013 Navigate to your personal settings and find your user ID under the ",(0,t.jsx)(n.strong,{children:"Account"})," section."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Personal Access Token (PAT)"})," \u2013 In the same personal settings page, go to the ",(0,t.jsx)(n.strong,{children:"Security"})," section to generate or copy your ",(0,t.jsx)(n.a,{href:"https://docs.clarifai.com/control/authentication/pat",children:"PAT"}),". This token is used to securely authenticate your connection to the Clarifai platform."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["You can then set the PAT as an environment variable using ",(0,t.jsx)(n.code,{children:"CLARIFAI_PAT"}),", which is important when running inference with your models."]}),"\n",(0,t.jsxs)(r.A,{groupId:"code",children:[(0,t.jsx)(i.A,{value:"bash",label:"Unix-Like Systems",children:(0,t.jsx)(s.A,{className:"language-bash",children:"export CLARIFAI_PAT=YOUR_PERSONAL_ACCESS_TOKEN_HERE"})}),(0,t.jsx)(i.A,{value:"bash2",label:"Windows",children:(0,t.jsx)(s.A,{className:"language-bash",children:"set CLARIFAI_PAT=YOUR_PERSONAL_ACCESS_TOKEN_HERE"})})]}),"\n",(0,t.jsx)(n.h3,{id:"install-the-clarifai-cli",children:"Install the Clarifai CLI"}),"\n",(0,t.jsxs)(n.p,{children:["Install the latest version of the ",(0,t.jsx)(n.a,{href:"https://docs.clarifai.com/sdk/cli",children:"Clarifai CLI"}),", which includes built-in support for Local Runners."]}),"\n",(0,t.jsx)(r.A,{groupId:"code",children:(0,t.jsx)(i.A,{value:"bash",label:"Bash",children:(0,t.jsx)(s.A,{className:"language-bash",children:"pip install --upgrade clarifai"})})}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Note:"})," You must have ",(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.a,{href:"https://docs.clarifai.com/resources/api-overview/python-sdk#python-requirements",children:"Python 3.11 or 3.12"})}),"  installed to use Local Runners."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"install-openai-package",children:"Install OpenAI Package"}),"\n",(0,t.jsxs)(n.p,{children:["Install the ",(0,t.jsx)(n.code,{children:"openai"})," package, which is required when performing inference with models using the ",(0,t.jsx)(n.a,{href:"https://docs.clarifai.com/compute/inference/#predict-with-openai-compatible-format",children:"OpenAI-compatible format"}),"."]}),"\n",(0,t.jsx)(r.A,{groupId:"code",children:(0,t.jsx)(i.A,{value:"bash",label:"Python",children:(0,t.jsx)(s.A,{className:"language-bash",children:" pip install openai "})})}),"\n",(0,t.jsx)(n.h2,{id:"step-2-initialize-a-model-from-ollama",children:"Step 2: Initialize a Model From Ollama"}),"\n",(0,t.jsx)(n.p,{children:"You can use the Clarifai CLI to download and initialize any model available in the Ollama library directly into your local environment."}),"\n",(0,t.jsxs)(n.p,{children:["For example, here's how to initialize the ",(0,t.jsx)(n.a,{href:"https://ollama.com/library/llama3.2",children:(0,t.jsx)(n.code,{children:"llama3.2"})})," model in your current directory:"]}),"\n",(0,t.jsx)(r.A,{groupId:"code",children:(0,t.jsx)(i.A,{value:"bash",label:"CLI",children:(0,t.jsx)(s.A,{className:"language-bash",children:"clarifai model init --toolkit ollama"})})}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Note:"})," The above command will create a new model directory structure that is compatible with the Clarifai platform. You can customize or optimize the generated model by modifying the ",(0,t.jsx)(n.code,{children:"1/model.py"})," file as needed."]}),"\n"]}),"\n",(0,t.jsxs)(l,{children:[(0,t.jsx)("summary",{children:"Example Output"}),(0,t.jsx)(s.A,{className:"language-text",children:c})]}),"\n",(0,t.jsx)(n.p,{children:"You can customize model initialization from the Ollama library using the Clarifai CLI with the following options:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"--model-name"})," \u2013 Name of the Ollama model to use (default: ",(0,t.jsx)(n.code,{children:"llama3.2"}),"). This lets you specify any model from the Ollama library"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"--port"})," \u2013 Port to run the model on (default: ",(0,t.jsx)(n.code,{children:"23333"}),")"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"--context-length"})," \u2013 Context window size for the model in tokens (default: ",(0,t.jsx)(n.code,{children:"8192"}),")"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"--verbose"})," \u2013 Enables detailed Ollama logs during execution. By default, logs are suppressed unless this flag is provided."]}),"\n"]}),"\n",(0,t.jsx)(n.admonition,{title:"tip",type:"note",children:(0,t.jsxs)(n.p,{children:["You can use Ollama commands such as ",(0,t.jsx)(n.code,{children:"ollama list"})," to list downloaded models and ",(0,t.jsx)(n.code,{children:"ollama rm"})," to remove a model. Run ",(0,t.jsx)(n.code,{children:"ollama --help"})," to see the full list of available commands."]})}),"\n",(0,t.jsx)(n.h2,{id:"step-3-log-in-to-clarifai",children:"Step 3: Log In to Clarifai"}),"\n",(0,t.jsxs)(n.p,{children:["Use the following command to log in to the Clarifai platform to create a configuration ",(0,t.jsx)(n.a,{href:"/compute/local-runners/#step-2-create-a-context-optional",children:"context"})," and establish a connection:"]}),"\n",(0,t.jsx)(r.A,{groupId:"code",children:(0,t.jsx)(i.A,{value:"bash",label:"CLI",children:(0,t.jsx)(s.A,{className:"language-bash",children:"clarifai login"})})}),"\n",(0,t.jsx)(n.p,{children:"After running the command, you'll be prompted to provide a few details for authentication:"}),"\n",(0,t.jsx)(r.A,{groupId:"code",children:(0,t.jsx)(i.A,{value:"bash",label:"CLI",children:(0,t.jsx)(s.A,{className:"language-bash",children:(0,t.jsx)(n.p,{children:'context name (default: "default"):\nuser id:\npersonal access token value (default: "ENVVAR" to get our env var rather than config):'})})})}),"\n",(0,t.jsx)(n.p,{children:"Here\u2019s what each field means:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Context name"})," \u2013 You can assign a custom name to this configuration context, or simply press Enter to use the default name, ",(0,t.jsx)(n.code,{children:'"default"'}),". This is useful if you manage multiple environments or configurations."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"User ID"})," \u2013 Enter your Clarifai user ID."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Personal Access Token (PAT)"})," \u2013 Paste your Clarifai PAT here. If you've already set the ",(0,t.jsx)(n.code,{children:"CLARIFAI_PAT"})," environment variable, you can just press Enter to use it automatically."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"step-4-start-your-local-runner",children:"Step 4: Start Your Local Runner"}),"\n",(0,t.jsx)(n.p,{children:"Start a local runner using the following command:"}),"\n",(0,t.jsx)(r.A,{groupId:"code",children:(0,t.jsx)(i.A,{value:"bash",label:"CLI",children:(0,t.jsx)(s.A,{className:"language-bash",children:"clarifai model local-runner"})})}),"\n",(0,t.jsx)(n.p,{children:"If the necessary context configurations aren\u2019t detected, the CLI will guide you through creating them using default values."}),"\n",(0,t.jsxs)(n.p,{children:["This setup ensures all required components \u2014 such as compute clusters, nodepools, and deployments \u2014 are properly included in your configuration context, which are described ",(0,t.jsx)(n.a,{href:"/compute/local-runners/#step-2-create-a-context-optional",children:"here"}),". Simply review each prompt and confirm to proceed."]}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Note"}),": Use the ",(0,t.jsx)(n.code,{children:"--verbose"})," option to show detailed logs from the Ollama server, which is helpful for debugging: ",(0,t.jsx)(n.code,{children:"clarifai model local-runner --verbose"}),"."]}),"\n"]}),"\n",(0,t.jsxs)(l,{children:[(0,t.jsx)("summary",{children:"Example Output"}),(0,t.jsx)(s.A,{className:"language-text",children:d})]}),"\n",(0,t.jsx)(n.h2,{id:"step-5-run-inference",children:"Step 5: Run Inference"}),"\n",(0,t.jsx)(n.p,{children:"When the local runner starts, it displays a public URL where your model is hosted and provides a sample client code snippet for quick testing."}),"\n",(0,t.jsx)(n.p,{children:"Pulling a model from Ollama may take some time depending on your machine\u2019s resources, but once the download finishes, you can run the snippet in a separate terminal within the same directory to get the model\u2019s response."}),"\n",(0,t.jsx)(n.p,{children:"Below is an example of running inference using the OpenAI-compatible format:"}),"\n",(0,t.jsx)(r.A,{groupId:"code",children:(0,t.jsx)(i.A,{value:"python",label:"Python",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import os\nfrom openai import OpenAI\n\n# Initialize the OpenAI client with Clarifai\'s OpenAI-compatible endpoint\nclient = OpenAI(\n    base_url="https://api.clarifai.com/v2/ext/openai/v1",\n    api_key=os.environ[\'CLARIFAI_PAT\'],\n)\n\n# Replace \'user-id\' with your actual Clarifai user ID\nresponse = client.chat.completions.create(\n    model="https://clarifai.com/user-id/local-runner-app/models/local-runner-model",\n    messages=[\n        {"role": "system", "content": "Talk like a pirate."},\n        {"role": "user", "content": "How do I check if a Python object is an instance of a class?"},\n    ],\n    temperature=0.7,\n    stream=False,  # Set to True for streaming responses\n)\n\n# Print the full response\nprint(response)\n\n# Example for handling a streaming response:\n# if stream=True, uncomment below to print chunks as they arrive\n# for chunk in response:\n#     print(chunk.choices[0].message[\'content\'], end=\'\')\n'})})})}),"\n",(0,t.jsxs)(l,{children:[(0,t.jsx)("summary",{children:"Example Output"}),(0,t.jsx)(s.A,{className:"language-text",children:u})]}),"\n",(0,t.jsx)(n.p,{children:"When you're done, just close the terminal running the local runner to shut it down."}),"\n",(0,t.jsx)(n.h2,{id:"additional-examples",children:"Additional Examples"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://github.com/ollama/ollama-python/tree/main/examples",children:"More examples of calling Ollama models"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://docs.clarifai.com/compute/inference/clarifai/api",children:"Clarifai-specific inference examples with Ollama models"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://github.com/Clarifai/runners-examples/tree/main/local-runners/ollama-model-upload",children:"Example for running Ollama models locally with Clarifai\u2019s Local Runners"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://www.youtube.com/watch?v=TfS2p8LZYBE",children:"YouTube video on running OpenAI\u2019s open-source GPT-OSS-20B model locally with Ollama"})}),"\n"]})]})}function y(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(g,{...e})}):g(e)}}}]);
"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[9796],{17596:(n,e,t)=>{t.d(e,{A:()=>o});const o=t.p+"assets/images/workflow_predict_try_your_own_image-99ac7bcdbb420ea93cffc15f50753346.png"},27713:(n,e,t)=>{t.d(e,{A:()=>o});const o=t.p+"assets/images/workflow_prediction_output-9812ab06bbee854e404e80a9ad54dbd3.png"},38660:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>Sn,contentTitle:()=>Pn,default:()=>Ln,frontMatter:()=>jn,metadata:()=>o,toc:()=>Cn});const o=JSON.parse('{"id":"create/workflows/inference","title":"Workflow Inferences","description":"Make predictions with your workflows","source":"@site/docs/create/workflows/inference.md","sourceDirName":"create/workflows","slug":"/create/workflows/inference","permalink":"/create/workflows/inference","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"description":"Make predictions with your workflows","sidebar_position":1,"toc_max_heading_level":4},"sidebar":"tutorialSidebar","previous":{"title":"Create Workflows","permalink":"/create/workflows/create"},"next":{"title":"Manage Workflows","permalink":"/create/workflows/manage"}}');var r=t(74848),a=t(28453),i=t(65537),s=t(79329),c=t(58069);const l='from clarifai.client.workflow import Workflow\n\n# Your PAT (Personal Access Token) can be found in the Account\'s Security section\n\n# Initialize the workflow_url\nworkflow_url = "https://clarifai.com/clarifai/main/workflows/Text-Moderation"\ntext_classfication_workflow = Workflow(\n    url= workflow_url , pat="YOUR_PAT"\n)\nraw_text = b"I love your product very much"\nresult = text_classfication_workflow.predict_by_bytes(raw_text, input_type="text")\nprint(result.results[0].outputs[0].data)',u='import { Workflow } from "clarifai-nodejs";\n\n\nconst workflowUrl =\n  "https://clarifai.com/clarifai/main/workflows/Text-Moderation";\n\nconst textClassificationWorkflow = new Workflow({\n  url: workflowUrl,\n  authConfig: {\n    pat: process.env.CLARIFAI_PAT,\n  },\n});\n\nconst rawText = Buffer.from("I love your product very much");\nconst prediction = await textClassificationWorkflow.predictByBytes(\n  rawText,\n  "text",\n);\nconsole.log(prediction.resultsList?.[0]?.outputsList?.[0]?.data);\n',d='from clarifai.client.workflow import Workflow\n\n\nworkflow_url = "https://clarifai.com/clarifai/Text/workflows/summary"\n# creating the workflow\ntext_summarization_workflow = Workflow(\n    url= workflow_url, pat="YOUR_PAT"\n)\n# initialize the input text (minimum 200 characters)\ntext = b"""The word pollution was derived from the Latin word pollution, which means to make dirty. Pollution is the process of making the environment pollute the water and the air by adding harmful substances. Pollution causes an imbalance in the environment. This imbalance threatened the very survival of all forms of life. It\'s a threat to the whole world. India ranks at the bottom of 125 of the 132 countries in the 2012 Environmental Performance Index. This report is produced by researchers at Yale and Columbia University in association with the World Economic Forum.\n\nPollution of the environment is a serious problem of industrialized societies. Industrial development and the green revolution have had a negative impact on the environment. People have converted the life support system of all living people into their own resources and have greatly disrupted the natural ecological balance. Serious degradation and depletion have been caused due to overuse, abuse and mismanagement of resources to meet human greed."""\n\n\nresult = text_summarization_workflow.predict_by_bytes(text, input_type="text")\nprint(result.results[0].outputs[0].data.text.raw)\n',p='import { Workflow } from "clarifai-nodejs";\n\n\nconst workflowUrl = "https://clarifai.com/clarifai/Text/workflows/summary";\n// creating the workflow\nconst textSummarizationWorkflow = new Workflow({\n  url: workflowUrl,\n  authConfig: {\n    pat: process.env.CLARIFAI_PAT,\n  },\n});\n// initialize the input text (minimum 200 characters)\nconst text =\n  Buffer.from(`The word pollution was derived from the Latin word pollution, which means to make dirty. Pollution is the process of making the environment pollute the water and the air by adding harmful substances. Pollution causes an imbalance in the environment. This imbalance threatened the very survival of all forms of life. It\'s a threat to the whole world. India ranks at the bottom of 125 of the 132 countries in the 2012 Environmental Performance Index. This report is produced by researchers at Yale and Columbia University in association with the World Economic Forum.\n\nPollution of the environment is a serious problem of industrialized societies. Industrial development and the green revolution have had a negative impact on the environment. People have converted the life support system of all living people into their own resources and have greatly disrupted the natural ecological balance. Serious degradation and depletion have been caused due to overuse, abuse and mismanagement of resources to meet human greed.`);\n\nconst prediction = await textSummarizationWorkflow.predictByBytes(\n  text,\n  "text",\n);\nconsole.log(prediction.resultsList?.[0]?.outputsList?.[0]?.data?.text?.raw);\n',v='from clarifai.client.workflow import Workflow\n\n\nworkflow_url = "https://clarifai.com/clarifai/Text/workflows/text-generation"\n# Creating the workflow\ntext_generation_workflow = Workflow(\n    url=workflow_url,\n    pat="YOUR_PAT",\n)\n\n# getting the prediction\nresult = text_generation_workflow.predict_by_bytes(\n    b"Good Morning! I think it\'s going to be a great day!", input_type="text"\n)\nprint(result.results[0].outputs[0].data.text.raw)',f='import { Workflow } from "clarifai-nodejs";\n\n\nconst workflowUrl =\n  "https://clarifai.com/clarifai/Text/workflows/text-generation";\n// Creating the workflow\nconst textGenerationWorkflow = new Workflow({\n  url: workflowUrl,\n  authConfig: {\n    pat: process.env.CLARIFAI_PAT,\n  },\n});\n\n// Getting the prediction\nconst prediction = await textGenerationWorkflow.predictByBytes(\n  Buffer.from("Good Morning! I think it\'s going to be a great day!"),\n  "text",\n);\nconsole.log(prediction.resultsList?.[0]?.outputsList?.[0]?.data?.text?.raw);\n',h='from clarifai.client.workflow import Workflow\n\nimage_url =     "https://cdn-bfgco.nitrocdn.com/PLGJnButkKeCQigeKyiwHwnQLZJDOQZI/assets/images/optimized/rev-96fa12a/delamere.com/wp-content/uploads/2022/02/Picture1-1024x683.jpg"\nworkflow_url = "https://clarifai.com/clarifai/main/workflows/Moderation"\n# Creating the workflow\nimage_classifcation_workflow = Workflow(\n    url=workflow_url, pat="YOUR_PAT"\n)\n\n# Getting the predictions\nresult = image_classifcation_workflow.predict_by_url(image_url ,\n    input_type="image",\n)\nprint(result.results[0].outputs[0].data)',m='import { Workflow } from "clarifai-nodejs";\n\n\nconst imageUrl =\n  "https://2.img-dpreview.com/files/p/E~C1000x0S4000x4000T1200x1200~articles/3925134721/0266554465.jpeg";\nconst workflowUrl = "https://clarifai.com/clarifai/main/workflows/Moderation";\n\n// Creating the workflow\nconst imageClassificationWorkflow = new Workflow({\n  url: workflowUrl,\n  authConfig: {\n    pat: process.env.CLARIFAI_PAT,\n  },\n});\n\n// Getting the predictions\nconst prediction = await imageClassificationWorkflow.predictByUrl(\n  imageUrl,\n  "image",\n);\nconsole.log(prediction.resultsList?.[0]?.outputsList?.[0]?.data);\n',w='from clarifai.client.workflow import Workflow\nimport cv2\n\n\nimage_url = "https://s3.amazonaws.com/samples.clarifai.com/featured-models/general-elephants.jpg"\nworkflow_url = "https://clarifai.com/clarifai/main/workflows/Visual-Segmenter"\n# Creating the workflow\nimage_segmentation_workflow = Workflow(\n    url=workflow_url, pat="YOUR_PAT"\n)\n# Getting the predictions\nresult = image_segmentation_workflow.predict_by_url(\nimage_url ,\n    input_type="image",\n)\nim_b = result.results[0].outputs[0].data.regions[0].region_info.mask.image.base64\nimage_np = np.frombuffer(im_b, np.uint8)\nimg_np = cv2.imdecode(image_np, cv2.IMREAD_COLOR)\n# Display the image\nplt.axis("off")\nplt.imshow(img_np[..., ::-1])\n',g='import { Workflow } from "clarifai-nodejs";\nimport fs from "fs";\n\n\nconst imageUrl =\n  "https://s3.amazonaws.com/samples.clarifai.com/featured-models/general-elephants.jpg";\nconst workflowUrl =\n  "https://clarifai.com/clarifai/main/workflows/Visual-Segmenter";\n\n// Creating the workflow\nconst imageSegmentationWorkflow = new Workflow({\n  url: workflowUrl,\n  authConfig: {\n    pat: process.env.CLARIFAI_PAT,\n  },\n});\n\n// Getting the predictions\nconst prediction = await imageSegmentationWorkflow.predictByUrl(\n  imageUrl,\n  "image",\n);\n\nconst imB =\n  prediction.resultsList?.[0]?.outputsList?.[0]?.data?.regionsList?.[0]\n    ?.regionInfo?.mask?.image?.base64 ?? "";\n\n// Write the output to a file\nfs.writeFileSync("image.png", imB, "base64");\n',_='from clarifai.client.workflow import Workflow\nfrom clarifai.client.user import User\n\n# Your PAT (Personal Access Token) can be found in the Account\'s Security section\n# Specify the correct user_id/app_id pairings\n# Since you\'re making inferences outside your app\'s scope\nUSER_ID = "user_id"\nAPP_ID = "app_id"\n\napp = User(user_id=USER_ID, pat="YOUR_PAT").create_app(\n    app_id=APP_ID, base_workflow="Empty"\n)\nimage_url =  "https://s3.amazonaws.com/samples.clarifai.com/ocr_img_00417302.jpg"\n# create a yaml file specifying the workflow structure\n# eg:\n"""language_aware_ocr.yml\nworkflow:\n  id: wf-ocr\n  nodes:\n    - id: ocr-workflow\n      model:\n          model_id: language-aware-multilingual-ocr-multiplex\n\n    - id: text-aggregator\n      model:\n          model_id: text-aggregation\n          model_type_id: text-aggregation-operator\n          output_info:\n            params:\n              avg_word_width_window_factor: 2.0\n              avg_word_height_window_factor: 1.0\n\n      node_inputs:\n        - node_id: ocr-workflow\n\n    - id: language-id-operator\n      model:\n          model_id: language-id\n          model_type_id: language-id-operator\n          output_info:\n            params:\n              library: "fasttext"\n              topk: 1\n              threshold:  0.1\n              lowercase: true\n\n      node_inputs:\n        - node_id: text-aggregator\n\n\n"""\n\n\nocr_workflow = app.create_workflow(config_filepath="configs/language_aware_ocr.yml")\nprediction = ocr_workflow.predict_by_url(\n   image_url ,\n    input_type="image",\n)\n\n# Get workflow results\nprint(prediction.results[0].outputs[-1].data)\n',I='import { App, User, Workflow } from "clarifai-nodejs";\nimport path from "path";\n\n\nconst appId = "test_app"; // Placeholder for application ID\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\n// Specify the correct userId/appId pairings\n// Since you\'re making inferences outside your app\'s scope\nconst client = new User({\n  userId: process.env.CLARIFAI_USER_ID,\n  pat: process.env.CLARIFAI_PAT,\n  appId,\n});\n\nawait client.createApp({\n  appId,\n  baseWorkflow: "Empty",\n});\n\nconst app = new App({\n  authConfig: {\n    userId: process.env.CLARIFAI_USER_ID,\n    pat: process.env.CLARIFAI_PAT,\n    appId,\n  },\n});\n\nconst imageUrl =\n  "https://s3.amazonaws.com/samples.clarifai.com/ocr_img_00417302.jpg";\n// create a yaml file specifying the workflow structure\n// eg:\n/* languageAwareOcr.yml\nworkflow:\n  id: wfOcr\n  nodes:\n      - id: ocrWorkflow\n          model:\n                  modelId: languageAwareMultilingualOcrMultiplex\n\n      - id: textAggregator\n          model:\n                  modelId: textAggregation\n                  modelTypeId: textAggregationOperator\n                  outputInfo:\n                      params:\n                          avgWordWidthWindowFactor: 2.0\n                          avgWordHeightWindowFactor: 1.0\n\n          nodeInputs:\n              - nodeId: ocrWorkflow\n\n      - id: languageIdOperator\n          model:\n                  modelId: languageId\n                  modelTypeId: languageIdOperator\n                  outputInfo:\n                      params:\n                          library: "fasttext"\n                          topk: 1\n                          threshold: 0.1\n                          lowercase: true\n\n          nodeInputs:\n              - nodeId: textAggregator\n*/\n\nconst ocrWorkflow = await app.createWorkflow({\n  configFilePath: path.resolve(\n    __dirname,\n    "../../assets/configs/language_aware_ocr.yml",\n  ),\n});\n\nconst workflow = new Workflow({\n  workflowId: ocrWorkflow.id,\n  authConfig: {\n    userId: process.env.CLARIFAI_USER_ID,\n    pat: process.env.CLARIFAI_PAT,\n    appId,\n  },\n});\n\nconst prediction = await workflow.predictByUrl(imageUrl, "image");\n\n// Get workflow results\nconsole.log(\n  prediction.resultsList?.[0]?.outputsList?.[\n    prediction?.resultsList?.[0]?.outputsList?.length - 1\n  ].data,\n);\n',A='from clarifai.client.workflow import Workflow\nimport cv2\n\n\nworkflow_url ="https://clarifai.com/clarifai/Visual/workflows/image-generation"\n# Creating the workflow\nimage_generation_workflow = Workflow(\n    url=workflow_url,\n    pat="YOUR_PAT",\n)\n\n# Getting the predictions\nresult = image_generation_workflow.predict_by_bytes(\n    b"cat with a hat", input_type="text"\n)\n# Extract the base64-encoded image data from the result object\nim_b = result.results[0].outputs[0].data.image.base64\n# Convert the base64-encoded image data to a NumPy array of uint8 values\nimage_np = np.frombuffer(im_b, np.uint8)\n# Decode the NumPy array to obtain the image in OpenCV format (BGR color)\nimg_np = cv2.imdecode(image_np, cv2.IMREAD_COLOR)\n\n# Display the image\nplt.axis("off")\nplt.imshow(img_np[..., ::-1])\n',b='import { Workflow } from "clarifai-nodejs";\nimport fs from "fs";\n\n\nconst workflowUrl =\n  "https://clarifai.com/clarifai/Visual/workflows/image-generation";\n\n// Creating the workflow\nconst imageGenerationWorkflow = new Workflow({\n  url: workflowUrl,\n  authConfig: {\n    pat: process.env.CLARIFAI_PAT,\n  },\n});\n\n// Getting the predictions\nconst prediction = await imageGenerationWorkflow.predictByBytes(\n  Buffer.from("cat with a hat"),\n  "text",\n);\n// Extract the base64-encoded image data from the result object\nconst imB =\n  prediction.resultsList?.[0]?.outputsList?.[0]?.data?.image?.base64 ?? "";\n\n// Write the output to a file\nfs.writeFileSync("image.png", imB, "base64");\n',x='from clarifai.client.workflow import Workflow\nimport cv2\n\n\n\nworkflow_url = "https://clarifai.com/clarifai/Visual/workflows/upscale-workflow"\nimage_url = "https://s3.amazonaws.com/samples.clarifai.com/featured-models/image-captioning-statue-of-liberty.jpeg"\n\n# Creating the workflow\nimage_upscale_workflow = Workflow(\n    url=workflow_url,\n    pat="YOUR_PAT",\n)\n\n# Getting the predictions\nresult = image_upscale_workflow.predict_by_url(\n    image_url ,\n    input_type="image",\n)\n\nim_b = result.results[0].outputs[0].data.image.base64\nimage_np = np.frombuffer(im_b, np.uint8)\nimg_np = cv2.imdecode(image_np, cv2.IMREAD_COLOR)\n# Display the image\nplt.axis("off")\nplt.imshow(img_np[..., ::-1])',k='import { Workflow } from "clarifai-nodejs";\nimport fs from "fs";\n\n\nconst workflowUrl =\n  "https://clarifai.com/clarifai/Visual/workflows/upscale-workflow";\nconst imageUrl =\n  "https://s3.amazonaws.com/samples.clarifai.com/featured-models/image-captioning-statue-of-liberty.jpeg";\n\n// Creating the workflow\nconst imageUpscaleWorkflow = new Workflow({\n  url: workflowUrl,\n  authConfig: {\n    pat: process.env.CLARIFAI_PAT,\n  },\n});\n\n// Getting the predictions\nconst result = await imageUpscaleWorkflow.predictByUrl(imageUrl, "image");\n\nconst imB =\n  result.resultsList?.[0]?.outputsList?.[0]?.data?.image?.base64 ?? "";\n\nconsole.log(imB);\n\n// Write the output to a file\nfs.writeFileSync("image.png", imB, "base64");\n',E='from clarifai.client.workflow import Workflow\n\n\nworkflow_url = "https://clarifai.com/clarifai/Text/workflows/text-to-audio"\n# Creating thr workflow\ntext_to_audio_workflow = Workflow(\n    url=workflow_url,\n    pat="YOUR_PAT",\n)\n\n# Getting the predictions\nresult = text_to_audio_workflow.predict_by_bytes(\n    b"I love your product very much", input_type="text"\n)\n\n# Save the audio file\nwith open("output_audio.wav", mode="bx") as f:\n    f.write(result.results[0].outputs[0].data.audio.base64)',y='import { Workflow } from "clarifai-nodejs";\nimport fs from "fs";\n\n\nconst workflowUrl =\n  "https://clarifai.com/clarifai/Text/workflows/text-to-audio";\n\n// Creating the workflow\nconst textToAudioWorkflow = new Workflow({\n  url: workflowUrl,\n  authConfig: {\n    pat: process.env.CLARIFAI_PAT,\n  },\n});\n\n// Getting the predictions\nconst prediction = await textToAudioWorkflow.predictByBytes(\n  Buffer.from("I love your product very much"),\n  "text",\n);\n\nconst imgB =\n  prediction.resultsList?.[0]?.outputsList?.[0]?.data?.audio?.base64 ?? "";\n\n// Save the audio file\nfs.writeFileSync("audio.wav", imgB, "base64");\n',T='from clarifai.client.user import User\n\n\n# Create a new app (replace USER_ID and APP_ID with your own)\napp = User(user_id="user_id", pat="YOUR_PAT").create_app(\n    app_id="APP_ID", base_workflow="Empty"\n)\n# Create ASR-Sentiment workflow\nasr_sentiment_workflow = app.create_workflow(\n    config_filepath="configs/asr_sentiment.yml"\n)\n\naudio_url = "https://s3.amazonaws.com/samples.clarifai.com/GreatPresentation2.wav"\n\n# Infer workflow on an audio input\nprediction = asr_sentiment_workflow.predict_by_url(\n    audio_url,\n    input_type="audio",\n)\n\n# Get workflow results\nprint(prediction.results[0].outputs[-1].data)\n',R='import { App, User, Workflow } from "clarifai-nodejs";\nimport path from "path";\n\n\nconst appId = "test_app"; // Placeholder for application ID\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\n// Specify the correct userId/appId pairings\n// Since you\'re making inferences outside your app\'s scope\nconst client = new User({\n  userId: process.env.CLARIFAI_USER_ID,\n  pat: process.env.CLARIFAI_PAT,\n  appId,\n});\n\nawait client.createApp({\n  appId,\n  baseWorkflow: "Empty",\n});\n\nconst app = new App({\n  authConfig: {\n    userId: process.env.CLARIFAI_USER_ID,\n    pat: process.env.CLARIFAI_PAT,\n    appId,\n  },\n});\n\n// Create ASR-Sentiment workflow\nconst asrSentimentWorkflow = await app.createWorkflow({\n  configFilePath: path.resolve(\n    __dirname,\n    "../../assets/configs/asr_sentiment.yml",\n  ),\n});\n\nconst audioUrl =\n  "https://s3.amazonaws.com/samples.clarifai.com/GreatPresentation2.wav";\n\nconst workflow = new Workflow({\n  workflowId: asrSentimentWorkflow.id,\n  authConfig: {\n    userId: process.env.CLARIFAI_USER_ID,\n    pat: process.env.CLARIFAI_PAT,\n    appId,\n  },\n});\n\n// Infer workflow on an audio input\nconst prediction = await workflow.predictByUrl(audioUrl, "audio");\n\n// Get workflow results\nconsole.log(\n  prediction.resultsList?.[0]?.outputsList?.[\n    prediction.resultsList?.[0]?.outputsList.length - 1\n  ].data,\n);\n',O='from clarifai.client.workflow import Workflow\n\n\nworkflow_url = "https://clarifai.com/clarifai/main/workflows/Face"\nimage_url = "https://samples.clarifai.com/face-det.jpg"\n# creating the workflow\nface_search_workflow = Workflow(\n    url=workflow_url, pat="YOUR_PAT"\n)\n\n# Getting the predictions\nresult = face_search_workflow.predict_by_url(\n    url=image_url, input_type="image"\n)\n',j='import { Workflow } from "clarifai-nodejs";\n\n\nconst workflowUrl = "https://clarifai.com/clarifai/main/workflows/Face";\nconst imageUrl = "https://samples.clarifai.com/face-det.jpg";\n\n// Creating the workflow\nconst faceSearchWorkflow = new Workflow({\n  url: workflowUrl,\n  authConfig: {\n    pat: process.env.CLARIFAI_PAT,\n  },\n});\n\n// Getting the predictions\nconst prediction = await faceSearchWorkflow.predictByUrl(imageUrl, "image");\nconsole.log(prediction.resultsList?.[0]?.outputsList?.[0]?.data);\n',P='from clarifai.client.workflow import Workflow\n\n\nworkflow_url = "https://clarifai.com/clarifai/main/workflows/General-Detection"\nvideo_url = "https://samples.clarifai.com/beer.mp4"\n# Creating the workflows\nobject_search_workflow = Workflow(\n    url=workflow_url, pat="YOUR_PAT"\n)\n# getting the predictions\nresult = object_search_workflow.predict_by_url(\n    url=video_url, input_type="video"\n)\nprint(result.results[0].outputs[-1].data)',S='import { Workflow } from "clarifai-nodejs";\n\n\nconst workflowUrl =\n  "https://clarifai.com/clarifai/main/workflows/General-Detection";\nconst videoUrl = "https://samples.clarifai.com/beer.mp4";\n\n// Creating the workflows\nconst objectSearchWorkflow = new Workflow({\n  url: workflowUrl,\n  authConfig: {\n    pat: process.env.CLARIFAI_PAT,\n  },\n});\n\n// Getting the predictions\nconst prediction = await objectSearchWorkflow.predictByUrl(videoUrl, "video");\n\nconsole.log(\n  prediction.resultsList?.[0]?.outputsList?.[\n    prediction.resultsList?.[0]?.outputsList?.length - 1\n  ].data,\n);\n',C='from clarifai.client.workflow import Workflow\n\n\n# input text\ntext = b"""The word pollution was derived from the Latin word pollution, which means to make dirty. Pollution is the process of making the environment pollute the water and the air by adding harmful substances. Pollution causes an imbalance in the environment. This imbalance threatened the very survival of all forms of life. It\'s a threat to the whole world. India ranks at the bottom of 125 of the 132 countries in the 2012 Environmental Performance Index. This report is produced by researchers at Yale and Columbia University in association with the World Economic Forum.\nPollution of the environment is a serious problem of industrialized societies. Industrial development and the green revolution have had a negative impact on the environment. People have converted the life support system of all living people into their own resources and have greatly disrupted the natural ecological balance. Serious degradation and depletion have been caused due to overuse, abuse and mismanagement of resources to meet human greed."""\n\nworkflow_url = "https://clarifai.com/clarifai/main/workflows/Language-Understanding"\n# Creating the workflow\ntext_embedding_workflow = Workflow(\n    url= workflow_url,\n    pat="YOUR_PAT",\n)\n\n\n\n# getting the predictions\nresult = text_embedding_workflow.predict_by_bytes(text, input_type="text")\nprint(result.results[0].outputs[-1].data)',D='import { Workflow } from "clarifai-nodejs";\n\n\n// input text\nconst text =\n  Buffer.from(`The word pollution was derived from the Latin word pollution, which means to make dirty. Pollution is the process of making the environment pollute the water and the air by adding harmful substances. Pollution causes an imbalance in the environment. This imbalance threatened the very survival of all forms of life. It\'s a threat to the whole world. India ranks at the bottom of 125 of the 132 countries in the 2012 Environmental Performance Index. This report is produced by researchers at Yale and Columbia University in association with the World Economic Forum.\nPollution of the environment is a serious problem of industrialized societies. Industrial development and the green revolution have had a negative impact on the environment. People have converted the life support system of all living people into their own resources and have greatly disrupted the natural ecological balance. Serious degradation and depletion have been caused due to overuse, abuse and mismanagement of resources to meet human greed.`);\n\nconst workflowUrl =\n  "https://clarifai.com/clarifai/main/workflows/Language-Understanding";\n// Creating the workflow\nconst textEmbeddingWorkflow = new Workflow({\n  url: workflowUrl,\n  authConfig: {\n    pat: process.env.CLARIFAI_PAT,\n  },\n});\n\n// getting the predictions\nconst result = await textEmbeddingWorkflow.predictByBytes(text, "text");\nconsole.log(\n  result.resultsList?.[0]?.outputsList?.[\n    result.resultsList?.[0]?.outputsList?.length - 1\n  ].data,\n);\n',L='from clarifai.client.input import Inputs\nfrom clarifai.client.workflow import Workflow\n\n# setting up input image and prompt\nprompt = "What time of day is it?"\nimage_url = "https://samples.clarifai.com/metro-north.jpg"\n\nworkflow_url = "https://clarifai.com/clarifai/Visual/workflows/multimodal-to-text"\n\nmulti_input = Inputs.get_multimodal_input(input_id="", image_url=image_url, raw_text=prompt)\n\n# Creating the workflow\ntext_embedding_workflow = Workflow(\n    url=workflow_url,\n    pat="YOUR_PAT",\n)\n\n# Getting the predictions\nresult = text_embedding_workflow.predict(\n    inputs=[\n        multi_input\n    ]\n)\nprint(result.results[0].outputs[-1].data)',U='import { Input, Workflow } from "clarifai-nodejs";\n\n\n// Setting up input image and prompt\nconst prompt = "What time of day is it?";\nconst imageUrl = "https://samples.clarifai.com/metro-north.jpg";\n\nconst workflowUrl =\n  "https://clarifai.com/clarifai/Visual/workflows/multimodal-to-text";\n\nconst multiInput = Input.getMultimodalInput({\n  inputId: "multi-modal-input",\n  imageUrl,\n  rawText: prompt,\n});\n\n// Creating the workflow\nconst textEmbeddingWorkflow = new Workflow({\n  url: workflowUrl,\n  authConfig: {\n    pat: process.env.CLARIFAI_PAT,\n  },\n});\n\n// Getting the predictions\nconst result = await textEmbeddingWorkflow.predict({\n  inputs: [multiInput],\n});\n\nconsole.log(\n  result.resultsList?.[0]?.outputsList?.[\n    result?.resultsList?.[0]?.outputsList?.length - 1\n  ]?.data,\n);\n',W='# Create or load an app\n\n\nfrom clarifai.client.user import User\nfrom clarifai.client.app import App\n\n\n# Create or load app (replace USER_ID and APP_ID with your own)\nAPP_ID = "APP_ID"\nUSER_ID="USER_ID"\ntry:\n app = User(user_id=USER_ID).create_app(app_id=APP_ID, base_workflow="Empty")\nexcept Exception as e:\n app = App(app_id=APP_ID, user_id=USER_ID)\n\n\n# # Create workflow from yaml file\n\n\n"""bytetrack.yaml\nworkflow:\n id: demo-bytetrack\n nodes:\n   - id: object-detection\n     model:\n       user_id: clarifai\n       app_id: main\n       model_id: person-vehicle-detection-yolov5\n   - id: track\n     model:\n       model_id: "byte-tracker"\n       model_type_id: "byte-tracker"\n       output_info:\n         params:\n           min_visible_frames: 1\n           max_disappeared: 30\n           confidence_thresh:  0.1\n     node_inputs:\n       - node_id: object-detection\n"""\n\n\nworkflow = app.create_workflow(config_filepath="./bytetrack.yaml")\n\n\nresult = workflow.predict_by_url("https://samples.clarifai.com/beer.mp4", input_type="video")\nprint(result.results[0].outputs[-1].data)\n',N='import { User, App, Workflow } from "clarifai-nodejs";\nimport path from "path";\n\n\n// Create or load an app\nconst appId = "test_app";\n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\n// Specify the correct userId/appId pairings\n// Since you\'re making inferences outside your app\'s scope\nconst client = new User({\n  userId: process.env.CLARIFAI_USER_ID,\n  pat: process.env.CLARIFAI_PAT,\n  appId,\n});\n\nawait client.createApp({\n  appId,\n  baseWorkflow: "Empty",\n});\n\nconst app = new App({\n  authConfig: {\n    userId: process.env.CLARIFAI_USER_ID,\n    pat: process.env.CLARIFAI_PAT,\n    appId,\n  },\n});\n\n// Create workflow from yaml file\n/**\nworkflow:\n  id: demoBytetrack\n  nodes:\n      - id: objectDetection\n          model:\n              userId: clarifai\n              appId: main\n              modelId: personVehicleDetectionYolov5\n      - id: track\n          model:\n              modelId: "byteTracker"\n              modelTypeId: "byteTracker"\n              outputInfo:\n                  params:\n                      minVisibleFrames: 1\n                      maxDisappeared: 30\n                      confidenceThresh: 0.1\n          nodeInputs:\n              - nodeId: objectDetection\n*/\n\nconst visualDetectorWorkflow = await app.createWorkflow({\n  configFilePath: path.resolve(\n    __dirname,\n    "../../assets/configs/byte_tracker.yml",\n  ),\n});\n\nconst workflow = new Workflow({\n  authConfig: {\n    userId: process.env.CLARIFAI_USER_ID,\n    pat: process.env.CLARIFAI_PAT,\n    appId,\n  },\n  workflowId: visualDetectorWorkflow.id,\n});\n\nconst prediction = await workflow.predictByUrl(\n  "https://samples.clarifai.com/beer.mp4",\n  "video",\n);\n\nconsole.log(\n  prediction.resultsList?.[0]?.outputsList?.[\n    prediction.resultsList?.[0]?.outputsList?.length - 1\n  ].data,\n);\n',F='from clarifai.client.input import Inputs\nfrom clarifai.client.model import Model\nfrom clarifai.client.workflow import Workflow\n\nmodel_url = "https://clarifai.com/clarifai/main/models/general-image-recognition"\nimage_url = "https://samples.clarifai.com/metro-north.jpg"\n\n# here is an example of creating an input proto list of size 32\nproto_list=[]\nfor i in range(0,32):\n    proto_list.append(Inputs.get_input_from_url(input_id = f\'demo_{i}\', image_url=image_url))\n\n\nworkflow_url = "https://clarifai.com/clarifai/main/workflows/Moderation"\n# Creating the workflow\nimage_classifcation_workflow = Workflow(\n    url=workflow_url, pat="YOUR_PAT"\n)\n\n# Getting the predictions\nresult = image_classifcation_workflow.predict(proto_list)\nprint(len(result.results))',$='import { Input, Workflow } from "clarifai-nodejs";\nimport { Input as GrpcInput } from "clarifai-nodejs-grpc/proto/clarifai/api/resources_pb";\n\n\nconst imageUrl = "https://samples.clarifai.com/metro-north.jpg";\n\n// Here is an example of creating an input proto list of size 32\nconst protoList: GrpcInput[] = [];\nfor (let i = 0; i < 32; i++) {\n  protoList.push(Input.getInputFromUrl({ inputId: `demo_${i}`, imageUrl }));\n}\n\nconst workflowUrl = "https://clarifai.com/clarifai/main/workflows/Moderation";\n// Creating the workflow\nconst imageClassificationWorkflow = new Workflow({\n  url: workflowUrl,\n  authConfig: {\n    pat: process.env.CLARIFAI_PAT,\n  },\n});\n\n// Getting the predictions\nconst result = await imageClassificationWorkflow.predict({\n  inputs: protoList,\n});\nconsole.log(result.resultsList.length);\n',B='concepts {\n\n  id: "ai_pkltpBZ7"\n\n  name: "toxic"\n\n  value: 0.996599138\n\n  app_id: "main"\n\n}\n\nconcepts {\n\n  id: "ai_PlX1h51x"\n\n  name: "insult"\n\n  value: 0.515220046\n\n  app_id: "main"\n\n}\n\nconcepts {\n\n  id: "ai_jf9vs404"\n\n  name: "obscene"\n\n  value: 0.0849464387\n\n  app_id: "main"\n\n}\n\nconcepts {\n\n  id: "ai_2Dv0D4Nm"\n\n  name: "identity_hate"\n\n  value: 0.0308352802\n\n  app_id: "main"\n\n}\n\nconcepts {\n\n...\n\n  value: 0.00677669281\n\n  app_id: "main"\n\n}',Y='- Pollution, taken from the Latin word meaning "to make dirty", is the act of using harmful substances to dirty the air and water around us, to the point where it threatens all life forms\n\n- India ranks terribly low on the Environmental Performance Index, an evaluation of countries\' environmental statuses, produced by researchers at Yale and Columbia University in collaboration with the World Economic Forum\n\n- While pollution is a problem for all societies, industrialized societies are particularly afflicted due to the further damage industrial development has caused to the environment\n\n- However, the worst impact of pollution may be yet to come, as warns the author, and the best course of action going forward needs to be evaluated. \n\nWould you like help with anything else regarding this summary?',H="Good morning! I'm glad to hear you're starting the day with such a positive attitude. How can I assist you today to make it even better?\n",G='concepts {\n\n  id: "ai_8QQwMjQR"\n\n  name: "drug"\n\n  value: 0.99999994\n\n  app_id: "main"\n\n}\n\nconcepts {\n\n  id: "ai_QD1zClSd"\n\n  name: "safe"\n\n  value: 4.93858607e-08\n\n  app_id: "main"\n\n}\n\nconcepts {\n\n  id: "ai_V76bvrtj"\n\n  name: "explicit"\n\n  value: 3.21394289e-09\n\n  app_id: "main"\n\n}\n\nconcepts {\n\n  id: "ai_kBBGf7r8"\n\n  name: "gore"\n\n  value: 1.82284843e-09\n\n  app_id: "main"\n\n}\n\nconcepts {\n\n...\n\n  value: 8.18517032e-10\n\n  app_id: "main"\n\n}',M='2024-01-11 09:09:54 INFO     clarifai:  wf-ocr Workflow is still deploying, please wait...           workflow.py:89\n\nINFO:clarifai:wf-ocr Workflow is still deploying, please wait...\n\n2024-01-11 09:10:57 INFO     clarifai:  wf-ocr Workflow is still deploying, please wait...           workflow.py:89\n\nINFO:clarifai:wf-ocr Workflow is still deploying, please wait...\n\ntext {\n\n  raw: "H WerP\\nRADOAS TESTXIT\\nTRRTESTINCEERINCMODEB\\nEPAGSTED\\nONETIMEIISE\\nDITED1YEARWPRRNTY"\n\n  text_info {\n\n    encoding: "UnknownTextEnc"\n\n  }\n\n}',K='concepts {\n\n  id: "LABEL_2"\n\n  name: "LABEL_2"\n\n  value: 0.9802844524383545\n\n  app_id: "text-classification"\n\n}\n\nconcepts {\n\n  id: "LABEL_1"\n\n  name: "LABEL_1"\n\n  value: 0.017947228625416756\n\n  app_id: "text-classification"\n\n}\n\nconcepts {\n\n  id: "LABEL_0"\n\n  name: "LABEL_0"\n\n  value: 0.0017683181213214993\n\n  app_id: "text-classification"\n\n}',V='regions {\n\n  id: "e2cb37cbc6a0e59f7afc4d74b55ecb91"\n\n  region_info {\n\n    bounding_box {\n\n      top_row: 0.265738964\n\n      left_col: 0.189666823\n\n      bottom_row: 0.519693851\n\n      right_col: 0.326894283\n\n    }\n\n  }\n\n  data {\n\n    clusters {\n\n      id: "428_280"\n\n      projection: -0.032409668\n\n      projection: 0.20954895\n\n    }\n\n  }\n\n}\n\nregions {\n\n  id: "abf63d045dae71ef5c6532e8edfda65f"\n\n  region_info {\n\n    bounding_box {\n\n      top_row: 0.174794614\n\n      left_col: 0.665749371\n\n      bottom_row: 0.395655245\n\n...\n\n      projection: -0.0704193115\n\n      projection: -0.0369262695\n\n    }\n\n  }\n\n}\n',q='frames {\n\n  frame_info {\n\n    time: 500\n\n  }\n\n  data {\n\n  }\n\n  id: "faa9f3d5c8569123d8bea365ea478031"\n\n}\n\nframes {\n\n  frame_info {\n\n    index: 1\n\n    time: 1500\n\n  }\n\n  data {\n\n    regions {\n\n      id: "c58812cf4c3066e55fb16532dfde6986"\n\n      region_info {\n\n        bounding_box {\n\n          top_row: 0.0844013914\n\n          bottom_row: 1\n\n          right_col: 0.858514369\n\n        }\n\n      }\n    }\n    \n    \n    }\n}',z='clusters {\n\n  id: "24_21"\n\n  projection: 0.203612357\n\n  projection: -0.0755017698\n\n}',X='\ntext {\n\n  raw: "Based on the lighting and the sky\\\'s color in the image, it appears to be either dawn or dusk. The sky has a mix of light and dark hues, which is typical during twilight hours when the sun is below the horizon and the light is diffused. However, without additional context, it\\\'s not possible to determine with certainty whether it\\\'s morning or evening. The presence of snow on the ground suggests that it might be during the winter months when the days are shorter."\n\n  text_info {\n\n    encoding: "UnknownTextEnc"\n\n  }\n\n}',J='frames {\n\n  frame_info {\n\n    time: 500\n\n  }\n\n  data {\n\n    regions {\n\n      id: "172c80386cbad83149135e715200f5a8"\n\n      region_info {\n\n        bounding_box {\n\n          top_row: 0.6157978773117065\n\n          left_col: 0.45200759172439575\n\n          bottom_row: 0.6784101724624634\n\n          right_col: 0.49515417218208313\n\n        }\n\n      }\n\n      data {\n\n        concepts {\n\n          id: "ai_BPXQSP1m"\n\n          name: "vehicle"\n\n          value: 0.2520264685153961\n\n          app_id: "main"\n\n        }\n\n      }\n\n      value: 0.2520264685153961\n\n    }\n\n  }\n\n  id: "faa9f3d5c8569123d8bea365ea478031"\n\n}\n\nframes {\n\n  frame_info {\n\n    index: 1\n\n    time: 1500\n\n  }\n\n  data {\n\n  }\n\n  id: "8e5f8672bdda2f2682d59ccc019d48c0"\n\n}\n\nframes {\n\n  frame_info {\n\n    index: 2\n\n    time: 2500\n\n  }\n\n  data {\n\n  }\n\n  id: "3f4cd8b6cbe2361de2d3a3f84906723c"\n\n}\n\nframes {\n\n  frame_info {\n\n    index: 3\n\n    time: 3500\n\n  }\n\n  data {\n\n  }\n\n  id: "049f7331f17764126fa433ccc7eb27a6"\n\n}\n\nframes {\n\n  frame_info {\n\n    index: 4\n\n    time: 4500\n\n  }\n\n  data {\n\n  }\n\n  id: "a815862119825cfb037834ec5dc24619"\n\n}\n\nframes {\n\n  frame_info {\n\n    index: 5\n\n    time: 5500\n\n  }\n\n  data {\n\n  }\n\n  id: "14572f138018d23fcd39a87f0c51880e"\n\n}\n\nframes {\n\n  frame_info {\n\n    index: 6\n\n    time: 6500\n\n  }\n\n  data {\n\n  }\n\n  id: "7790d9923639183be7213e8330736ea5"\n\n}\n\nframes {\n\n  frame_info {\n\n    index: 7\n\n    time: 7500\n\n  }\n\n  data {\n\n  }\n\n  id: "32224efe9c43139c9f3070930bae4e6c"\n\n}\n\nframes {\n\n  frame_info {\n\n    index: 8\n\n    time: 8500\n\n  }\n\n  data {\n\n  }\n\n  id: "0c4dd5d602afa6754bcfd441998412af"\n\n}\n\ntracks {\n\n  id: "1"\n\n  time_info {\n\n  }\n\n}',Z="32",Q='##############################################################################\n# In this section, we set the user authentication, app ID, workflow ID, and\n# image URL. Change these strings to run your own example.\n##############################################################################\n\n# Your PAT (Personal Access Token) can be found in the Account\'s Security section\nPAT = "YOUR_PAT_HERE"\nUSER_ID = "clarifai"\nAPP_ID = "main"\n# Change these to make your own predictions\nWORKFLOW_ID = "Face-Sentiment"\nIMAGE_URL = "https://samples.clarifai.com/celebrity.jpeg"\n# Or, to use a local text file, assign the location variable\n# IMAGE_FILE_LOCATION = "YOUR_IMAGE_FILE_LOCATION_HERE"\n\n##########################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n##########################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (("authorization", "Key " + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(\n    user_id=USER_ID, app_id=APP_ID\n)  # The userDataObject is required when using a PAT\n\n# To use a local text file, uncomment the following lines\n# with open(IMAGE_FILE_LOCATION, "rb") as f:\n#    file_bytes = f.read()\n\npost_workflow_results_response = stub.PostWorkflowResults(\n    service_pb2.PostWorkflowResultsRequest(\n        user_app_id=userDataObject,\n        workflow_id=WORKFLOW_ID,\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    image=resources_pb2.Image(\n                        url=IMAGE_URL\n                      # base64=file_bytes\n                    )\n                )\n            )\n        ],\n    ),\n    metadata=metadata,\n)\nif post_workflow_results_response.status.code != status_code_pb2.SUCCESS:\n    print(post_workflow_results_response.status)\n    raise Exception(\n        "Post workflow results failed, status: "\n        + post_workflow_results_response.status.description\n    )\n\n# We\'ll get one WorkflowResult for each input we used above. Because of one input, we have here one WorkflowResult\nresults = post_workflow_results_response.results[0]\n\n# Each model we have in the workflow will produce one output.\nfor output in results.outputs:\n    model = output.model\n\n    print("Predicted concepts for the model `%s`" % model.id)\n\n    for concept in output.data.regions:\n        for item in concept.data.concepts:\n            print("\\t%s %.2f" % (item.name, item.value))\n\n# Uncomment this line to print the raw output\n# print(results)\n',nn='\x3c!--index.html file--\x3e\n\n<script>\n  ////////////////////////////////////////////////////////////////////////////////////////\n  //  In this section, we set the user authentication, app ID, workflow ID, and\n  // image URL. Change these strings to run your own example.\n  ///////////////////////////////////////////////////////////////////////////////////////\n\n  // Your PAT (Personal Access Token) can be found in the Account\'s Security section\n  const PAT = "YOUR_PAT_HERE";\n  const USER_ID = "clarifai";\n  const APP_ID = "main";\n  // Change these to make your own predictions\n  const WORKFLOW_ID = "Face-Sentiment";\n  const IMAGE_URL = "https://samples.clarifai.com/celebrity.jpeg";\n\n  ///////////////////////////////////////////////////////////////////////////////////\n  // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n  /////////////////////////////////////////////////////////////////////////////////// \n\n  const raw = JSON.stringify({\n    "user_app_id": {\n      "user_id": USER_ID,\n      "app_id": APP_ID\n    },\n    "inputs": [\n      {\n        "data": {\n          "image": {\n            "url": IMAGE_URL\n          }\n        }\n      }\n    ]\n  });\n\n  const requestOptions = {\n    method: \'POST\',\n    headers: {\n      \'Accept\': \'application/json\',\n      \'Authorization\': \'Key \' + PAT\n    },\n    body: raw\n  };\n\n  fetch(`https://api.clarifai.com/v2/workflows/${WORKFLOW_ID}/results`, requestOptions)\n    .then(response => response.text())\n    .then(result => console.log(result))\n    .catch(error => console.log(\'error\', error));\n<\/script>',en='//index.js file\n\n///////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, app ID, workflow ID, and\n// image URL. Change these strings to run your own example.\n///////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\nconst PAT = "YOUR_PAT_HERE";\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\nconst USER_ID = "clarifai";\nconst APP_ID = "main";\n// Change these to make your own predictions\nconst WORKFLOW_ID = "Face-Sentiment";\nconst IMAGE_URL = "https://samples.clarifai.com/celebrity.jpeg";\n// Or, to use a local text file, assign the location variable\n// const IMAGE_FILE_LOCATION = "YOUR_IMAGE_FILE_LOCATION_HERE";\n\n/////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n/////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\n// To use a local text file, uncomment the following lines\n// const fs = require("fs");\n// const imageBytes = fs.readFileSync(IMAGE_FILE_LOCATION);\n\nstub.PostWorkflowResults({\n        user_app_id: {\n            "user_id": USER_ID,\n            "app_id": APP_ID,\n        },\n        workflow_id: WORKFLOW_ID,\n        inputs: [{\n            data: {\n                image: {\n                    url: IMAGE_URL,\n                    // base64: imageBytes\n                }\n            }\n        }],\n    },\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            throw new Error(\n                "Post workflow results failed, status: " + response.status.description\n            );\n        }\n\n        // We\'ll get one WorkflowResult for each input we used above. Because of one input, we have here \n        // one WorkflowResult\n        const results = response.results[0];\n\n        // Each model we have in the workflow will produce one output.\n        for (const output of results.outputs) {\n            const model = output.model;\n\n            console.log(`Predicted concepts for the model \'${model.id}\'`);\n\n            for (const concept of output.data.regions) {\n                for (const item of concept.data.concepts) {\n                    console.log(`\\t${item.name} ${item.value.toFixed(2)}`);\n                }\n            }\n        }\n        // Uncomment this line to print the raw output\n        // console.log(results);\n    }\n);',tn='package com.clarifai.example;\n\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.grpc.api.status.StatusCode;\n\nimport com.google.protobuf.ByteString;\nimport java.io.File;\nimport java.io.IOException;\nimport java.nio.file.Files;\n\npublic class ClarifaiExample {\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, app ID, workflow ID, and\n    // image URL. Change these strings to run your own example.\n    ///////////////////////////////////////////////////////////////////////////////////\n    \n    //Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    static final String USER_ID = "clarifai";\n    static final String APP_ID = "main";\n    // Change these to make your own predictions\n    static final String WORKFLOW_ID = "Face-Sentiment";\n    static final String IMAGE_URL = "https://samples.clarifai.com/celebrity.jpeg";\n    // Or, to use a local text file, assign the location variable\n    // static final String IMAGE_FILE_LOCATION = "YOUR_IMAGE_FILE_LOCATION_HERE";\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n    \n    public static void main(String[] args) throws IOException {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n                .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        PostWorkflowResultsResponse postWorkflowResultsResponse = stub.postWorkflowResults(\n                PostWorkflowResultsRequest.newBuilder()\n                        .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n                        .setWorkflowId(WORKFLOW_ID)\n                        .addInputs(\n                                Input.newBuilder().setData(\n                                        Data.newBuilder().setImage(\n                                                Image.newBuilder().setUrl(IMAGE_URL)\n                                                //  To use a local text file, uncomment the following lines\n                                                //Image.newBuilder().setBase64(ByteString.copyFrom(Files.readAllBytes(\n                                                       // new File(IMAGE_FILE_LOCATION).toPath()\n                                                //)))\n                                        )\n                                )\n                        )\n                        .build()\n        );\n\n        if (postWorkflowResultsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("Post workflow results failed, status: " + postWorkflowResultsResponse.getStatus());\n        }\n\n        // We\'ll get one WorkflowResult for each input we used above. Because of one input, we have here\n        // one WorkflowResult\n        WorkflowResult results = postWorkflowResultsResponse.getResults(0);\n\n        // Each model we have in the workflow will produce its output       \n        for (Output output : results.getOutputsList()) {\n            Model model = output.getModel();\n\n            System.out.println("Predicted concepts for the model \'" + model.getId() + "\'");\n\n            for (Region concept : output.getData().getRegionsList()) {\n                for (Concept item : concept.getData().getConceptsList()) {\n                    System.out.printf("\\t%s %.2f%n", item.getName(), item.getValue());\n                }\n            }\n        }\n\n        // Uncomment this line to print the raw output\n        // System.out.println(results);\n    }\n\n}\n',on='curl -X POST "https://api.clarifai.com/v2/users/clarifai/apps/main/workflows/Face-Sentiment/results" \\\n  -H "authorization: Key YOUR_PAT_HERE" \\\n  -H "content-type: application/json" \\\n  -d \'{\n    "inputs": [\n        {\n          "data": {\n            "image": {\n              "url": "https://samples.clarifai.com/celebrity.jpeg"\n          }\n        }\n      }\n    ]\n}\'',rn='<?php\n\nrequire __DIR__ . "/vendor/autoload.php";\n\n/////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, app ID, workflow ID, and\n// image URL. Change these strings to run your own example.\n/////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\n$PAT = "YOUR_PAT_HERE";\n$USER_ID = "clarifai";\n$APP_ID = "main";\n// Change these to make your own predictions\n$WORKFLOW_ID = "Face-Sentiment";\n$IMAGE_URL = "https://samples.clarifai.com/celebrity.jpeg";\n# Or, to use a local text file, assign the location variable\n// $IMAGE_BYTES_STRING = "YOUR_IMAGE_FILE_LOCATION_HERE";\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Image;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\PostWorkflowResultsRequest;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ["Authorization" => ["Key " . $PAT]];\n\n$userDataObject = new UserAppIDSet([\n    "user_id" => $USER_ID,\n    "app_id" => $APP_ID,\n]);\n\n// To use a local text file, uncomment the following lines\n// $imageData = file_get_contents($IMAGE_BYTES_STRING); \n\n// Let\'s make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client\n    ->PostWorkflowResults(\n        // The request object carries the request along with the request status and other metadata related to the request itself\n        new PostWorkflowResultsRequest([\n            "user_app_id" => $userDataObject,\n            "workflow_id" => $WORKFLOW_ID,\n            "inputs" => [\n                new Input([\n                    // The Input object wraps the Data object in order to meet the API specification\n                    "data" => new Data([\n                        // The Data object is constructed around the Image object. It offers a container that has additional image independent\n                        // metadata. In this particular use case, no other metadata is needed to be specified\n                        "image" => new Image([\n                            // In the Clarifai platform, an image is defined by a special Image object\n                           "url" => $IMAGE_URL,\n                            // "base64" => $imageData,\n                        ]),\n                    ]),\n                ]),\n            ],\n        ]),\n        $metadata\n    )\n    ->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception("Error: {$status->details}");\n}\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure\n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    throw new Exception(\n        "Failure response: " .\n            $response->getStatus()->getDescription() .\n            " " .\n            $response->getStatus()->getDetails()\n    );\n}\n\n// We\'ll get one WorkflowResult for each input we used above. Because of one input, we have here one WorkflowResult\n$results = $response->getResults()[0];\n\n// Each model we have in the workflow will produce one output\nforeach ($results->getOutputs() as $output) {\n    $model = $output->getModel();\n\n    echo "Predicted concepts for the model \'{$model->getId()}\'" . "\\n";\n\n    foreach ($output->getData()->getRegions() as $concept) {\n        foreach ($concept->getData()->getConcepts() as $item) {\n            echo "\\t{$item->getName()} {$item->getValue()}" . "\\n";\n        }\n    }\n}\n\n// Uncomment this line to print the raw output\n// print_r($results);\n\n?>',an="Predicted concepts for the model `face-detection`\n        BINARY_POSITIVE 1.00\nPredicted concepts for the model `margin-110-image-crop`\nPredicted concepts for the model `face-sentiment-recognition`\n        happiness 1.00\n        disgust 0.00\n        fear 0.00\n        sadness-contempt 0.00\n        surprise 0.00\n        anger 0.00\n        neutral 0.00",sn='status {\n  code: SUCCESS\n  description: "Ok"\n}\ninput {\n  id: "5865e5d55a164beebc6f7a5682269cb4"\n  data {\n    image {\n      url: "https://samples.clarifai.com/celebrity.jpeg"\n    }\n  }\n}\noutputs {\n  id: "01e76acba82d4453a1d3c10b1777066e"\n  status {\n    code: SUCCESS\n    description: "Ok"\n  }\n  created_at {\n    seconds: 1700656970\n    nanos: 361613613\n  }\n  model {\n    id: "face-detection"\n    name: "Face"\n    created_at {\n      seconds: 1606323024\n      nanos: 453038000\n    }\n    modified_at {\n      seconds: 1665509418\n      nanos: 21257000\n    }\n    app_id: "main"\n    model_version {\n      id: "6dc7e46bc9124c5c8824be4822abe105"\n      created_at {\n        seconds: 1614879626\n        nanos: 81729000\n      }\n      status {\n        code: MODEL_TRAINED\n        description: "Model is trained and ready"\n      }\n      visibility {\n        gettable: PUBLIC\n      }\n      app_id: "main"\n      user_id: "clarifai"\n      metadata {\n      }\n    }\n    user_id: "clarifai"\n    model_type_id: "visual-detector"\n    visibility {\n      gettable: PUBLIC\n    }\n    workflow_recommended {\n    }\n  }\n  data {\n    regions {\n      id: "32b383f26ce26a4ff16642447f9317b8"\n      region_info {\n        bounding_box {\n          top_row: 0.151694223\n          left_col: 0.285768479\n          bottom_row: 0.614028037\n          right_col: 0.762517869\n        }\n      }\n      data {\n        concepts {\n          id: "ai_b1b1b1b1"\n          name: "BINARY_POSITIVE"\n          value: 0.999997377\n          app_id: "main"\n        }\n      }\n      value: 0.999997377\n    }\n  }\n}\noutputs {\n  id: "a521f59aeaf94d06901da76ab01fd0e0"\n  status {\n    code: SUCCESS\n    description: "Ok"\n  }\n  created_at {\n    seconds: 1700656970\n    nanos: 361621134\n  }\n  model {\n    id: "margin-110-image-crop"\n    name: "margin-110"\n    created_at {\n      seconds: 1590505298\n      nanos: 387731000\n    }\n    modified_at {\n      seconds: 1634716390\n      nanos: 69050000\n    }\n    app_id: "main"\n    model_version {\n      id: "b9987421b40a46649566826ef9325303"\n      created_at {\n        seconds: 1590505298\n        nanos: 387731000\n      }\n      status {\n        code: MODEL_TRAINED\n        description: "Model is trained and ready"\n      }\n      visibility {\n        gettable: PUBLIC\n      }\n      app_id: "main"\n      user_id: "clarifai"\n      metadata {\n      }\n    }\n    display_name: "margin-110-image-crop"\n    user_id: "clarifai"\n    model_type_id: "image-crop"\n    visibility {\n      gettable: PUBLIC\n    }\n    workflow_recommended {\n    }\n  }\n  data {\n    regions {\n      id: "6b5ea07bdaec9a8e48ff9424bf1f03b7"\n      region_info {\n        bounding_box {\n          top_row: 0.12857753\n          left_col: 0.261931\n          bottom_row: 0.637144744\n          right_col: 0.786355317\n        }\n      }\n      data {\n        image {\n          base64: "\\377\\330\\377\\340\\000\\020JFIF\\000\\001\\001\\000\\000\\001\\000\\001\\000\\000\\377\\333\\000C\\000\\010\\006\\006\\007\\006\\005\\010\\007\\007\\007\\t\\t\\010\\n\\014\\024\\r\\014\\013\\013\\014\\031\\022\\023\\017\\024\\035\\032\\037\\036\\035\\032\\034\\034 $.\\\' \\",#\\034\\034(7),01444\\037\\\'9=82<.342\\377\\333\\000C\\001\\t\\t\\t\\014\\013\\014\\030\\r\\r\\0302!\\034!22222222222222222222222222222222222222222222222222\\377\\300\\000\\021\\010\\000\\373\\000\\267\\003\\001\\"\\000\\002\\021\\001\\003\\021\\001\\377\\304\\000\\037\\000\\000\\001\\005\\001\\001\\001\\001\\001\\001\\000\\000\\000\\000\\000\\000\\000\\000\\001\\002\\003\\004\\005\\006\\007\\010\\t\\n\\013\\377\\304\\000\\265\\020\\000\\002\\001\\003\\003\\002\\004\\003\\005\\005\\004\\004\\000\\000\\001}\\001\\002\\003\\000\\004\\021\\005\\022!1A\\006\\023Qa\\007\\"q\\0242\\201\\221\\241\\010#B\\261\\301\\025R\\321\\360$3br\\202\\t\\n\\026\\027\\030\\031\\032%&\\\'()*456789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz\\203\\204\\205\\206\\207\\210\\211\\212\\222\\223\\224\\225\\226\\227\\230\\231\\232\\242\\243\\244\\245\\246\\247\\250\\251\\252\\262\\263\\264\\265\\266\\267\\270\\271\\272\\302\\303\\304\\305\\306\\307\\310\\311\\312\\322\\323\\324\\325\\326\\327\\330\\331\\332\\341\\342\\343\\344\\345\\346\\347\\350\\351\\352\\361\\362\\363\\364\\365\\366\\367\\370\\371\\372\\377\\304\\000\\037\\001\\000\\003\\001\\001\\001\\001\\001\\001\\001\\001\\001\\000\\000\\000\\000\\000\\000\\001\\002\\003\\004\\005\\006\\007\\010\\t\\n\\013\\377\\304\\000\\265\\021\\000\\002\\001\\002\\004\\004\\003\\004\\007\\005\\004\\004\\000\\001\\002w\\000\\001\\002\\003\\021\\004\\005!1\\006\\022AQ\\007aq\\023\\"2\\201\\010\\024B\\221\\241\\261\\301\\t#3R\\360\\025br\\321\\n\\026$4\\341%\\361\\027\\030\\031\\032&\\\'()*56789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz\\202\\203\\204\\205\\206\\207\\210\\211\\212\\222\\223\\224\\225\\226\\227\\230\\231\\232\\242\\243\\244\\245\\246\\247\\250\\251\\252\\262\\263\\264\\265\\266\\267\\270\\271\\272\\302\\303\\304\\305\\306\\307\\310\\311\\312\\322\\323\\324\\325\\326\\327\\330\\331\\332\\342\\343\\344\\345\\346\\347\\350\\351\\352\\362\\363\\364\\365\\366\\367\\370\\371\\372\\377\\332\\000\\014\\003\\001\\000\\002\\021\\003\\021\\000?\\000\\267\\343\\235r\\342\\037\\t\\333E\\013\\235\\363I\\2169\\030Q\\237\\347\\\\\\265\\215\\203jSGi\\031;\\374\\244s\\307^I\\374\\263Z^(\\267i\\264\\235.$\\004\\262\\242\\310A\\350s\\237\\360\\244\\360\\254e\\365\\213F3}\\236\\342%\\302\\223\\321\\306\\343\\3764\\001\\351\\332T\\253\\035\\252Z_),\\000\\344\\216\\234W-\\343\\035\\021\\217\\225%\\244\\212\\033\\177,8;{\\217\\344kw\\\\\\274\\362\\356\\360\\315\\203\\264d\\017\\245ajz\\221\\273\\323\\221\\320\\375\\322:\\367\\355@\\036s\\250xZ\\005i\\245\\211\\260\\n\\347\\216\\233\\272g\\333>\\225\\227\\006\\200\\310\\025\\267\\t\\037~\\000\\000\\355\\353\\324\\237\\351]\\253\\022f6\\345\\306\\030\\202\\247\\361\\310\\255\\333-\\r\\036\\305rw\\004l\\341G\\271\\377\\000\\032\\000\\363m?\\303\\327S\\3523\\302\\301v\\344\\202q\\216\\343\\374kN\\037\\006;\\224\\215\\260VF\\034\\036\\307\\034\\327\\253\\330\\370~<\\264\\221\\242\\251f\\335\\300\\357\\232\\261.\\206\\360\\344\\214\\026 \\250\\366\\316y\\240\\016\\"\\307B\\032\\177\\237:\\"\\220cs\\273\\035\\277\\310\\247Ec$\\020:\\020\\024\\2220\\007\\030\\256\\272{IJ\\371J\\277\\"\\205\\014{\\340\\036\\237\\245U\\276fX\\302\\210\\301y\\216\\336?\\204\\347\\257\\363\\240\\014\\030\\356e\\265\\272DV\\312\\217\\223\\036\\265\\\'\\372F\\307|1\\031\\013\\237\\344+F\\332\\311d\\236Id\\371QO\\310H\\357\\334\\377\\000:\\275\\025\\210r&e;\\006B(\\356Oz\\000\\347\\247\\232D\\266\\\\\\256\\013\\026\\177\\256zT6o<\\022<\\216>|\\371\\216\\007n0\\005uR\\351RN\\342F^\\177\\204zS\\323\\303\\341U\\211o\\235\\310$\\216\\302\\2009i/\\356\\r\\350\\220\\234mR\\t=NNO\\341\\305Nu\\2276R+g\\356\\005\\006\\266\\245\\360\\366\\340W\\037/Oz\\241q\\243,\\205c\\013\\373\\265\\345\\273~\\024\\001\\315\\245\\334\\205\\240NA\\003%\\207\\247\\245hG\\256I#\\3033\\266\\317(\\025\\367=\\305O>\\230\\322\\254\\202%\\332\\25200?J\\306\\271\\323\\347\\267u.\\273Q\\017\\034\\346\\200:\\353\\rN;\\253\\313\\270\\311\\300h9\\317J\\3263\\304\\342&\\016\\016r\\016\\017\\260\\2578\\266\\2756\\333\\211\\353\\214u\\353\\351Zv\\032\\226\\350\\330<\\234\\242\\234z\\002h\\003\\244\\273\\324\\0323\\264\\267\\312\\t\\340\\037j~\\241j\\232\\305\\206\\340\\271\\223\\030$\\236\\325\\312_j\\r$\\214K\\3416\\200=\\316k_N\\325\\231-d\\001\\263\\200[\\035\\370\\240\\016CP\\265kk\\247\\211\\307*v\\321Z\\272\\233\\303\\177p\\262\\2467\\225\\303\\003\\353E\\000u\\257\\341\\370u\\037\\rXN\\337,\\221B\\252\\304}3\\237\\314\\326\\236\\211\\242i\\255hM\\324)\\347!\\311n\\230\\367\\025\\223\\341\\335Y\\227G\\212)C\\035\\243\\033\\010\\350*\\245\\376\\245${\\326#\\264\\\'\\336\\301\\352(\\003/\\304\\372\\203C\\2513#\\356D\\316\\016y=\\205gAr\\346\\312x\\316?\\325\\356\\307\\247z\\212X^\\372\\340H\\352|\\265\\301,\\303\\001\\217`*\\255\\373\\0338\\2340\\313\\310v\\214~\\202\\200\\036\\203\\315\\273\\261d\\\\\\310X\\222\\276\\243\\322\\275kD\\264\\217\\354\\352\\0356\\260\\025\\346>\\036\\322\\346\\325<Ug\\032\\037\\335Y\\307\\346JG|\\364\\025\\354\\321F\\021@\\034z\\320\\004\\211\\032F\\273Tq\\232qU#\\007\\030\\244\\343<\\322|\\304\\216(\\001\\277g\\214\\251\\033G=j\\205\\336\\233\\033+2\\24785\\250\\240\\016i~\\\\r3@\\0306Z:\\340\\263\\216;\\003\\332\\265\\026\\326%P6\\214\\212\\263\\200\\017#\\360\\246\\266Nx\\240\\010X/L\\016;Ty\\300\\341A\\\'\\323\\245Y\\362\\301\\031\\301\\246\\371aE\\000SeG\\0075Vh\\"+\\200:v\\305h\\262\\214\\236*\\273 \\310\\376T\\001\\2155\\266\\341\\205L`\\366\\025\\205\\253X\\374\\247 \\202Eu\\323\\241*~`3\\336\\262.\\255\\325\\211\\016\\t\\343\\363\\240\\017>\\273\\260*\\300\\356\\3713\\220j \\202+g;\\260O\\312:\\364\\356k\\256\\222\\3364gP\\204\\203\\323w\\364\\254k\\210\\222&`\\352\\014x\\310R?\\255\\000b\\223\\373\\264g\\371\\202\\345\\261\\370\\325h\\365S\\000wn\\025br\\336\\331\\007\\217\\326\\254\\336L\\233\\260\\321\\252\\2560\\025{\\366\\305s\\372\\204M\\345\\262\\026t\\36306u\\300\\240\\013\\376\\036\\327\\343.^\\3458\\031\\000\\217\\322\\212\\311\\264\\267`\\256\\233F\\305\\352\\336\\2474P\\007\\277\\315e\\021\\214In\\361\\355\\003<\\014\\223X3\\350\\357p\\314\\315\\014\\222\\023\\317M\\240Um\\007Y\\221\\003yw,\\253\\350Fq\\3765\\253<wW\\270\\377\\000\\211\\204\\254\\247\\000\\202\\330^~\\224\\001\\314jWv\\372_\\311\\2729$\\350\\261\\203\\234\\037\\177LV\\034v\\363\\334\\317&\\243x\\017\\244Jz\\022z`Wc\\250h\\232.\\222\\206\\346\\376\\345e\\003\\222\\250r\\307\\330zVW\\205\\235\\274W\\2565\\371\\203\\311\\322l\\330\\210#\\376\\373z\\232\\000\\355<!\\242\\r#IS(\\037j\\233\\347\\224\\217~\\337\\205t\\200\\214\\340\\032\\256\\216U@\\343\\350*T9 c\\221@\\023\\237\\347M\\030\\335\\326\\202s\\212\\\\\\216\\224\\000\\345\\037\\205(\\004\\365\\241A\\3174\\376\\270\\240\\010\\302\\363\\311\\240\\376\\224\\363\\223\\326\\230\\001\\351\\332\\200\\024\\023A\\300\\240\\374\\275)\\t\\343\\332\\200\\030\\300c\\007\\255U\\220\\000j\\314\\215\\305T\\231\\266\\343\\034\\212\\000\\206^\\325R]\\205J\\262\\3765;\\261\\301\\006\\253JKpzP\\006\\035\\364,\\23709\\003\\277z\\240cYa\\"E\\030c\\234\\343\\245l\\314NJ\\2209\\252\\022\\241\\213\\250\\001\\033\\217\\245\\000sZ\\215\\202\\014\\200\\210\\007P\\007J\\345u\\013;\\250\\013\\375\\235\\304\\201\\316Hn\\253]\\305\\3623\\206\\034nO\\302\\271K\\331\\266F\\305\\230\\206\\355\\333\\360\\240\\014(%h\\243dh\\360\\304\\374\\300\\234QD\\227K.\\010\\3032\\365\\337E\\000oCt-\\333z\\034\\251\\3523Zp\\353~J\\022\\315\\2663\\334\\265r3j0\\305\\302\\262\\273\\001\\323\\251\\025\\231s\\252\\\\J\\305Ps\\330\\365\\305\\000t\\272\\275\\343k\\027\\221XB\\371y\\231Q\\025[$\\347\\2515\\354\\332.\\231o\\243i\\026\\266P(\\013\\032\\000}\\317rk\\310\\276\\030i?i\\325\\337Q\\234\\026\\021ga=Kz\\327\\262+\\226l\\364\\240\\013\\3611\\351\\351V\\324n\\301\\025\\235\\021\\310\\034\\326\\204O\\362\\216E\\000J\\026\\225z\\364\\244\\335\\307\\024\\354c\\221@\\013\\234\\323\\263\\232`<\\360)w{P\\002\\367\\034Q\\317^\\242\\202h\\317\\313@\\r9\\357HN3\\315) \\212\\211\\330\\347\\212\\000l\\234\\367\\346\\251J\\333z\\232\\264\\315\\200j\\235\\307Pq@\\025d`:\\032\\205\\232\\245\\224t8\\346\\241 \\234\\373\\320\\0059\\2241\\306MF\\310\\n\\225\\353\\237Z\\236^\\270\\3075\\024\\2146\\373\\032\\000\\311\\274\\2044{\\270\\035\\363\\374\\253\\216\\324\\204n\\\\\\224\\034\\251\\312\\221\\320\\327es4e\\nd\\025c\\264z\\203\\\\\\027\\210&{y\\231\\362IS\\363/\\257\\257\\351@\\034\\325\\355\\273\\t\\314\\260\\253\\025=\\207Ph\\246=\\314\\236yh\\231\\260\\334\\2145\\024\\000\\267\\260\\375\\214\\006p7\\036p:/\\370\\232\\312\\336\\323L\\002\\214\\0268\\000Qqy-\\323\\226\\221\\263\\316y\\251\\264\\225\\337\\252\\332\\217\\372h(\\003\\334\\274\\\'g\\016\\223\\241\\304\\230U\\302\\006v?\\255A7\\304M\\"\\033\\267\\201\\030\\220\\274o\\317\\004\\373W5\\250__j\\302=\\023M\\334A\\000\\\\88\\037\\356\\347\\371\\325\\330\\276\\027D\\360|\\316\\212\\344`\\220I\\240\\r1\\361KJV+\\363px\\000WI\\243\\370\\327L\\324\\231B\\316\\253\\273\\200\\254y5\\347\\223|4\\236\\021\\225\\270\\214c\\257\\313\\367\\253<\\370v\\357O\\271R\\244\\200\\2479\\006\\200=\\372;\\204\\221@G\\006\\245\\317C\\232\\363\\037\\013\\353\\363DV\\031\\3130^2s^\\203oy\\034\\3439\\340\\320\\005\\360h,\\000\\357P\\211\\007c\\305<\\236\\347\\232\\000p;\\273\\364\\2449\\307\\007\\245B\\322\\0055\\237\\250\\352\\253gn\\354\\010\\335\\320s@\\027\\345\\270H\\207\\316\\352\\276\\344\\342\\262o|I\\247\\331g\\316\\270E\\300\\317Z\\363m{_\\276\\232\\340\\204i\\030\\366\\332+\\232\\223G\\325\\365I\\204\\263\\006,\\307\\030c\\322\\200=B\\357\\342>\\215\\006vLd#\\262\\214\\326L\\337\\023\\364\\361!-\\033l\\355\\201\\315qi\\360\\373Q.\\t%\\201\\351\\216\\200\\326\\245\\207\\3039\\311\\3374\\215\\274t\\\'\\374(\\003\\252\\265\\361\\356\\215r\\312\\257r\\020\\236\\306\\272\\010\\256\\241\\236!,,\\035\\010\\340\\203^e}\\340\\023\\n\\223\\261\\213\\003\\367\\223\\255g\\332j\\272\\267\\206.>m\\363Z\\217\\225\\220\\366\\240\\017T\\270\\234\\003\\3375FK\\236\\240`Vu\\236\\265o\\252\\331\\245\\315\\273eXr;\\203U.\\3570\\335(\\000\\325.\\214E\\231c\\335\\225\\344\\017\\342\\025\\304\\370\\222\\364I\\n4l_\\003\\031\\376\\360\\367\\367\\025\\257{~\\300\\375\\354\\257S\\354k\\216\\326.\\243\\226]\\3216\\033\\270\\316CP\\006ls\\274O\\224\\3069\\371H\\310\\037\\235\\025\\017\\037\\375j(\\002\\032\\273\\246\\022\\267\\321\\262\\214\\260\\316\\337\\256*\\216kOB]\\372\\274\\003\\031\\031\\311\\372b\\200=\\223\\302\\332|\\032}\\212\\310\\374\\312\\377\\00037\\326\\272t\\272\\007\\201^r\\372\\371\\203dQ\\236\\247\\004\\3243\\370\\345-\\031QX7\\030f\\\'\\200\\177\\n\\000\\365 \\206\\3410z\\223Yz\\226\\215&\\326oQ\\315y\\342\\374C\\215o\\221\\336y\\314D\\r\\376_\\312\\007\\323\\275Y\\377\\000\\204\\372\\322\\346\\351\\201\\270\\274\\2113\\362\\026\\223\\250\\307\\241\\343\\326\\200.Ne\\261\\270\\310\\007\\000\\372WC\\242k\\245\\230)=;\\023X\\021_C\\254ea\\270[\\206\\003$\\343k/\\324w\\250|\\251-&\\334\\231\\340\\363\\305\\000z\\315\\235\\302\\314\\231\\004sW\\310!s\\236\\265\\315\\370js4+\\237\\316\\272Y~X\\375q@\\031\\327s\\210\\262A\\351\\\\\\236\\2538\\234\\341\\316@<\\001[Z\\224\\340\\006$\\376\\025\\306\\335\\\\\\031f(9\\240\\002\\025\\217\\314\\312\\307\\320\\365\\002\\266-\\"g#\\344\\343\\351Y\\261\\315\\r\\244`\\277\\007\\323\\031\\346\\2337\\211\\\'\\264\\031\\021C\\002u\\335p\\370\\375\\005\\000v\\020)E\\031^\\235\\252S0\\0305\\300\\311\\343k\\225\\212I~\\323dc\\214)%Q\\261\\317\\276j\\025\\361\\344>c+\\354r:\\264\\017\\237\\320\\320\\007\\241<\\210\\337\\210\\357Xz\\266\\231k{\\023#D\\231n\\370\\254\\253/\\024[_\\251he\\014\\243\\257\\265^\\376\\322\\216U\\371\\\\\\037\\306\\200<\\372\\346\\326_\\njd\\306\\316\\326\\222\\237\\233\\330\\325\\313\\213\\360\\321\\356V\\030<\\203Z\\372\\3641_\\332\\272\\267$\\202\\001\\035Eq\\0214\\220\\333\\233yO*p>\\224\\001WV\\272|\\374\\2140G\\"\\260\\035\\211bOS[\\2271\\006L\\267^\\325\\227<\\014\\024\\222=\\350\\002\\256h\\244\\242\\200\\021\\3431\\271Rs\\203[\\036\\031\\266\\226\\343Qo$\\002\\352\\204\\200{\\326k\\220\\343\\236\\243\\371Wc\\360\\336\\317\\355\\032\\274\\255\\214\\205Q@\\030\\372\\325\\304\\221H\\312\\021\\342byF\\0042\\236\\377\\000\\205c[[Kw:\\306\\210\\356I\\347h\\316\\005{\\256\\271\\243G\\251D\\366\\361X\\2031\\340J\\313\\362\\250\\365\\367\\247h^\\031]\\n\\315a\\206$bG\\317&\\316X\\320\\007\\221\\352>\\026\\274Ia6v\\256\\321\\310\\240\\000:\\206\\367\\253#\\300^\\"\\220+\\0356vR\\200)\\310\\340\\373\\347\\265{\\2641o\\330\\246\\331>NG\\313\\322\\264\\225.\\\'M\\245B\\363\\351@\\036Q\\\'\\201\\256t\\333{y\\364\\206\\232;\\270\\243\\006A\\267\\211\\030\\016\\303\\353]u\\235\\215\\305\\377\\000\\207\\342\\270\\276\\265\\362/TbU\\365\\367\\256\\276;R\\000\\334rEA\\250\\355\\216\\335\\300<\\343\\237z\\000\\316\\360\\314%\\006\\010\\357[\\367\\317\\262,\\017\\312\\251hv\\306(A\\365\\251\\2657\\371v\\320\\007+\\252\\273\\2630N\\265\\026\\217\\240\\033\\200\\322\\271\\307\\275Z\\232\\"\\362\\217C[\\272>\\321j` \\014\\346\\200<\\307\\304~$\\264\\323\\356Z\\316\\3126\\236\\355N\\001U\\310\\007\\353Qi\\337\\017\\365\\rv\\321\\365\\rN\\354\\215\\310\\305`L\\023\\234w&\\2756m\\006\\331\\030\\225\\265\\2079\\316\\340\\2035\\037\\331!R\\240\\240M\\277\\335\\342\\200>b!\\343\\027\\021\\3102W\\013\\206r\\n\\220};\\324R,\\221\\025\\334@\\334\\241\\206\\030\\036?\\n\\367\\253\\337\\006\\350W\\006`\\360\\306\\246^X\\205\\347>\\271\\256~\\177\\207\\332Kp\\234v8\\034\\375E\\000yu\\246\\253sf\\341\\342|0\\353\\236\\343\\320\\372\\327M\\242j\\315\\3441yX\\314\\355\\300=\\251u_\\207w\\360>\\3759^\\342>\\352F\\030\\177\\215Y\\321|7u\\001\\204\\311l\\353:\\311\\226\\363T\\200\\007\\265\\000t\\221\\244\\262\\333\\006*@\\"\\261.\\264\\360\\362\\226a\\217J\\364[]9V\\313.0q\\323\\025\\313\\353H\\250\\330\\000~\\024\\001\\305\\337\\300\\253\\036\\007P;U\\033\\273\\177*\\325$P\\013\\021\\223\\315j\\335:\\253\\355+\\222GAP\\334\\306\\222\\330\\034G\\363\\001\\307l\\320\\007\\"\\337x\\343\\326\\212V]\\254G\\241\\242\\200-\\317lcp\\303\\247Z\\364/\\205\\360\\005\\202\\346~\\357&\\321\\370\\n\\345\\257\\341X,\\313u%\\000\\317\\340k\\264\\370s\\031]\\0266\\035Y\\330\\376\\264\\001\\352\\326I\\033\\"\\356\\\\\\326\\232B\\244}\\321\\212\\315\\323\\301\\"\\266c\\031Z\\000\\214B\\243\\242\\216)J\\201V6\\344}*\\\'\\024\\001\\004\\207\\007\\247\\025\\203\\251\\312e\\224F\\243\\2775\\265t\\352\\220\\261$\\014\\016\\365\\207`\\246\\362\\351\\244# \\036(\\003r\\301Dv\\312=\\253;R\\220\\031\\017?\\205l\\010\\202F@\\364\\256gTvF>\\335(\\002\\263\\221\\346\\n\\323\\323\\376\\367N\\225\\315\\303xL\\300\\036y\\256\\237Le\\220\\016}\\350\\003]\\016\\341\\206\\024\\311-\\243q\\310\\372\\324\\301G\\245!\\\\s\\322\\2003\\246\\322\\255\\344\\004\\355\\025\\007\\366T\\013\\320sZ\\215\\320\\217J\\201\\370$\\320\\005Qf\\240\\0001\\365\\025\\024\\220\\242\\222@\\031\\251\\236}\\247\\031\\342\\250\\334\\335\\016I4\\001R\\376\\\\!\\031\\256\\023\\\\\\224e\\210\\346\\272}F\\360mc\\234`W\\237\\353w\\237+\\022\\324\\001\\211wq\\275\\213\\203\\323\\201\\212#\\235\\232\\035\\256\\273\\243+\\216:\\203Y\\023^\\035\\344g\\275Y\\264\\272\\362\\340U\\007\\234\\367\\035\\215\\000d\\\\\\200\\263\\270\\034\\340\\365\\365\\242\\226g\\01736\\321\\317j(\\003\\245\\361S\\254@F\\270\\034\\355\\037\\205v\\377\\000\\017v\\215\\026\\330\\036\\371?\\255y\\217\\210u\\024\\3245F1g\\312O\\225k\\321|\\007(\\032]\\262\\347\\267\\365\\240\\017]\\261\\n\\020`\\365\\255T\\340qXZl\\243n3\\316+\\\\J\\252\\271\\310\\034P\\005\\226uQ\\232\\315\\272\\324b\\213?0\\252\\367\\332\\200\\211\\033\\006\\261\\326\\316]E\\332G,\\023\\327\\326\\200\\027R\\325\\267\\302\\334\\341O\\025\\241\\240<-\\000!\\201\\317J\\362?\\031x\\212\\347A\\277\\223L(X\\201\\271\\030\\367\\006\\261t\\177\\210\\02762r\\314\\253\\237\\273\\234\\212\\000\\372VFA\\021\\344t\\2567[\\275\\267B\\353\\271Kt\\034\\327\\233]|S\\236H\\331\\025\\233$W%?\\214\\257f\\234\\310\\006\\356{\\232\\000\\365\\253u\\337.\\345\\025\\245m\\177\\366\\031\\201f\\342\\274\\237O\\361\\375\\305\\263\\001u\\006\\007\\250\\251\\365?\\034\\244\\361\\376\\340\\222O@(\\003\\336\\255\\265(\\247EepA\\025m$\\017\\320\\365\\257*\\360,\\272\\245\\376\\223\\366\\211I\\306\\357\\222\\272\\3305)\\355\\247\\362\\347R\\255\\333=\\350\\003\\253\\000\\021\\322\\252\\316\\241G\\2756\\332\\365fQ\\315\\023>T\\232\\000\\312\\271b\\271\\037\\344V\\035\\355\\307\\004V\\305\\353\\360Oz\\346\\257\\245\\000\\034\\342\\2002\\365+\\203\\214g\\250\\346\\270\\035n\\34030\\311\\300\\342\\272\\235Fs\\222Mp\\272\\264\\305\\230\\364\\240\\014\\263\\206#\\236\\264\\371\\\'\\371\\021A\\035\\016\\177:\\257\\223\\270\\232N\\364\\000\\346b\\3074U\\324\\261\\337\\246\\265\\321b\\010`\\000\\366\\242\\2003s\\236k\\321\\374\\027u\\215:\\334\\017\\341%O\\347^l+\\263\\360d\\245\\240\\226<\\375\\307\\007\\363\\377\\000\\365P\\007\\263\\351\\227\\300\\250\\311\\347\\025\\2515\\360Hr\\307\\363\\256KJ\\\'+\\357\\355[\\363\\333\\264\\266\\307\\035\\372\\032\\000\\316I\\344\\325u/%[\\367hr\\344\\036\\202\\272\\370\\212E\\010\\2165\\300\\003\\025\\205\\244\\330\\303\\246\\332\\223\\307\\230\\347s1\\352i\\367\\376\\"\\323\\264\\250\\032k\\313\\230\\341A\\375\\343\\311\\372P\\006W\\213\\374\\re\\342\\242\\222\\311#Cr\\203\\013*\\214\\361\\350}\\253\\313\\265\\317\\205\\332\\276\\225\\033On\\313w\\010\\376\\340\\303~U\\334\\\\\\374Z\\322\\225\\312[@\\362\\217\\357\\023\\2675=\\277\\304\\313I\\206^\\317\\344\\030\\316$\\024\\001\\343\\013\\246\\310\\271\\016\\245[\\241\\004r*h4\\271$\\177.$gs\\306\\024W\\264\\333\\352\\036\\016\\327\\245\\222Y\\255B\\314\\243s\\356^\\337\\205[\\267\\324\\274#\\246e\\255\\243Tb:\\210\\215\\000y}\\217\\303-f\\370y\\222\\005\\204\\036@s\\315ni\\337\\010\\245\\022\\253^^\\250\\214\\036V1\\311\\374\\353\\273\\377\\000\\204\\317CB\\000\\271\\333\\236r\\312kB\\327]\\323\\257\\0245\\275\\334NO`\\324\\001kK\\323\\340\\323l\\243\\265\\201\\002\\307\\030\\332\\242\\233\\252\\332\\307sl@\\0370\\345H\\353\\232\\177\\332\\224\\216\\032\\221\\245\\004u\\3104\\001\\211\\246\\337\\024c\\013\\237\\231N+`\\335\\202\\235A\\025\\316k6\\257mp\\267p)\\3018p)\\022\\361\\232.N\\r\\000]\\275\\272R\\017\\"\\271\\235J}\\303\\nFMK}t\\352\\016\\017\\342k\\032y\\031\\311$\\376T\\001\\233\\250\\022P\\201\\311\\355\\\\\\215\\362\\206\\007 dWYp\\254\\3753\\317j\\300\\324!\\332\\033\\201\\232\\000\\347[\\216)\\326\\320\\233\\211\\2261\\336\\222p\\025\\310\\024\\266\\263<\\022\\356NX\\214P\\007k\\243h\\277\\333Z\\315\\266\\211\\027\\372\\250\\3432\\316\\376\\234p?2(\\257@\\360\\026\\223\\026\\221\\2424\\316\\237\\351\\267{d\\225\\317Q\\334/\\345E\\000|\\376+\\245\\360e\\307\\225\\252I\\021<H\\237\\310\\3277\\232\\275\\244\\334\\375\\223T\\267\\233\\260p\\017\\320\\361@\\036\\353\\244\\220\\0311\\322\\273\\033xL\\221\\006\\035\\205q:K\\215\\250A\\353\\214W\\240iN\\257\\006\\334\\320\\0075\\254\\255\\3540H\\360F\\317\\267\\\'\\002\\276~\\361\\006\\257w\\253jr\\275\\303\\266\\325b\\252\\204\\375\\332\\372\\314\\304\\204\\036\\0075\\341\\337\\025<\\024\\260j\\203T\\323\\321Q\\\'\\037\\274\\214\\014\\r\\343\\277\\343@\\036qa\\241jZ\\225\\264\\327\\026\\226\\317$P\\214\\273\\016\\325\\320\\332|;\\361d\\206\\333\\313\\262u\\027+\\2712\\334c\\336\\272\\277\\205\\336#\\322\\364\\233;\\273MJx\\241/\\367\\226R\\007\\267z\\367\\013;\\273;\\264\\215\\255\\246\\212@\\027\\215\\214\\016\\007\\341@\\0374\\334x_\\304\\332\\r\\303%\\314\\027QH\\352q\\345|\\312\\376\\325V\\347\\303\\236/\\020\\255\\324\\232}\\312\\240\\030\\017\\267\\025\\365-\\305\\274S\\264~b\\253ml\\214\\323o\\0266\\265ulm\\307~\\224\\001\\362E\\325\\226\\275m\\t\\236\\342\\033\\225\\210pX\\216*\\240\\237R\\211D\\352fE^7\\250 \\017\\306\\276\\252\\327\\364\\353I\\364\\031\\255\\345\\2161\\031\\\\\\034\\201^{\\343\\213]#M\\360\\352i\\266\\220\\242\\3119X\\325\\207\\\\\\236\\246\\200<\\313K\\361\\366\\271\\247:\\206\\2717\\021\\016\\251\\\'\\247\\326\\275\\007E\\370\\211c\\250\\354\\216F0Jx*\\347\\277\\265y\\336\\263\\243Z\\330\\334\\332[\\2271\\202\\205\\234\\236\\244V~\\213\\241j\\032\\356\\244\\266\\332l.\\355\\273\\357\\364\\010=I\\240\\017\\240\\215\\3547P|\\254\\034\\021\\326\\251Kdb^\\001\\372\\325\\275\\037\\303?\\3316\\021D\\363\\031\\245\\n\\003\\261=O\\255^\\272\\211v\\355\\316q@\\034u\\364d\\251\\025\\232-\\t\\346\\272;\\270\\027q\\307j\\242\\361\\355R1@\\030\\023\\306\\0279\\256oU\\306\\327\\302\\223\\306q\\351]E\\351\\332H5\\305\\353W\\030,\\007Lc\\212\\000\\347\\347?\\274<\\346\\257\\370v\\320_k\\326\\220\\2666\\264\\203v};\\326[\\034\\234\\326\\327\\205\\031\\223^\\205\\301\\373\\240\\237\\322\\200=\\252\\363S\\216\\336U\\265\\214\\025M\\240\\223\\236\\364W%\\254\\335\\307\\r\\314/+\\223\\373\\275\\334t\\311\\242\\200<\\216\\224\\034RQ@\\036\\271\\340\\335T^\\351\\321\\2536dN\\010\\372W\\247\\350\\327\\253\\201\\333\\327\\025\\363\\237\\205\\365f\\323\\265\\025R\\330\\216N\\277Z\\366}\\022\\377\\000s#\\003\\303s@\\036\\220\\256\\031{r+\\220\\361\\325\\217\\333\\264Y\\325rdE.\\200z\\212\\350\\255&\\337\\030\\365\\252\\272\\242\\357\\211\\201\\031\\355@\\0372H\\366WW\\031\\2342\\311\\234\\0208\\255\\255;L\\236/\\336\\350\\272\\235\\345\\264\\354061\\371\\275\\270\\253\\3762\\360d\\242\\362K\\35557\\0079x\\307\\\\\\372\\212\\347t\\275GS\\321\\256A\\362\\244F^\\205\\201\\030\\240\\016\\204j\\177\\020t\\373\\230\\330jwnS;|\\307\\3349\\366j\\237P\\327\\274q\\251\\351\\362[_j\\350\\220H0\\352\\252\\001#\\352\\005^\\203\\306\\0272K\\r\\304\\3067eS\\234\\340\\326F\\275\\343\\031o\\025\\221\\2216\\360\\000E\\003\\247\\320P\\006n\\241\\252\\370\\210X\\233\\013\\255vy-\\3602\\214\\304\\344u\\353\\326\\2625\\035F\\366\\365\\340\\373f\\242\\363\\030T\\010\\311\\352\\005K\\366\\035GW`-\\355&r{\\355\\300\\307\\326\\272\\215\\017\\341\\314\\222:\\313\\2521\\307\\374\\363S\\374\\315\\000d\\350\\332E\\357\\212\\365$\\226R\\356\\027\\001\\344\\220q\\201\\330W\\271xoA\\264\\321\\254\\226+h\\202\\223\\367\\237\\034\\261\\252ZF\\223oa\\002G\\014A\\025G\\000\\016\\225\\321\\300\\307!E\\000X\\333\\234\\344t\\254\\333\\250\\302\\263\\036+Wn\\330\\311j\\310\\276\\225y\\307z\\000\\307\\225\\003\\312y\\342\\251^ U8\\0305x\\261\\031=\\253/Q\\272TS\\316O\\265\\000r\\372\\274\\253\\031c\\221\\221^u\\251]\\031\\245+\\236\\225\\324x\\233P*\\215\\317,{W\\022\\314Y\\211=\\350\\001\\246\\265\\264\\t\\326\\336\\361\\244l\\234\\251Q\\217z\\3115n\\300\\220de\\\'r\\000\\343\\352\\r\\000tZ\\315\\324\\227VV\\227\\n\\331\\300(\\331\\365\\242\\253\\3463\\031\\266\\220\\200\\214w\\251\\3169\\357E\\000r\\335h\\242\\2279\\000zP\\002\\202A\\004\\032\\364\\357\\005\\352\\222\\313\\014Ipp\\330\\312\\347\\270\\365\\2570\\357Z\\232V\\2555\\206\\241\\025\\301b@\\300#=\\250\\003\\351\\215.\\340<K\\223Z7\\021\\231a#\\326\\270\\215\\003WK\\210cpr\\010\\007\\212\\354\\255\\256<\\305\\344\\346\\2009\\313\\313&26\\006k\\"\\343D\\023\\022\\036\\020A\\366\\256\\342[q$\\231\\355O\\212\\321\\030\\014\\257\\343@\\036n\\336\\020\\261\\221\\376{Q\\317\\267Z\\277k\\341K\\030@\\333g\\027\\035\\366\\327\\241\\213\\010\\217%\\006}i>\\304\\2128\\035z\\320\\007/\\026\\233\\024*\\004q\\205\\307\\240\\253\\021\\332\\205#\\217\\306\\266\\315\\230\\335\\234S\\205\\242\\343$P\\006zC\\264\\021\\216\\225n\\004\\331\\320sS:\\004\\007\\212\\256\\323\\004\\3474\\000\\373\\231\\210C\\315a\\\\\\266\\362w\\034{\\232\\236\\366\\354\\355!M`\\335\\337\\252d\\356\\372P\\003\\357.B+\\021\\315q\\332\\346\\256\\261+\\020y\\366\\246\\353:\\350\\215\\030\\207\\300\\035\\253\\317\\265-VK\\331[\\007\\3444\\001\\026\\243z\\327\\227,\\354x\\317\\025N\\223\\245\\\'S@\\001\\253\\232`f\\273\\001:\\3438\\317Z\\246MZ\\323]\\322\\365\\nu\\351@\\032m\\021\\273s\\000\\307\\313\\363\\017\\306\\212\\275\\004\\r\\r\\364\\222\\205\\371\\037\\246}h\\240\\016B\\224\\243\\005\\016G\\004\\340SsZ\\006h\\344\\320\\026-\\243\\315\\206\\340\\266{\\225e\\377\\000\\021\\372\\320\\003\\277\\263\\310\\265\\016A\\004&\\366\\374z\\017\\313\\237\\306\\251\\306\\233\\330\\214\\200\\000\\\'&\\264\\236\\355M\\274\\212:\\030\\366\\203\\370\\017\\360\\254\\326p|\\302\\000\\344\\342\\200;\\217\\004kf<\\332H\\374\\251\\371F{W\\255i\\272\\232\\262\\257<\\032\\371\\306\\326yl\\256c\\231r\\247\\257=\\305z6\\221\\342%1\\241\\337\\236\\231\\240\\017d\\216\\355_\\201Z08\\003\\232\\363\\213=}p\\016\\377\\000\\326\\267m\\365\\356\\203w\\353@\\035\\242\\277Jv29\\256Z=pn\\004\\2605|k\\211\\264|\\302\\2005\\330\\016\\375\\2151\\346D\\007\\245bM\\256 \\\'\\347\\030\\254\\373\\215uUX\\226\\037\\215\\000l\\\\\\336\\204\\r\\310\\305b]j+\\264\\220\\337\\255s\\227\\376!BH\\337\\\\\\346\\241\\342%\\215Nd\\343\\0353@\\0355\\376\\250\\250\\255\\363r}\\372\\327\\025\\254x\\201c\\334<\\317\\326\\260\\265\\037\\022\\274\\233\\2226\\3115\\316K$\\263\\266\\347\\311\\240\\0137\\332\\203\\335\\310r\\307mT\\003\\214\\346\\236\\210;\\216i\\316\\000\\007\\024\\001\\r\\024\\231\\245\\024\\000\\204\\325\\213\\031\\204\\027q\\310T\\020\\017qU\\315[\\261\\212Gf\\"=\\351\\2140\\316(\\003\\253k\\241k:\\272\\223\\345\\3122\\271\\301\\000\\372QX\\261\\316m\\024Gp\\331\\210\\363\\033u\\307\\261\\242\\2009\\354\\322\\251 \\020\\016\\001\\340\\321\\212\\231,\\356d\\031H$a\\354\\246\\200\\"9S\\212\\003e\\271\\351\\232\\234[J\\3213\\224#a\\332s\\353H\\266\\2638\\312!a\\3543@\\014\\232f\\232M\\314z\\000\\007\\260\\253\\026\\327OnA\\004\\200j\\365\\247\\206\\257n`i\\014l\\230\\344\\002:\\325\\033\\253I-d1\\310\\2440\\365\\240\\016\\202\\327Y\\221\\025K7\\312z\\232\\334\\265\\327N\\321\\206 W\\007\\003\\272\\020\\001\\343\\371\\326\\305\\263\\0020T\\202{\\251\\240\\016\\3115\\351U\\211/\\322\\244\\036\\"v\\377\\000\\226\\2035\\310\\0377\\202\\254\\307\\352*2.2:\\343\\324\\320\\007b\\376 \\220\\003\\227\\254\\273\\317\\021q\\203&\\177\\032\\347\\245Y\\316>r\\rW6N\\355\\363\\261\\317\\326\\200-]\\353\\354\\344\\204\\344\\326\\\\\\255ux\\3376pj\\362X\\242\\002B\\375j\\302\\306\\241@\\003\\247S@\\031\\261X\\004\\033\\237\\222(h\\325[\\247\\036\\225\\240\\370\\031\\007\\223Te<\\344\\363\\217z\\000\\211\\276^\\265ZF\\353\\374\\252G\\221\\216y5]\\271&\\200\\033J))h\\000\\247\\3073\\307\\235\\214@\\356=i\\206\\222\\2004\\"\\324\\314h\\020\\304\\254\\203\\370I\\310\\242\\263\\350\\240\\017y\\321\\376\\033\\350\\332n\\014\\361y\\362c\\253t\\256\\217\\376\\021\\335?f\\324\\201\\000\\372U\\370\\376\\351\\2531\\216\\007\\322\\200<\\263\\304\\336\\010\\363\\\'qk\\204I>\\366\\007\\353\\365\\251t\\237\\r\\332\\331\\230\\266\\303\\227\\214`q^\\207x\\212I\\310\\025V\\030\\243\\005\\210A\\232\\000\\307M8ua\\222{b\\270\\317\\032\\370dKm\\366\\270\\023\\014\\234\\266\\007Q^\\240\\31288\\355Yz\\252+\\332\\312\\030\\002\\n\\320\\007\\201G\\010\\316;\\326\\235\\254d\\036\\237\\235Gv\\252\\232\\224\\310\\243\\n\\034\\360*\\344\\003\\030\\305\\000ZX\\307\\007oZR\\213\\323\\212U<\\023A\\352h\\002\\002\\243\\323\\212\\0361\\216;\\366\\024\\366\\357H\\334\\n\\000\\210\\306v\\001\\330\\324\\022\\355^\\230\\253c\\221\\315U\\237\\225\\003\\265\\000R\\235\\200\\310\\007\\267Z\\250\\303 zU\\211G\\312\\247\\275Vj\\000\\202C\\200x\\252\\335\\352i\\017\\312j\\032\\000\\005\\024\\n;P\\002\\036\\264R\\236\\264P\\001E\\\'\\255\\024\\001\\377\\331"\n          image_info {\n            width: 183\n            height: 251\n            format: "JPEG"\n            color_mode: "UnknownColorMode"\n          }\n        }\n      }\n    }\n  }\n}\noutputs {\n  id: "7ab40bb98cad4133b490d7183095d9b5"\n  status {\n    code: SUCCESS\n    description: "Ok"\n  }\n  created_at {\n    seconds: 1700656970\n    nanos: 361626613\n  }\n  model {\n    id: "face-sentiment-recognition"\n    name: "face-sentiment"\n    created_at {\n      seconds: 1620837542\n      nanos: 718331000\n    }\n    modified_at {\n      seconds: 1652994708\n      nanos: 222496000\n    }\n    app_id: "main"\n    model_version {\n      id: "a5d7776f0c064a41b48c3ce039049f65"\n      created_at {\n        seconds: 1620837542\n        nanos: 812738000\n      }\n      status {\n        code: MODEL_TRAINED\n        description: "Model is trained and ready"\n      }\n      visibility {\n        gettable: PUBLIC\n      }\n      app_id: "main"\n      user_id: "clarifai"\n      metadata {\n      }\n    }\n    user_id: "clarifai"\n    model_type_id: "visual-classifier"\n    visibility {\n      gettable: PUBLIC\n    }\n    workflow_recommended {\n    }\n  }\n  data {\n    regions {\n      id: "6b5ea07bdaec9a8e48ff9424bf1f03b7"\n      region_info {\n        bounding_box {\n          top_row: 0.12857753\n          left_col: 0.261931\n          bottom_row: 0.637144744\n          right_col: 0.786355317\n        }\n      }\n      data {\n        concepts {\n          id: "ai_CrBPDCM6"\n          name: "happiness"\n          value: 0.999999821\n          app_id: "main"\n        }\n        concepts {\n          id: "ai_5fbLSP06"\n          name: "disgust"\n          value: 3.61372668e-006\n          app_id: "main"\n        }\n        concepts {\n          id: "ai_8PZvz0N1"\n          name: "fear"\n          value: 1.39605234e-007\n          app_id: "main"\n        }\n        concepts {\n          id: "ai_KdS5fmgb"\n          name: "sadness-contempt"\n          value: 4.07719938e-008\n          app_id: "main"\n        }\n        concepts {\n          id: "ai_59ZvTKz7"\n          name: "surprise"\n          value: 1.09932232e-008\n          app_id: "main"\n        }\n        concepts {\n          id: "ai_96KLdq72"\n          name: "anger"\n          value: 6.72241196e-009\n          app_id: "main"\n        }\n        concepts {\n          id: "ai_MqGSWdbN"\n          name: "neutral"\n          value: 4.21860102e-009\n          app_id: "main"\n        }\n      }\n    }\n  }\n}',cn='######################################################################################################\n# In this section, we set the user authentication, user and app ID, workflow ID, and the text \n# we want as an input. Change these strings to run your own example.\n######################################################################################################\n\n# Your PAT (Personal Access Token) can be found in the Account\'s Security section\nPAT = "YOUR_PAT_HERE"\nUSER_ID = "clarifai"\nAPP_ID = "main"\n# Change these to make your own predictions\nWORKFLOW_ID = "Language-Understanding"\nRAW_TEXT = "This is a test text for testing"\n# To use a hosted text file, assign the URL variable\n# TEXT_FILE_URL = "https://samples.clarifai.com/negative_sentence_12.txt"\n# Or, to use a local text file, assign the location variable\n# TEXT_FILE_LOCATION = "YOUR_TEXT_FILE_LOCATION_HERE"\n\n############################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n############################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (("authorization", "Key " + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\n# To use a local text file, uncomment the following lines\n# with open(TEXT_FILE_LOCATION, "rb") as f:\n#    file_bytes = f.read()\n\npost_workflow_results_response = stub.PostWorkflowResults(\n    service_pb2.PostWorkflowResultsRequest(\n        user_app_id=userDataObject,\n        workflow_id=WORKFLOW_ID,\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    text=resources_pb2.Text(\n                        raw=RAW_TEXT\n                        # url=TEXT_FILE_URL\n                        # raw=file_bytes\n                    )\n                )\n            )\n        ],\n    ),\n    metadata=metadata,\n)\nif post_workflow_results_response.status.code != status_code_pb2.SUCCESS:\n    print(post_workflow_results_response.status)\n    raise Exception(\n        "Post workflow results failed, status: "\n        + post_workflow_results_response.status.description\n    )\n\n# We\'ll get one WorkflowResult for each input we used above. Because of one input, we have here one WorkflowResult\nresults = post_workflow_results_response.results[0]\n\n# Each model we have in the workflow will produce one output.\nfor output in results.outputs:\n    model = output.model\n\n    print("Predicted concepts for the model `%s`" % model.id)\n    print(output.data)\n\n# Uncomment this line to print the raw output\n# print(results)\n',ln='\x3c!--index.html file--\x3e\n\n<script>\n  ////////////////////////////////////////////////////////////////////////////////////////////////////\n  // In this section, we set the user authentication, user and app ID, workflow ID, and the text \n  // we want as an input. Change these strings to run your own example.\n  ///////////////////////////////////////////////////////////////////////////////////////////////////\n\n  // Your PAT (Personal Access Token) can be found in the Account\'s Security section\n  const PAT = "YOUR_PAT_HERE";\n  const USER_ID = "clarifai";\n  const APP_ID = "main";\n  // Change these to make your own predictions\n  const WORKFLOW_ID = "Language-Understanding";\n  const RAW_TEXT = "This is a test text for testing";\n  // To use a hosted text file, assign the URL variable\n  // const TEXT_FILE_URL = "https://samples.clarifai.com/negative_sentence_12.txt";\n\n  ///////////////////////////////////////////////////////////////////////////////////\n  // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n  /////////////////////////////////////////////////////////////////////////////////// \n\n  const raw = JSON.stringify({\n    "user_app_id": {\n      "user_id": USER_ID,\n      "app_id": APP_ID\n    },\n    "inputs": [\n      {\n        "data": {\n          "text": {\n            "raw": RAW_TEXT\n            // "url": TEXT_FILE_URL\n          }\n        }\n      }\n    ]\n  });\n\n  const requestOptions = {\n    method: "POST",\n    headers: {\n      "Accept": "application/json",\n      "Authorization": "Key " + PAT\n    },\n    body: raw\n  };\n\n  fetch(`https://api.clarifai.com/v2/workflows/${WORKFLOW_ID}/results`, requestOptions)\n    .then(response => response.text())\n    .then(result => console.log(result))\n    .catch(error => console.log("error", error));\n\n<\/script>',un='//index.js file\n\n//////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, workflow ID, and the text \n// we want as an input. Change these strings to run your own example.\n/////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\nconst PAT = "YOUR_PAT_HERE";\nconst USER_ID = "clarifai";\nconst APP_ID = "main";\n// Change these to make your own predictions\nconst WORKFLOW_ID = "Language-Understanding";\nconst RAW_TEXT = "This is a test text for testing";\n// To use a hosted text file, assign the URL variable\n// const TEXT_FILE_URL = "https://samples.clarifai.com/negative_sentence_12.txt"\n// Or, to use a local text file, assign the location variable\n// TEXT_FILE_LOCATION = "YOUR_TEXT_FILE_LOCATION_HERE"\n\n/////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n/////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\n// To use a local text file, uncomment the following lines\n// const fs = require("fs");\n// const fileBytes = fs.readFileSync(TEXT_FILE_LOCATION);\n\nstub.PostWorkflowResults({\n    user_app_id: {\n        "user_id": USER_ID,\n        "app_id": APP_ID,\n    },\n    workflow_id: WORKFLOW_ID,\n    inputs: [{\n        data: {\n            text: {\n                raw: RAW_TEXT\n                // url: TEXT_FILE_URL,\n                // raw: fileBytes\n            }\n        }\n    }],\n},\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            throw new Error(\n                "Post workflow results failed, status: " + response.status.description\n            );\n        }\n\n        // We\'ll get one WorkflowResult for each input we used above. Because of one input, we have here \n        // one WorkflowResult\n        const results = response.results[0];\n\n        // Each model we have in the workflow will produce one output.\n        for (const output of results.outputs) {\n            const model = output.model;\n\n            console.log(`Predicted concepts for the model \'${model.id}\'`);\n            console.log(output.data);\n\n        }\n        // Uncomment this line to print the raw output\n        // console.log(results);\n    }\n);',dn='package com.clarifai.example;\n\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.grpc.api.status.StatusCode;\n\nimport com.google.protobuf.ByteString;\nimport java.io.File;\nimport java.io.IOException;\nimport java.nio.file.Files;\n\npublic class ClarifaiExample {\n\n    /////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, workflow ID, and the text \n    // we want as an input. Change these strings to run your own example.\n    /////////////////////////////////////////////////////////////////////////////////////////////////////\n    \n    //Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    static final String USER_ID = "clarifai";\n    static final String APP_ID = "main";\n    // Change these to make your own predictions\n    static final String WORKFLOW_ID = "Language-Understanding";\n    static final String RAW_TEXT = "This is a test text for testing";    \n    // To use a hosted text file, assign the URL variable\n    // static final String TEXT_FILE_URL = "https://samples.clarifai.com/negative_sentence_12.txt";\n    // Or, to use a local text file, assign the location variable\n    // static final String TEXT_FILE_LOCATION = "YOUR_TEXT_FILE_LOCATION_HERE";\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n    \n    public static void main(String[] args) throws IOException {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n                .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        PostWorkflowResultsResponse postWorkflowResultsResponse = stub.postWorkflowResults(\n                PostWorkflowResultsRequest.newBuilder()\n                        .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n                        .setWorkflowId(WORKFLOW_ID)\n                        .addInputs(\n                                Input.newBuilder().setData(\n                                        Data.newBuilder().setText(\n                                                Text.newBuilder().setRaw(RAW_TEXT)\n                                                // Text.newBuilder().setUrl(TEXT_FILE_URL)\n                                                // Text.newBuilder().setRawBytes(ByteString.copyFrom(Files.readAllBytes(\n                                                       // new File(TEXT_FILE_LOCATION).toPath()\n                                                //)))\n                                        )\n                                )\n                        )\n                        .build()\n        );\n\n        if (postWorkflowResultsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("Post workflow results failed, status: " + postWorkflowResultsResponse.getStatus());\n        }\n\n        // We\'ll get one WorkflowResult for each input we used above. Because of one input, we have here\n        // one WorkflowResult\n        WorkflowResult results = postWorkflowResultsResponse.getResults(0);\n\n        // Each model we have in the workflow will produce its output       \n        for (Output output : results.getOutputsList()) {\n            Model model = output.getModel();\n\n            System.out.println("Predicted concepts for the model \'" + model.getId() + "\'");\n            \n            System.out.println(output.getData());\n \n        }\n\n        // Uncomment this line to print the raw output\n        // System.out.println(results);\n    }\n\n}\n',pn='curl -X POST "https://api.clarifai.com/v2/users/clarifai/apps/main/workflows/Language-Understanding/results" \\\n  -H "authorization: Key YOUR_PAT_HERE" \\\n  -H "content-type: application/json" \\\n  -d \'{\n    "inputs": [\n        {\n          "data": {\n            "text": {\n              "raw": "This is a test text for testing"\n          }\n        }\n      }\n    ]\n}\'',vn='<?php\n\nrequire __DIR__ . "/vendor/autoload.php";\n\n/////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, workflow ID, and the text \n// we want as an input. Change these strings to run your own example.\n/////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\n$PAT = "YOUR_PAT_HERE";\n$USER_ID = "clarifai";\n$APP_ID = "main";\n// Change these to make your own predictions\n$WORKFLOW_ID = "Language-Understanding";\n$RAW_TEXT = "This is a test text for testing";\n// To use a hosted text file, assign the URL variable\n// $TEXT_FILE_URL = "https://samples.clarifai.com/negative_sentence_12.txt";\n// Or, to use a local text file, assign the location variable\n// $TEXT_FILE_LOCATION = "YOUR_TEXT_FILE_LOCATION_HERE";\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Text;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\PostWorkflowResultsRequest;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ["Authorization" => ["Key " . $PAT]];\n\n$userDataObject = new UserAppIDSet([\n    "user_id" => $USER_ID,\n    "app_id" => $APP_ID,\n]);\n\n// To use a local text file, uncomment the following lines\n// $textData = file_get_contents($TEXT_FILE_LOCATION); \n\n// Let\'s make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client\n    ->PostWorkflowResults(\n        // The request object carries the request along with the request status and other metadata related to the request itself\n        new PostWorkflowResultsRequest([\n            "user_app_id" => $userDataObject,\n            "workflow_id" => $WORKFLOW_ID,\n            "inputs" => [\n                new Input([\n                    // The Input object wraps the Data object in order to meet the API specification\n                    "data" => new Data([\n                        // The Data object is constructed around the Image object. It offers a container that has additional image independent\n                        // metadata. In this particular use case, no other metadata is needed to be specified\n                        "text" => new Text([\n                            // In the Clarifai platform, a text is defined by a special Text object\n                            "raw" => $RAW_TEXT\n                            // "url" => $TEXT_FILE_URL \n                            // "raw" => $textData \n                        ]),\n                    ]),\n                ]),\n            ],\n        ]),\n        $metadata\n    )\n    ->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception("Error: {$status->details}");\n}\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure\n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    throw new Exception(\n        "Failure response: " .\n            $response->getStatus()->getDescription() .\n            " " .\n            $response->getStatus()->getDetails()\n    );\n}\n\n// We\'ll get one WorkflowResult for each input we used above. Because of one input, we have here one WorkflowResult\n$results = $response->getResults()[0];\n\n// Each model we have in the workflow will produce one output\nforeach ($results->getOutputs() as $output) {\n    $model = $output->getModel();\n\n    echo "Predicted concepts for the model \'{$model->getId()}\'" . "\\n";\n\n    $convertDataToJSONString = $output->getData()->serializeToJsonString();\n\n    echo  $convertDataToJSONString . "\\n";\n}\n\n// Uncomment this line to print the raw output\n// print_r($results);\n\n?>',fn='Predicted concepts for the model `multilingual-text-embedding`\nembeddings {\n  vector: 0.0255603846\n  vector: 0.0256410129\n  vector: 0.0107929539\n  vector: 0.030718796\n  vector: 0.0150386961\n  vector: -0.0226166341\n  vector: -0.0263089519\n  vector: 0.00326520158\n  vector: 0.0102104917\n  vector: -0.044386141\n  vector: -0.0195556302\n  vector: 0.00893216766\n  vector: 0.015003683\n  vector: -0.0130465208\n  vector: -0.0104514267\n  vector: 0.0217112433\n  vector: -0.0362542719\n  vector: -0.00232617464\n  vector: 0.0566842183\n  vector: -0.0483675748\n  vector: -0.010495048\n  vector: 0.0236451961\n  vector: -0.0139718978\n  vector: -0.0204272885\n  vector: 0.00862451177\n  vector: 0.0366721936\n  vector: 0.0400005206\n  vector: -0.0113559328\n  vector: -0.0424929187\n  vector: -0.0034513874\n  vector: -0.0322748311\n  vector: -0.00985202659\n  vector: -0.012448323\n  vector: -0.0394972041\n  vector: 0.0184240248\n  vector: 0.000438908\n  vector: -0.0020233281\n  vector: 0.0129011944\n  vector: 0.0250889361\n  vector: -0.0152506949\n  vector: 0.0270063598\n  vector: -0.0264613442\n  vector: 0.0258420501\n  vector: -0.0289224759\n  vector: -0.0264949258\n  vector: 0.0334232263\n  vector: -0.0338085629\n  vector: 0.0334763639\n  vector: -0.03762725\n  vector: 0.00782276411\n  vector: -0.00409373501\n  vector: 0.0120968325\n  vector: -0.0106019555\n  vector: -0.0269890483\n  vector: -0.072263509\n  vector: -0.012660739\n  vector: 0.0308089778\n  vector: 0.0298142675\n  vector: -0.0146902641\n  vector: 0.0218613446\n  vector: 0.0323576666\n  vector: -0.0436149947\n  vector: -0.0174307581\n  vector: 0.084372744\n  vector: 0.0181334354\n  vector: -0.0199910812\n  vector: 0.0123974094\n  vector: -0.0193333048\n  vector: 0.0339736864\n  vector: -0.0253562946\n  vector: 0.00288021402\n  vector: 0.0169731714\n  vector: -0.0200157464\n  vector: -0.0637308881\n  vector: 7.40556279e-005\n  vector: 0.00873292703\n  vector: -0.0170422606\n  vector: 0.0415907614\n  vector: -0.00372848427\n  vector: 0.0341904387\n  vector: 0.0267014429\n  vector: -0.0209374689\n  vector: -0.0327630155\n  vector: -0.0457439\n  vector: 0.00402727\n  vector: -0.00840501208\n  vector: 0.0180109739\n  vector: -0.00936154835\n  vector: -0.0110226534\n  vector: -0.00436874479\n  vector: 0.0163043\n  vector: 0.0262424368\n  vector: 0.0101943593\n  vector: -0.0400694683\n  vector: -0.00884656888\n  vector: -0.0427878574\n  vector: -0.0641586557\n  vector: -0.0212010648\n  vector: -0.00159631309\n  vector: 0.0310680382\n  vector: -0.0647975504\n  vector: -0.023672644\n  vector: 0.0459937714\n  vector: -0.00774210179\n  vector: 0.000529117067\n  vector: -0.0292916577\n  vector: -0.0145822288\n  vector: 0.00338335894\n  vector: -0.0156141864\n  vector: -0.00935915578\n  vector: 0.0299793016\n  vector: -0.00355648622\n  vector: 0.0202946737\n  vector: 0.0302724876\n  vector: 0.00297537982\n  vector: 0.0380662605\n  vector: 0.0350826\n  vector: 0.0141671756\n  vector: 0.0307802558\n  vector: 0.0251820423\n  vector: -0.04314005\n  vector: 0.0967362\n  vector: 0.0179795\n  vector: -0.0144064706\n  vector: 0.0614442118\n  vector: 0.0418301858\n  vector: 0.0298902467\n  vector: -0.00762633048\n  vector: 0.00442519924\n  vector: 0.000885691727\n  vector: -0.0406515226\n  vector: -0.0188011918\n  vector: 0.0137273492\n  vector: 0.00622383412\n  vector: 0.0335491821\n  vector: 0.00737484824\n  vector: 0.0139906872\n  vector: 0.0109717203\n  vector: 0.010755497\n  vector: 0.0112457387\n  vector: -0.00598319899\n  vector: 0.0259225015\n  vector: -0.000439705298\n  vector: -0.017860774\n  vector: -0.0371661447\n  vector: -0.0182125829\n  vector: -0.000586743816\n  vector: 0.0030561774\n  vector: 0.00668949727\n  vector: -0.0123718204\n  vector: -0.0365500264\n  vector: 0.0134482831\n  vector: -0.0129413838\n  vector: -0.00557328854\n  vector: 0.0504806265\n  vector: 0.0707531124\n  vector: 0.0188564844\n  vector: 0.0097397631\n  vector: -0.0409301817\n  vector: 0.00957701914\n  vector: -0.0113784261\n  vector: 0.0362381637\n  vector: 0.00238611782\n  vector: -0.0190066174\n  vector: -0.0160514135\n  vector: -0.0437530763\n  vector: -0.00567471469\n  vector: -0.0242700893\n  vector: 0.0125929378\n  vector: -0.00250009913\n  vector: -0.0128744598\n  vector: -0.0602364838\n  vector: -0.0363118686\n  vector: 0.0310818329\n  vector: -0.0280554648\n  vector: 0.00686237263\n  vector: 0.051632829\n  vector: -0.0259241946\n  vector: 0.000656295859\n  vector: -0.0104174372\n  vector: 0.0143860672\n  vector: -0.0219465848\n  vector: -0.0635990351\n  vector: 0.00397105515\n  vector: -0.0171896238\n  vector: -0.000487698242\n  vector: 0.0557192415\n  vector: 0.0290173776\n  vector: -0.0216272268\n  vector: 0.0197027903\n  vector: 0.00293333107\n  vector: -0.0256508272\n  vector: 0.0145954657\n  vector: 0.0147802\n  vector: 0.0199410208\n  vector: 0.0362814479\n  vector: 0.0114512853\n  vector: 0.0516074747\n  vector: -0.00228457758\n  vector: -0.0247946437\n  vector: 0.0531231686\n  vector: 0.0412405729\n  vector: -0.00312332762\n  vector: -0.0384900719\n  vector: 0.0265060011\n  vector: 0.0188652594\n  vector: 0.0254785381\n  vector: -0.0120410575\n  vector: -0.055106923\n  vector: -0.041226916\n  vector: -0.0110674649\n  vector: -0.00518939504\n  vector: 0.0258076955\n  vector: -4.47058883e-005\n  vector: 0.00584268896\n  vector: 0.0291903764\n  vector: 0.0544795282\n  vector: -0.0141743645\n  vector: 0.0383740328\n  vector: -0.0609933324\n  vector: 0.0196725428\n  vector: -0.0131683592\n  vector: 0.0928284079\n  vector: 0.0304285903\n  vector: -0.0431602523\n  vector: -0.000314288016\n  vector: 0.0481073223\n  vector: -0.03109896\n  vector: 0.0305516273\n  vector: -0.0531298704\n  vector: -0.0364271179\n  vector: 0.0249502454\n  vector: -0.035180334\n  vector: -0.00412273454\n  vector: 0.0286418777\n  vector: 0.00197291095\n  vector: -0.0143354721\n  vector: 0.0143140703\n  vector: 0.044354897\n  vector: 0.0567986146\n  vector: 0.0701035857\n  vector: 0.010885776\n  vector: -0.00677968934\n  vector: -0.0355549529\n  vector: 0.0214009341\n  vector: -0.0396741442\n  vector: 0.0010890587\n  vector: 0.0230288804\n  vector: 0.0160423983\n  vector: 0.00770209916\n  vector: -0.00134117063\n  vector: 0.00584157603\n  vector: -0.0506436527\n  vector: 0.0167286471\n  vector: -0.0331318229\n  vector: 0.0315938741\n  vector: -0.0301273353\n  vector: 0.00558987679\n  vector: 0.00451010419\n  vector: -0.0279915575\n  vector: -0.0209878609\n  vector: -0.0199963469\n  vector: -0.000246938725\n  vector: -0.0405185111\n  vector: 0.00242121704\n  vector: 0.00407472113\n  vector: -0.00968741067\n  vector: -0.038693171\n  vector: -0.0187584516\n  vector: 0.0341708027\n  vector: -0.0132718869\n  vector: 0.00288945087\n  vector: 0.0163044799\n  vector: -0.0546093583\n  vector: -0.0190316942\n  vector: 0.0211417172\n  vector: 0.0336544812\n  vector: -0.00693607656\n  vector: 0.0127230892\n  vector: 0.0135776838\n  vector: -0.0617968142\n  vector: -0.00195178052\n  vector: 0.0471284464\n  vector: -0.0395634659\n  vector: 0.0295374077\n  vector: -0.0362583138\n  vector: 0.0599530078\n  vector: -0.0492380969\n  vector: 0.0120025882\n  vector: -0.0152681777\n  vector: 0.00973533373\n  vector: 0.0677181706\n  vector: 0.0222762376\n  vector: -0.0271166284\n  vector: 0.0360873379\n  vector: -0.0109270047\n  vector: 0.00856078602\n  vector: 0.0292164441\n  vector: -0.0243707\n  vector: -0.0110178702\n  vector: 0.0125099961\n  vector: -0.000995316426\n  vector: 0.00649002707\n  vector: -0.0221561305\n  vector: 0.0468515716\n  vector: 0.00950729568\n  vector: -0.0216504335\n  vector: -0.0135809742\n  vector: -0.587396502\n  vector: -0.0500078537\n  vector: 0.0219854936\n  vector: -0.01205255\n  vector: 0.0175891258\n  vector: -0.0113249039\n  vector: 0.0286584757\n  vector: 0.00650032377\n  vector: -0.00326811569\n  vector: 0.022440603\n  vector: -0.0182182249\n  vector: 0.0761179253\n  vector: -0.0167741235\n  vector: 0.0361539535\n  vector: -0.0113675604\n  vector: -0.00895621907\n  vector: -0.0258418024\n  vector: 0.0181844737\n  vector: 0.0115667935\n  vector: 0.0318501815\n  vector: 0.00093767792\n  vector: 0.00287587265\n  vector: 0.0216217488\n  vector: 0.0230962206\n  vector: -0.0256820396\n  vector: 0.0218300279\n  vector: -0.0249342\n  vector: 0.0561292693\n  vector: -0.0231751911\n  vector: 0.016221609\n  vector: 0.00636604\n  vector: -0.0150087075\n  vector: -0.00729617896\n  vector: 0.031151155\n  vector: -0.0164434742\n  vector: -0.00316140847\n  vector: 0.0480747186\n  vector: -0.00684076874\n  vector: -0.0348987654\n  vector: 0.0300664771\n  vector: -0.0080304658\n  vector: -0.00318912556\n  vector: 0.0232989155\n  vector: -0.0248846486\n  vector: -0.074139826\n  vector: -0.017566992\n  vector: 0.00341211492\n  vector: -0.00683950912\n  vector: 0.0451601073\n  vector: -0.0306811985\n  vector: -0.0506822355\n  vector: 0.0148111014\n  vector: -0.0168116689\n  vector: 0.0245187134\n  vector: -0.0385940857\n  vector: -0.00666388543\n  vector: 0.00905002933\n  vector: 0.0306002442\n  vector: 0.0104561793\n  vector: -0.0616600476\n  vector: -0.000910751347\n  vector: -0.0383770578\n  vector: -0.0319519192\n  vector: -0.0191159304\n  vector: 0.00798001699\n  vector: 0.0162921902\n  vector: -0.00722851697\n  vector: -0.00984974951\n  vector: 0.00443824846\n  vector: 0.0458599813\n  vector: 0.00428895839\n  vector: -0.016468009\n  vector: 0.00984302443\n  vector: -0.0848148167\n  vector: 0.0138867963\n  vector: -0.0347199\n  vector: -0.012926463\n  vector: -0.0195842963\n  vector: 0.00821305159\n  vector: 0.0116449213\n  vector: -0.0596969128\n  vector: 0.0139209842\n  vector: 0.0124949655\n  vector: 0.0564839914\n  vector: 0.073116228\n  vector: 0.0054912949\n  vector: 0.008373552\n  vector: 0.049428314\n  vector: 0.00387572078\n  vector: 0.00328427833\n  vector: 0.0574303158\n  vector: -0.0401720814\n  vector: 0.00543978252\n  vector: 0.0125507237\n  vector: -0.0223911144\n  vector: 0.0530512854\n  vector: -0.0193119962\n  vector: -0.056736242\n  vector: 0.028417591\n  vector: 0.0039758184\n  vector: 0.0603403524\n  vector: 0.00940856431\n  vector: -0.00775589608\n  vector: -0.00202059839\n  vector: -0.0325067\n  vector: 0.00763982581\n  vector: -0.0420214422\n  vector: -0.0248172414\n  vector: 0.0121375453\n  vector: -0.0279675424\n  vector: -0.0139899682\n  vector: 0.0103907259\n  vector: 0.00846104417\n  vector: 0.000477065099\n  vector: -0.0172488894\n  vector: 0.0307512395\n  vector: -0.0159618128\n  vector: -0.0448177345\n  vector: -0.0378222652\n  vector: -0.00430452963\n  vector: -0.0221919119\n  vector: -0.0052908631\n  vector: -0.0408164859\n  vector: -0.0093395263\n  vector: -0.0526061207\n  vector: -0.0033937979\n  vector: -0.0111951698\n  vector: -0.018287722\n  vector: 0.0332923383\n  vector: 0.0202295\n  vector: -0.0026297241\n  vector: -0.0169538446\n  vector: -0.000453287736\n  vector: 0.0429562218\n  vector: -0.000680702738\n  vector: 0.0273506679\n  vector: -0.0683761761\n  vector: 0.0667696\n  vector: 0.0106630186\n  vector: -0.0298528746\n  vector: -0.000488598191\n  vector: -0.0440973416\n  vector: 0.0534471236\n  vector: -0.0181944631\n  vector: -0.00166536344\n  vector: -0.0655773953\n  vector: 0.0274203978\n  vector: 0.00751716364\n  vector: 0.012719023\n  vector: -0.0299365614\n  vector: 0.0668713\n  vector: 0.0176321361\n  vector: 0.00121316453\n  vector: 0.0211630538\n  vector: -0.0457814448\n  vector: 0.0290366914\n  vector: 0.0472914241\n  vector: 1.481e-005\n  vector: -0.0201600771\n  vector: 0.0741583481\n  vector: -0.0177378468\n  vector: -0.018208934\n  vector: -0.0424425118\n  vector: -0.0322192535\n  vector: 0.0212140493\n  vector: -0.0333384909\n  vector: -0.0089503089\n  vector: 0.00865053944\n  vector: -0.0114420308\n  vector: -0.0323484875\n  vector: -0.0319988914\n  vector: -0.0404753834\n  vector: -0.0135618644\n  vector: 0.0243302248\n  vector: -0.00248846621\n  vector: -0.00764315343\n  vector: 0.0407035202\n  vector: 0.0072440342\n  vector: -0.00446285773\n  vector: -0.00628219312\n  vector: 0.00204555667\n  vector: -0.0171564016\n  vector: 0.00325665134\n  vector: -0.0169214662\n  vector: -0.00940683484\n  vector: 0.0141208908\n  vector: 0.00711128581\n  vector: -0.0218882859\n  vector: -0.0365912281\n  vector: -0.0138345128\n  vector: 0.028417252\n  vector: -0.0632902831\n  vector: 0.0563294031\n  vector: -0.0139016444\n  vector: 0.0173191428\n  vector: 0.0109188249\n  vector: -0.00881743524\n  vector: 0.0335570648\n  vector: 0.0302902944\n  vector: 0.00306223263\n  vector: 0.0300811697\n  vector: 0.0165142342\n  vector: -0.0220398791\n  vector: -0.0190337524\n  vector: -0.0218247\n  vector: -0.00892979838\n  vector: 0.00296077109\n  vector: -0.00667882059\n  vector: 0.0369906649\n  vector: 0.00189560978\n  vector: 0.0496911258\n  vector: -0.0371349975\n  vector: -0.0232151151\n  vector: 0.0153016197\n  vector: -0.00322092674\n  vector: -0.0189977195\n  vector: -0.0241388399\n  vector: -0.0209848098\n  vector: -0.00240087532\n  vector: -0.0097488109\n  vector: 0.0277912915\n  vector: 0.0129486732\n  vector: -0.0418183915\n  vector: -0.0172175094\n  vector: 0.014063701\n  vector: 0.0179245677\n  vector: 0.0329503492\n  vector: -0.0451413542\n  vector: 0.0375629142\n  vector: -0.00336613436\n  vector: 0.0837872624\n  vector: 0.0155244442\n  vector: -0.0319327973\n  vector: 0.0177341402\n  vector: -0.0694914684\n  vector: -0.0230065882\n  vector: -0.0349770822\n  vector: -0.000637284247\n  vector: -0.0171240233\n  vector: -0.000385974447\n  vector: -0.0154445628\n  vector: -0.00813626312\n  vector: -0.0018971978\n  vector: -0.0101503106\n  vector: -0.0577919446\n  vector: 0.00862083118\n  vector: 0.0169755798\n  vector: -0.0585522167\n  vector: -0.0303873625\n  vector: 0.0278049447\n  vector: 0.000132163666\n  vector: 0.0184809174\n  vector: 0.0111429971\n  vector: 0.0186991747\n  vector: 0.0336764\n  vector: 0.00854624715\n  vector: 0.0118956529\n  vector: 5.09967458e-006\n  vector: -0.0374510773\n  vector: 0.0125189032\n  vector: 0.0146504\n  vector: 0.0372200981\n  vector: -0.0323317759\n  vector: 0.0184809044\n  vector: -0.0637908429\n  vector: 0.00348871434\n  vector: 0.0116535509\n  vector: -0.0170193538\n  vector: -0.00596837653\n  vector: 0.00763026299\n  vector: -0.0396812037\n  vector: 0.00393839693\n  vector: 0.0243215691\n  vector: 0.0736935437\n  vector: 0.0649354607\n  vector: -0.0693660229\n  vector: -0.0173935127\n  vector: 0.0645925924\n  vector: 0.0218239073\n  vector: 0.0443394184\n  vector: 0.00201784237\n  vector: 0.000589759089\n  vector: 0.0140056768\n  vector: 0.0208582152\n  vector: -0.0178501364\n  vector: 0.0198773723\n  vector: 0.0119620683\n  vector: -0.00607993035\n  vector: 0.0137936343\n  vector: 0.015486001\n  vector: 0.0217939913\n  vector: 0.000516918313\n  vector: -0.0155286901\n  vector: -0.0236418\n  vector: 0.00602063863\n  vector: -0.0117947338\n  vector: -0.0382487215\n  vector: -0.0253913198\n  vector: 0.0540902093\n  vector: -0.00069479784\n  vector: -0.0030842768\n  vector: 0.00678593758\n  vector: -0.0158268567\n  vector: -0.0301091801\n  vector: -0.047721222\n  vector: 0.0486889854\n  vector: -0.0031820375\n  vector: -0.00780140189\n  vector: -0.0473188572\n  vector: -0.0377694\n  vector: 0.00300174044\n  vector: 0.00725157047\n  vector: 0.0334912054\n  vector: -0.0190392211\n  vector: -0.00786152\n  vector: -0.00393901\n  vector: 0.0346906558\n  vector: -0.0048984047\n  vector: -0.0289619621\n  vector: -0.0132993627\n  vector: -0.0260517057\n  vector: -0.0194860194\n  vector: -0.00903460104\n  vector: -0.0720684156\n  vector: -0.0326718502\n  vector: -0.0178757478\n  vector: 0.0116493832\n  vector: -0.0249003451\n  vector: -0.012765849\n  vector: -0.0367143154\n  vector: 0.0253556\n  vector: -0.000750295934\n  vector: 0.00800141\n  vector: -0.0399938263\n  vector: -0.0156591497\n  vector: 0.00355092739\n  vector: 0.00527952937\n  vector: -0.00371489814\n  vector: 0.00979584455\n  vector: 0.0128817623\n  vector: -0.0257944819\n  vector: 0.00532698631\n  vector: -0.0054745716\n  vector: -0.0402425155\n  vector: 0.0023749\n  vector: -0.0191466119\n  vector: -0.000185457975\n  vector: 0.0123729743\n  vector: -0.0285796933\n  vector: 0.027248418\n  vector: -0.00682593649\n  vector: -0.027824128\n  vector: -0.0483085215\n  vector: 0.0509723\n  vector: 0.0150561249\n  vector: -0.0671003759\n  vector: -0.0302604195\n  vector: 0.0335228\n  vector: -0.00811030716\n  vector: -0.0476540774\n  vector: -0.0483691692\n  vector: 0.0257905629\n  vector: 0.0216702223\n  vector: 0.0756625161\n  vector: -0.0114407977\n  vector: 0.0250642728\n  vector: 0.0492128\n  vector: 0.0104585467\n  vector: 0.0354731493\n  vector: 0.0342974328\n  vector: -0.0315436497\n  vector: -0.0416372307\n  vector: 0.0251945127\n  vector: 0.0550534874\n  vector: -0.00769103458\n  vector: -0.0269156434\n  vector: 0.0629401803\n  vector: 0.00849406514\n  vector: 0.0277529527\n  vector: 0.00905152876\n  vector: -0.0214481559\n  vector: -0.0336390026\n  vector: 0.0172805414\n  vector: -0.0254873466\n  vector: 0.00181326456\n  vector: -0.00262851967\n  vector: 0.00212677\n  vector: 0.021096956\n  vector: -0.0310921967\n  vector: 0.0090319775\n  vector: -0.0244406015\n  vector: -0.00450366875\n  vector: 0.0024087145\n  vector: -0.021715302\n  vector: -0.0187266618\n  vector: 0.00639256975\n  vector: 0.00592296245\n  vector: -0.0126604829\n  vector: -0.00361264497\n  vector: 0.018905757\n  vector: 0.0180640183\n  vector: -0.0142810205\n  vector: 0.0201476328\n  vector: -0.0371763296\n  vector: -0.016901549\n  vector: 0.0173841435\n  vector: 0.0144714024\n  vector: 0.0371279158\n  vector: -0.0259519368\n  vector: -0.0135354148\n  vector: 0.0480304547\n  vector: 0.0241338\n  vector: 0.00995781645\n  vector: -0.0145506114\n  vector: -0.0132244434\n  vector: 0.0104087936\n  vector: -0.0019786309\n  vector: -0.010059624\n  vector: 0.0418220572\n  vector: 0.00906385668\n  vector: -0.0608912781\n  vector: 0.0269323979\n  vector: 0.0267148074\n  vector: 0.0114678359\n  vector: -0.00542484596\n  vector: 0.0605565161\n  vector: 0.00729974685\n  vector: 0.0284503475\n  vector: 0.000257256615\n  vector: -0.029183466\n  vector: 0.029569421\n  vector: 0.000915852608\n  vector: 0.0109864781\n  vector: 0.0160177927\n  vector: -0.0311278421\n  vector: -0.00568914367\n  vector: -0.045658119\n  vector: -0.0380534567\n  vector: 0.00769931311\n  vector: -0.0169286355\n  vector: -0.0414473452\n  vector: 0.0346236937\n  vector: 0.00468268059\n  vector: -0.000171230145\n  vector: -0.0555126853\n  vector: -0.0213408619\n  vector: -0.00562082976\n  vector: -0.042139709\n  vector: 0.00932554808\n  vector: 0.0544001274\n  vector: 0.0186972376\n  vector: 0.0127014797\n  vector: 0.0039651487\n  vector: 0.0317394622\n  vector: 0.00775914779\n  vector: -0.0223883521\n  vector: -0.0120635182\n  vector: 0.0196055751\n  vector: 0.0291586574\n  vector: 0.0337944\n  num_dimensions: 768\n}\n\nPredicted concepts for the model `multilingual-text-clustering`\nclusters {\n  id: "9_17"\n  projection: 0.210836634\n  projection: -0.248410583\n}\n',hn='status {\n  code: SUCCESS\n  description: "Ok"\n}\ninput {\n  id: "f58574ce6a304c39ba298ecb4f4eef5e"\n  data {\n    text {\n      raw: "This is a test text for testing"\n      url: "https://samples.clarifai.com/placeholder.gif"\n    }\n  }\n}\noutputs {\n  id: "0ff24540c786470e912984865c03efc0"\n  status {\n    code: SUCCESS\n    description: "Ok"\n  }\n  created_at {\n    seconds: 1700723375\n    nanos: 640229252\n  }\n  model {\n    id: "multilingual-text-embedding"\n    name: "multilingual-text-embedding"\n    created_at {\n      seconds: 1581694729\n      nanos: 522174000\n    }\n    modified_at {\n      seconds: 1655211303\n      nanos: 454205000\n    }\n    app_id: "main"\n    model_version {\n      id: "9b33adf15280465b857163ddaaacdcb1"\n      created_at {\n        seconds: 1606747915\n        nanos: 848030000\n      }\n      status {\n        code: MODEL_TRAINED\n        description: "Model is trained and ready"\n      }\n      visibility {\n        gettable: PUBLIC\n      }\n      app_id: "main"\n      user_id: "clarifai"\n      metadata {\n      }\n    }\n    user_id: "clarifai"\n    model_type_id: "text-embedder"\n    visibility {\n      gettable: PUBLIC\n    }\n    workflow_recommended {\n    }\n  }\n  data {\n    embeddings {\n      vector: 0.0255603846\n      vector: 0.0256410129\n      vector: 0.0107929539\n      vector: 0.030718796\n      vector: 0.0150386961\n      vector: -0.0226166341\n      vector: -0.0263089519\n      vector: 0.00326520158\n      vector: 0.0102104917\n      vector: -0.044386141\n      vector: -0.0195556302\n      vector: 0.00893216766\n      vector: 0.015003683\n      vector: -0.0130465208\n      vector: -0.0104514267\n      vector: 0.0217112433\n      vector: -0.0362542719\n      vector: -0.00232617464\n      vector: 0.0566842183\n      vector: -0.0483675748\n      vector: -0.010495048\n      vector: 0.0236451961\n      vector: -0.0139718978\n      vector: -0.0204272885\n      vector: 0.00862451177\n      vector: 0.0366721936\n      vector: 0.0400005206\n      vector: -0.0113559328\n      vector: -0.0424929187\n      vector: -0.0034513874\n      vector: -0.0322748311\n      vector: -0.00985202659\n      vector: -0.012448323\n      vector: -0.0394972041\n      vector: 0.0184240248\n      vector: 0.000438908\n      vector: -0.0020233281\n      vector: 0.0129011944\n      vector: 0.0250889361\n      vector: -0.0152506949\n      vector: 0.0270063598\n      vector: -0.0264613442\n      vector: 0.0258420501\n      vector: -0.0289224759\n      vector: -0.0264949258\n      vector: 0.0334232263\n      vector: -0.0338085629\n      vector: 0.0334763639\n      vector: -0.03762725\n      vector: 0.00782276411\n      vector: -0.00409373501\n      vector: 0.0120968325\n      vector: -0.0106019555\n      vector: -0.0269890483\n      vector: -0.072263509\n      vector: -0.012660739\n      vector: 0.0308089778\n      vector: 0.0298142675\n      vector: -0.0146902641\n      vector: 0.0218613446\n      vector: 0.0323576666\n      vector: -0.0436149947\n      vector: -0.0174307581\n      vector: 0.084372744\n      vector: 0.0181334354\n      vector: -0.0199910812\n      vector: 0.0123974094\n      vector: -0.0193333048\n      vector: 0.0339736864\n      vector: -0.0253562946\n      vector: 0.00288021402\n      vector: 0.0169731714\n      vector: -0.0200157464\n      vector: -0.0637308881\n      vector: 7.40556279e-005\n      vector: 0.00873292703\n      vector: -0.0170422606\n      vector: 0.0415907614\n      vector: -0.00372848427\n      vector: 0.0341904387\n      vector: 0.0267014429\n      vector: -0.0209374689\n      vector: -0.0327630155\n      vector: -0.0457439\n      vector: 0.00402727\n      vector: -0.00840501208\n      vector: 0.0180109739\n      vector: -0.00936154835\n      vector: -0.0110226534\n      vector: -0.00436874479\n      vector: 0.0163043\n      vector: 0.0262424368\n      vector: 0.0101943593\n      vector: -0.0400694683\n      vector: -0.00884656888\n      vector: -0.0427878574\n      vector: -0.0641586557\n      vector: -0.0212010648\n      vector: -0.00159631309\n      vector: 0.0310680382\n      vector: -0.0647975504\n      vector: -0.023672644\n      vector: 0.0459937714\n      vector: -0.00774210179\n      vector: 0.000529117067\n      vector: -0.0292916577\n      vector: -0.0145822288\n      vector: 0.00338335894\n      vector: -0.0156141864\n      vector: -0.00935915578\n      vector: 0.0299793016\n      vector: -0.00355648622\n      vector: 0.0202946737\n      vector: 0.0302724876\n      vector: 0.00297537982\n      vector: 0.0380662605\n      vector: 0.0350826\n      vector: 0.0141671756\n      vector: 0.0307802558\n      vector: 0.0251820423\n      vector: -0.04314005\n      vector: 0.0967362\n      vector: 0.0179795\n      vector: -0.0144064706\n      vector: 0.0614442118\n      vector: 0.0418301858\n      vector: 0.0298902467\n      vector: -0.00762633048\n      vector: 0.00442519924\n      vector: 0.000885691727\n      vector: -0.0406515226\n      vector: -0.0188011918\n      vector: 0.0137273492\n      vector: 0.00622383412\n      vector: 0.0335491821\n      vector: 0.00737484824\n      vector: 0.0139906872\n      vector: 0.0109717203\n      vector: 0.010755497\n      vector: 0.0112457387\n      vector: -0.00598319899\n      vector: 0.0259225015\n      vector: -0.000439705298\n      vector: -0.017860774\n      vector: -0.0371661447\n      vector: -0.0182125829\n      vector: -0.000586743816\n      vector: 0.0030561774\n      vector: 0.00668949727\n      vector: -0.0123718204\n      vector: -0.0365500264\n      vector: 0.0134482831\n      vector: -0.0129413838\n      vector: -0.00557328854\n      vector: 0.0504806265\n      vector: 0.0707531124\n      vector: 0.0188564844\n      vector: 0.0097397631\n      vector: -0.0409301817\n      vector: 0.00957701914\n      vector: -0.0113784261\n      vector: 0.0362381637\n      vector: 0.00238611782\n      vector: -0.0190066174\n      vector: -0.0160514135\n      vector: -0.0437530763\n      vector: -0.00567471469\n      vector: -0.0242700893\n      vector: 0.0125929378\n      vector: -0.00250009913\n      vector: -0.0128744598\n      vector: -0.0602364838\n      vector: -0.0363118686\n      vector: 0.0310818329\n      vector: -0.0280554648\n      vector: 0.00686237263\n      vector: 0.051632829\n      vector: -0.0259241946\n      vector: 0.000656295859\n      vector: -0.0104174372\n      vector: 0.0143860672\n      vector: -0.0219465848\n      vector: -0.0635990351\n      vector: 0.00397105515\n      vector: -0.0171896238\n      vector: -0.000487698242\n      vector: 0.0557192415\n      vector: 0.0290173776\n      vector: -0.0216272268\n      vector: 0.0197027903\n      vector: 0.00293333107\n      vector: -0.0256508272\n      vector: 0.0145954657\n      vector: 0.0147802\n      vector: 0.0199410208\n      vector: 0.0362814479\n      vector: 0.0114512853\n      vector: 0.0516074747\n      vector: -0.00228457758\n      vector: -0.0247946437\n      vector: 0.0531231686\n      vector: 0.0412405729\n      vector: -0.00312332762\n      vector: -0.0384900719\n      vector: 0.0265060011\n      vector: 0.0188652594\n      vector: 0.0254785381\n      vector: -0.0120410575\n      vector: -0.055106923\n      vector: -0.041226916\n      vector: -0.0110674649\n      vector: -0.00518939504\n      vector: 0.0258076955\n      vector: -4.47058883e-005\n      vector: 0.00584268896\n      vector: 0.0291903764\n      vector: 0.0544795282\n      vector: -0.0141743645\n      vector: 0.0383740328\n      vector: -0.0609933324\n      vector: 0.0196725428\n      vector: -0.0131683592\n      vector: 0.0928284079\n      vector: 0.0304285903\n      vector: -0.0431602523\n      vector: -0.000314288016\n      vector: 0.0481073223\n      vector: -0.03109896\n      vector: 0.0305516273\n      vector: -0.0531298704\n      vector: -0.0364271179\n      vector: 0.0249502454\n      vector: -0.035180334\n      vector: -0.00412273454\n      vector: 0.0286418777\n      vector: 0.00197291095\n      vector: -0.0143354721\n      vector: 0.0143140703\n      vector: 0.044354897\n      vector: 0.0567986146\n      vector: 0.0701035857\n      vector: 0.010885776\n      vector: -0.00677968934\n      vector: -0.0355549529\n      vector: 0.0214009341\n      vector: -0.0396741442\n      vector: 0.0010890587\n      vector: 0.0230288804\n      vector: 0.0160423983\n      vector: 0.00770209916\n      vector: -0.00134117063\n      vector: 0.00584157603\n      vector: -0.0506436527\n      vector: 0.0167286471\n      vector: -0.0331318229\n      vector: 0.0315938741\n      vector: -0.0301273353\n      vector: 0.00558987679\n      vector: 0.00451010419\n      vector: -0.0279915575\n      vector: -0.0209878609\n      vector: -0.0199963469\n      vector: -0.000246938725\n      vector: -0.0405185111\n      vector: 0.00242121704\n      vector: 0.00407472113\n      vector: -0.00968741067\n      vector: -0.038693171\n      vector: -0.0187584516\n      vector: 0.0341708027\n      vector: -0.0132718869\n      vector: 0.00288945087\n      vector: 0.0163044799\n      vector: -0.0546093583\n      vector: -0.0190316942\n      vector: 0.0211417172\n      vector: 0.0336544812\n      vector: -0.00693607656\n      vector: 0.0127230892\n      vector: 0.0135776838\n      vector: -0.0617968142\n      vector: -0.00195178052\n      vector: 0.0471284464\n      vector: -0.0395634659\n      vector: 0.0295374077\n      vector: -0.0362583138\n      vector: 0.0599530078\n      vector: -0.0492380969\n      vector: 0.0120025882\n      vector: -0.0152681777\n      vector: 0.00973533373\n      vector: 0.0677181706\n      vector: 0.0222762376\n      vector: -0.0271166284\n      vector: 0.0360873379\n      vector: -0.0109270047\n      vector: 0.00856078602\n      vector: 0.0292164441\n      vector: -0.0243707\n      vector: -0.0110178702\n      vector: 0.0125099961\n      vector: -0.000995316426\n      vector: 0.00649002707\n      vector: -0.0221561305\n      vector: 0.0468515716\n      vector: 0.00950729568\n      vector: -0.0216504335\n      vector: -0.0135809742\n      vector: -0.587396502\n      vector: -0.0500078537\n      vector: 0.0219854936\n      vector: -0.01205255\n      vector: 0.0175891258\n      vector: -0.0113249039\n      vector: 0.0286584757\n      vector: 0.00650032377\n      vector: -0.00326811569\n      vector: 0.022440603\n      vector: -0.0182182249\n      vector: 0.0761179253\n      vector: -0.0167741235\n      vector: 0.0361539535\n      vector: -0.0113675604\n      vector: -0.00895621907\n      vector: -0.0258418024\n      vector: 0.0181844737\n      vector: 0.0115667935\n      vector: 0.0318501815\n      vector: 0.00093767792\n      vector: 0.00287587265\n      vector: 0.0216217488\n      vector: 0.0230962206\n      vector: -0.0256820396\n      vector: 0.0218300279\n      vector: -0.0249342\n      vector: 0.0561292693\n      vector: -0.0231751911\n      vector: 0.016221609\n      vector: 0.00636604\n      vector: -0.0150087075\n      vector: -0.00729617896\n      vector: 0.031151155\n      vector: -0.0164434742\n      vector: -0.00316140847\n      vector: 0.0480747186\n      vector: -0.00684076874\n      vector: -0.0348987654\n      vector: 0.0300664771\n      vector: -0.0080304658\n      vector: -0.00318912556\n      vector: 0.0232989155\n      vector: -0.0248846486\n      vector: -0.074139826\n      vector: -0.017566992\n      vector: 0.00341211492\n      vector: -0.00683950912\n      vector: 0.0451601073\n      vector: -0.0306811985\n      vector: -0.0506822355\n      vector: 0.0148111014\n      vector: -0.0168116689\n      vector: 0.0245187134\n      vector: -0.0385940857\n      vector: -0.00666388543\n      vector: 0.00905002933\n      vector: 0.0306002442\n      vector: 0.0104561793\n      vector: -0.0616600476\n      vector: -0.000910751347\n      vector: -0.0383770578\n      vector: -0.0319519192\n      vector: -0.0191159304\n      vector: 0.00798001699\n      vector: 0.0162921902\n      vector: -0.00722851697\n      vector: -0.00984974951\n      vector: 0.00443824846\n      vector: 0.0458599813\n      vector: 0.00428895839\n      vector: -0.016468009\n      vector: 0.00984302443\n      vector: -0.0848148167\n      vector: 0.0138867963\n      vector: -0.0347199\n      vector: -0.012926463\n      vector: -0.0195842963\n      vector: 0.00821305159\n      vector: 0.0116449213\n      vector: -0.0596969128\n      vector: 0.0139209842\n      vector: 0.0124949655\n      vector: 0.0564839914\n      vector: 0.073116228\n      vector: 0.0054912949\n      vector: 0.008373552\n      vector: 0.049428314\n      vector: 0.00387572078\n      vector: 0.00328427833\n      vector: 0.0574303158\n      vector: -0.0401720814\n      vector: 0.00543978252\n      vector: 0.0125507237\n      vector: -0.0223911144\n      vector: 0.0530512854\n      vector: -0.0193119962\n      vector: -0.056736242\n      vector: 0.028417591\n      vector: 0.0039758184\n      vector: 0.0603403524\n      vector: 0.00940856431\n      vector: -0.00775589608\n      vector: -0.00202059839\n      vector: -0.0325067\n      vector: 0.00763982581\n      vector: -0.0420214422\n      vector: -0.0248172414\n      vector: 0.0121375453\n      vector: -0.0279675424\n      vector: -0.0139899682\n      vector: 0.0103907259\n      vector: 0.00846104417\n      vector: 0.000477065099\n      vector: -0.0172488894\n      vector: 0.0307512395\n      vector: -0.0159618128\n      vector: -0.0448177345\n      vector: -0.0378222652\n      vector: -0.00430452963\n      vector: -0.0221919119\n      vector: -0.0052908631\n      vector: -0.0408164859\n      vector: -0.0093395263\n      vector: -0.0526061207\n      vector: -0.0033937979\n      vector: -0.0111951698\n      vector: -0.018287722\n      vector: 0.0332923383\n      vector: 0.0202295\n      vector: -0.0026297241\n      vector: -0.0169538446\n      vector: -0.000453287736\n      vector: 0.0429562218\n      vector: -0.000680702738\n      vector: 0.0273506679\n      vector: -0.0683761761\n      vector: 0.0667696\n      vector: 0.0106630186\n      vector: -0.0298528746\n      vector: -0.000488598191\n      vector: -0.0440973416\n      vector: 0.0534471236\n      vector: -0.0181944631\n      vector: -0.00166536344\n      vector: -0.0655773953\n      vector: 0.0274203978\n      vector: 0.00751716364\n      vector: 0.012719023\n      vector: -0.0299365614\n      vector: 0.0668713\n      vector: 0.0176321361\n      vector: 0.00121316453\n      vector: 0.0211630538\n      vector: -0.0457814448\n      vector: 0.0290366914\n      vector: 0.0472914241\n      vector: 1.481e-005\n      vector: -0.0201600771\n      vector: 0.0741583481\n      vector: -0.0177378468\n      vector: -0.018208934\n      vector: -0.0424425118\n      vector: -0.0322192535\n      vector: 0.0212140493\n      vector: -0.0333384909\n      vector: -0.0089503089\n      vector: 0.00865053944\n      vector: -0.0114420308\n      vector: -0.0323484875\n      vector: -0.0319988914\n      vector: -0.0404753834\n      vector: -0.0135618644\n      vector: 0.0243302248\n      vector: -0.00248846621\n      vector: -0.00764315343\n      vector: 0.0407035202\n      vector: 0.0072440342\n      vector: -0.00446285773\n      vector: -0.00628219312\n      vector: 0.00204555667\n      vector: -0.0171564016\n      vector: 0.00325665134\n      vector: -0.0169214662\n      vector: -0.00940683484\n      vector: 0.0141208908\n      vector: 0.00711128581\n      vector: -0.0218882859\n      vector: -0.0365912281\n      vector: -0.0138345128\n      vector: 0.028417252\n      vector: -0.0632902831\n      vector: 0.0563294031\n      vector: -0.0139016444\n      vector: 0.0173191428\n      vector: 0.0109188249\n      vector: -0.00881743524\n      vector: 0.0335570648\n      vector: 0.0302902944\n      vector: 0.00306223263\n      vector: 0.0300811697\n      vector: 0.0165142342\n      vector: -0.0220398791\n      vector: -0.0190337524\n      vector: -0.0218247\n      vector: -0.00892979838\n      vector: 0.00296077109\n      vector: -0.00667882059\n      vector: 0.0369906649\n      vector: 0.00189560978\n      vector: 0.0496911258\n      vector: -0.0371349975\n      vector: -0.0232151151\n      vector: 0.0153016197\n      vector: -0.00322092674\n      vector: -0.0189977195\n      vector: -0.0241388399\n      vector: -0.0209848098\n      vector: -0.00240087532\n      vector: -0.0097488109\n      vector: 0.0277912915\n      vector: 0.0129486732\n      vector: -0.0418183915\n      vector: -0.0172175094\n      vector: 0.014063701\n      vector: 0.0179245677\n      vector: 0.0329503492\n      vector: -0.0451413542\n      vector: 0.0375629142\n      vector: -0.00336613436\n      vector: 0.0837872624\n      vector: 0.0155244442\n      vector: -0.0319327973\n      vector: 0.0177341402\n      vector: -0.0694914684\n      vector: -0.0230065882\n      vector: -0.0349770822\n      vector: -0.000637284247\n      vector: -0.0171240233\n      vector: -0.000385974447\n      vector: -0.0154445628\n      vector: -0.00813626312\n      vector: -0.0018971978\n      vector: -0.0101503106\n      vector: -0.0577919446\n      vector: 0.00862083118\n      vector: 0.0169755798\n      vector: -0.0585522167\n      vector: -0.0303873625\n      vector: 0.0278049447\n      vector: 0.000132163666\n      vector: 0.0184809174\n      vector: 0.0111429971\n      vector: 0.0186991747\n      vector: 0.0336764\n      vector: 0.00854624715\n      vector: 0.0118956529\n      vector: 5.09967458e-006\n      vector: -0.0374510773\n      vector: 0.0125189032\n      vector: 0.0146504\n      vector: 0.0372200981\n      vector: -0.0323317759\n      vector: 0.0184809044\n      vector: -0.0637908429\n      vector: 0.00348871434\n      vector: 0.0116535509\n      vector: -0.0170193538\n      vector: -0.00596837653\n      vector: 0.00763026299\n      vector: -0.0396812037\n      vector: 0.00393839693\n      vector: 0.0243215691\n      vector: 0.0736935437\n      vector: 0.0649354607\n      vector: -0.0693660229\n      vector: -0.0173935127\n      vector: 0.0645925924\n      vector: 0.0218239073\n      vector: 0.0443394184\n      vector: 0.00201784237\n      vector: 0.000589759089\n      vector: 0.0140056768\n      vector: 0.0208582152\n      vector: -0.0178501364\n      vector: 0.0198773723\n      vector: 0.0119620683\n      vector: -0.00607993035\n      vector: 0.0137936343\n      vector: 0.015486001\n      vector: 0.0217939913\n      vector: 0.000516918313\n      vector: -0.0155286901\n      vector: -0.0236418\n      vector: 0.00602063863\n      vector: -0.0117947338\n      vector: -0.0382487215\n      vector: -0.0253913198\n      vector: 0.0540902093\n      vector: -0.00069479784\n      vector: -0.0030842768\n      vector: 0.00678593758\n      vector: -0.0158268567\n      vector: -0.0301091801\n      vector: -0.047721222\n      vector: 0.0486889854\n      vector: -0.0031820375\n      vector: -0.00780140189\n      vector: -0.0473188572\n      vector: -0.0377694\n      vector: 0.00300174044\n      vector: 0.00725157047\n      vector: 0.0334912054\n      vector: -0.0190392211\n      vector: -0.00786152\n      vector: -0.00393901\n      vector: 0.0346906558\n      vector: -0.0048984047\n      vector: -0.0289619621\n      vector: -0.0132993627\n      vector: -0.0260517057\n      vector: -0.0194860194\n      vector: -0.00903460104\n      vector: -0.0720684156\n      vector: -0.0326718502\n      vector: -0.0178757478\n      vector: 0.0116493832\n      vector: -0.0249003451\n      vector: -0.012765849\n      vector: -0.0367143154\n      vector: 0.0253556\n      vector: -0.000750295934\n      vector: 0.00800141\n      vector: -0.0399938263\n      vector: -0.0156591497\n      vector: 0.00355092739\n      vector: 0.00527952937\n      vector: -0.00371489814\n      vector: 0.00979584455\n      vector: 0.0128817623\n      vector: -0.0257944819\n      vector: 0.00532698631\n      vector: -0.0054745716\n      vector: -0.0402425155\n      vector: 0.0023749\n      vector: -0.0191466119\n      vector: -0.000185457975\n      vector: 0.0123729743\n      vector: -0.0285796933\n      vector: 0.027248418\n      vector: -0.00682593649\n      vector: -0.027824128\n      vector: -0.0483085215\n      vector: 0.0509723\n      vector: 0.0150561249\n      vector: -0.0671003759\n      vector: -0.0302604195\n      vector: 0.0335228\n      vector: -0.00811030716\n      vector: -0.0476540774\n      vector: -0.0483691692\n      vector: 0.0257905629\n      vector: 0.0216702223\n      vector: 0.0756625161\n      vector: -0.0114407977\n      vector: 0.0250642728\n      vector: 0.0492128\n      vector: 0.0104585467\n      vector: 0.0354731493\n      vector: 0.0342974328\n      vector: -0.0315436497\n      vector: -0.0416372307\n      vector: 0.0251945127\n      vector: 0.0550534874\n      vector: -0.00769103458\n      vector: -0.0269156434\n      vector: 0.0629401803\n      vector: 0.00849406514\n      vector: 0.0277529527\n      vector: 0.00905152876\n      vector: -0.0214481559\n      vector: -0.0336390026\n      vector: 0.0172805414\n      vector: -0.0254873466\n      vector: 0.00181326456\n      vector: -0.00262851967\n      vector: 0.00212677\n      vector: 0.021096956\n      vector: -0.0310921967\n      vector: 0.0090319775\n      vector: -0.0244406015\n      vector: -0.00450366875\n      vector: 0.0024087145\n      vector: -0.021715302\n      vector: -0.0187266618\n      vector: 0.00639256975\n      vector: 0.00592296245\n      vector: -0.0126604829\n      vector: -0.00361264497\n      vector: 0.018905757\n      vector: 0.0180640183\n      vector: -0.0142810205\n      vector: 0.0201476328\n      vector: -0.0371763296\n      vector: -0.016901549\n      vector: 0.0173841435\n      vector: 0.0144714024\n      vector: 0.0371279158\n      vector: -0.0259519368\n      vector: -0.0135354148\n      vector: 0.0480304547\n      vector: 0.0241338\n      vector: 0.00995781645\n      vector: -0.0145506114\n      vector: -0.0132244434\n      vector: 0.0104087936\n      vector: -0.0019786309\n      vector: -0.010059624\n      vector: 0.0418220572\n      vector: 0.00906385668\n      vector: -0.0608912781\n      vector: 0.0269323979\n      vector: 0.0267148074\n      vector: 0.0114678359\n      vector: -0.00542484596\n      vector: 0.0605565161\n      vector: 0.00729974685\n      vector: 0.0284503475\n      vector: 0.000257256615\n      vector: -0.029183466\n      vector: 0.029569421\n      vector: 0.000915852608\n      vector: 0.0109864781\n      vector: 0.0160177927\n      vector: -0.0311278421\n      vector: -0.00568914367\n      vector: -0.045658119\n      vector: -0.0380534567\n      vector: 0.00769931311\n      vector: -0.0169286355\n      vector: -0.0414473452\n      vector: 0.0346236937\n      vector: 0.00468268059\n      vector: -0.000171230145\n      vector: -0.0555126853\n      vector: -0.0213408619\n      vector: -0.00562082976\n      vector: -0.042139709\n      vector: 0.00932554808\n      vector: 0.0544001274\n      vector: 0.0186972376\n      vector: 0.0127014797\n      vector: 0.0039651487\n      vector: 0.0317394622\n      vector: 0.00775914779\n      vector: -0.0223883521\n      vector: -0.0120635182\n      vector: 0.0196055751\n      vector: 0.0291586574\n      vector: 0.0337944\n      num_dimensions: 768\n    }\n  }\n}\noutputs {\n  id: "ce36cd3d9aa645338238341bfbdfcf45"\n  status {\n    code: SUCCESS\n    description: "Ok"\n  }\n  created_at {\n    seconds: 1700723375\n    nanos: 640238148\n  }\n  model {\n    id: "multilingual-text-clustering"\n    name: "multilingual-text-clustering"\n    created_at {\n      seconds: 1607379316\n      nanos: 936028000\n    }\n    modified_at {\n      seconds: 1657111032\n      nanos: 332505000\n    }\n    app_id: "main"\n    model_version {\n      id: "f3f0dbe5e9ec4072ae4aa2794021982b"\n      created_at {\n        seconds: 1607365607\n        nanos: 249885000\n      }\n      status {\n        code: MODEL_TRAINED\n        description: "Model is trained and ready"\n      }\n      visibility {\n        gettable: PUBLIC\n      }\n      app_id: "main"\n      user_id: "clarifai"\n      metadata {\n      }\n    }\n    user_id: "clarifai"\n    model_type_id: "clusterer"\n    visibility {\n      gettable: PUBLIC\n    }\n    workflow_recommended {\n    }\n  }\n  data {\n    clusters {\n      id: "9_17"\n      projection: 0.210836634\n      projection: -0.248410583\n    }\n  }\n}\n',mn='######################################################################################################\n# In this section, we set the user authentication, user and app ID, workflow ID, video input,\n# and sample_ms. Change these strings to run your own example.\n######################################################################################################\n\n# Your PAT (Personal Access Token) can be found in the Account\'s Security section\nPAT = "YOUR_PAT_HERE"\nUSER_ID = "YOUR_USER_ID_HERE"\nAPP_ID = "YOUR_APP_ID_HERE"\n# Change these to make your own predictions\nWORKFLOW_ID = "YOUR_WORKFLOW_ID_HERE"\nVIDEO_URL = "https://samples.clarifai.com/beer.mp4"\n# Or, to use a local video file, assign the location variable\n# VIDEO_FILE_LOCATION = "YOUR_VIDEO_FILE_LOCATION_HERE"\n# Change this to configure the FPS rate (If it\'s not configured, it defaults to 1 FPS) \nSAMPLE_MS = 500\n\n############################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n############################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (("authorization", "Key " + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\n# To use a local video file, uncomment the following lines\n# with open(VIDEO_FILE_LOCATION, "rb") as f:\n   # file_bytes = f.read()\n\npost_workflow_results_response = stub.PostWorkflowResults(\n    service_pb2.PostWorkflowResultsRequest(\n        user_app_id=userDataObject,\n        workflow_id=WORKFLOW_ID,\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    video=resources_pb2.Video(\n                        url=VIDEO_URL,\n                        # base64=file_bytes\n                    )\n                )\n            )\n        ],\n        output_config=resources_pb2.OutputConfig(\n            sample_ms=SAMPLE_MS\n        )\n    ),\n    metadata=metadata,\n)\nif post_workflow_results_response.status.code != status_code_pb2.SUCCESS:\n    print(post_workflow_results_response.status)\n    raise Exception(\n        "Post workflow results failed, status: "\n        + post_workflow_results_response.status.description\n    )\n\n# We\'ll get one WorkflowResult for each input we used above. Because of one input, we have here one WorkflowResult\nresults = post_workflow_results_response.results[0]\n\n# Uncomment this line to print the raw output\nprint(results)\n',wn='\x3c!--index.html file--\x3e\n\n<script>\n  ////////////////////////////////////////////////////////////////////////////////////////////////////\n  // In this section, we set the user authentication, user and app ID, workflow ID, video input,\n  // and sample_ms. Change these strings to run your own example.\n  ///////////////////////////////////////////////////////////////////////////////////////////////////\n\n  // Your PAT (Personal Access Token) can be found in the Account\'s Security section\n  const PAT = "YOUR_PAT_HERE";\n  const USER_ID = "YOUR_USER_ID_HERE";\n  const APP_ID = "YOUR_APP_ID_HERE";\n  // Change these to make your own predictions\n  const WORKFLOW_ID = "YOUR_WORKFLOW_ID_HERE";\n  const VIDEO_URL = "https://samples.clarifai.com/beer.mp4";\n  // Change this to configure the FPS rate (If it\'s not configured, it defaults to 1 FPS) \n  const SAMPLE_MS = 500;\n\n  ///////////////////////////////////////////////////////////////////////////////////\n  // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n  /////////////////////////////////////////////////////////////////////////////////// \n\n  const raw = JSON.stringify({\n    "user_app_id": {\n      "user_id": USER_ID,\n      "app_id": APP_ID\n    },\n    "inputs": [\n      {\n        "data": {\n          "video": {            \n            "url": VIDEO_URL\n          }\n        }\n      }\n    ],\n    "output_config": {\n      "sample_ms": SAMPLE_MS\n    }\n  });\n\n  const requestOptions = {\n    method: "POST",\n    headers: {\n      "Accept": "application/json",\n      "Authorization": "Key " + PAT\n    },\n    body: raw\n  };\n\n  fetch(`https://api.clarifai.com/v2/workflows/${WORKFLOW_ID}/results`, requestOptions)\n    .then(response => response.text())\n    .then(result => console.log(result))\n    .catch(error => console.log("error", error));\n\n<\/script>',gn='//index.js file\n\n//////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, workflow ID, video input,\n// and sample_ms. Change these strings to run your own example.\n/////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\nconst PAT = "YOUR_PAT_HERE";\nconst USER_ID = "YOUR_USER_ID_HERE";\nconst APP_ID = "YOUR_APP_ID_HERE";\n// Change these to make your own predictions\nconst WORKFLOW_ID = "YOUR_WORKFLOW_ID_HERE";\nconst VIDEO_URL = "https://samples.clarifai.com/beer.mp4";\n// Or, to use a local video file, assign the location variable\n// const VIDEO_FILE_LOCATION = "YOUR_VIDEO_FILE_LOCATION_HERE"\n// Change this to configure the FPS rate (If it\'s not configured, it defaults to 1 FPS) \nconst SAMPLE_MS = 500;\n\n/////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n/////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\n// To use a local text file, uncomment the following lines\n// const fs = require("fs");\n// const videoBytes = fs.readFileSync(VIDEO_FILE_LOCATION);\n\nstub.PostWorkflowResults({\n    user_app_id: {\n        "user_id": USER_ID,\n        "app_id": APP_ID,\n    },\n    workflow_id: WORKFLOW_ID,\n    inputs: [{\n        data: {\n            video: {\n                url: VIDEO_URL,\n                // base64: videoBytes\n            }\n        }\n    }],\n    output_config: {\n        sample_ms: SAMPLE_MS\n      }  \n},\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            throw new Error(\n                "Post workflow results failed, status: " + response.status.description\n            );\n        }\n\n        // We\'ll get one WorkflowResult for each input we used above. Because of one input, we have here \n        // one WorkflowResult\n        const results = response.results[0];\n\n        // Uncomment this line to print the raw output\n        console.log(results);\n    }\n);\n',_n='package com.clarifai.example;\n\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.grpc.api.status.StatusCode;\n\nimport com.google.protobuf.ByteString;\nimport java.io.File;\nimport java.io.IOException;\nimport java.nio.file.Files;\n\npublic class ClarifaiExample {\n\n    /////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, workflow ID, video input,\n    // and sample_ms. Change these strings to run your own example.\n    /////////////////////////////////////////////////////////////////////////////////////////////////////\n\n    //Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    static final String USER_ID = "YOUR_USER_ID_HERE";\n    static final String APP_ID = "YOUR_APP_ID_HERE";\n    // Change these to make your own predictions\n    static final String WORKFLOW_ID = "YOUR_WORKFLOW_ID_HERE";\n    static final String VIDEO_URL = "https://samples.clarifai.com/beer.mp4";\n    // Or, to use a local video file, assign the location variable\n    // static final String VIDEO_FILE_LOCATION = "YOUR_VIDEO_FILE_LOCATION_HERE";\n    // Change this to configure the FPS rate (If it\'s not configured, it defaults to 1 FPS) \n    static final int SAMPLE_MS = 500;\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    public static void main(String[] args) throws IOException {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n                .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        PostWorkflowResultsResponse postWorkflowResultsResponse = stub.postWorkflowResults(\n                PostWorkflowResultsRequest.newBuilder()\n                        .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n                        .setWorkflowId(WORKFLOW_ID)\n                        .addInputs(\n                                Input.newBuilder().setData(\n                                        Data.newBuilder().setVideo(\n                                                Video.newBuilder().setUrl(VIDEO_URL)\n                                        // Video.newBuilder().setBase64(ByteString.copyFrom(Files.readAllBytes(\n                                        // new File(VIDEO_FILE_LOCATION).toPath()\n                                        //)))\n                                        )\n                                )\n                        )\n                        .setOutputConfig(OutputConfig.newBuilder()\n                        \t.setSampleMs(SAMPLE_MS)                        \t\t\n                        )\n                        .build()\n        );\n\n        if (postWorkflowResultsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("Post workflow results failed, status: " + postWorkflowResultsResponse.getStatus());\n        }\n\n        // We\'ll get one WorkflowResult for each input we used above. Because of one input, we have here\n        // one WorkflowResult\n        WorkflowResult results = postWorkflowResultsResponse.getResults(0);\n\n        // Uncomment this line to print the raw output\n        System.out.println(results);\n    }\n\n}\n',In='curl -X POST "https://api.clarifai.com/v2/users/YOUR_USER_ID_HERE/apps/YOUR_APP_ID_HERE/workflows/YOUR_WORKFLOW_ID_HERE/results" \\\n  -H "authorization: Key YOUR_PAT_HERE" \\\n  -H "content-type: application/json" \\\n  -d \'{\n    "inputs": [\n        {\n          "data": {\n            "video": {\n              "url": "https://samples.clarifai.com/beer.mp4"\n          }\n        }\n      }\n    ],\n    "output_config": {\n        "sample_ms": 500\n    }\n}\'',An='<?php\n\nrequire __DIR__ . "/vendor/autoload.php";\n\n/////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, workflow ID, video input,\n// and sample_ms. Change these strings to run your own example.\n/////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\n$PAT = "YOUR_PAT_HERE";\n$USER_ID = "YOUR_USER_ID_HERE";\n$APP_ID = "YOUR_APP_ID_HERE";\n// Change these to make your own predictions\n$WORKFLOW_ID = "YOUR_WORKFLOW_ID_HERE";\n$VIDEO_URL = "https://samples.clarifai.com/beer.mp4";\n# Or, to use a local video file, assign the location variable\n# $VIDEO_FILE_LOCATION = "YOUR_VIDEO_FILE_LOCATION_HERE";\n# Change this to configure the FPS rate (If it\'s not configured, it defaults to 1 FPS) \n$SAMPLE_MS = 500;\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Video;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\PostWorkflowResultsRequest;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\nuse Clarifai\\Api\\OutputConfig;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ["Authorization" => ["Key " . $PAT]];\n\n$userDataObject = new UserAppIDSet([\n    "user_id" => $USER_ID,\n    "app_id" => $APP_ID,\n]);\n\n// To use a local text file, uncomment the following lines\n// $videoData = file_get_contents($VIDEO_FILE_LOCATION); \n\n// Let\'s make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client\n    ->PostWorkflowResults(\n        // The request object carries the request along with the request status and other metadata related to the request itself\n        new PostWorkflowResultsRequest([\n            "user_app_id" => $userDataObject,\n            "workflow_id" => $WORKFLOW_ID,\n            "inputs" => [\n                new Input([\n                    // The Input object wraps the Data object in order to meet the API specification\n                    "data" => new Data([\n                        // The Data object is constructed around the Video object. It offers a container that has additional independent\n                        // metadata. In this particular use case, no other metadata is needed to be specified\n                        "video" => new Video([\n                            // In the Clarifai platform, a Video is defined by a special Video object                            \n                            "url" => $VIDEO_URL\n                            // "base64" => $videoData \n                        ]),\n                    ]),\n                ]),\n            ],\n            "output_config" => new OutputConfig([\n                "sample_ms" => $SAMPLE_MS\n            ])\n        ]),\n        $metadata\n    )\n    ->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception("Error: {$status->details}");\n}\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure\n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    throw new Exception(\n        "Failure response: " .\n            $response->getStatus()->getDescription() .\n            " " .\n            $response->getStatus()->getDetails()\n    );\n}\n\n// We\'ll get one WorkflowResult for each input we used above. Because of one input, we have here one WorkflowResult\n$results = $response->getResults()[0];\n\n// Uncomment this line to print the raw output\nprint_r($results);\n\n?>',bn='###########################################################################################\n# In this section, we set the user authentication, user and app ID, workflow ID, and\n# audio URL. Change these strings to run your own example.\n##########################################################################################\n\n# Your PAT (Personal Access Token) can be found in the Account\'s Security section\nPAT = "YOUR_PAT_HERE"\nUSER_ID = "clarifai"\nAPP_ID = "main"\n# Change these to make your own predictions\nWORKFLOW_ID = "asr-sentiment"\nAUDIO_URL = "https://samples.clarifai.com/negative_sentence_1.wav"\n# Or, to use a local audio file, assign the location variable\n# AUDIO_FILE_LOCATION = "YOUR_AUDIO_FILE_LOCATION_HERE"\n\n##########################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n##########################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (("authorization", "Key " + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(\n    user_id=USER_ID, app_id=APP_ID\n)  # The userDataObject is required when using a PAT\n\n# To use a local video file, uncomment the following lines\n# with open(AUDIO_FILE_LOCATION, "rb") as f:\n# audio_bytes = f.read()\n\npost_workflow_results_response = stub.PostWorkflowResults(\n    service_pb2.PostWorkflowResultsRequest(\n        user_app_id=userDataObject,\n        workflow_id=WORKFLOW_ID,\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    audio=resources_pb2.Audio(\n                        url=AUDIO_URL,\n                        # base64=audio_bytes\n                    )\n                )\n            )\n        ],\n    ),\n    metadata=metadata,\n)\nif post_workflow_results_response.status.code != status_code_pb2.SUCCESS:\n    print(post_workflow_results_response.status)\n    raise Exception(\n        "Post workflow results failed, status: "\n        + post_workflow_results_response.status.description\n    )\n\n# We\'ll get one WorkflowResult for each input we used above. Because of one input, we have here one WorkflowResult\nresults = post_workflow_results_response.results[0]\n\n# Each model we have in the workflow will produce its output\nfor output in results.outputs:\n    model = output.model\n    print("Output for the model: `%s`" % model.id)\n    for concept in output.data.concepts:\n        print("\\t%s %.2f" % (concept.name, concept.value))\n    print(output.data.text.raw)\n\n# Uncomment this line to print the raw output\n# print(results)\n',xn='\x3c!--index.html file--\x3e\n\n<script>\n  ////////////////////////////////////////////////////////////////////////////////////////////////////\n  // In this section, we set the user authentication, user and app ID, workflow ID, and\n  // audio URL. Change these strings to run your own example.\n  ///////////////////////////////////////////////////////////////////////////////////////////////////\n\n  // Your PAT (Personal Access Token) can be found in the Account\'s Security section\n  const PAT = "YOUR_PAT_HERE";\n  const USER_ID = "clarifai";\n  const APP_ID = "main";\n  // Change these to make your own predictions\n  const WORKFLOW_ID = "asr-sentiment";\n  const AUDIO_URL = "https://samples.clarifai.com/negative_sentence_1.wav";\n\n  ///////////////////////////////////////////////////////////////////////////////////\n  // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n  /////////////////////////////////////////////////////////////////////////////////// \n\n  const raw = JSON.stringify({\n    "user_app_id": {\n      "user_id": USER_ID,\n      "app_id": APP_ID\n    },\n    "inputs": [\n      {\n        "data": {\n          "audio": {            \n            "url": AUDIO_URL\n          }\n        }\n      }\n    ]\n  });\n\n  const requestOptions = {\n    method: "POST",\n    headers: {\n      "Accept": "application/json",\n      "Authorization": "Key " + PAT\n    },\n    body: raw\n  };\n\n  fetch(`https://api.clarifai.com/v2/workflows/${WORKFLOW_ID}/results`, requestOptions)\n    .then(response => response.text())\n    .then(result => console.log(result))\n    .catch(error => console.log("error", error));\n\n<\/script>',kn='\n//index.js file\n\n//////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, workflow ID, and\n// audio URL. Change these strings to run your own example.\n/////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\nconst PAT = "YOUR_PAT_HERE";\nconst USER_ID = "clarifai";\nconst APP_ID = "main";\n// Change these to make your own predictions\nconst WORKFLOW_ID = "asr-sentiment";\nconst AUDIO_URL = "https://samples.clarifai.com/negative_sentence_1.wav";\n// Or, to use a local audio file, assign the location variable\n// const AUDIO_FILE_LOCATION = "YOUR_AUDIO_FILE_LOCATION_HERE";\n\n/////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n/////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\n// To use a local text file, uncomment the following lines\n// const fs = require("fs");\n// const audioBytes = fs.readFileSync(AUDIO_FILE_LOCATION);\n\nstub.PostWorkflowResults(\n    {\n        user_app_id: {\n            "user_id": USER_ID,\n            "app_id": APP_ID,\n        },\n        workflow_id: WORKFLOW_ID,\n        inputs: [{\n            data: {\n                audio: {\n                    url: AUDIO_URL,\n                    // base64: audioBytes\n                }\n            }\n        }],\n    },\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            throw new Error(\n                "Post workflow results failed, status: " + response.status.description\n            );\n        }\n\n        // We\'ll get one WorkflowResult for each input we used above. Because of one input, we have here \n        // one WorkflowResult\n        const results = response.results[0];\n\n        // Each model we have in the workflow will produce its output   \n        for (const output of results.outputs) {\n            const model = output.model;\n            console.log("Output for the model: `" + model.id + "`");\n            for (const concept of output.data.concepts) {\n                console.log("\\t" + concept.name + " " + concept.value);\n            }\n            if (output.data.text) {\n                console.log(output.data.text.raw);\n            }\n        }\n    }\n);\n',En='package com.clarifai.example;\n\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.grpc.api.status.StatusCode;\n\nimport com.google.protobuf.ByteString;\nimport java.io.File;\nimport java.io.IOException;\nimport java.nio.file.Files;\n\npublic class ClarifaiExample {\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, app ID, workflow ID, and\n    // audio URL. Change these strings to run your own example.\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    //Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    static final String USER_ID = "clarifai";\n    static final String APP_ID = "main";\n    // Change these to make your own predictions\n    static final String WORKFLOW_ID = "asr-sentiment";\n    static final String AUDIO_URL = "https://samples.clarifai.com/negative_sentence_1.wav";\n    // Or, to use a local audio file, assign the location variable\n    // static final String AUDIO_FILE_LOCATION = "YOUR_IMAGE_FILE_LOCATION_HERE";\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    public static void main(String[] args) throws IOException {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n                .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        PostWorkflowResultsResponse postWorkflowResultsResponse = stub.postWorkflowResults(\n                PostWorkflowResultsRequest.newBuilder()\n                        .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n                        .setWorkflowId(WORKFLOW_ID)\n                        .addInputs(\n                                Input.newBuilder().setData(\n                                        Data.newBuilder().setAudio(\n                                                Audio.newBuilder().setUrl(AUDIO_URL)\n                                        //  To use a local text file, uncomment the following lines\n                                        //Audio.newBuilder().setBase64(ByteString.copyFrom(Files.readAllBytes(\n                                        // new File(AUDIO_FILE_LOCATION).toPath()\n                                        //)))\n                                        )\n                                )\n                        )\n                        .build()\n        );\n\n        if (postWorkflowResultsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("Post workflow results failed, status: " + postWorkflowResultsResponse.getStatus());\n        }\n\n        // We\'ll get one WorkflowResult for each input we used above. Because of one input, we have here\n        // one WorkflowResult\n        WorkflowResult results = postWorkflowResultsResponse.getResults(0);\n\n        // Each model we have in the workflow will produce its output\n        for (Output output : results.getOutputsList()) {\n            Model model = output.getModel();\n            System.out.println("Output for the model: `" + model.getId() + "`");\n            for (Concept concept : output.getData().getConceptsList()) {\n                System.out.printf("%s %.2f%n", concept.getName(), concept.getValue());\n            }\n            System.out.println(output.getData().getText().getRaw());\n        }\n\n    }\n\n}\n',yn='curl -X POST "https://api.clarifai.com/v2/users/clarifai/apps/main/workflows/asr-sentiment/results" \\\n  -H "authorization: Key YOUR_PAT_HERE" \\\n  -H "content-type: application/json" \\\n  -d \'{\n    "inputs": [\n        {\n          "data": {\n            "audio": {\n              "url": "https://samples.clarifai.com/negative_sentence_1.wav"\n          }\n        }\n      }\n    ]\n}\'',Tn='<?php\n\nrequire __DIR__ . "/vendor/autoload.php";\n\n///////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, workflow ID, and\n// audio URL. Change these strings to run your own example.\n////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\n$PAT = "YOUR_PAT_HERE";\n$USER_ID = "clarifai";\n$APP_ID = "main";\n// Change these to make your own predictions\n$WORKFLOW_ID = "asr-sentiment";\n$AUDIO_URL = "https://samples.clarifai.com/negative_sentence_1.wav";\n# Or, to use a local audio file, assign the location variable\n# $AUDIO_FILE_LOCATION = "YOUR_AUDIO_FILE_LOCATION_HERE";\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Audio;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\PostWorkflowResultsRequest;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ["Authorization" => ["Key " . $PAT]];\n\n$userDataObject = new UserAppIDSet([\n    "user_id" => $USER_ID,\n    "app_id" => $APP_ID,\n]);\n\n// To use a local text file, uncomment the following lines\n//$audioData = file_get_contents($AUDIO_FILE_LOCATION);\n\n// Let\'s make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client\n    ->PostWorkflowResults(\n        // The request object carries the request along with the request status and other metadata related to the request itself\n        new PostWorkflowResultsRequest([\n            "user_app_id" => $userDataObject,\n            "workflow_id" => $WORKFLOW_ID,\n            "inputs" => [\n                new Input([\n                    // The Input object wraps the Data object in order to meet the API specification\n                    "data" => new Data([\n                        // The Data object is constructed around the Audio object. It offers a container that has additional independent\n                        // metadata. In this particular use case, no other metadata is needed to be specified\n                        "audio" => new Audio([\n                            // In the Clarifai platform, a audio is defined by a special Audio object\n                            "url" => $AUDIO_URL,\n                            //"base64" => $audioData\n                        ]),\n                    ]),\n                ]),\n            ],\n        ]),\n        $metadata\n    )\n    ->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception("Error: {$status->details}");\n}\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure\n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    throw new Exception(\n        "Failure response: " .\n            $response->getStatus()->getDescription() .\n            " " .\n            $response->getStatus()->getDetails()\n    );\n}\n\n// We\'ll get one WorkflowResult for each input we used above. Because of one input, we have here one WorkflowResult\n$results = $response->getResults()[0];\n\n// Each model we have in the workflow will produce its output\nforeach ($results->getOutputs() as $output) {\n    $model = $output->getModel();\n    echo "Output for the model: `" . $model->getId() . "`" . "<br>";\n    foreach ($output->getData()->getConcepts() as $concept) {\n        echo $concept->getName() .\n            " " .\n            number_format($concept->getValue(), 2) .\n            "<br>";\n    }\n    $textData = $output->getData()->getText();\n\n    if ($textData !== null) {\n        echo $textData->getRaw() . "<br>";\n    }\n}\n\n// Uncomment this line to print the raw output\n// echo $results->serializeToJsonString();\n',Rn="Output for the model: `asr-wav2vec2-large-robust-ft-swbd-300h-english`\nI AM NOT FLYING TO ENGLAND\nOutput for the model: `sentiment-analysis-twitter-roberta-base`\n        LABEL_0 0.92\n        LABEL_1 0.07\n        LABEL_2 0.01",On='status {\n  code: SUCCESS\n  description: "Ok"\n}\ninput {\n  id: "c7b258c785614694bc1d9982e847e327"\n  data {\n    audio {\n      url: "https://samples.clarifai.com/negative_sentence_1.wav"\n    }\n  }\n}\noutputs {\n  id: "b562c938bb4545199a6908eabd8f6295"\n  status {\n    code: SUCCESS\n    description: "Ok"\n  }\n  created_at {\n    seconds: 1700762370\n    nanos: 800729365\n  }\n  model {\n    id: "asr-wav2vec2-large-robust-ft-swbd-300h-english"\n    name: "wav2vec2-large-robust-ft-swbd-300"\n    created_at {\n      seconds: 1636021464\n      nanos: 884891000\n    }\n    modified_at {\n      seconds: 1659644487\n      nanos: 107647000\n    }\n    app_id: "asr"\n    model_version {\n      id: "7adce5efc90744ed986fbd0bdc40000f"\n      created_at {\n        seconds: 1638786626\n        nanos: 104602000\n      }\n      status {\n        code: MODEL_TRAINED\n        description: "Model is trained and ready"\n      }\n      visibility {\n        gettable: PUBLIC\n      }\n      app_id: "asr"\n      user_id: "facebook"\n      metadata {\n      }\n      license: "Apache-2.0"\n    }\n    user_id: "facebook"\n    model_type_id: "audio-to-text"\n    visibility {\n      gettable: PUBLIC\n    }\n    workflow_recommended {\n    }\n  }\n  data {\n    text {\n      raw: "I AM NOT FLYING TO ENGLAND"\n      text_info {\n        encoding: "UnknownTextEnc"\n      }\n    }\n  }\n}\noutputs {\n  id: "1a3815e9c46b425e8247a4c491cfa0f2"\n  status {\n    code: SUCCESS\n    description: "Ok"\n  }\n  created_at {\n    seconds: 1700762370\n    nanos: 800742640\n  }\n  model {\n    id: "sentiment-analysis-twitter-roberta-base"\n    name: "sentiment-analysis-twitter-roberta-base"\n    created_at {\n      seconds: 1656525158\n      nanos: 299847000\n    }\n    modified_at {\n      seconds: 1659564125\n      nanos: 82152000\n    }\n    app_id: "text-classification"\n    model_version {\n      id: "f7f3df02b79d4080a0233ec1fb6404bd"\n      created_at {\n        seconds: 1656525158\n        nanos: 310142000\n      }\n      status {\n        code: MODEL_TRAINED\n        description: "Model is trained and ready"\n      }\n      visibility {\n        gettable: PUBLIC\n      }\n      app_id: "text-classification"\n      user_id: "erfan"\n      metadata {\n        fields {\n          key: "Model version logs zipped"\n          value {\n            string_value: "https://s3.amazonaws.com/clarifai-temp/prod/f7f3df02b79d4080a0233ec1fb6404bd.zip"\n          }\n        }\n      }\n    }\n    user_id: "erfan"\n    model_type_id: "text-classifier"\n    task: "text-classification"\n    visibility {\n      gettable: PUBLIC\n    }\n    workflow_recommended {\n    }\n  }\n  data {\n    concepts {\n      id: "LABEL_0"\n      name: "LABEL_0"\n      value: 0.91823113\n      app_id: "text-classification"\n    }\n    concepts {\n      id: "LABEL_1"\n      name: "LABEL_1"\n      value: 0.0743510351\n      app_id: "text-classification"\n    }\n    concepts {\n      id: "LABEL_2"\n      name: "LABEL_2"\n      value: 0.00741776684\n      app_id: "text-classification"\n    }\n  }\n}',jn={description:"Make predictions with your workflows",sidebar_position:1,toc_max_heading_level:4},Pn="Workflow Inferences",Sn={},Cn=[{value:"Predict via the UI",id:"predict-via-the-ui",level:2},{value:"Predict via the API",id:"predict-via-the-api",level:2},{value:"Text Classification",id:"text-classification",level:3},{value:"Text-to-Text - Summarization",id:"text-to-text---summarization",level:3},{value:"Text-to-Text - Generation",id:"text-to-text---generation",level:3},{value:"Text-to-Image",id:"text-to-image",level:3},{value:"Text-to-Audio",id:"text-to-audio",level:3},{value:"Text-to-Embeddings",id:"text-to-embeddings",level:3},{value:"Example 1",id:"example-1",level:4},{value:"Example 2",id:"example-2",level:4},{value:"Visual Classification",id:"visual-classification",level:3},{value:"Example 1",id:"example-1-1",level:4},{value:"Example 2",id:"example-2-1",level:4},{value:"Visual Segmentation",id:"visual-segmentation",level:3},{value:"Image-to-Text",id:"image-to-text",level:3},{value:"Image-to-Image",id:"image-to-image",level:3},{value:"Audio-to-Text",id:"audio-to-text",level:3},{value:"Example 1",id:"example-1-2",level:4},{value:"Example 2",id:"example-2-2",level:4},{value:"Visual Detection - Object Search",id:"visual-detection---object-search",level:3},{value:"Visual Detection - Face Search",id:"visual-detection---face-search",level:3},{value:"Visual Detection  - Custom Workflow",id:"visual-detection----custom-workflow",level:3},{value:"Multimodal Inputs",id:"multimodal-inputs",level:3},{value:"Batch Prediction - Workflows",id:"batch-prediction---workflows",level:3}];function Dn(n){const e={a:"a",admonition:"admonition",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",img:"img",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...n.components},{Details:o}=e;return o||function(n,e){throw new Error("Expected "+(e?"component":"object")+" `"+n+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.header,{children:(0,r.jsx)(e.h1,{id:"workflow-inferences",children:"Workflow Inferences"})}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Make predictions with your workflows"})}),"\n",(0,r.jsx)("hr",{}),"\n",(0,r.jsx)(e.p,{children:"You can make predictions using all the models in a workflow with a single API call. The cost of making predictions with a workflow is the same as calling individual models."}),"\n",(0,r.jsxs)(e.p,{children:["The request body follows the same structure as a standard prediction call. In the ",(0,r.jsx)(e.a,{href:"https://docs.clarifai.com/resources/api-overview/api-outputs",children:"response"}),", a ",(0,r.jsx)(e.code,{children:"results"})," object will contain the outputs from each model in the workflow, presented in the order they were defined."]}),"\n",(0,r.jsx)(e.admonition,{type:"note",children:(0,r.jsx)(e.p,{children:"The maximum number of inputs that can be processed at once with any given workflow is 32."})}),"\n",(0,r.jsx)(e.h2,{id:"predict-via-the-ui",children:"Predict via the UI"}),"\n",(0,r.jsx)(e.p,{children:"You can use a workflow to make predictions by following these steps:"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsx)(e.li,{children:"Go to the workflow's overview page;"}),"\n",(0,r.jsx)(e.li,{children:"Select a workflow version and upload an image or video;"}),"\n",(0,r.jsx)(e.li,{children:"View the predictions returned by each model in the workflow."}),"\n"]}),"\n",(0,r.jsxs)(e.p,{children:["For example, let's use the custom workflow we ",(0,r.jsx)(e.a,{href:"/create/workflows/create#create-via-the-ui",children:"created previously"})," to extract text from ",(0,r.jsx)(e.a,{href:"https://samples.clarifai.com/featured-models/ocr-woman-holding-sold-sign.jpg",children:"this image"}),", and then translate the extracted text into Spanish."]}),"\n",(0,r.jsxs)(e.p,{children:["To do so, go to the workflow's overview page, select the version you want to use, and click the blue ",(0,r.jsx)(e.strong,{children:'"+"'})," button."]}),"\n",(0,r.jsxs)(e.p,{children:["Next, select the ",(0,r.jsx)(e.strong,{children:"Try your own image or video"})," option on the modal that appears. The small window that pops up allows you to upload the image."]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.img,{alt:"alt_text",src:t(17596).A+"",width:"1795",height:"792"})}),"\n",(0,r.jsx)(e.p,{children:"After the image has been uploaded and processed, the output will contain the predictions each model in the workflow returns."}),"\n",(0,r.jsx)(e.p,{children:"You can see in the screenshot below that the text was successfully extracted from the image and then translated into Spanish."}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.img,{alt:"alt_text",src:t(27713).A+"",width:"1809",height:"891"})}),"\n",(0,r.jsx)(e.h2,{id:"predict-via-the-api",children:"Predict via the API"}),"\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n",(0,r.jsx)(e.admonition,{type:"info",children:(0,r.jsxs)(e.p,{children:["Before using the ",(0,r.jsx)(e.a,{href:"https://docs.clarifai.com/additional-resources/api-overview/python-sdk",children:"Python SDK"}),", ",(0,r.jsx)(e.a,{href:"https://docs.clarifai.com/additional-resources/api-overview/nodejs-sdk",children:"Node.js SDK"}),", or any of our ",(0,r.jsx)(e.a,{href:"https://docs.clarifai.com/additional-resources/api-overview/grpc-clients",children:"gRPC clients"}),", ensure they are properly installed on your machine. Refer to their respective installation guides for instructions on how to install and initialize them."]})}),"\n",(0,r.jsx)(e.admonition,{type:"tip",children:(0,r.jsxs)(e.p,{children:["If you want to make a predict call with an external workflow that is outside the scope of your app, you need to use a PAT while specifying the ",(0,r.jsx)(e.code,{children:"app_id"})," and the ",(0,r.jsx)(e.code,{children:"user_id"})," associated with the workflow you want to use."]})}),"\n",(0,r.jsx)(e.h3,{id:"text-classification",children:"Text Classification"}),"\n",(0,r.jsx)(e.p,{children:"You can use a workflow to categorize and analyze text data."}),"\n",(0,r.jsxs)(i.A,{groupId:"code",children:[(0,r.jsxs)(s.A,{value:"python",label:"Python SDK",children:[(0,r.jsx)(c.A,{className:"language-python",children:l}),(0,r.jsxs)(o,{children:[(0,r.jsx)("summary",{children:"Output"}),(0,r.jsx)(c.A,{className:"language-text",children:B})]})]}),(0,r.jsx)(s.A,{value:"typescript",label:"Node.js SDK",children:(0,r.jsx)(c.A,{className:"language-typescript",children:u})})]}),"\n",(0,r.jsx)(e.h3,{id:"text-to-text---summarization",children:"Text-to-Text - Summarization"}),"\n",(0,r.jsx)(e.p,{children:"You can use a workflow to generate concise summaries by extracting key insights from lengthy or complex textual content."}),"\n",(0,r.jsxs)(i.A,{groupId:"code",children:[(0,r.jsxs)(s.A,{value:"python",label:"Python SDK",children:[(0,r.jsx)(c.A,{className:"language-python",children:d}),(0,r.jsxs)(o,{children:[(0,r.jsx)("summary",{children:"Output"}),(0,r.jsx)(c.A,{className:"language-text",children:Y})]})]}),(0,r.jsx)(s.A,{value:"typescript",label:"Node.js SDK",children:(0,r.jsx)(c.A,{className:"language-typescript",children:p})})]}),"\n",(0,r.jsx)(e.h3,{id:"text-to-text---generation",children:"Text-to-Text - Generation"}),"\n",(0,r.jsx)(e.p,{children:"You can use a workflow to generate textual content based on a given prompt. This enables dynamic text generation for a wide range of use cases."}),"\n",(0,r.jsxs)(i.A,{groupId:"code",children:[(0,r.jsxs)(s.A,{value:"python",label:"Python SDK",children:[(0,r.jsx)(c.A,{className:"language-python",children:v}),(0,r.jsxs)(o,{children:[(0,r.jsx)("summary",{children:"Output"}),(0,r.jsx)(c.A,{className:"language-text",children:H})]})]}),(0,r.jsx)(s.A,{value:"typescript",label:"Node.js SDK",children:(0,r.jsx)(c.A,{className:"language-typescript",children:f})})]}),"\n",(0,r.jsx)(e.h3,{id:"text-to-image",children:"Text-to-Image"}),"\n",(0,r.jsx)(e.p,{children:"You can use a workflow generate images from textual prompts. This enables creative visual content generation based on descriptive input."}),"\n",(0,r.jsxs)(i.A,{groupId:"code",children:[(0,r.jsxs)(s.A,{value:"python",label:"Python SDK",children:[(0,r.jsx)(c.A,{className:"language-python",children:A}),(0,r.jsxs)(o,{children:[(0,r.jsx)("summary",{children:"Image Output"}),(0,r.jsx)("img",{src:"/img/python-sdk/TI.png"})]})]}),(0,r.jsx)(s.A,{value:"typescript",label:"Node.js SDK",children:(0,r.jsx)(c.A,{className:"language-typescript",children:b})})]}),"\n",(0,r.jsx)(e.h3,{id:"text-to-audio",children:"Text-to-Audio"}),"\n",(0,r.jsx)(e.p,{children:"You can use a workflow to convert written text into high-quality audio. Simply submit text as input to generate natural-sounding speech."}),"\n",(0,r.jsxs)(i.A,{groupId:"code",children:[(0,r.jsx)(s.A,{value:"python",label:"Python SDK",children:(0,r.jsx)(c.A,{className:"language-python",children:E})}),(0,r.jsx)(s.A,{value:"typescript",label:"Node.js SDK",children:(0,r.jsx)(c.A,{className:"language-typescript",children:y})})]}),"\n",(0,r.jsx)(e.h3,{id:"text-to-embeddings",children:"Text-to-Embeddings"}),"\n",(0,r.jsx)(e.h4,{id:"example-1",children:"Example 1"}),"\n",(0,r.jsx)(e.p,{children:"You can use a workflow that combines embeddings and clustering to process, organize, and analyze diverse text data. This enables advanced language understanding and categorization."}),"\n",(0,r.jsxs)(i.A,{groupId:"code",children:[(0,r.jsxs)(s.A,{value:"python",label:"Python SDK",children:[(0,r.jsx)(c.A,{className:"language-python",children:C}),(0,r.jsxs)(o,{children:[(0,r.jsx)("summary",{children:"Output"}),(0,r.jsx)(c.A,{className:"language-text",children:z})]})]}),(0,r.jsx)(s.A,{value:"typescript",label:"Node.js SDK",children:(0,r.jsx)(c.A,{className:"language-typescript",children:D})})]}),"\n",(0,r.jsx)(e.h4,{id:"example-2",children:"Example 2"}),"\n",(0,r.jsxs)(e.p,{children:["Let's illustrate how you would produce embeddings and clusters from text inputs using Clarifai's ",(0,r.jsx)(e.a,{href:"https://clarifai.com/clarifai/main/workflows/Language-Understanding",children:"Language-Understanding"})," text workflow."]}),"\n",(0,r.jsxs)(i.A,{groupId:"code",children:[(0,r.jsx)(s.A,{value:"python",label:"Python (gRPC)",children:(0,r.jsx)(c.A,{className:"language-python",children:cn})}),(0,r.jsx)(s.A,{value:"js_rest",label:"JavaScript (REST)",children:(0,r.jsx)(c.A,{className:"language-javascript",children:ln})}),(0,r.jsx)(s.A,{value:"nodejs",label:"Node.js (gRPC)",children:(0,r.jsx)(c.A,{className:"language-javascript",children:un})}),(0,r.jsx)(s.A,{value:"java",label:"Java (gRPC)",children:(0,r.jsx)(c.A,{className:"language-java",children:dn})}),(0,r.jsx)(s.A,{value:"php",label:"PHP (gRPC)",children:(0,r.jsx)(c.A,{className:"language-php",children:vn})}),(0,r.jsx)(s.A,{value:"curl",label:"cURL",children:(0,r.jsx)(c.A,{className:"language-bash",children:pn})})]}),"\n",(0,r.jsxs)(o,{children:[(0,r.jsx)("summary",{children:"Text Output Example"}),(0,r.jsx)(c.A,{className:"language-text",children:fn})]}),"\n",(0,r.jsxs)(o,{children:[(0,r.jsx)("summary",{children:"Raw Output Example"}),(0,r.jsx)(c.A,{className:"language-javascript",children:hn})]}),"\n",(0,r.jsx)(e.h3,{id:"visual-classification",children:"Visual Classification"}),"\n",(0,r.jsx)(e.p,{children:"You can use a workflow for visual classification tasks. This enables accurate and efficient categorization of images based on learned visual patterns."}),"\n",(0,r.jsx)(e.h4,{id:"example-1-1",children:"Example 1"}),"\n",(0,r.jsx)(e.p,{children:"Let's illustrate how you would get predictions from a visual classifier workflow."}),"\n",(0,r.jsxs)(i.A,{groupId:"code",children:[(0,r.jsxs)(s.A,{value:"python",label:"Python SDK",children:[(0,r.jsx)(c.A,{className:"language-python",children:h}),(0,r.jsxs)(o,{children:[(0,r.jsx)("summary",{children:"Output"}),(0,r.jsx)(c.A,{className:"language-text",children:G})]})]}),(0,r.jsx)(s.A,{value:"typescript",label:"Node.js SDK",children:(0,r.jsx)(c.A,{className:"language-typescript",children:m})})]}),"\n",(0,r.jsx)(e.h4,{id:"example-2-1",children:"Example 2"}),"\n",(0,r.jsxs)(e.p,{children:["Let's illustrate how you would get predictions from image inputs using Clarifai's ",(0,r.jsx)(e.a,{href:"https://clarifai.com/clarifai/main/workflows/Face-Sentiment",children:"Face-Sentiment"})," workflow. The workflow combines these three models:"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"A visual detector model that detects bounding box regions in an image;"}),"\n",(0,r.jsx)(e.li,{children:"An image cropper model that extracts the specific region of interest from an image;"}),"\n",(0,r.jsx)(e.li,{children:"A visual classifier model that classifies an image into a set of concepts."}),"\n"]}),"\n",(0,r.jsxs)(e.p,{children:["Note that the ",(0,r.jsx)(e.code,{children:"base64"})," output representation of the image in bytes is already in binary format. It is not encoded, so you do not need to decode it for further downstream tasks."]}),"\n",(0,r.jsxs)(i.A,{groupId:"code",children:[(0,r.jsx)(s.A,{value:"python",label:"Python (gRPC)",children:(0,r.jsx)(c.A,{className:"language-python",children:Q})}),(0,r.jsx)(s.A,{value:"js_rest",label:"JavaScript (REST)",children:(0,r.jsx)(c.A,{className:"language-javascript",children:nn})}),(0,r.jsx)(s.A,{value:"nodejs",label:"Node.js (gRPC)",children:(0,r.jsx)(c.A,{className:"language-javascript",children:en})}),(0,r.jsx)(s.A,{value:"java",label:"Java (gRPC)",children:(0,r.jsx)(c.A,{className:"language-java",children:tn})}),(0,r.jsx)(s.A,{value:"php",label:"PHP (gRPC)",children:(0,r.jsx)(c.A,{className:"language-php",children:rn})}),(0,r.jsx)(s.A,{value:"curl",label:"cURL",children:(0,r.jsx)(c.A,{className:"language-bash",children:on})})]}),"\n",(0,r.jsxs)(o,{children:[(0,r.jsx)("summary",{children:"Text Output Example"}),(0,r.jsx)(c.A,{className:"language-text",children:an})]}),"\n",(0,r.jsxs)(o,{children:[(0,r.jsx)("summary",{children:"Raw Output Example"}),(0,r.jsx)(c.A,{className:"language-javascript",children:sn})]}),"\n",(0,r.jsx)(e.h3,{id:"visual-segmentation",children:"Visual Segmentation"}),"\n",(0,r.jsx)(e.p,{children:"You can use a workflow for visual segmentation tasks. This enables precise categorization of distinct regions within an image."}),"\n",(0,r.jsxs)(i.A,{groupId:"code",children:[(0,r.jsxs)(s.A,{value:"python",label:"Python SDK",children:[(0,r.jsx)(c.A,{className:"language-python",children:w}),(0,r.jsxs)(o,{children:[(0,r.jsx)("summary",{children:"Image Output"}),(0,r.jsx)("img",{src:"/img/python-sdk/visual_seg.png"})]})]}),(0,r.jsx)(s.A,{value:"typescript",label:"Node.js SDK",children:(0,r.jsx)(c.A,{className:"language-typescript",children:g})})]}),"\n",(0,r.jsx)(e.h3,{id:"image-to-text",children:"Image-to-Text"}),"\n",(0,r.jsx)(e.p,{children:"You can use a workflow to extract and interpret text from images. This enables OCR applications to efficiently recognize, process, and convert visual text into machine-readable format."}),"\n",(0,r.jsxs)(i.A,{groupId:"code",children:[(0,r.jsxs)(s.A,{value:"python",label:"Python SDK",children:[(0,r.jsx)(c.A,{className:"language-python",children:_}),(0,r.jsxs)(o,{children:[(0,r.jsx)("summary",{children:"Output"}),(0,r.jsx)(c.A,{className:"language-text",children:M})]})]}),(0,r.jsx)(s.A,{value:"typescript",label:"Node.js SDK",children:(0,r.jsx)(c.A,{className:"language-typescript",children:I})})]}),"\n",(0,r.jsx)(e.h3,{id:"image-to-image",children:"Image-to-Image"}),"\n",(0,r.jsx)(e.p,{children:"You can use a workflow to enhance and transform images, including upscaling for higher resolution and improved visual quality. This delivers a superior visual experience for end users."}),"\n",(0,r.jsxs)(i.A,{groupId:"code",children:[(0,r.jsxs)(s.A,{value:"python",label:"Python SDK",children:[(0,r.jsx)(c.A,{className:"language-python",children:x}),(0,r.jsxs)(o,{children:[(0,r.jsx)("summary",{children:"Image Output"}),(0,r.jsx)("img",{src:"/img/python-sdk/II.png"})]})]}),(0,r.jsx)(s.A,{value:"typescript",label:"Node.js SDK",children:(0,r.jsx)(c.A,{className:"language-typescript",children:k})})]}),"\n",(0,r.jsx)(e.h3,{id:"audio-to-text",children:"Audio-to-Text"}),"\n",(0,r.jsx)(e.h4,{id:"example-1-2",children:"Example 1"}),"\n",(0,r.jsx)(e.p,{children:"You can use a workflow to convert audio into text by transcribing the provided spoken content."}),"\n",(0,r.jsxs)(i.A,{groupId:"code",children:[(0,r.jsxs)(s.A,{value:"python",label:"Python SDK",children:[(0,r.jsx)(c.A,{className:"language-python",children:T}),(0,r.jsxs)(o,{children:[(0,r.jsx)("summary",{children:"Output"}),(0,r.jsx)(c.A,{className:"language-text",children:K})]})]}),(0,r.jsx)(s.A,{value:"typescript",label:"Node.js SDK",children:(0,r.jsx)(c.A,{className:"language-typescript",children:R})})]}),"\n",(0,r.jsx)(e.h4,{id:"example-2-2",children:"Example 2"}),"\n",(0,r.jsxs)(e.p,{children:["Let's illustrate how you would get the sentiment of an audio input using Clarifai's ",(0,r.jsx)(e.a,{href:"https://clarifai.com/clarifai/main/workflows/asr-sentiment",children:"asr-sentiment"})," workflow."]}),"\n",(0,r.jsxs)(i.A,{groupId:"code",children:[(0,r.jsx)(s.A,{value:"python",label:"Python (gRPC)",children:(0,r.jsx)(c.A,{className:"language-python",children:bn})}),(0,r.jsx)(s.A,{value:"js_rest",label:"JavaScript (REST)",children:(0,r.jsx)(c.A,{className:"language-javascript",children:xn})}),(0,r.jsx)(s.A,{value:"nodejs",label:"Node.js (gRPC)",children:(0,r.jsx)(c.A,{className:"language-javascript",children:kn})}),(0,r.jsx)(s.A,{value:"java",label:"Java (gRPC)",children:(0,r.jsx)(c.A,{className:"language-java",children:En})}),(0,r.jsx)(s.A,{value:"php",label:"PHP (gRPC)",children:(0,r.jsx)(c.A,{className:"language-php",children:Tn})}),(0,r.jsx)(s.A,{value:"curl",label:"cURL",children:(0,r.jsx)(c.A,{className:"language-bash",children:yn})})]}),"\n",(0,r.jsxs)(o,{children:[(0,r.jsx)("summary",{children:"Text Output Example"}),(0,r.jsx)(c.A,{className:"language-text",children:Rn})]}),"\n",(0,r.jsxs)(o,{children:[(0,r.jsx)("summary",{children:"Raw Output Example"}),(0,r.jsx)(c.A,{className:"language-javascript",children:On})]}),"\n",(0,r.jsx)(e.h3,{id:"visual-detection---object-search",children:"Visual Detection - Object Search"}),"\n",(0,r.jsx)(e.p,{children:"You can use a workflow to detect common objects in images and generate embeddings to enable fast, accurate visual search based on each object\u2019s unique features."}),"\n",(0,r.jsxs)(e.admonition,{type:"note",children:[(0,r.jsx)(e.p,{children:"When you input a video into the Workflow Predict API, the response includes a list of predicted concepts for each frame of the video. By default, the video is processed at 1 frame per second (FPS), but this rate can be customized in the predict request. This means you\u2019ll receive a set of concepts for every second (1000 milliseconds) of your video."}),(0,r.jsxs)(e.p,{children:["To adjust the FPS rate, use the ",(0,r.jsx)(e.code,{children:"sample_ms"})," parameter in your predict request. The ",(0,r.jsx)(e.code,{children:"sample_ms"})," value specifies the time interval (in milliseconds) between frames selected for inference, determining how frequently frames are processed."]}),(0,r.jsxs)(e.p,{children:["The valid range for ",(0,r.jsx)(e.code,{children:"sample_ms"})," is between 100 and 60,000 milliseconds."]}),(0,r.jsxs)(e.blockquote,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Note:"})," FPS is calculated as: FPS = 1000 / sample_ms"]}),"\n"]}),(0,r.jsxs)(e.p,{children:["For example, if ",(0,r.jsx)(e.code,{children:"sample_ms"})," is set to 1000, the FPS rate will be 1 (the default value)."]}),(0,r.jsxs)(e.p,{children:["The Workflow Predict API has size and duration limitations for ",(0,r.jsx)(e.a,{href:"https://docs.clarifai.com/create/inputs/upload/#videos",children:"video inputs"}),":"]}),(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Videos uploaded via URL can be up to 300MB in size or 10 minutes in length."}),"\n",(0,r.jsx)(e.li,{children:"Videos sent as byte data are limited to 128MB in size."}),"\n"]}),(0,r.jsx)(e.p,{children:"If your video exceeds these limits, you can split it into smaller segments for processing. Exceeding these limits may cause the process to time out and result in an error response."})]}),"\n",(0,r.jsxs)(i.A,{groupId:"code",children:[(0,r.jsxs)(s.A,{value:"python",label:"Python SDK",children:[(0,r.jsx)(c.A,{className:"language-python",children:P}),(0,r.jsxs)(o,{children:[(0,r.jsx)("summary",{children:"Output"}),(0,r.jsx)(c.A,{className:"language-text",children:q})]})]}),(0,r.jsx)(s.A,{value:"typescript",label:"Node.js SDK",children:(0,r.jsx)(c.A,{className:"language-typescript",children:S})}),(0,r.jsx)(s.A,{value:"python2",label:"Python (gRPC)",children:(0,r.jsx)(c.A,{className:"language-python",children:mn})}),(0,r.jsx)(s.A,{value:"js_rest",label:"JavaScript (REST)",children:(0,r.jsx)(c.A,{className:"language-javascript",children:wn})}),(0,r.jsx)(s.A,{value:"nodejs",label:"Node.js (gRPC)",children:(0,r.jsx)(c.A,{className:"language-javascript",children:gn})}),(0,r.jsx)(s.A,{value:"java",label:"Java (gRPC)",children:(0,r.jsx)(c.A,{className:"language-java",children:_n})}),(0,r.jsx)(s.A,{value:"php",label:"PHP (gRPC)",children:(0,r.jsx)(c.A,{className:"language-php",children:An})}),(0,r.jsx)(s.A,{value:"curl",label:"cURL",children:(0,r.jsx)(c.A,{className:"language-bash",children:In})})]}),"\n",(0,r.jsx)(e.h3,{id:"visual-detection---face-search",children:"Visual Detection - Face Search"}),"\n",(0,r.jsx)(e.p,{children:"You can use a workflow that combines face detection, recognition, and embedding to accurately identify facial landmarks. This approach not only detects faces but also generates distinctive embeddings, enabling efficient visual searches based on the unique features of each face."}),"\n",(0,r.jsxs)(i.A,{groupId:"code",children:[(0,r.jsxs)(s.A,{value:"python",label:"Python SDK",children:[(0,r.jsx)(c.A,{className:"language-python",children:O}),(0,r.jsxs)(o,{children:[(0,r.jsx)("summary",{children:"Output"}),(0,r.jsx)(c.A,{className:"language-text",children:V})]})]}),(0,r.jsx)(s.A,{value:"typescript",label:"Node.js SDK",children:(0,r.jsx)(c.A,{className:"language-typescript",children:j})})]}),"\n",(0,r.jsx)(e.h3,{id:"visual-detection----custom-workflow",children:"Visual Detection  - Custom Workflow"}),"\n",(0,r.jsx)(e.p,{children:"You can use a workflow to detect a wide range of common objects within a video. It processes each frame to identify and localize objects, generating regions of interest that outline where the detected objects appear."}),"\n",(0,r.jsxs)(i.A,{groupId:"code",children:[(0,r.jsxs)(s.A,{value:"python",label:"Python SDK",children:[(0,r.jsx)(c.A,{className:"language-python",children:W}),(0,r.jsxs)(o,{children:[(0,r.jsx)("summary",{children:"Output"}),(0,r.jsx)(c.A,{className:"language-text",children:J})]})]}),(0,r.jsx)(s.A,{value:"typescript",label:"Node.js SDK",children:(0,r.jsx)(c.A,{className:"language-typescript",children:N})})]}),"\n",(0,r.jsx)(e.h3,{id:"multimodal-inputs",children:"Multimodal Inputs"}),"\n",(0,r.jsx)(e.p,{children:"You can provide data in multiple formats \u2014 such as text, images, or a combination of both \u2014 to a workflow and receive prediction results tailored to your input."}),"\n",(0,r.jsxs)(i.A,{groupId:"code",children:[(0,r.jsxs)(s.A,{value:"python",label:"Python SDK",children:[(0,r.jsx)(c.A,{className:"language-python",children:L}),(0,r.jsxs)(o,{children:[(0,r.jsx)("summary",{children:"Output"}),(0,r.jsx)(c.A,{className:"language-text",children:X})]})]}),(0,r.jsx)(s.A,{value:"typescript",label:"Node.js SDK",children:(0,r.jsx)(c.A,{className:"language-typescript",children:U})})]}),"\n",(0,r.jsx)(e.h3,{id:"batch-prediction---workflows",children:"Batch Prediction - Workflows"}),"\n",(0,r.jsx)(e.p,{children:"You can use a workflow to process multiple inputs in a single request. You can submit a batch of data and receive comprehensive predictions for all inputs."}),"\n",(0,r.jsx)(e.admonition,{type:"info",children:(0,r.jsx)(e.p,{children:"The batch size should not exceed 32."})}),"\n",(0,r.jsxs)(i.A,{groupId:"code",children:[(0,r.jsxs)(s.A,{value:"python",label:"Python SDK",children:[(0,r.jsx)(c.A,{className:"language-python",children:F}),(0,r.jsxs)(o,{children:[(0,r.jsx)("summary",{children:"Output"}),(0,r.jsx)(c.A,{className:"language-text",children:Z})]})]}),(0,r.jsx)(s.A,{value:"typescript",label:"Node.js SDK",children:(0,r.jsx)(c.A,{className:"language-typescript",children:$})})]})]})}function Ln(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,r.jsx)(e,{...n,children:(0,r.jsx)(Dn,{...n})}):Dn(n)}},65537:(n,e,t)=>{t.d(e,{A:()=>b});var o=t(96540),r=t(18215),a=t(65627),i=t(56347),s=t(50372),c=t(30604),l=t(11861),u=t(78749);function d(n){return o.Children.toArray(n).filter((n=>"\n"!==n)).map((n=>{if(!n||(0,o.isValidElement)(n)&&function(n){const{props:e}=n;return!!e&&"object"==typeof e&&"value"in e}(n))return n;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof n.type?n.type:n.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function p(n){const{values:e,children:t}=n;return(0,o.useMemo)((()=>{const n=e??function(n){return d(n).map((n=>{let{props:{value:e,label:t,attributes:o,default:r}}=n;return{value:e,label:t,attributes:o,default:r}}))}(t);return function(n){const e=(0,l.XI)(n,((n,e)=>n.value===e.value));if(e.length>0)throw new Error(`Docusaurus error: Duplicate values "${e.map((n=>n.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(n),n}),[e,t])}function v(n){let{value:e,tabValues:t}=n;return t.some((n=>n.value===e))}function f(n){let{queryString:e=!1,groupId:t}=n;const r=(0,i.W6)(),a=function(n){let{queryString:e=!1,groupId:t}=n;if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:e,groupId:t});return[(0,c.aZ)(a),(0,o.useCallback)((n=>{if(!a)return;const e=new URLSearchParams(r.location.search);e.set(a,n),r.replace({...r.location,search:e.toString()})}),[a,r])]}function h(n){const{defaultValue:e,queryString:t=!1,groupId:r}=n,a=p(n),[i,c]=(0,o.useState)((()=>function(n){let{defaultValue:e,tabValues:t}=n;if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!v({value:e,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${t.map((n=>n.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const o=t.find((n=>n.default))??t[0];if(!o)throw new Error("Unexpected error: 0 tabValues");return o.value}({defaultValue:e,tabValues:a}))),[l,d]=f({queryString:t,groupId:r}),[h,m]=function(n){let{groupId:e}=n;const t=function(n){return n?`docusaurus.tab.${n}`:null}(e),[r,a]=(0,u.Dv)(t);return[r,(0,o.useCallback)((n=>{t&&a.set(n)}),[t,a])]}({groupId:r}),w=(()=>{const n=l??h;return v({value:n,tabValues:a})?n:null})();(0,s.A)((()=>{w&&c(w)}),[w]);return{selectedValue:i,selectValue:(0,o.useCallback)((n=>{if(!v({value:n,tabValues:a}))throw new Error(`Can't select invalid tab value=${n}`);c(n),d(n),m(n)}),[d,m,a]),tabValues:a}}var m=t(9136);const w={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var g=t(74848);function _(n){let{className:e,block:t,selectedValue:o,selectValue:i,tabValues:s}=n;const c=[],{blockElementScrollPositionUntilNextRender:l}=(0,a.a_)(),u=n=>{const e=n.currentTarget,t=c.indexOf(e),r=s[t].value;r!==o&&(l(e),i(r))},d=n=>{let e=null;switch(n.key){case"Enter":u(n);break;case"ArrowRight":{const t=c.indexOf(n.currentTarget)+1;e=c[t]??c[0];break}case"ArrowLeft":{const t=c.indexOf(n.currentTarget)-1;e=c[t]??c[c.length-1];break}}e?.focus()};return(0,g.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,r.A)("tabs",{"tabs--block":t},e),children:s.map((n=>{let{value:e,label:t,attributes:a}=n;return(0,g.jsx)("li",{role:"tab",tabIndex:o===e?0:-1,"aria-selected":o===e,ref:n=>{c.push(n)},onKeyDown:d,onClick:u,...a,className:(0,r.A)("tabs__item",w.tabItem,a?.className,{"tabs__item--active":o===e}),children:t??e},e)}))})}function I(n){let{lazy:e,children:t,selectedValue:a}=n;const i=(Array.isArray(t)?t:[t]).filter(Boolean);if(e){const n=i.find((n=>n.props.value===a));return n?(0,o.cloneElement)(n,{className:(0,r.A)("margin-top--md",n.props.className)}):null}return(0,g.jsx)("div",{className:"margin-top--md",children:i.map(((n,e)=>(0,o.cloneElement)(n,{key:e,hidden:n.props.value!==a})))})}function A(n){const e=h(n);return(0,g.jsxs)("div",{className:(0,r.A)("tabs-container",w.tabList),children:[(0,g.jsx)(_,{...e,...n}),(0,g.jsx)(I,{...e,...n})]})}function b(n){const e=(0,m.A)();return(0,g.jsx)(A,{...n,children:d(n.children)},String(e))}},79329:(n,e,t)=>{t.d(e,{A:()=>i});t(96540);var o=t(18215);const r={tabItem:"tabItem_Ymn6"};var a=t(74848);function i(n){let{children:e,hidden:t,className:i}=n;return(0,a.jsx)("div",{role:"tabpanel",className:(0,o.A)(r.tabItem,i),hidden:t,children:e})}}}]);
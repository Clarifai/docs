"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[7777],{3905:function(e,t,n){n.d(t,{Zo:function(){return p},kt:function(){return m}});var s=n(7294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function a(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);t&&(s=s.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,s)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?a(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):a(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function i(e,t){if(null==e)return{};var n,s,o=function(e,t){if(null==e)return{};var n,s,o={},a=Object.keys(e);for(s=0;s<a.length;s++)n=a[s],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(s=0;s<a.length;s++)n=a[s],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var l=s.createContext({}),u=function(e){var t=s.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},p=function(e){var t=u(e.components);return s.createElement(l.Provider,{value:t},e.children)},c={inlineCode:"code",wrapper:function(e){var t=e.children;return s.createElement(s.Fragment,{},t)}},d=s.forwardRef((function(e,t){var n=e.components,o=e.mdxType,a=e.originalType,l=e.parentName,p=i(e,["components","mdxType","originalType","parentName"]),d=u(n),m=o,_=d["".concat(l,".").concat(m)]||d[m]||c[m]||a;return n?s.createElement(_,r(r({ref:t},p),{},{components:n})):s.createElement(_,r({ref:t},p))}));function m(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var a=n.length,r=new Array(a);r[0]=d;var i={};for(var l in t)hasOwnProperty.call(t,l)&&(i[l]=t[l]);i.originalType=e,i.mdxType="string"==typeof e?e:o,r[1]=i;for(var u=2;u<a;u++)r[u]=n[u];return s.createElement.apply(null,r)}return s.createElement.apply(null,n)}d.displayName="MDXCreateElement"},8215:function(e,t,n){var s=n(7294);t.Z=function(e){var t=e.children,n=e.hidden,o=e.className;return s.createElement("div",{role:"tabpanel",hidden:n,className:o},t)}},6396:function(e,t,n){n.d(t,{Z:function(){return d}});var s=n(7462),o=n(7294),a=n(2389),r=n(9443);var i=function(){var e=(0,o.useContext)(r.Z);if(null==e)throw new Error('"useUserPreferencesContext" is used outside of "Layout" component.');return e},l=n(3616),u=n(6010),p="tabItem_vU9c";function c(e){var t,n,a,r=e.lazy,c=e.block,d=e.defaultValue,m=e.values,_=e.groupId,h=e.className,f=o.Children.map(e.children,(function(e){if((0,o.isValidElement)(e)&&void 0!==e.props.value)return e;throw new Error("Docusaurus error: Bad <Tabs> child <"+("string"==typeof e.type?e.type:e.type.name)+'>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.')})),v=null!=m?m:f.map((function(e){var t=e.props;return{value:t.value,label:t.label,attributes:t.attributes}})),b=(0,l.lx)(v,(function(e,t){return e.value===t.value}));if(b.length>0)throw new Error('Docusaurus error: Duplicate values "'+b.map((function(e){return e.value})).join(", ")+'" found in <Tabs>. Every value needs to be unique.');var g=null===d?d:null!=(t=null!=d?d:null==(n=f.find((function(e){return e.props.default})))?void 0:n.props.value)?t:null==(a=f[0])?void 0:a.props.value;if(null!==g&&!v.some((function(e){return e.value===g})))throw new Error('Docusaurus error: The <Tabs> has a defaultValue "'+g+'" but none of its children has the corresponding value. Available values are: '+v.map((function(e){return e.value})).join(", ")+". If you intend to show no default tab, use defaultValue={null} instead.");var y=i(),w=y.tabGroupChoices,k=y.setTabGroupChoices,x=(0,o.useState)(g),T=x[0],C=x[1],E=[],N=(0,l.o5)().blockElementScrollPositionUntilNextRender;if(null!=_){var P=w[_];null!=P&&P!==T&&v.some((function(e){return e.value===P}))&&C(P)}var D=function(e){var t=e.currentTarget,n=E.indexOf(t),s=v[n].value;s!==T&&(N(t),C(s),null!=_&&k(_,s))},O=function(e){var t,n=null;switch(e.key){case"ArrowRight":var s=E.indexOf(e.currentTarget)+1;n=E[s]||E[0];break;case"ArrowLeft":var o=E.indexOf(e.currentTarget)-1;n=E[o]||E[E.length-1]}null==(t=n)||t.focus()};return o.createElement("div",{className:"tabs-container"},o.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,u.Z)("tabs",{"tabs--block":c},h)},v.map((function(e){var t=e.value,n=e.label,a=e.attributes;return o.createElement("li",(0,s.Z)({role:"tab",tabIndex:T===t?0:-1,"aria-selected":T===t,key:t,ref:function(e){return E.push(e)},onKeyDown:O,onFocus:D,onClick:D},a,{className:(0,u.Z)("tabs__item",p,null==a?void 0:a.className,{"tabs__item--active":T===t})}),null!=n?n:t)}))),r?(0,o.cloneElement)(f.filter((function(e){return e.props.value===T}))[0],{className:"margin-vert--md"}):o.createElement("div",{className:"margin-vert--md"},f.map((function(e,t){return(0,o.cloneElement)(e,{key:t,hidden:e.props.value!==T})}))))}function d(e){var t=(0,a.Z)();return o.createElement(c,(0,s.Z)({key:String(t)},e))}},7810:function(e,t,n){n.r(t),n.d(t,{frontMatter:function(){return u},contentTitle:function(){return p},metadata:function(){return c},toc:function(){return d},default:function(){return _}});var s=n(7462),o=n(3366),a=(n(7294),n(3905)),r=n(6396),i=n(8215),l=["components"],u={description:"Develop your own custom text classifier.",sidebar_position:4},p="Custom Text Model",c={unversionedId:"api-guide/model/custom-text-model-walkthrough",id:"api-guide/model/custom-text-model-walkthrough",title:"Custom Text Model",description:"Develop your own custom text classifier.",source:"@site/docs/api-guide/model/custom-text-model-walkthrough.md",sourceDirName:"api-guide/model",slug:"/api-guide/model/custom-text-model-walkthrough",permalink:"/api-guide/model/custom-text-model-walkthrough",editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/api-guide/model/custom-text-model-walkthrough.md",tags:[],version:"current",sidebarPosition:4,frontMatter:{description:"Develop your own custom text classifier.",sidebar_position:4},sidebar:"tutorialSidebar",previous:{title:"Custom Models",permalink:"/api-guide/model/custom-model-walkthrough"},next:{title:"Create, Get, Update, Delete",permalink:"/api-guide/model/create-get-update-and-delete"}},d=[{value:"Create a new application",id:"create-a-new-application",children:[],level:2},{value:"Add a batch of texts",id:"add-a-batch-of-texts",children:[],level:2},{value:"Wait for inputs to download",id:"wait-for-inputs-to-download",children:[],level:2},{value:"Create a custom model",id:"create-a-custom-model",children:[],level:2},{value:"Train the model",id:"train-the-model",children:[],level:2},{value:"Wait for model training to complete",id:"wait-for-model-training-to-complete",children:[],level:2},{value:"Predict on new inputs",id:"predict-on-new-inputs",children:[],level:2},{value:"Start model evaluation",id:"start-model-evaluation",children:[],level:2},{value:"Wait for model evaluation results",id:"wait-for-model-evaluation-results",children:[],level:2}],m={toc:d};function _(e){var t=e.components,n=(0,o.Z)(e,l);return(0,a.kt)("wrapper",(0,s.Z)({},m,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"custom-text-model"},"Custom Text Model"),(0,a.kt)("p",null,"The Clarifai API has the ability not only to learn concepts from images and videos, but from text as well."),(0,a.kt)("p",null,"In this walkthrough, you'll learn how to create and use a custom text model, learn from your own text data using the power of the Clarifai's base Text model, and predict on new text examples."),(0,a.kt)("p",null,"The steps below can all be done via ",(0,a.kt)("a",{parentName:"p",href:"https://portal.clarifai.com"},"the Clarifai's portal"),". But here you'll learn how to do them programmatically via API, using our ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/Clarifai/clarifai-python-grpc"},"gRPC Python client"),"."),(0,a.kt)("p",null,"The examples below map directly to any of our other gRPC clients."),(0,a.kt)("p",null,"The walkthrough assumes you have already created your Clarifai's user account and the ",(0,a.kt)("a",{parentName:"p",href:"https://portal.clarifai.com/settings/authentication"},"Personal Access Token"),". Also, first set up the gRPC Python client together with the initial code, see ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/Clarifai/docs/tree/1c1d25cdd43190c38a2edb313297c0d566b3a0e3/api-guide/api-overview/api-clients/README.md#client-installation-instructions"},"Client Installation Instructions"),"."),(0,a.kt)("p",null,"For debugging purposes, each response returned by a method call can be printed to the console, and its entire data and structure will be shown verbosely."),(0,a.kt)("h2",{id:"create-a-new-application"},"Create a new application"),(0,a.kt)("p",null,"The first step is manual: in the Clarifai Portal, ",(0,a.kt)("a",{parentName:"p",href:"https://docs.clarifai.com/getting-started/applications/create-an-application"},"create an new application")," with ",(0,a.kt)("strong",{parentName:"p"},"Text")," selected as the Base Workflow."),(0,a.kt)("p",null,"Afterward, copy the newly-created application's ",(0,a.kt)("em",{parentName:"p"},"API key")," and set it in the variable below. This variable is going to be used, for authorization purposes, by all Clarifai API calls that follow."),(0,a.kt)(r.Z,{mdxType:"Tabs"},(0,a.kt)(i.Z,{value:"grpc_python",label:"gRPC Python",mdxType:"TabItem"},(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"# Insert here the initialization code as outlined on this page:\n# https://docs.clarifai.com/api-guide/api-overview/api-clients#client-installation-instructions\n\napi_key_metadata = (('authorization', 'Key ' + post_keys_response.keys[0].id),)\n")))),(0,a.kt)("h2",{id:"add-a-batch-of-texts"},"Add a batch of texts"),(0,a.kt)("p",null,"We'll now add several text inputs that we will later use as a training data in our custom model. The idea is that we'll create a model which can differentiate between positive and negative sentences ","(","in a grammatical sense",")",". We'll mark each input with one of the two concepts: ",(0,a.kt)("inlineCode",{parentName:"p"},"positive")," or ",(0,a.kt)("inlineCode",{parentName:"p"},"negative"),"."),(0,a.kt)("p",null,"The text can be added either directly ","(","it's called raw",")",", or from a URL."),(0,a.kt)(r.Z,{mdxType:"Tabs"},(0,a.kt)(i.Z,{value:"grpc_python",label:"gRPC Python",mdxType:"TabItem"},(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'positive_raw_texts = [\n    "Marie is a published author.",\n    "In three years, everyone will be happy.",\n    "Nora Roberts is the most prolific romance writer the world has ever known.",\n    "She has written more than 225 books.",\n    "If you walk into Knoxville, you\'ll find a shop named Rala.",\n    "There are more than 850 miles of hiking trails in the Great Smoky Mountains.",\n    "Harrison Ford is 6\'1\\".",\n    "According to Reader\'s Digest, in the original script of Return of The Jedi, Han Solo died.",\n    "Kate travels to Doolin, Ireland every year for a writers\' conference.",\n    "Fort Stevens was decommissioned by the United States military in 1947.",\n]\nnegative_text_urls = [\n    "https://samples.clarifai.com/negative_sentence_1.txt",\n    "https://samples.clarifai.com/negative_sentence_2.txt",\n    "https://samples.clarifai.com/negative_sentence_3.txt",\n    "https://samples.clarifai.com/negative_sentence_4.txt",\n    "https://samples.clarifai.com/negative_sentence_5.txt",\n    "https://samples.clarifai.com/negative_sentence_6.txt",\n    "https://samples.clarifai.com/negative_sentence_7.txt",\n    "https://samples.clarifai.com/negative_sentence_8.txt",\n    "https://samples.clarifai.com/negative_sentence_9.txt",\n    "https://samples.clarifai.com/negative_sentence_10.txt",\n]\n\npost_inputs_response = stub.PostInputs(\n    service_pb2.PostInputsRequest(\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    text=resources_pb2.Text(raw=raw_text),\n                    concepts=[resources_pb2.Concept(id="positive", value=1)]\n                )\n            )\n            for raw_text in positive_raw_texts\n        ] + [\n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    text=resources_pb2.Text(\n                        url=text_url,\n                        allow_duplicate_url=True\n                    ),\n                    concepts=[resources_pb2.Concept(id="negative", value=1)]\n                )\n            )\n            for text_url in negative_text_urls\n        ]\n    ),\n    metadata=api_key_metadata\n)\n\n# You may print the response to see what the structure and the data of the response is.\n# print(post_inputs_response)\n\nif post_inputs_response.status.code != status_code_pb2.SUCCESS:\n    print("There was an error with your request!")\n    print("\\tCode: {}".format(post_inputs_response.outputs[0].status.code))\n    print("\\tDescription: {}".format(post_inputs_response.outputs[0].status.description))\n    print("\\tDetails: {}".format(post_inputs_response.outputs[0].status.details))\n    raise Exception("Failed response, status: " + str(post_inputs_response.status))\n')))),(0,a.kt)("h2",{id:"wait-for-inputs-to-download"},"Wait for inputs to download"),(0,a.kt)("p",null,"Let's now wait for all the inputs to download."),(0,a.kt)(r.Z,{mdxType:"Tabs"},(0,a.kt)(i.Z,{value:"grpc_python",label:"gRPC Python",mdxType:"TabItem"},(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'import time\n\nwhile True:\n    list_inputs_response = stub.ListInputs(\n        service_pb2.ListInputsRequest(page=1, per_page=100),\n        metadata=api_key_metadata\n    )\n\n    if list_inputs_response.status.code != status_code_pb2.SUCCESS:\n        print("There was an error with your request!")\n        print("\\tCode: {}".format(list_inputs_response.outputs[0].status.code))\n        print("\\tDescription: {}".format(list_inputs_response.outputs[0].status.description))\n        print("\\tDetails: {}".format(list_inputs_response.outputs[0].status.details))\n        raise Exception("Failed response, status: " + str(list_inputs_response.status))\n\n    for the_input in list_inputs_response.inputs:\n        input_status_code = the_input.status.code\n        if input_status_code == status_code_pb2.INPUT_DOWNLOAD_SUCCESS:\n            continue\n        elif input_status_code in (status_code_pb2.INPUT_DOWNLOAD_PENDING, status_code_pb2.INPUT_DOWNLOAD_IN_PROGRESS):\n            print("Not all inputs have been downloaded yet. Checking again shortly.")\n            break\n        else:\n            error_message = (\n                    str(input_status_code) + " " +\n                    the_input.status.description + " " +\n                    the_input.status.details\n            )\n            raise Exception(\n                f"Expected inputs to download, but got {error_message}. Full response: {list_inputs_response}"\n            )\n    else:\n        # Once all inputs have been successfully downloaded, break the while True loop.\n        print("All inputs have been successfully downloaded.")\n        break\n    time.sleep(2)\n')))),(0,a.kt)("h2",{id:"create-a-custom-model"},"Create a custom model"),(0,a.kt)("p",null,"Now we can create a custom model that's going to be using the concepts ",(0,a.kt)("inlineCode",{parentName:"p"},"positive")," and ",(0,a.kt)("inlineCode",{parentName:"p"},"negative"),". All inputs ","(","in our application",")"," associated with these two concepts will be used as a training data, once we actually train the model."),(0,a.kt)(r.Z,{mdxType:"Tabs"},(0,a.kt)(i.Z,{value:"grpc_python",label:"gRPC Python",mdxType:"TabItem"},(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'post_models_response = stub.PostModels(\n    service_pb2.PostModelsRequest(\n        models=[\n            resources_pb2.Model(\n                id="my-text-model",\n                output_info=resources_pb2.OutputInfo(\n                    data=resources_pb2.Data(\n                        concepts=[\n                            resources_pb2.Concept(id="positive"),\n                            resources_pb2.Concept(id="negative"),\n                        ]\n                    ),\n                    output_config=resources_pb2.OutputConfig(closed_environment=True)\n                )\n            )\n        ]\n    ),\n    metadata=api_key_metadata\n)\n\nif post_models_response.status.code != status_code_pb2.SUCCESS:\n    print("There was an error with your request!")\n    print("\\tCode: {}".format(post_models_response.outputs[0].status.code))\n    print("\\tDescription: {}".format(post_models_response.outputs[0].status.description))\n    print("\\tDetails: {}".format(post_models_response.outputs[0].status.details))\n    raise Exception("Failed response, status: " + str(post_models_response.status))\n')))),(0,a.kt)("h2",{id:"train-the-model"},"Train the model"),(0,a.kt)("p",null,"Let's train the model, making it learn from the inputs, so we can later use it to predict new text examples!"),(0,a.kt)(r.Z,{mdxType:"Tabs"},(0,a.kt)(i.Z,{value:"grpc_python",label:"gRPC Python",mdxType:"TabItem"},(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'post_model_versions_response = stub.PostModelVersions(\n    service_pb2.PostModelVersionsRequest(model_id="my-text-model"),\n    metadata=api_key_metadata\n)\n\nif post_model_versions_response.status.code != status_code_pb2.SUCCESS:\n    print("There was an error with your request!")\n    print("\\tCode: {}".format(post_model_versions_response.outputs[0].status.code))\n    print("\\tDescription: {}".format(post_model_versions_response.outputs[0].status.description))\n    print("\\tDetails: {}".format(post_model_versions_response.outputs[0].status.details))\n    raise Exception("Failed response, status: " + str(post_model_versions_response.status))\n')))),(0,a.kt)("h2",{id:"wait-for-model-training-to-complete"},"Wait for model training to complete"),(0,a.kt)("p",null,"Let's wait for the model training to complete."),(0,a.kt)("p",null,"Each model training produces a new model version. See on the bottom of the code example, that we put the model version ID into its own variable. We'll be using it below to specify which specific model version we want to use ","(","since a model can have multiple versions",")","."),(0,a.kt)(r.Z,{mdxType:"Tabs"},(0,a.kt)(i.Z,{value:"grpc_python",label:"gRPC Python",mdxType:"TabItem"},(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'import time\n\nwhile True:\n    get_model_response = stub.GetModel(\n        service_pb2.GetModelRequest(model_id="my-text-model"),\n        metadata=api_key_metadata\n    )\n\n    if get_model_response.status.code != status_code_pb2.SUCCESS:\n        print("There was an error with your request!")\n        print("\\tCode: {}".format(get_model_response.outputs[0].status.code))\n        print("\\tDescription: {}".format(get_model_response.outputs[0].status.description))\n        print("\\tDetails: {}".format(get_model_response.outputs[0].status.details))\n        raise Exception("Failed response, status: " + str(get_model_response.status))\n\n    version_status_code = get_model_response.model.model_version.status.code\n    if version_status_code == status_code_pb2.MODEL_TRAINED:\n        print("The model has been successfully trained.")\n        break\n    elif version_status_code in (status_code_pb2.MODEL_QUEUED_FOR_TRAINING, status_code_pb2.MODEL_TRAINING):\n        print("The model hasn\'t been trained yet. Trying again shortly.")\n        time.sleep(2)\n    else:\n        error_message = (\n                str(get_model_response.status.code) + " " +\n                get_model_response.status.description + " " +\n                get_model_response.status.details\n        )\n        raise Exception(\n            f"Expected model to train, but got {error_message}. Full response: {get_model_response}"\n        )\n\nmodel_version_id = get_model_response.model.model_version.id\n')))),(0,a.kt)("h2",{id:"predict-on-new-inputs"},"Predict on new inputs"),(0,a.kt)("p",null,"Now we can use the new custom model to predict new text examples."),(0,a.kt)(r.Z,{mdxType:"Tabs"},(0,a.kt)(i.Z,{value:"grpc_python",label:"gRPC Python",mdxType:"TabItem"},(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'post_model_outputs_response = stub.PostModelOutputs(\n    service_pb2.PostModelOutputsRequest(\n        model_id="my-text-model",\n        # By default, the latest model version will be used, but it doesn\'t hurt to set it explicitly.\n        version_id=model_version_id,\n        inputs=[\n            resources_pb2.Input(data=resources_pb2.Data(text=resources_pb2.Text(raw="Butchart Gardens contains over 900 varieties of plants."))),\n            resources_pb2.Input(data=resources_pb2.Data(text=resources_pb2.Text(url="https://samples.clarifai.com/negative_sentence_12.txt"))),\n        ]\n    ),\n    metadata=api_key_metadata\n)\n\nif post_model_outputs_response.status.code != status_code_pb2.SUCCESS:\n    print("There was an error with your request!")\n    print("\\tCode: {}".format(post_model_outputs_response.outputs[0].status.code))\n    print("\\tDescription: {}".format(post_model_outputs_response.outputs[0].status.description))\n    print("\\tDetails: {}".format(post_model_outputs_response.outputs[0].status.details))\n    raise Exception("Failed response, status: " + str(post_model_outputs_response.status))\n\nfor output in post_model_outputs_response.outputs:\n    text_object = output.input.data.text\n    val = text_object.raw if text_object.raw else text_object.url\n\n    print(f"The following concepts were predicted for the input `{val}`:")\n    for concept in output.data.concepts:\n        print(f"\\t{concept.name}: {concept.value:.2f}")\n')))),(0,a.kt)("h2",{id:"start-model-evaluation"},"Start model evaluation"),(0,a.kt)("p",null,"Let's now test the performance of the model by using model evaluation. See the ",(0,a.kt)("a",{parentName:"p",href:"https://docs.clarifai.com/portal-guide/model/evaluate"},"the Model Evaluation page")," to learn more."),(0,a.kt)(r.Z,{mdxType:"Tabs"},(0,a.kt)(i.Z,{value:"grpc_python",label:"gRPC Python",mdxType:"TabItem"},(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'post_model_version_metrics = stub.PostModelVersionMetrics(\n    service_pb2.PostModelVersionMetricsRequest(\n        model_id="my-text-model",\n        version_id=model_version_id\n    ),\n    metadata=api_key_metadata\n)\n\nif post_model_version_metrics.status.code != status_code_pb2.SUCCESS:\n    print("There was an error with your request!")\n    print("\\tCode: {}".format(post_model_version_metrics.outputs[0].status.code))\n    print("\\tDescription: {}".format(post_model_version_metrics.outputs[0].status.description))\n    print("\\tDetails: {}".format(post_model_version_metrics.outputs[0].status.details))\n    raise Exception("Failed response, status: " + str(post_model_version_metrics.status))\n')))),(0,a.kt)("h2",{id:"wait-for-model-evaluation-results"},"Wait for model evaluation results"),(0,a.kt)("p",null,"Model evaluation takes some time, depending on the amount of data in our model. Let's wait for it to complete, and print all the results that it gives us."),(0,a.kt)(r.Z,{mdxType:"Tabs"},(0,a.kt)(i.Z,{value:"grpc_python",label:"gRPC Python",mdxType:"TabItem"},(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'import time\n\nwhile True:\n    get_model_version_metrics_response = stub.GetModelVersionMetrics(\n        service_pb2.GetModelVersionMetricsRequest(\n            model_id="my-text-model",\n            version_id=model_version_id,\n            fields=resources_pb2.FieldsValue(\n                confusion_matrix=True,\n                cooccurrence_matrix=True,\n                label_counts=True,\n                binary_metrics=True,\n                test_set=True,\n                metrics_by_area=True,\n                metrics_by_class=True,\n            )\n        ),\n        metadata=api_key_metadata\n    )\n\n    if get_model_version_metrics_response.status.code != status_code_pb2.SUCCESS:\n        print("There was an error with your request!")\n        print("\\tCode: {}".format(get_model_version_metrics_response.outputs[0].status.code))\n        print("\\tDescription: {}".format(get_model_version_metrics_response.outputs[0].status.description))\n        print("\\tDetails: {}".format(get_model_version_metrics_response.outputs[0].status.details))\n        raise Exception("Get model version metrics failed: " + str(get_model_version_metrics_response.status))\n\n    metrics_status_code = get_model_version_metrics_response.model_version.metrics.status.code\n    if metrics_status_code == status_code_pb2.MODEL_EVALUATED:\n        print("The model has been successfully evaluated.")\n        break\n    elif metrics_status_code in (status_code_pb2.MODEL_NOT_EVALUATED, status_code_pb2.MODEL_QUEUED_FOR_EVALUATION, status_code_pb2.MODEL_EVALUATING):\n        print("The model hasn\'t been evaluated yet. Trying again shortly.")\n        time.sleep(2)\n    else:\n        error_message = (\n                str(get_model_version_metrics_response.status.code) + " " +\n                get_model_version_metrics_response.status.description + " " +\n                get_model_version_metrics_response.status.details\n        )\n        raise Exception(\n            f"Expected model to evaluate, but got {error_message}. Full response: {get_model_version_metrics_response}"\n        )\n\nprint("The model metrics response object:")\nprint(get_model_version_metrics_response)\n')))))}_.isMDXComponent=!0}}]);
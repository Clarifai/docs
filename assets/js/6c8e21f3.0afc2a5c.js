"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[8261],{77517:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>g,contentTitle:()=>h,default:()=>v,frontMatter:()=>m,metadata:()=>f,toc:()=>I});var a=n(74848),i=n(28453),r=n(11470),o=n(19365),s=n(21432);const l='from clarifai.client.model import Model\nfrom clarifai.client.input import Inputs\n\n# Your PAT (Personal Access Token) can be found in the Account\'s Security section\n# Specify the correct user_id/app_id pairings\n# Since you\'re making inferences outside your app\'s scope\n#USER_ID = "openai"\n#APP_ID = "chat-completion"\n\n# You can set the model using model URL or model ID.\n# Change these to whatever model you want to use\n# eg : MODEL_ID = \'openai-gpt-4-vision\'\n# You can also set a particular model version by specifying the  version ID\n# eg: MODEL_VERSION_ID = \'model_version\'\n#  Model class objects can be inititalised by providing its URL or also by defining respective user_id, app_id and model_id\n\n# eg : model = Model(user_id="clarifai", app_id="main", model_id=MODEL_ID)\n\nprompt = "What time of day is it?"\nimage_url = "https://samples.clarifai.com/metro-north.jpg"\n\nmodel_url = "https://clarifai.com/openai/chat-completion/models/openai-gpt-4-vision"\ninference_params = dict(temperature=0.2, max_tokens=100)\nmulti_inputs = Inputs.get_multimodal_input(input_id="", image_url=image_url, raw_text=prompt)\n\n# Predicts the model based on the given inputs.\n\nmodel_prediction = Model(url=model_url, pat="YOUR_PAT").predict(\n    inputs=[\n        multi_inputs\n    ],\n    inference_params=inference_params,\n)\n\nprint(model_prediction.outputs[0].data.text.raw)',c='import { Model, Input } from "clarifai-nodejs";\n\n/**\n    Your PAT (Personal Access Token) can be found in the Account\'s Security section\n    Specify the correct userId/appId pairings\n    Since you\'re making inferences outside your app\'s scope\n    USER_ID = "openai"\n    APP_ID = "chat-completion"\n\n    You can set the model using model URL or model ID.\n    Change these to whatever model you want to use\n    eg : MODEL_ID = \'openai-gpt-4-vision\'\n    You can also set a particular model version by specifying the  version ID\n    eg: MODEL_VERSION_ID = "model_version"\n    Model class objects can be initialised by providing its URL or also by defining respective userId, appId and modelId\n\n    eg : \n    const model = new Model({\n        authConfig: {\n            userId: "clarifai",\n            appId: "main",\n            pat: process.env.CLARIFAI_PAT,\n        },\n        modelId: MODEL_ID,\n    });\n\n*/\n\nconst prompt = "What time of day is it?";\nconst imageUrl = "https://samples.clarifai.com/metro-north.jpg";\nconst modelUrl =\n  "https://clarifai.com/openai/chat-completion/models/openai-gpt-4-vision";\nconst inferenceParams = { temperature: 0.2, maxTokens: 100 };\nconst multiInputs = Input.getMultimodalInput({\n  inputId: "",\n  imageUrl,\n  rawText: prompt,\n});\n\n/*\n        The predict API gives flexibility to generate predictions for data provided through URL, Filepath and bytes format.\n\n        Example for prediction through Bytes:\n        const modelPrediction = await model.predictByBytes({\n                                    inputBytes: Bytes,\n                                    inputType: "image"\n                                });\n\n        Example for prediction through Filepath:\n        const modelPrediction = await model.predictByFilepath({\n                                    filepath,\n                                    inputType: "image",\n                                });\n\n    */\n\n\nconst model = new Model({\n  url: modelUrl,\n  authConfig: { pat: process.env.CLARIFAI_PAT },\n});\n\nconst modelPrediction = await model.predict({\n  inputs: [multiInputs],\n  inferenceParams,\n});\n\nconsole.log(modelPrediction?.[0]?.data?.text?.raw);\n',u='from clarifai.client.model import Model\nfrom clarifai.client.input import Inputs\n\nIMAGE_FILE_LOCATION = \'LOCAL IMAGE PATH\'\nwith open(IMAGE_FILE_LOCATION, "rb") as f:\n    file_bytes = f.read()\n\n\nprompt = "What time of day is it?"\ninference_params = dict(temperature=0.2, max_tokens=100)\n\nmodel_prediction = Model("https://clarifai.com/openai/chat-completion/models/openai-gpt-4-vision").predict(inputs = [Inputs.get_multimodal_input(input_id="", image_bytes = file_bytes, raw_text=prompt)], inference_params=inference_params)\nprint(model_prediction.outputs[0].data.text.raw)\n',d="import { Model } from 'clarifai';\nimport { Inputs } from 'clarifai';\n\nconst IMAGE_FILE_LOCATION = 'LOCAL IMAGE PATH';\nconst fs = require('fs');\n\nconst file_bytes = fs.readFileSync(IMAGE_FILE_LOCATION);\n\nconst prompt = \"What time of day is it?\";\nconst inference_params = { temperature: 0.2, max_tokens: 100 };\n\nconst model = new Model(\"https://clarifai.com/openai/chat-completion/models/openai-gpt-4-vision\");\nmodel.predict({\n    inputs: [\n        Inputs.getMultimodalInput({\n            input_id: \"\",\n            image_bytes: file_bytes,\n            raw_text: prompt\n        })\n    ],\n    inference_params: inference_params\n}).then((model_prediction) => {\n    console.log(model_prediction.outputs[0].data.text.raw);\n});\n\n",p="The time of day in the image appears to be either dawn or dusk, given the light in the sky. It's not possible to determine the exact time without additional context, but the sky has a mix of light and dark hues, which typically occurs during sunrise or sunset. The presence of snow and the lighting at the train station suggest that it might be winter, and depending on the location, this could influence whether it's morning or evening.",m={sidebar_position:4},h="MultiModal as Input",f={id:"sdk/Inference-from-AI-Models/Multimodal-as-Input",title:"MultiModal as Input",description:"Learn how to perform inference with multimodal inputs using Clarifai SDKs",source:"@site/docs/sdk/Inference-from-AI-Models/Multimodal-as-Input.md",sourceDirName:"sdk/Inference-from-AI-Models",slug:"/sdk/Inference-from-AI-Models/Multimodal-as-Input",permalink:"/sdk/Inference-from-AI-Models/Multimodal-as-Input",draft:!1,unlisted:!1,editUrl:"https://github.com/Clarifai/docs/blob/main/docs/sdk/Inference-from-AI-Models/Multimodal-as-Input.md",tags:[],version:"current",sidebarPosition:4,frontMatter:{sidebar_position:4},sidebar:"tutorialSidebar",previous:{title:"Audio as Input",permalink:"/sdk/Inference-from-AI-Models/Audio-as-Input"},next:{title:"Advanced Inference Options",permalink:"/sdk/Inference-from-AI-Models/Advance-Inference-Options"}},g={},I=[{value:"[Image,Text] to Text",id:"imagetext-to-text",level:2},{value:"Predict Via Image URL",id:"predict-via-image-url",level:3},{value:"Predict Via Local Image",id:"predict-via-local-image",level:3}];function b(e){const t={a:"a",h1:"h1",h2:"h2",h3:"h3",p:"p",strong:"strong",...(0,i.R)(),...e.components},{Details:n}=t;return n||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.h1,{id:"multimodal-as-input",children:"MultiModal as Input"}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.strong,{children:"Learn how to perform inference with multimodal inputs using Clarifai SDKs"})}),"\n",(0,a.jsx)("hr",{}),"\n",(0,a.jsx)(t.p,{children:"Multi-modal inputs refer to feeding multiple types of data into a single model for processing and analysis. These data types, or modalities, can be diverse, such as text, images, audio, video, sensor data, or any other form of structured or unstructured data."}),"\n",(0,a.jsx)(t.h2,{id:"imagetext-to-text",children:"[Image,Text] to Text"}),"\n",(0,a.jsxs)(t.p,{children:["Leverage the power of the Predict API to seamlessly process multimodal inputs and obtain accurate predictions. In this example, we demonstrate the capability to send both image and text inputs to a ",(0,a.jsx)(t.a,{href:"https://clarifai.com/openai/chat-completion/models/openai-gpt-4-vision",children:"model"}),", showcasing the versatility of the Predict API in handling diverse data types."]}),"\n",(0,a.jsx)(t.h3,{id:"predict-via-image-url",children:"Predict Via Image URL"}),"\n",(0,a.jsxs)(r.A,{children:[(0,a.jsxs)(o.A,{value:"python",label:"Python",children:[(0,a.jsx)(s.A,{className:"language-python",children:l}),(0,a.jsxs)(n,{children:[(0,a.jsx)("summary",{children:"Output"}),(0,a.jsx)(s.A,{className:"language-text",children:p})]})]}),(0,a.jsx)(o.A,{value:"typescript",label:"Typescript",children:(0,a.jsx)(s.A,{className:"language-typescript",children:c})})]}),"\n",(0,a.jsx)(t.h3,{id:"predict-via-local-image",children:"Predict Via Local Image"}),"\n",(0,a.jsxs)(r.A,{children:[(0,a.jsxs)(o.A,{value:"python",label:"Python",children:[(0,a.jsx)(s.A,{className:"language-python",children:u}),(0,a.jsxs)(n,{children:[(0,a.jsx)("summary",{children:"Output"}),(0,a.jsx)(s.A,{className:"language-text",children:p})]})]}),(0,a.jsx)(o.A,{value:"typescript",label:"Typescript",children:(0,a.jsx)(s.A,{className:"language-typescript",children:d})})]})]})}function v(e={}){const{wrapper:t}={...(0,i.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(b,{...e})}):b(e)}},19365:(e,t,n)=>{n.d(t,{A:()=>o});n(96540);var a=n(18215);const i={tabItem:"tabItem_Ymn6"};var r=n(74848);function o(e){let{children:t,hidden:n,className:o}=e;return(0,r.jsx)("div",{role:"tabpanel",className:(0,a.A)(i.tabItem,o),hidden:n,children:t})}},11470:(e,t,n)=>{n.d(t,{A:()=>x});var a=n(96540),i=n(18215),r=n(23104),o=n(56347),s=n(205),l=n(57485),c=n(31682),u=n(70679);function d(e){return a.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,a.isValidElement)(e)&&function(e){const{props:t}=e;return!!t&&"object"==typeof t&&"value"in t}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function p(e){const{values:t,children:n}=e;return(0,a.useMemo)((()=>{const e=t??function(e){return d(e).map((e=>{let{props:{value:t,label:n,attributes:a,default:i}}=e;return{value:t,label:n,attributes:a,default:i}}))}(n);return function(e){const t=(0,c.X)(e,((e,t)=>e.value===t.value));if(t.length>0)throw new Error(`Docusaurus error: Duplicate values "${t.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[t,n])}function m(e){let{value:t,tabValues:n}=e;return n.some((e=>e.value===t))}function h(e){let{queryString:t=!1,groupId:n}=e;const i=(0,o.W6)(),r=function(e){let{queryString:t=!1,groupId:n}=e;if("string"==typeof t)return t;if(!1===t)return null;if(!0===t&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:t,groupId:n});return[(0,l.aZ)(r),(0,a.useCallback)((e=>{if(!r)return;const t=new URLSearchParams(i.location.search);t.set(r,e),i.replace({...i.location,search:t.toString()})}),[r,i])]}function f(e){const{defaultValue:t,queryString:n=!1,groupId:i}=e,r=p(e),[o,l]=(0,a.useState)((()=>function(e){let{defaultValue:t,tabValues:n}=e;if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(t){if(!m({value:t,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${t}" but none of its children has the corresponding value. Available values are: ${n.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return t}const a=n.find((e=>e.default))??n[0];if(!a)throw new Error("Unexpected error: 0 tabValues");return a.value}({defaultValue:t,tabValues:r}))),[c,d]=h({queryString:n,groupId:i}),[f,g]=function(e){let{groupId:t}=e;const n=function(e){return e?`docusaurus.tab.${e}`:null}(t),[i,r]=(0,u.Dv)(n);return[i,(0,a.useCallback)((e=>{n&&r.set(e)}),[n,r])]}({groupId:i}),I=(()=>{const e=c??f;return m({value:e,tabValues:r})?e:null})();(0,s.A)((()=>{I&&l(I)}),[I]);return{selectedValue:o,selectValue:(0,a.useCallback)((e=>{if(!m({value:e,tabValues:r}))throw new Error(`Can't select invalid tab value=${e}`);l(e),d(e),g(e)}),[d,g,r]),tabValues:r}}var g=n(92303);const I={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var b=n(74848);function v(e){let{className:t,block:n,selectedValue:a,selectValue:o,tabValues:s}=e;const l=[],{blockElementScrollPositionUntilNextRender:c}=(0,r.a_)(),u=e=>{const t=e.currentTarget,n=l.indexOf(t),i=s[n].value;i!==a&&(c(t),o(i))},d=e=>{let t=null;switch(e.key){case"Enter":u(e);break;case"ArrowRight":{const n=l.indexOf(e.currentTarget)+1;t=l[n]??l[0];break}case"ArrowLeft":{const n=l.indexOf(e.currentTarget)-1;t=l[n]??l[l.length-1];break}}t?.focus()};return(0,b.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,i.A)("tabs",{"tabs--block":n},t),children:s.map((e=>{let{value:t,label:n,attributes:r}=e;return(0,b.jsx)("li",{role:"tab",tabIndex:a===t?0:-1,"aria-selected":a===t,ref:e=>l.push(e),onKeyDown:d,onClick:u,...r,className:(0,i.A)("tabs__item",I.tabItem,r?.className,{"tabs__item--active":a===t}),children:n??t},t)}))})}function y(e){let{lazy:t,children:n,selectedValue:i}=e;const r=(Array.isArray(n)?n:[n]).filter(Boolean);if(t){const e=r.find((e=>e.props.value===i));return e?(0,a.cloneElement)(e,{className:"margin-top--md"}):null}return(0,b.jsx)("div",{className:"margin-top--md",children:r.map(((e,t)=>(0,a.cloneElement)(e,{key:t,hidden:e.props.value!==i})))})}function _(e){const t=f(e);return(0,b.jsxs)("div",{className:(0,i.A)("tabs-container",I.tabList),children:[(0,b.jsx)(v,{...t,...e}),(0,b.jsx)(y,{...t,...e})]})}function x(e){const t=(0,g.A)();return(0,b.jsx)(_,{...e,children:d(e.children)},String(t))}}}]);
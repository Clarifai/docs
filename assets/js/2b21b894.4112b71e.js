"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[6257],{46598:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>m,contentTitle:()=>h,default:()=>x,frontMatter:()=>p,metadata:()=>r,toc:()=>f});const r=JSON.parse('{"id":"getting-started/upload-model","title":"Upload Your First Model","description":"Upload a model from Hugging Face to the Clarifai platform","source":"@site/docs/getting-started/upload-model.md","sourceDirName":"getting-started","slug":"/getting-started/upload-model","permalink":"/getting-started/upload-model","draft":false,"unlisted":false,"editUrl":"https://github.com/Clarifai/docs/blob/main/docs/getting-started/upload-model.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"description":"Upload a model from Hugging Face to the Clarifai platform","sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Deploy Your First Model","permalink":"/getting-started/first-deployment"},"next":{"title":"Compute Orchestration","permalink":"/compute/overview"}}');var a=n(74848),o=n(28453),s=n(65537),i=n(79329),l=n(58069);const d='from typing import List, Iterator\nfrom threading import Thread\nimport os\nimport torch\n\nfrom clarifai.runners.models.model_class import ModelClass\nfrom clarifai.utils.logging import logger\nfrom clarifai.runners.models.model_builder import ModelBuilder\nfrom clarifai.runners.utils.openai_convertor import openai_response\nfrom transformers import (AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer)\n\n\nclass MyModel(ModelClass):\n  """A custom runner for llama-3.2-1b-instruct llm that integrates with the Clarifai platform"""\n\n  def load_model(self):\n    """Load the model here."""\n    self.device = \'cuda\' if torch.cuda.is_available() else \'cpu\'\n    logger.info(f"Running on device: {self.device}")\n\n    # Load checkpoints\n    model_path = os.path.dirname(os.path.dirname(__file__))\n    builder = ModelBuilder(model_path, download_validation_only=True)\n    self.checkpoints = builder.download_checkpoints(stage="runtime")\n    \n    # Load model and tokenizer\n    self.tokenizer = AutoTokenizer.from_pretrained(self.checkpoints,)\n    self.tokenizer.pad_token = self.tokenizer.eos_token  # Set pad token to eos token\n    self.model = AutoModelForCausalLM.from_pretrained(\n        self.checkpoints,\n        low_cpu_mem_usage=True,\n        device_map=self.device,\n        torch_dtype=torch.bfloat16,\n    )\n    self.streamer = TextIteratorStreamer(tokenizer=self.tokenizer,)\n    self.chat_template = None\n    logger.info("Done loading!")\n\n  @ModelClass.method\n  def predict(self,\n              prompt: str ="",\n              chat_history: List[dict] = None,\n              max_tokens: int = 512,\n              temperature: float = 0.7,\n              top_p: float = 0.8) -> str:\n    """\n    Predict the response for the given prompt and chat history using the model.\n    """\n    # Construct chat-style messages\n    messages = chat_history if chat_history else []\n    if prompt:\n        messages.append({\n            "role": "user",\n            "content": [{"type": "text", "text": prompt}]\n        })\n\n    inputs = self.tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors="pt").to(self.model.device)\n\n    generation_kwargs = {\n        "input_ids": inputs["input_ids"],\n        "do_sample": True,\n        "max_new_tokens": max_tokens,\n        "temperature": temperature,\n        "top_p": top_p,\n        "eos_token_id": self.tokenizer.eos_token_id,\n    }\n\n    output = self.model.generate(**generation_kwargs)\n    generated_tokens = output[0][inputs["input_ids"].shape[-1]:]\n    return self.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n\n  @ModelClass.method\n  def generate(self,\n              prompt: str="",\n              chat_history: List[dict] = None,\n              max_tokens: int = 512,\n              temperature: float = 0.7,\n              top_p: float = 0.8) -> Iterator[str]:\n      """Stream generated text tokens from a prompt + optional chat history."""\n\n      # Construct chat-style messages\n      messages = chat_history if chat_history else []\n      if prompt:\n          messages.append({\n              "role": "user",\n              "content": [{"type": "text", "text": prompt}]\n          })\n      \n      response = self.chat(\n          messages=messages,\n          max_tokens=max_tokens,\n          temperature=temperature,\n          top_p=top_p\n      )\n      for each in response:\n        yield each[\'choices\'][0][\'delta\'][\'content\']\n\n\n  @ModelClass.method\n  def chat(self,\n          messages: List[dict],\n          max_tokens: int = 512,\n          temperature: float = 0.7,\n          top_p: float = 0.8) -> Iterator[dict]:\n      """\n      Stream back JSON dicts for assistant messages.\n      Example return format:\n      {"role": "assistant", "content": [{"type": "text", "text": "response here"}]}\n      """\n\n      # Tokenize using chat template\n      inputs = self.tokenizer.apply_chat_template(\n          messages,\n          tokenize=True,\n          add_generation_prompt=True,\n          return_tensors="pt"\n      ).to(self.model.device)\n\n      generation_kwargs = {\n          "input_ids": inputs["input_ids"],\n          "do_sample": True,\n          "max_new_tokens": max_tokens,\n          "temperature": temperature,\n          "top_p": top_p,\n          "eos_token_id": self.tokenizer.eos_token_id,\n          "streamer": self.streamer\n      }\n\n      thread = Thread(target=self.model.generate, kwargs=generation_kwargs)\n      thread.start()\n\n      # Accumulate response text\n      for token_text in self.streamer:\n         yield openai_response(token_text)\n\n      thread.join()\n\n\n  def test(self):\n    """Test the model here."""\n    try:\n      print("Testing predict...")\n      # Test predict\n      print(self.predict(prompt="What is the capital of India?",))\n    except Exception as e:\n      print("Error in predict", e)\n\n    try:\n      print("Testing generate...")\n      # Test generate\n      for each in self.generate(prompt="What is the capital of India?",):\n        print(each, end="")\n      print()\n    except Exception as e:\n      print("Error in generate", e)\n\n    try:\n      print("Testing chat...")\n      messages = [\n        {"role": "system", "content": "You are an helpful assistant."},\n        {"role": "user", "content": "What is the capital of India?"},\n      ]\n      for each in self.chat(messages=messages,):\n        print(each, end="")\n      print()\n    except Exception as e:\n      print("Error in generate", e)',c='model:\n  id: "llama_3_2_1b_instruct"\n  user_id: "user_id"\n  app_id: "app_id"\n  model_type_id: "text-to-text"\n\nbuild_info:\n  python_version: "3.11"\n\ninference_compute_info:\n  cpu_limit: "1"\n  cpu_memory: "13Gi"\n  num_accelerators: 1\n  accelerator_type: ["NVIDIA-*"]\n  accelerator_memory: "18Gi"\n\ncheckpoints:\n  type: "huggingface"\n  repo_id: "unsloth/Llama-3.2-1B-Instruct"\n  hf_token: "hf_token"\n  when: "runtime"',u="torch==2.5.1\ntokenizers>=0.21.0\ntransformers>=4.47.0\naccelerate>=1.2.0\nscipy==1.10.1\noptimum>=1.23.3\nprotobuf==5.27.3\neinops>=0.8.0\nrequests==2.32.3\nclarifai>=11.3.0",p={description:"Upload a model from Hugging Face to the Clarifai platform",sidebar_position:3},h="Upload Your First Model",m={},f=[{value:"Step 1: Perform Prerequisites",id:"step-1-perform-prerequisites",level:2},{value:"Install Clarifai Package",id:"install-clarifai-package",level:3},{value:"Set a PAT Key",id:"set-a-pat-key",level:3},{value:"Get a Hugging Face Access Token",id:"get-a-hugging-face-access-token",level:3},{value:"Step 2: Create Files",id:"step-2-create-files",level:2},{value:"<code>model.py</code>",id:"modelpy",level:3},{value:"<code>requirements.txt</code>",id:"requirementstxt",level:3},{value:"<code>config.yaml</code>",id:"configyaml",level:3},{value:"Step 3: Upload the Model",id:"step-3-upload-the-model",level:2}];function g(e){const t={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.header,{children:(0,a.jsx)(t.h1,{id:"upload-your-first-model",children:"Upload Your First Model"})}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.strong,{children:"Upload a model from Hugging Face to the Clarifai platform"})}),"\n",(0,a.jsx)("hr",{}),"\n",(0,a.jsx)(t.p,{children:"The Clarifai platform allows you to upload custom models for a wide range of use cases. With just a few simple steps, you can get your models up and running and leverage the platform\u2019s powerful capabilities."}),"\n",(0,a.jsxs)(t.p,{children:["Let's demonstrate how you can upload the ",(0,a.jsx)(t.a,{href:"https://github.com/Clarifai/runners-examples/tree/main/llm/hf-llama-3_2-1b-instruct",children:"Llama-3.2-1B-Instruct"})," model from Hugging Face to the Clarifai platform."]}),"\n",(0,a.jsx)(t.admonition,{type:"tip",children:(0,a.jsxs)(t.p,{children:["To learn more about how to upload different types of models, check out ",(0,a.jsx)(t.a,{href:"https://docs.clarifai.com/compute/models/model-upload/",children:"this comprehensive guide"}),"."]})}),"\n","\n","\n",(0,a.jsx)(t.h2,{id:"step-1-perform-prerequisites",children:"Step 1: Perform Prerequisites"}),"\n",(0,a.jsx)(t.h3,{id:"install-clarifai-package",children:"Install Clarifai Package"}),"\n",(0,a.jsxs)(t.p,{children:["Install the latest version of the ",(0,a.jsx)(t.code,{children:"clarifai"})," Python SDK. This also installs the Clarifai ",(0,a.jsx)(t.a,{href:"https://docs.clarifai.com/additional-resources/api-overview/cli",children:"Command Line Interface (CLI)"}),", which we'll use for uploading the model."]}),"\n",(0,a.jsx)(s.A,{children:(0,a.jsx)(i.A,{value:"bash",label:"Bash",children:(0,a.jsx)(l.A,{className:"language-bash",children:" pip install --upgrade clarifai "})})}),"\n",(0,a.jsx)(t.h3,{id:"set-a-pat-key",children:"Set a PAT Key"}),"\n",(0,a.jsxs)(t.p,{children:["You need to set the ",(0,a.jsx)(t.code,{children:"CLARIFAI_PAT"})," (Personal Access Token) as an environment variable. You can generate the PAT key in your personal settings page by navigating to the ",(0,a.jsx)(t.a,{href:"https://clarifai.com/settings/security",children:"Security section"}),"."]}),"\n",(0,a.jsx)(t.p,{children:"This token is essential for authenticating your connection to the Clarifai platform."}),"\n",(0,a.jsxs)(s.A,{children:[(0,a.jsx)(i.A,{value:"bash",label:"Unix-Like Systems",children:(0,a.jsx)(l.A,{className:"language-bash",children:" export CLARIFAI_PAT=YOUR_PERSONAL_ACCESS_TOKEN_HERE "})}),(0,a.jsx)(i.A,{value:"bash2",label:"Windows",children:(0,a.jsx)(l.A,{className:"language-bash",children:" set CLARIFAI_PAT=YOUR_PERSONAL_ACCESS_TOKEN_HERE "})})]}),"\n",(0,a.jsx)(t.h3,{id:"get-a-hugging-face-access-token",children:"Get a Hugging Face Access Token"}),"\n",(0,a.jsx)(t.p,{children:"To download models from the Hugging Face platform, you'll need to authenticate your connection. You can create a Hugging Face account, then generate an access token to authorize your downloads."}),"\n",(0,a.jsxs)(t.p,{children:["You can follow the guide ",(0,a.jsx)(t.a,{href:"https://huggingface.co/docs/hub/en/security-tokens",children:"here"})," to get it."]}),"\n",(0,a.jsx)(t.h2,{id:"step-2-create-files",children:"Step 2: Create Files"}),"\n",(0,a.jsx)(t.p,{children:"Create a project directory and organize your files as indicated below to fit the requirements of uploading models to the Clarifai platform."}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-text",children:"your_model_directory/\n\u251c\u2500\u2500 1/\n\u2502   \u2514\u2500\u2500 model.py\n\u251c\u2500\u2500 requirements.txt\n\u2514\u2500\u2500 config.yaml\n"})}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"your_model_directory/"})," \u2013 The main directory containing your model files.","\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"1/"})," \u2013 A subdirectory that holds the model file (",(0,a.jsxs)(t.em,{children:["Note that the folder is named as ",(0,a.jsx)(t.strong,{children:"1"})]}),").","\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"model.py"})," \u2013 Contains the code that defines your model, including loading the model and running inference."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"requirements.txt"})," \u2013 Lists the Python libraries and dependencies required to run your model."]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"config.yaml"})," \u2013 Contains model metadata and configuration details necessary for building the Docker image, defining compute resources, and uploading the model to Clarifai."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(t.p,{children:"Add the following snippets to each of the respective files."}),"\n",(0,a.jsx)(t.h3,{id:"modelpy",children:(0,a.jsx)(t.code,{children:"model.py"})}),"\n",(0,a.jsx)(s.A,{children:(0,a.jsx)(i.A,{value:"python",label:"Python",children:(0,a.jsx)(l.A,{className:"language-python",children:d})})}),"\n",(0,a.jsx)(t.h3,{id:"requirementstxt",children:(0,a.jsx)(t.code,{children:"requirements.txt"})}),"\n",(0,a.jsx)(s.A,{children:(0,a.jsx)(i.A,{value:"text",label:"Text",children:(0,a.jsx)(l.A,{className:"language-text",children:u})})}),"\n",(0,a.jsx)(t.h3,{id:"configyaml",children:(0,a.jsx)(t.code,{children:"config.yaml"})}),"\n",(0,a.jsx)(t.admonition,{title:"important",type:"info",children:(0,a.jsxs)(t.p,{children:["In the ",(0,a.jsx)(t.code,{children:"model"})," section of the ",(0,a.jsx)(t.code,{children:"config.yaml"})," file, specify your model ID, Clarifai user ID, and Clarifai app ID. These will define where your model will be uploaded on the Clarifai platform. You also need to specify the ",(0,a.jsx)(t.code,{children:"hf_token"})," to authenticate your connection to Hugging Face, as ",(0,a.jsx)(t.a,{href:"#get-a-hugging-face-access-token",children:"described"})," earlier."]})}),"\n",(0,a.jsx)(s.A,{children:(0,a.jsx)(i.A,{value:"yaml",label:"YAML",children:(0,a.jsx)(l.A,{className:"language-yaml",children:c})})}),"\n",(0,a.jsx)(t.h2,{id:"step-3-upload-the-model",children:"Step 3: Upload the Model"}),"\n",(0,a.jsx)(t.p,{children:"Once your custom model is ready, upload it to the Clarifai platform by navigating to the directory containing the model and running the following command:"}),"\n",(0,a.jsx)(s.A,{children:(0,a.jsx)(i.A,{value:"bash",label:"CLI",children:(0,a.jsx)(l.A,{className:"language-bash",children:" clarifai model upload "})})}),"\n",(0,a.jsx)(t.p,{children:"Congratulations \u2014 you've just uploaded your first model to the Clarifai platform!"}),"\n",(0,a.jsxs)(t.p,{children:["Now, you can ",(0,a.jsx)(t.a,{href:"https://docs.clarifai.com/compute/deployments/deploy-model",children:"deploy"})," the model to a cluster and nodepool. This allows you to cost-efficiently and scalably make ",(0,a.jsx)(t.a,{href:"https://docs.clarifai.com/compute/models/inference/api",children:"inferences"})," with it."]})]})}function x(e={}){const{wrapper:t}={...(0,o.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(g,{...e})}):g(e)}},65537:(e,t,n)=>{n.d(t,{A:()=>k});var r=n(96540),a=n(18215),o=n(65627),s=n(56347),i=n(50372),l=n(30604),d=n(11861),c=n(78749);function u(e){return r.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,r.isValidElement)(e)&&function(e){const{props:t}=e;return!!t&&"object"==typeof t&&"value"in t}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function p(e){const{values:t,children:n}=e;return(0,r.useMemo)((()=>{const e=t??function(e){return u(e).map((e=>{let{props:{value:t,label:n,attributes:r,default:a}}=e;return{value:t,label:n,attributes:r,default:a}}))}(n);return function(e){const t=(0,d.XI)(e,((e,t)=>e.value===t.value));if(t.length>0)throw new Error(`Docusaurus error: Duplicate values "${t.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[t,n])}function h(e){let{value:t,tabValues:n}=e;return n.some((e=>e.value===t))}function m(e){let{queryString:t=!1,groupId:n}=e;const a=(0,s.W6)(),o=function(e){let{queryString:t=!1,groupId:n}=e;if("string"==typeof t)return t;if(!1===t)return null;if(!0===t&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:t,groupId:n});return[(0,l.aZ)(o),(0,r.useCallback)((e=>{if(!o)return;const t=new URLSearchParams(a.location.search);t.set(o,e),a.replace({...a.location,search:t.toString()})}),[o,a])]}function f(e){const{defaultValue:t,queryString:n=!1,groupId:a}=e,o=p(e),[s,l]=(0,r.useState)((()=>function(e){let{defaultValue:t,tabValues:n}=e;if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(t){if(!h({value:t,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${t}" but none of its children has the corresponding value. Available values are: ${n.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return t}const r=n.find((e=>e.default))??n[0];if(!r)throw new Error("Unexpected error: 0 tabValues");return r.value}({defaultValue:t,tabValues:o}))),[d,u]=m({queryString:n,groupId:a}),[f,g]=function(e){let{groupId:t}=e;const n=function(e){return e?`docusaurus.tab.${e}`:null}(t),[a,o]=(0,c.Dv)(n);return[a,(0,r.useCallback)((e=>{n&&o.set(e)}),[n,o])]}({groupId:a}),x=(()=>{const e=d??f;return h({value:e,tabValues:o})?e:null})();(0,i.A)((()=>{x&&l(x)}),[x]);return{selectedValue:s,selectValue:(0,r.useCallback)((e=>{if(!h({value:e,tabValues:o}))throw new Error(`Can't select invalid tab value=${e}`);l(e),u(e),g(e)}),[u,g,o]),tabValues:o}}var g=n(9136);const x={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var _=n(74848);function y(e){let{className:t,block:n,selectedValue:r,selectValue:s,tabValues:i}=e;const l=[],{blockElementScrollPositionUntilNextRender:d}=(0,o.a_)(),c=e=>{const t=e.currentTarget,n=l.indexOf(t),a=i[n].value;a!==r&&(d(t),s(a))},u=e=>{let t=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const n=l.indexOf(e.currentTarget)+1;t=l[n]??l[0];break}case"ArrowLeft":{const n=l.indexOf(e.currentTarget)-1;t=l[n]??l[l.length-1];break}}t?.focus()};return(0,_.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,a.A)("tabs",{"tabs--block":n},t),children:i.map((e=>{let{value:t,label:n,attributes:o}=e;return(0,_.jsx)("li",{role:"tab",tabIndex:r===t?0:-1,"aria-selected":r===t,ref:e=>{l.push(e)},onKeyDown:u,onClick:c,...o,className:(0,a.A)("tabs__item",x.tabItem,o?.className,{"tabs__item--active":r===t}),children:n??t},t)}))})}function b(e){let{lazy:t,children:n,selectedValue:o}=e;const s=(Array.isArray(n)?n:[n]).filter(Boolean);if(t){const e=s.find((e=>e.props.value===o));return e?(0,r.cloneElement)(e,{className:(0,a.A)("margin-top--md",e.props.className)}):null}return(0,_.jsx)("div",{className:"margin-top--md",children:s.map(((e,t)=>(0,r.cloneElement)(e,{key:t,hidden:e.props.value!==o})))})}function j(e){const t=f(e);return(0,_.jsxs)("div",{className:(0,a.A)("tabs-container",x.tabList),children:[(0,_.jsx)(y,{...t,...e}),(0,_.jsx)(b,{...t,...e})]})}function k(e){const t=(0,g.A)();return(0,_.jsx)(j,{...e,children:u(e.children)},String(t))}},79329:(e,t,n)=>{n.d(t,{A:()=>s});n(96540);var r=n(18215);const a={tabItem:"tabItem_Ymn6"};var o=n(74848);function s(e){let{children:t,hidden:n,className:s}=e;return(0,o.jsx)("div",{role:"tabpanel",className:(0,r.A)(a.tabItem,s),hidden:n,children:t})}}}]);
"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[333],{85162:(e,t,a)=>{a.d(t,{Z:()=>l});var n=a(67294),r=a(86010);const o={tabItem:"tabItem_Ymn6"};function l(e){let{children:t,hidden:a,className:l}=e;return n.createElement("div",{role:"tabpanel",className:(0,r.Z)(o.tabItem,l),hidden:a},t)}},74866:(e,t,a)=>{a.d(t,{Z:()=>k});var n=a(87462),r=a(67294),o=a(86010),l=a(12466),i=a(16550),s=a(91980),d=a(67392),u=a(50012);function c(e){return function(e){return r.Children.map(e,(e=>{if(!e||(0,r.isValidElement)(e)&&function(e){const{props:t}=e;return!!t&&"object"==typeof t&&"value"in t}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}(e).map((e=>{let{props:{value:t,label:a,attributes:n,default:r}}=e;return{value:t,label:a,attributes:n,default:r}}))}function p(e){const{values:t,children:a}=e;return(0,r.useMemo)((()=>{const e=t??c(a);return function(e){const t=(0,d.l)(e,((e,t)=>e.value===t.value));if(t.length>0)throw new Error(`Docusaurus error: Duplicate values "${t.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[t,a])}function m(e){let{value:t,tabValues:a}=e;return a.some((e=>e.value===t))}function h(e){let{queryString:t=!1,groupId:a}=e;const n=(0,i.k6)(),o=function(e){let{queryString:t=!1,groupId:a}=e;if("string"==typeof t)return t;if(!1===t)return null;if(!0===t&&!a)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return a??null}({queryString:t,groupId:a});return[(0,s._X)(o),(0,r.useCallback)((e=>{if(!o)return;const t=new URLSearchParams(n.location.search);t.set(o,e),n.replace({...n.location,search:t.toString()})}),[o,n])]}function _(e){const{defaultValue:t,queryString:a=!1,groupId:n}=e,o=p(e),[l,i]=(0,r.useState)((()=>function(e){let{defaultValue:t,tabValues:a}=e;if(0===a.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(t){if(!m({value:t,tabValues:a}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${t}" but none of its children has the corresponding value. Available values are: ${a.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return t}const n=a.find((e=>e.default))??a[0];if(!n)throw new Error("Unexpected error: 0 tabValues");return n.value}({defaultValue:t,tabValues:o}))),[s,d]=h({queryString:a,groupId:n}),[c,_]=function(e){let{groupId:t}=e;const a=function(e){return e?`docusaurus.tab.${e}`:null}(t),[n,o]=(0,u.Nk)(a);return[n,(0,r.useCallback)((e=>{a&&o.set(e)}),[a,o])]}({groupId:n}),v=(()=>{const e=s??c;return m({value:e,tabValues:o})?e:null})();(0,r.useLayoutEffect)((()=>{v&&i(v)}),[v]);return{selectedValue:l,selectValue:(0,r.useCallback)((e=>{if(!m({value:e,tabValues:o}))throw new Error(`Can't select invalid tab value=${e}`);i(e),d(e),_(e)}),[d,_,o]),tabValues:o}}var v=a(72389);const f={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};function g(e){let{className:t,block:a,selectedValue:i,selectValue:s,tabValues:d}=e;const u=[],{blockElementScrollPositionUntilNextRender:c}=(0,l.o5)(),p=e=>{const t=e.currentTarget,a=u.indexOf(t),n=d[a].value;n!==i&&(c(t),s(n))},m=e=>{let t=null;switch(e.key){case"Enter":p(e);break;case"ArrowRight":{const a=u.indexOf(e.currentTarget)+1;t=u[a]??u[0];break}case"ArrowLeft":{const a=u.indexOf(e.currentTarget)-1;t=u[a]??u[u.length-1];break}}t?.focus()};return r.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,o.Z)("tabs",{"tabs--block":a},t)},d.map((e=>{let{value:t,label:a,attributes:l}=e;return r.createElement("li",(0,n.Z)({role:"tab",tabIndex:i===t?0:-1,"aria-selected":i===t,key:t,ref:e=>u.push(e),onKeyDown:m,onClick:p},l,{className:(0,o.Z)("tabs__item",f.tabItem,l?.className,{"tabs__item--active":i===t})}),a??t)})))}function b(e){let{lazy:t,children:a,selectedValue:n}=e;const o=(Array.isArray(a)?a:[a]).filter(Boolean);if(t){const e=o.find((e=>e.props.value===n));return e?(0,r.cloneElement)(e,{className:"margin-top--md"}):null}return r.createElement("div",{className:"margin-top--md"},o.map(((e,t)=>(0,r.cloneElement)(e,{key:t,hidden:e.props.value!==n}))))}function y(e){const t=_(e);return r.createElement("div",{className:(0,o.Z)("tabs-container",f.tabList)},r.createElement(g,(0,n.Z)({},e,t)),r.createElement(b,(0,n.Z)({},e,t)))}function k(e){const t=(0,v.Z)();return r.createElement(y,(0,n.Z)({key:String(t)},e))}},88892:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>v,contentTitle:()=>h,default:()=>y,frontMatter:()=>m,metadata:()=>_,toc:()=>f});var n=a(87462),r=(a(67294),a(3905)),o=a(74866),l=a(85162),i=a(90814);const s='import os\nfrom clarifai.client.user import User\n#importing load_module_dataloader for calling the dataloader object in dataset.py in the local data folder\nfrom clarifai.datasets.upload.utils import load_module_dataloader\n\n#Replace your PAT\nos.environ[\'CLARIFAI_PAT\'] = "YOUR_PAT"\n\n#replace your "user_id"\nclient = User(user_id="user_id")\napp = client.create_app(app_id="demo_train", base_workflow="Universal")\n\n# Construct the path to the dataset folder\nCSV_PATH = os.path.join(os.getcwd().split(\'/models/model_train\')[0],\'datasets/upload/data/imdb.csv\')\n\n# Create a Clarifai dataset with the specified dataset_id \ndataset = app.create_dataset(dataset_id="text_dataset")\n# Upload the dataset using the provided dataloader and get the upload status\ndataset.upload_from_csv(csv_path=CSV_PATH,input_type=\'text\',csv_type=\'raw\', labels=True)\n\nMODEL_ID = "model_text_classifier"\nMODEL_TYPE_ID = "text-classifier"\n\n# Create a model by passing the model name and model type as parameter\nmodel = app.create_model(model_id=MODEL_ID, model_type_id=MODEL_TYPE_ID)\n\n# get the model parameters\nmodel_params = model.get_params(template=\'HuggingFace_AdvancedConfig\')\nconcepts = [concept.id for concept in app.list_concepts()]\n# update the concept field in model parameters\nmodel.update_params(dataset_id = \'text_dataset\',concepts = ["id-pos","id-neg"])\n\nimport time\n#Starting the training\nmodel_version_id = model.train()\n\n#Checking the status of training\nwhile True:\n    status = model.training_status(version_id=model_version_id,training_logs=False)\n    if status.code == 21106: #MODEL_TRAINING_FAILED\n        print(status)\n        break\n    elif status.code == 21100: #MODEL_TRAINED\n        print(status)\n        break\n    else:\n        print("Current Status:",status)\n        print("Waiting---")\n        time.sleep(120)\n',d="#importing load_module_dataloader for calling the dataloader object in dataset.py in the local data folder\nfrom clarifai.datasets.upload.utils import load_module_dataloader\n\n\n# Evaluate the model using the specified dataset ID 'text_dataset' and evaluation ID 'one'.\nmodel.evaluate(dataset_id='text_dataset', eval_id='one')\n\n# Retrieve the evaluation result for the evaluation ID 'one'.\ntrain_result = model.get_eval_by_id(eval_id=\"one\")\n\n# Construct the path to the test dataset folder\nCSV_PATH = os.path.join(os.getcwd().split('/models/model_train')[0],'datasets/upload/data/test_imdb.csv')\n\n# Create a Clarifai dataset with the specified dataset_id\ntest_dataset = app.create_dataset(dataset_id=\"test_text_dataset\")\n# Upload the dataset using the provided dataloader and get the upload status\ntest_dataset.upload_from_csv(csv_path=CSV_PATH,input_type='text',csv_type='raw', labels=True)\n\n# Evaluate the model using the specified test text dataset identified as 'test_text_dataset'\n# and the evaluation identifier 'two'.\nmodel.evaluate(dataset_id='test_text_dataset', eval_id='two')\n\n# Retrieve the evaluation result with the identifier 'two'.\ntest_result = model.get_eval_by_id(\"two\")\n\n# Print the summary of the evaluation result.\nprint(\"train result:\",train_result.summary)\nprint(\"test result:\",test_result.summary)\n\n",u="from clarifai.utils.evaluation import EvalResultCompare\n\n# Creating an instance of EvalResultCompare class with specified models and datasets\neval_result = EvalResultCompare(models=[model], datasets=[dataset, test_dataset])\n\n# Printing a detailed summary of the evaluation result\nprint(eval_result.detailed_summary())",c="train result:\nmacro_avg_roc_auc: 0.6499999761581421\nmacro_std_roc_auc: 0.07468751072883606\nmacro_avg_f1_score: 0.75\nmacro_avg_precision: 0.6000000238418579\nmacro_avg_recall: 0.5\ntest result:\nmacro_avg_roc_auc: 0.6161290407180786\nmacro_std_roc_auc: 0.1225806474685669\nmacro_avg_f1_score: 0.7207207679748535\nmacro_avg_precision: 0.5633803009986877\nmacro_avg_recall: 0.5\n",p="(  Concept  Accuracy (ROC AUC)  Total Labeled  Total Predicted  True Positives  \\\n 0  id-pos               0.725             80                0               0   \n 0  id-neg               0.575            120              200             120   \n 0  id-pos               0.739             31                0               0   \n 0  id-neg               0.494             40               71              40   \n \n    False Negatives  False Positives  Recall  Precision        F1  \\\n 0               80                0     0.0     1.0000  0.000000   \n 0                0               80     1.0     0.6000  0.750000   \n 0               31                0     0.0     1.0000  0.000000   \n 0                0               31     1.0     0.5634  0.720737   \n \n               Dataset  \n 0       text_dataset2  \n 0       text_dataset2  \n 0  test_text_dataset3  \n 0  test_text_dataset3  ,\n                 Total Concept  Accuracy (ROC AUC)  Total Labeled  \\\n 0       Dataset:text_dataset2            0.650000            200   \n 0  Dataset:test_text_dataset3            0.616129             71   \n \n    Total Predicted  True Positives  False Negatives  False Positives   Recall  \\\n 0              200             120               80               80  0.60000   \n 0               71              40               31               31  0.56338   \n \n    Precision        F1  \n 0   0.760000  0.670588  \n 0   0.754028  0.644909  )",m={sidebar_position:7},h="Model Training And Evaluation Overview",_={unversionedId:"python-sdk/model-train-and-eval",id:"python-sdk/model-train-and-eval",title:"Model Training And Evaluation Overview",description:"Get a brief overview about model training and evaluation using Clarifai Python SDK",source:"@site/docs/python-sdk/model-train-and-eval.md",sourceDirName:"python-sdk",slug:"/python-sdk/model-train-and-eval",permalink:"/python-sdk/model-train-and-eval",draft:!1,editUrl:"https://github.com/Clarifai/docs/blob/main/docs/python-sdk/model-train-and-eval.md",tags:[],version:"current",sidebarPosition:7,frontMatter:{sidebar_position:7},sidebar:"tutorialSidebar",previous:{title:"Inference from Workflows",permalink:"/python-sdk/Building-Workflow-Graphs/Inference-from-Workflows/"},next:{title:"Model Training Tutorial",permalink:"/python-sdk/Model-Training-Tutorial/"}},v={},f=[{value:"Model Training",id:"model-training",level:2},{value:"Model Evaluation",id:"model-evaluation",level:2}],g={toc:f},b="wrapper";function y(e){let{components:t,...a}=e;return(0,r.kt)(b,(0,n.Z)({},g,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"model-training-and-evaluation-overview"},"Model Training And Evaluation Overview"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Get a brief overview about model training and evaluation using Clarifai Python SDK")),(0,r.kt)("hr",null),(0,r.kt)("h2",{id:"model-training"},"Model Training"),(0,r.kt)("p",null,"Model training is the process of feeding data to an algorithm and iteratively adjusting its internal parameters to enable it to make accurate predictions on unseen data. After defining the model architecture, you can initiate the training process using the Clarifai Python SDK. During training, the SDK provides valuable feedback on the model's progress, allowing you to monitor metrics such as accuracy and loss. The structure followed during model training is app creation -> data upload -> model creation -> setting training configuration -> model training."),(0,r.kt)("p",null,"Click ",(0,r.kt)("a",{parentName:"p",href:"https://docs.clarifai.com/python-sdk/Model-Training-Tutorial/"},"here")," to learn more about model training."),(0,r.kt)("admonition",{type:"tip"},(0,r.kt)("p",{parentName:"admonition"},"Clone ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/Clarifai/examples.git"},"this")," repository to get the dataset used for training.")),(0,r.kt)(o.Z,{mdxType:"Tabs"},(0,r.kt)(l.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,r.kt)(i.Z,{className:"language-python",mdxType:"CodeBlock"},s))),(0,r.kt)("details",null,(0,r.kt)("summary",null,"Output"),(0,r.kt)("img",{src:"/img/python-sdk/tc_imt.png",width:"700",height:"700"})),(0,r.kt)("h2",{id:"model-evaluation"},"Model Evaluation"),(0,r.kt)("p",null,"Model evaluation is the process by which we monitor the model's performance on the dataset. The Clarifai Python SDK allows you to evaluate the model in two ways. Firstly, you can receive evaluation metrics for each dataset split separately.  The ",(0,r.kt)("inlineCode",{parentName:"p"},"Mode.evaluate()")," method will run the evaluation on the model by using the dataset passed as a parameter. Each evaluation is marked by ",(0,r.kt)("inlineCode",{parentName:"p"},"eval_id"),". This allows users to run multiple evaluations using different datasets."),(0,r.kt)("admonition",{type:"info"},(0,r.kt)("p",{parentName:"admonition"},"Evaluation is currently supported for the following model types: Embedding Classifier, Text Classifier, Visual Classifier, and Visual Detector.")),(0,r.kt)(o.Z,{mdxType:"Tabs"},(0,r.kt)(l.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,r.kt)(i.Z,{className:"language-python",mdxType:"CodeBlock"},d))),(0,r.kt)("details",null,(0,r.kt)("summary",null,"Output"),(0,r.kt)(i.Z,{className:"language-text",mdxType:"CodeBlock"},c)),(0,r.kt)("p",null,"The SDK also has a feature called ",(0,r.kt)("inlineCode",{parentName:"p"},"EvalResultCompare"),". This method allows users to compare the outputs from different evaluations."),(0,r.kt)(o.Z,{mdxType:"Tabs"},(0,r.kt)(l.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,r.kt)(i.Z,{className:"language-python",mdxType:"CodeBlock"},u))),(0,r.kt)("details",null,(0,r.kt)("summary",null,"Output"),(0,r.kt)(i.Z,{className:"language-text",mdxType:"CodeBlock"},p)))}y.isMDXComponent=!0}}]);
"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[8969],{2947:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>m,contentTitle:()=>u,default:()=>_,frontMatter:()=>h,metadata:()=>i,toc:()=>g});const i=JSON.parse('{"id":"create/models/templates/custom","title":"Advanced Config","description":"Learn how to create your own custom deep fine-tuned template","source":"@site/docs/create/models/templates/custom.md","sourceDirName":"create/models/templates","slug":"/create/models/templates/custom","permalink":"/create/models/templates/custom","draft":false,"unlisted":false,"editUrl":"https://github.com/Clarifai/docs/blob/main/docs/create/models/templates/custom.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"description":"Learn how to create your own custom deep fine-tuned template","sidebar_position":6},"sidebar":"tutorialSidebar","previous":{"title":"Text Fine-Tuning Templates","permalink":"/create/models/templates/text"},"next":{"title":"Model Versions","permalink":"/create/models/model-versions/"}}');var a=t(74848),o=t(28453),r=t(65537),s=t(79329),c=t(58069);const l="# Base Configuration File\n# This configuration file extends an existing YOLOF model configuration.\n\n_base_ = '/mmdetection/configs/yolof/yolof_r50-c5_8xb8-1x_coco.py'\n\n# Load from a checkpoint\n# load_from = '<add_url_with_checkpoint_file>'\n\n# Minimum number of samples per epoch\nmin_samples_per_epoch = 30\n\n# Dataset configuration\ndataset_type = 'CocoDataset'\n\n# Model configuration\nmodel = dict(\n    type='YOLOF',  # Specifies that the model type is YOLOF\n    data_preprocessor=dict(\n        type='DetDataPreprocessor',  # Specifies the type of data preprocessor\n        mean=[123.675, 116.28, 103.53],  # Mean values used to pre-train the backbone models, ordered in R, G, B\n        std=[1.0, 1.0, 1.0],  # Standard variance used to pre-train the backbone models, ordered in R, G, B\n        bgr_to_rgb=True,  # Whether to convert image from BGR to RGB\n        pad_mask=True,  # Whether to pad instance masks\n        pad_size_divisor=32,  # The size of the padded image should be divisible by pad_size_divisor, ensuring compatibility with the network's downsampling operations\n    ),\n    backbone=dict(\n        type='ResNet',  # Specifies the backbone network (e.g., 'ResNet')\n        depth=50,  # Specify the depth of the backbone (e.g., ResNet-50)\n        frozen_stages=1,  # The first stage of the backbone will not be updated during training, helping to preserve low-level features\n        init_cfg=dict(),  # Indicates how the weights of the backbone network are initialized\n    ),\n    neck=dict(\n        block_dilations=[2, 4, 6, 8],\n        block_mid_channels=128,\n        in_channels=2048,\n        num_residual_blocks=4,\n        out_channels=512,\n        type='DilatedEncoder'\n    ),\n    bbox_head=dict(\n        anchor_generator=dict(\n            ratios=[1.0],\n            scales=[1, 2, 4, 8, 16],\n            strides=[32],\n            type='AnchorGenerator'\n        ),\n        bbox_coder=dict(\n            add_ctr_clamp=True,\n            ctr_clamp=32,\n            target_means=[0.0, 0.0, 0.0, 0.0],\n            target_stds=[1.0, 1.0, 1.0, 1.0],\n            type='DeltaXYWHBBoxCoder'\n        ),\n        in_channels=512,\n        loss_bbox=dict(loss_weight=1.0, type='GIoULoss'),\n        loss_cls=dict(\n            alpha=0.25,\n            gamma=2.0,\n            loss_weight=1.0,\n            type='FocalLoss',\n            use_sigmoid=True\n        ),\n        num_classes=80,\n        reg_decoded_bbox=True,\n        type='YOLOFHead'\n    ),\n)\n\n# Optimizer configuration\noptim_wrapper = dict(\n    type='OptimWrapper',  # Type of optimizer wrapper, you can switch to AmpOptimWrapper to enable mixed precision training\n    optimizer=dict(  # Optimizer configuration, supports various PyTorch optimizers, please refer to https://pytorch.org/docs/stable/optim.html#algorithms\n        type='SGD',  # SGD\n        lr=0.001875,  # Base learning rate\n        momentum=0.9,  # SGD with momentum\n        weight_decay=0.0001,  # Weight decay\n    ),\n    paramwise_cfg=dict(\n        norm_decay_mult=0.0, custom_keys=dict(backbone=dict(lr_mult=0.3333))\n    ),\n    clip_grad=dict(\n        max_norm=8, norm_type=2\n    ),  # Configuration for gradient clipping, set to None to disable. For usage, please see https://mmengine.readthedocs.io/en/latest/tutorials/optimizer.html\n)\n\n# Parameter scheduler configuration\nparam_scheduler = [\n    dict(\n        type='LinearLR',  # Use linear learning rate warmup\n        start_factor=0.00066667,  # Coefficient for learning rate warmup\n        by_epoch=False,  # Update the learning rate during warmup at each iteration\n        begin=0,  # Start updating the parameters from the first iteration\n        end=500,  # End the warmup at the 500th iteration\n    ),\n    dict(\n        type='MultiStepLR',  # Use multi-step learning rate strategy during training\n        by_epoch=True,\n        begin=0,\n        end=12,  # Ending at the 12th epoch\n        milestones=[8, 12],  # Learning rate decay at which epochs\n        gamma=0.1,  # Learning rate decay coefficient\n    ),\n]\n\n# Hook configuration\ncustom_hooks = [\n    dict(type='CheckInvalidLossHook', interval=50)\n]  # Regularly checks if the loss is valid during training; checks every 50 iterations\n\n# Dataset and evaluator configuration\ntrain_pipeline = [  # Training data processing pipeline\n    dict(type='LoadImageFromFile'),  # First pipeline to load images from file path\n    dict(type='LoadAnnotations', with_bbox=True),  # Second pipeline to load annotations for current image\n    dict(type='Resize', scale=(768, 512), keep_ratio=1.5),  # Pipeline that resizes the images and their annotations\n    dict(type='RandomFlip', prob=0.5),  # Augmentation pipeline that flips the images and their annotations\n    dict(type='RandomShift', prob=0.5, max_shift_px=32),\n    dict(type='PackDetInputs'),  # Pipeline that formats the annotation data and decides which keys in the data should be packed into data_samples\n]\n\ntest_pipeline = None  # Testing data processing pipeline\n\ntrain_dataloader = dict(  # Train dataloader config\n    batch_size=16,  # Batch size of a single GPU\n    persistent_workers=True,  # If True, the dataloader will not shut down the worker processes after an epoch end, which can accelerate training speed\n    sampler=dict(\n        type='DefaultSampler', shuffle=True\n    ),  # Default sampler, supports both distributed and non-distributed training\n    batch_sampler=dict(\n        type='AspectRatioBatchSampler'\n    ),  # Default batch_sampler, used to ensure that images in the batch have similar aspect ratios, so as to better utilize graphics memory\n    dataset=dict(  # Train dataset config\n        type=dataset_type,\n        data_root='',\n        ann_file='',  # Path of annotation file\n        data_prefix=dict(img=''),  # Prefix of image path\n        metainfo=dict(classes=()),\n        filter_cfg=dict(\n            filter_empty_gt=True, min_size=32\n        ),  # Config of filtering images and annotations\n        pipeline=train_pipeline,\n    ),\n)\n# In version 3.x, validation and test dataloaders can be configured independently\nval_dataloader = None  # Validation dataloader config\n\nval_evaluator = None  # Validation evaluator config\n\n# Training and testing configuration\ntrain_cfg = dict(\n    type='EpochBasedTrainLoop',  # Type of training loop, please refer to https://github.com/open-mmlab/mmengine/blob/main/mmengine/runner/loops.py\n    max_epochs=15,  # Maximum number of training epochs\n    val_interval=1,  # Validation intervals. Run validation every epoch\n)\n\ndefault_hooks = dict(\n    checkpoint=dict(type='CheckpointHook', max_keep_ckpts=2)\n)  # CheckpointHook is a default hook that saves checkpoints at specified intervals. To limit the number of saved checkpoints, use the max_keep_ckpts parameter, which deletes older checkpoints once the limit is exceeded\n\ntest_cfg = dict(type='TestLoop')  # Type of testing loop\n\nval_cfg = None  # Type of validation loop\n",d="_base_ = '/mmdetection/configs/yolof/yolof_r50_c5_8x8_1x_coco.py'\nmodel=dict(\n  bbox_head=dict(num_classes=0))\ndata=dict(\n  train=dict(\n    ann_file='',\n    img_prefix='',\n    classes=''\n    ),\n  val=dict(\n    ann_file='',\n    img_prefix='',\n    classes=''))\noptimizer=dict(\n  _delete_=True,\n  type='Adam',\n  lr=0.0001,\n  weight_decay=0.0001)\nlr_config = dict(\n  _delete_=True,\n  policy='CosineAnnealing',\n  warmup='linear',\n  warmup_iters=1000,\n  warmup_ratio=0.1,\n  min_lr_ratio=1e-5)\nrunner = dict(\n  _delete_=True,\n  type='EpochBasedRunner',\n  max_epochs=10)\n\n",p='########################################################################################\n# In this section, we set the user authentication, app ID, model ID, and concept ID.\n# Change these strings to run your own example.\n########################################################################################\n\nUSER_ID = "YOUR_USER_ID_HERE"\n# Your PAT (Personal Access Token) can be found in the Account\'s Security section\nPAT = "YOUR_PAT_HERE"\nAPP_ID = "YOUR_APP_ID_HERE"\n# Change this to train your own model\nMODEL_ID = "test_config"\nCONCEPT_ID_1 = "house"\n\n##########################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n##########################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\nfrom google.protobuf.struct_pb2 import Struct\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nparams = Struct()\nparams.update({"template": "MMDetection"})\n\nCONFIG_FILE = \'training_config.py\'\nparams.update({"custom_config": open(CONFIG_FILE, "r").read()})\n\nmetadata = (("authorization", "Key " + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\npost_model_versions = stub.PostModelVersions(\n    service_pb2.PostModelVersionsRequest(\n        user_app_id=userDataObject,\n        model_id=MODEL_ID,\n        model_versions=[\n            resources_pb2.ModelVersion(\n                train_info=resources_pb2.TrainInfo(\n                    params=params,\n                ),\n                output_info=resources_pb2.OutputInfo(\n                    data=resources_pb2.Data(\n                        concepts=[\n                            resources_pb2.Concept(id=CONCEPT_ID_1, value=1)\n                        ]\n                    ),\n                )\n            )\n        ],\n    ),\n    metadata=metadata,\n)\n\nif post_model_versions.status.code != status_code_pb2.SUCCESS:\n    print(post_model_versions.status)\n    raise Exception(\n        "Post models versions failed, status: " + post_model_versions.status.description\n    )\n\nprint(post_model_versions)\n',h={description:"Learn how to create your own custom deep fine-tuned template",sidebar_position:6},u="Advanced Config",m={},g=[{value:"MMDetection",id:"mmdetection",level:2},{value:"Base Configuration",id:"base-configuration",level:3},{value:"Load From a Checkpoint",id:"load-from-a-checkpoint",level:3},{value:"Minimum Samples Per Epoch",id:"minimum-samples-per-epoch",level:3},{value:"Dataset Configuration",id:"dataset-configuration",level:3},{value:"Model Configuration",id:"model-configuration",level:3},{value:"Optimizer Configuration",id:"optimizer-configuration",level:3},{value:"Parameter Scheduler Configuration",id:"parameter-scheduler-configuration",level:3},{value:"Hook Configuration",id:"hook-configuration",level:3},{value:"Dataset and Evaluator Configuration",id:"dataset-and-evaluator-configuration",level:3},{value:"Training and Testing Configuration",id:"training-and-testing-configuration",level:3},{value:"Example 1",id:"example-1",level:2},{value:"Example 2",id:"example-2",level:2}];function f(e){const n={a:"a",admonition:"admonition",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"advanced-config",children:"Advanced Config"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Learn how to create your own custom deep fine-tuned template"})}),"\n",(0,a.jsx)("hr",{}),"\n",(0,a.jsx)(n.p,{children:"The Clarifai platform empowers advanced users to create their deep fine-tuned templates. You can customize your own templates to suit your specific needs and tasks."}),"\n",(0,a.jsx)(n.p,{children:"This flexibility allows you to leverage Clarifai's advanced machine learning capabilities and customize various template hyperparameters to influence \u201chow\u201d your model learns."}),"\n",(0,a.jsx)(n.p,{children:"If you select any of the following barebone templates when setting up a visual detection, visual classification, or  visual segmentation model:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"MMDetection_AdvancedConfig"}),";"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"MMClassification_AdvancedConfig"}),"; or,"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"MMSegmentation_AdvancedConfig"}),"."]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["Then, click the ",(0,a.jsx)(n.strong,{children:"Show Training Settings (optional)"})," button, a ",(0,a.jsx)(n.strong,{children:"Custom config"})," field will appear that allows you to provide a Python file that details the configurations of your template."]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{src:t(78974).A+"",width:"1818",height:"812"})}),"\n",(0,a.jsx)("br",{}),"\n",(0,a.jsxs)(n.p,{children:["If you click the pencil icon within the ",(0,a.jsx)(n.strong,{children:"Custom Config"})," field, a development environment will appear, enabling you to configure your template seamlessly without navigating away from the current screen. You can also click the upload button to upload a pre-configured Python file."]}),"\n",(0,a.jsx)(n.admonition,{title:"info",type:"warning",children:(0,a.jsxs)(n.p,{children:["Choosing non-barebone templates like ",(0,a.jsx)(n.a,{href:"https://docs.clarifai.com/portal-guide/model/deep-training/visual-detection-templates",children:(0,a.jsx)(n.strong,{children:"MMDetection_YoloF"})}),", ",(0,a.jsx)(n.a,{href:"https://docs.clarifai.com/portal-guide/model/deep-training/visual-classification-templates",children:(0,a.jsx)(n.strong,{children:"MMClassification_ResNet_50_RSB_A1"})}),", or ",(0,a.jsx)(n.a,{href:"https://docs.clarifai.com/portal-guide/model/deep-training/visual-segmenter-templates",children:(0,a.jsx)(n.strong,{children:"MMSegmentation_SegFormer"})})," grants you access to pre-configured templates. These templates come with default settings, allowing you to use them as is or conveniently customize their settings on the UI to align with your specific use case."]})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.admonition,{type:"note",children:(0,a.jsx)(n.p,{children:"In this example, we\u2019ll demonstrate how to create your own template using the MMDetection open-source toolbox for visual detection tasks. You can also adapt these steps to create customized templates for visual classification and visual segmentation tasks."})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"mmdetection",children:"MMDetection"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.a,{href:"https://mmdetection.readthedocs.io/en/latest/overview.html",children:"MMDetection"})," is a powerful open-source toolbox developed as part of the OpenMMLab project. It is based on PyTorch and provides a flexible and extensible framework for object detection and instance segmentation tasks."]}),"\n",(0,a.jsx)(n.p,{children:"You can configure the MMDetection toolbox and create a unique model template with its own hyperparameters. By tweaking the various settings, you can tailor the template to match your specific object detection tasks."}),"\n",(0,a.jsxs)(n.p,{children:["Let's demonstrate how you can leverage MMDetection's flexibility to create a custom training configuration file for the ",(0,a.jsx)(n.a,{href:"https://github.com/open-mmlab/mmdetection/tree/v3.1.0/configs/yolof",children:"YOLOF"})," model, tailored to a specific dataset and training requirements."]}),"\n",(0,a.jsx)(n.p,{children:"You can check the completed Python configuration file at the bottom of this page."}),"\n",(0,a.jsx)(n.admonition,{type:"tip",children:(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.a,{href:"https://mmdetection.readthedocs.io/en/dev-3.x/user_guides/config.html",children:"Click here"})," to learn more about how to configure the MMDetection toolbox."]})}),"\n",(0,a.jsx)(n.h3,{id:"base-configuration",children:"Base Configuration"}),"\n",(0,a.jsxs)(n.p,{children:["MMDetection uses a modular configuration system that allows you to easily customize and extend configurations. It provides base configurations for many models, which you can then customize conveniently. You can find all available pre-build configs ",(0,a.jsx)(n.a,{href:"https://github.com/open-mmlab/mmdetection/tree/v3.1.0/configs",children:"here"}),"."]}),"\n",(0,a.jsxs)(n.p,{children:["You can base your custom configurations on existing ones by using the ",(0,a.jsx)(n.code,{children:" _base_"})," variable, which points to a config file relative to the parent directory ",(0,a.jsx)(n.code,{children:"/mmdetection/"}),". This inheritance mechanism lets you leverage tried-and-tested configurations while customizing specific components."]}),"\n",(0,a.jsx)(n.p,{children:"Here is an example:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"_base_ = '/mmdetection/configs/yolof/yolof_r50-c5_8xb8-1x_coco.py'\n"})}),"\n",(0,a.jsxs)(n.p,{children:["In the above example, the ",(0,a.jsx)(n.code,{children:"_base_"})," field indicates that this configuration file is based on another existing configuration file located at ",(0,a.jsx)(n.code,{children:"/mmdetection/configs/yolof/yolof_r50-c5_8xb8-1x_coco.py"}),". This means that the current ",(0,a.jsx)(n.a,{href:"https://mmdetection.readthedocs.io/en/dev/tutorials/config.html",children:"configuration file"})," inherits settings and parameters from the existing YOLOF model with a ResNet-50 backbone trained on the COCO dataset."]}),"\n",(0,a.jsx)(n.p,{children:"This base configuration file serves as a template or starting point, providing the fundamental settings and components for the detector model \u2014 and any modifications made in the current file will override or extend the base configuration."}),"\n",(0,a.jsx)(n.h3,{id:"load-from-a-checkpoint",children:"Load From a Checkpoint"}),"\n",(0,a.jsx)(n.p,{children:"You can specify a source URL to load a model checkpoint as a pre-trained model. This allows you to initialize your model with pre-trained weights, accelerating training and leveraging existing knowledge."}),"\n",(0,a.jsxs)(n.blockquote,{children:["\n",(0,a.jsxs)(n.p,{children:["After training a model version, a checkpoint file is typically created. You can then perform ",(0,a.jsx)(n.a,{href:"https://docs.clarifai.com/api-guide/model/deep-training/#incrementally-train-a-model",children:"incremental training"})," using this checkpoint, updating your model with new data without retraining from scratch."]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"You can upload your own pre-trained checkpoint to a URL and load it as follows:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"load_from='<url_with_checkpoint_file>'\n"})}),"\n",(0,a.jsx)(n.h3,{id:"minimum-samples-per-epoch",children:"Minimum Samples Per Epoch"}),"\n",(0,a.jsx)(n.p,{children:"You can specify the minimum number of samples to be processed in one epoch during training, particularly useful for very small datasets. This ensures that a sufficient number of samples are processed in each epoch to provide meaningful training updates."}),"\n",(0,a.jsx)(n.p,{children:"Here is an example:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"min_samples_per_epoch = 30\n"})}),"\n",(0,a.jsx)(n.h3,{id:"dataset-configuration",children:"Dataset Configuration"}),"\n",(0,a.jsx)(n.p,{children:"You can define the type of dataset to be used. For this example, let's set it to 'CocoDataset', which is a common dataset format for object detection tasks."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"dataset_type = 'CocoDataset'\n"})}),"\n",(0,a.jsx)(n.h3,{id:"model-configuration",children:"Model Configuration"}),"\n",(0,a.jsx)(n.p,{children:"This is the most vital part of the detection model. It defines the architecture and various key components of the deep learning model."}),"\n",(0,a.jsxs)(n.p,{children:["Here are some of the neural network components you can set using the ",(0,a.jsx)(n.code,{children:"model"})," field:"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"type"})," \u2014 MMDetection contains high-quality codebases for many popular models and task-oriented modules, which you can specify to customize your detection model. You can find a list of all the pre-built model types it supports ",(0,a.jsx)(n.a,{href:"https://github.com/open-mmlab/mmdetection/blob/main/docs/en/model_zoo.md",children:"here"}),"."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"data_preprocessor"})," \u2014 Specifies how images are preprocessed before feeding them to the model. It's responsible for processing a batch of data output by the dataloader. Examples of attributes you can specify include mean subtraction, standard deviation normalization, converting BGR images to RGB, and padding masks."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"backbone"})," \u2014 This is the part of the architecture that transforms the input images into raw feature maps. It is typically a pre-trained model, such as ResNet-50 or MobileNet, that has been trained on a large dataset of images."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"neck"})," \u2014 This is the component that connects the backbone with heads and performs reconfigurations and refinements on the raw feature maps so that heads can further process them."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"bbox_head"})," \u2014 Defines the head of the model responsible for bounding box predictions. The ",(0,a.jsx)(n.code,{children:"num_classes"})," field, which specifies the number of object classes in your dataset for classification, must be included with any value in order to be compatible with Clarifai's system."]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Here is an example:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"model = dict(\n    type='YOLOF',  # Specifies that the model type is YOLOF\n    data_preprocessor=dict(\n        type='DetDataPreprocessor',  # Specifies the type of data preprocessor\n        mean=[123.675, 116.28, 103.53],  # Mean values used to pre-train the backbone models, ordered in R, G, B\n        std=[1.0, 1.0, 1.0],  # Standard variance used to pre-train the backbone models, ordered in R, G, B\n        bgr_to_rgb=True,  # Whether to convert image from BGR to RGB\n        pad_mask=True,  # Whether to pad instance masks\n        pad_size_divisor=32,  # The size of the padded image should be divisible by pad_size_divisor, ensuring compatibility with the network's downsampling operations\n    ),\n    backbone=dict(\n        type='ResNet',  # Specifies the backbone network (e.g., 'ResNet')\n        depth=50,  # Specify the depth of the backbone (e.g., ResNet-50)\n        frozen_stages=1,  # The first stage of the backbone will not be updated during training, helping to preserve low-level features\n        init_cfg=dict(),  # Indicates how the weights of the backbone network are initialized\n    ),\n    neck=dict(\n        block_dilations=[2, 4, 6, 8],\n        block_mid_channels=128,\n        in_channels=2048,\n        num_residual_blocks=4,\n        out_channels=512,\n        type='DilatedEncoder'\n    ),\n    bbox_head=dict(\n        anchor_generator=dict(\n            ratios=[1.0],\n            scales=[1, 2, 4, 8, 16],\n            strides=[32],\n            type='AnchorGenerator'\n        ),\n        bbox_coder=dict(\n            add_ctr_clamp=True,\n            ctr_clamp=32,\n            target_means=[0.0, 0.0, 0.0, 0.0],\n            target_stds=[1.0, 1.0, 1.0, 1.0],\n            type='DeltaXYWHBBoxCoder'\n        ),\n        in_channels=512,\n        loss_bbox=dict(loss_weight=1.0, type='GIoULoss'),\n        loss_cls=dict(\n            alpha=0.25,\n            gamma=2.0,\n            loss_weight=1.0,\n            type='FocalLoss',\n            use_sigmoid=True\n        ),\n        num_classes=80,\n        reg_decoded_bbox=True,\n        type='YOLOFHead'\n    ),\n)\n"})}),"\n",(0,a.jsx)(n.h3,{id:"optimizer-configuration",children:"Optimizer Configuration"}),"\n",(0,a.jsxs)(n.p,{children:["In an MMDetection file, the optimizer settings are specified using the ",(0,a.jsx)(n.code,{children:"optim_wrapper"})," field. The optimizer is a crucial component of training deep learning models, and is responsible for updating the model's weights during the training process."]}),"\n",(0,a.jsxs)(n.p,{children:["MMDetection already supports all the ",(0,a.jsx)(n.a,{href:"https://mmdetection.readthedocs.io/en/dev/tutorials/customize_runtime.html",children:"optimizers implemented in PyTorch"}),". So, you can conveniently adjust the optimizer choice, learning rate, and other hyperparameters."]}),"\n",(0,a.jsx)(n.p,{children:"Here is an example:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'optim_wrapper = dict(\n    type="OptimWrapper",  # Type of optimizer wrapper, you can switch to AmpOptimWrapper to enable mixed precision training\n    optimizer=dict(  # Optimizer configuration, supports various PyTorch optimizers, please refer to https://pytorch.org/docs/stable/optim.html#algorithms\n        type="SGD",  # SGD\n        lr=0.001875,  # Base learning rate\n        momentum=0.9,  # SGD with momentum\n        weight_decay=0.0001,\n    ),  # Weight decay\n    paramwise_cfg=dict(\n        norm_decay_mult=0.0, custom_keys=dict(backbone=dict(lr_mult=0.3333))\n    ),\n    clip_grad=dict(\n        max_norm=8, norm_type=2\n    ),  # Configuration for gradient clipping, set to None to disable. For usage, please see https://mmengine.readthedocs.io/en/latest/tutorials/optimizer.html\n)\n\n'})}),"\n",(0,a.jsx)(n.h3,{id:"parameter-scheduler-configuration",children:"Parameter Scheduler Configuration"}),"\n",(0,a.jsxs)(n.p,{children:["The ",(0,a.jsx)(n.a,{href:"https://mmengine.readthedocs.io/en/latest/tutorials/param_scheduler.html",children:(0,a.jsx)(n.code,{children:"param_scheduler"})})," field in MMDetection is used to configure the strategies for adjusting optimization hyperparameters during training, such as learning rate and momentum. By specifying different types of schedulers, you can control how these parameters change over time to improve training efficiency and model performance."]}),"\n",(0,a.jsx)(n.p,{children:"You can combine multiple schedulers, such as linear warmup and multi-step decay, to create a tailored parameter adjustment strategy that suits your specific training requirements. This flexibility allows for fine-tuning of the learning process, helping to achieve better convergence and more accurate models."}),"\n",(0,a.jsx)(n.p,{children:"Here is an example:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"param_scheduler = [\n    dict(\n        type='LinearLR',  # Use linear learning rate warmup\n        start_factor=0.00066667, # Coefficient for learning rate warmup\n        by_epoch=False,  # Update the learning rate during warmup at each iteration\n        begin=0,  # Start updating the parameters from the first iteration\n        end=500),  # End the warmup at the 500th iteration\n    dict(\n        type='MultiStepLR',  # Use multi-step learning rate strategy during training\n        by_epoch=True,  \n        begin=0,   \n        end=12,  # Ending at the 12th epoch\n        milestones=[8,12],  # Learning rate decay at which epochs\n        gamma=0.1  # Learning rate decay coefficient\n        )  \n]\n"})}),"\n",(0,a.jsx)(n.h3,{id:"hook-configuration",children:"Hook Configuration"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.a,{href:"https://mmengine.readthedocs.io/en/latest/tutorials/hook.html",children:"Hooks"})," in MMDetection allow you to set specific mount points in your code where additional functions can be executed. When the program reaches these points, all methods registered to the hook are automatically called."]}),"\n",(0,a.jsx)(n.p,{children:"If the built-in hooks provided by the MMEngine do not meet your needs, you can create custom hooks."}),"\n",(0,a.jsx)(n.p,{children:"For example, you can create a custom hook to check whether the loss value is valid (i.e., not infinite) during training. This check will be performed after each training iteration."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"custom_hooks = [dict(type='CheckInvalidLossHook', interval=50)]  # Regularly checks if the loss is valid during training; checks every 50 iterations\n"})}),"\n",(0,a.jsx)(n.p,{children:"This configuration ensures that the loss validity is monitored at regular intervals, helping to detect and address any issues promptly during training."}),"\n",(0,a.jsx)(n.h3,{id:"dataset-and-evaluator-configuration",children:"Dataset and Evaluator Configuration"}),"\n",(0,a.jsxs)(n.p,{children:["In the MMEngine's training pipeline, ",(0,a.jsx)(n.a,{href:"https://mmengine.readthedocs.io/en/latest/tutorials/dataset.html",children:"datasets and dataloaders"})," are essential components. Dataloaders are needed for training, validation, and testing of the runner. To build a dataloader, you need to configure both the dataset and the data pipeline."]}),"\n",(0,a.jsx)(n.p,{children:"These concepts are derived from and consistent with PyTorch. Typically:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Dataset"}),": Defines the quantity, parsing, and preprocessing of the data."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Dataloader"}),": Iteratively loads data based on settings such as ",(0,a.jsx)(n.code,{children:"batch_size"}),"."]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Datasets are encapsulated with dataloaders, and they together constitute the data source for the model."}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.a,{href:"https://mmengine.readthedocs.io/en/latest/tutorials/evaluation.html",children:"Evaluators"})," are used to compute metrics for the trained model on the validation and testing datasets. Quantitative evaluation of model accuracy during validation and testing is crucial. This is done by specifying the evaluation metrics in the configuration file."]}),"\n",(0,a.jsx)(n.p,{children:"Here is an example:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"train_pipeline = [ # Training data processing pipeline\n    dict(type='LoadImageFromFile'), # First pipeline to load images from file path\n    dict(type='LoadAnnotations', with_bbox=True), # Second pipeline to load annotations for current image\n    dict(type='Resize', scale=(768,512), keep_ratio=1.5), # Pipeline that resizes the images and their annotations\n    dict(type='RandomFlip', prob=0.5), # Augmentation pipeline that flips the images and their annotations\n    dict(type='RandomShift', prob=0.5, max_shift_px=32),\n    dict(type='PackDetInputs') # Pipeline that formats the annotation data and decides which keys in the data should be packed into data_samples\n]\n\ntest_pipeline = None  # Testing data processing pipeline\n\ntrain_dataloader = dict( # Train dataloader config\n    batch_size=16, # Batch size of a single GPU\n    persistent_workers=True,  # If True, the dataloader will not shut down the worker processes after an epoch end, which can accelerate training speed\n    sampler=dict(type='DefaultSampler', shuffle=True),  # Default sampler, supports both distributed and non-distributed training\n    batch_sampler=dict(type='AspectRatioBatchSampler'),  # Default batch_sampler, used to ensure that images in the batch have similar aspect ratios, so as to better utilize graphics memory\n    dataset=dict( # Train dataset config\n        type=dataset_type,\n        data_root='',\n        ann_file='', # Path of annotation file\n        data_prefix=dict(img=''), # Prefix of image path\n        metainfo=dict(classes=()),\n        filter_cfg=dict(filter_empty_gt=True, min_size=32), # Config of filtering images and annotations\n        pipeline=train_pipeline)\n    )\n# In version 3.x, validation and test dataloaders can be configured independently\nval_dataloader = None  # Validation dataloader config\n\nval_evaluator = None # Validation evaluator config\n\n"})}),"\n",(0,a.jsx)(n.h3,{id:"training-and-testing-configuration",children:"Training and Testing Configuration"}),"\n",(0,a.jsxs)(n.p,{children:["The MMEngine\u2019s runner uses ",(0,a.jsx)(n.code,{children:"Loop"})," to control the training, validation, and testing processes. This modular configuration allows users to set parameters like the maximum number of training epochs and validation intervals."]}),"\n",(0,a.jsx)(n.p,{children:"Here is an example:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"train_cfg = dict(\n    type='EpochBasedTrainLoop',  # Type of training loop, please refer to https://github.com/open-mmlab/mmengine/blob/main/mmengine/runner/loops.py\n    max_epochs=15,  # Maximum number of training epochs\n    val_interval=1  # Validation intervals. Run validation every epoch\n)\n\ndefault_hooks = dict(checkpoint=dict(type='CheckpointHook', max_keep_ckpts=2)) # CheckpointHook is default hook that saves checkpoints at specified intervals. To limit the number of saved checkpoints, use the max_keep_ckpts parameter, which deletes older checkpoints once the limit is exceeded\n\ntest_cfg = dict(type='TestLoop')  # Type of testing loop\n\nval_cfg = None  # The type of validation loop\n"})}),"\n",(0,a.jsx)(n.h2,{id:"example-1",children:"Example 1"}),"\n",(0,a.jsxs)(n.p,{children:["Here is the final ",(0,a.jsx)(n.code,{children:"config.py"})," file from the previous steps for creating an advanced configuration template using the MMDetection toolbox."]}),"\n","\n",(0,a.jsx)(r.A,{groupId:"code",children:(0,a.jsx)(s.A,{value:"python",label:"Python",children:(0,a.jsx)(c.A,{className:"language-python",children:l})})}),"\n",(0,a.jsx)(n.h2,{id:"example-2",children:"Example 2"}),"\n",(0,a.jsx)(n.p,{children:"As mentioned previously, you can create your own deep fine-tuned template and use it to train a model."}),"\n",(0,a.jsxs)(n.p,{children:["You need to create a Python configuration file and pass it as a training parameter to the ",(0,a.jsx)(n.code,{children:"PostModelVersions"})," endpoint."]}),"\n",(0,a.jsxs)(n.p,{children:["Here is an example of a ",(0,a.jsx)(n.code,{children:"training_config.py"})," file for creating a custom deep fine-tuned template using the MMDetection open source toolbox for visual detection tasks."]}),"\n",(0,a.jsx)(r.A,{groupId:"code",children:(0,a.jsx)(s.A,{value:"python",label:"Python",children:(0,a.jsx)(c.A,{className:"language-python",children:d})})}),"\n",(0,a.jsx)(n.p,{children:"Here is how you could use the custom template to train a deep fine-tuned model."}),"\n",(0,a.jsx)(r.A,{groupId:"code",children:(0,a.jsx)(s.A,{value:"python",label:"Python (gRPC)",children:(0,a.jsx)(c.A,{className:"language-python",children:p})})})]})}function _(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(f,{...e})}):f(e)}},65537:(e,n,t)=>{t.d(n,{A:()=>j});var i=t(96540),a=t(18215),o=t(65627),r=t(56347),s=t(50372),c=t(30604),l=t(11861),d=t(78749);function p(e){return i.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,i.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function h(e){const{values:n,children:t}=e;return(0,i.useMemo)((()=>{const e=n??function(e){return p(e).map((e=>{let{props:{value:n,label:t,attributes:i,default:a}}=e;return{value:n,label:t,attributes:i,default:a}}))}(t);return function(e){const n=(0,l.XI)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,t])}function u(e){let{value:n,tabValues:t}=e;return t.some((e=>e.value===n))}function m(e){let{queryString:n=!1,groupId:t}=e;const a=(0,r.W6)(),o=function(e){let{queryString:n=!1,groupId:t}=e;if("string"==typeof n)return n;if(!1===n)return null;if(!0===n&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:n,groupId:t});return[(0,c.aZ)(o),(0,i.useCallback)((e=>{if(!o)return;const n=new URLSearchParams(a.location.search);n.set(o,e),a.replace({...a.location,search:n.toString()})}),[o,a])]}function g(e){const{defaultValue:n,queryString:t=!1,groupId:a}=e,o=h(e),[r,c]=(0,i.useState)((()=>function(e){let{defaultValue:n,tabValues:t}=e;if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(n){if(!u({value:n,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${n}" but none of its children has the corresponding value. Available values are: ${t.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return n}const i=t.find((e=>e.default))??t[0];if(!i)throw new Error("Unexpected error: 0 tabValues");return i.value}({defaultValue:n,tabValues:o}))),[l,p]=m({queryString:t,groupId:a}),[g,f]=function(e){let{groupId:n}=e;const t=function(e){return e?`docusaurus.tab.${e}`:null}(n),[a,o]=(0,d.Dv)(t);return[a,(0,i.useCallback)((e=>{t&&o.set(e)}),[t,o])]}({groupId:a}),_=(()=>{const e=l??g;return u({value:e,tabValues:o})?e:null})();(0,s.A)((()=>{_&&c(_)}),[_]);return{selectedValue:r,selectValue:(0,i.useCallback)((e=>{if(!u({value:e,tabValues:o}))throw new Error(`Can't select invalid tab value=${e}`);c(e),p(e),f(e)}),[p,f,o]),tabValues:o}}var f=t(9136);const _={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var b=t(74848);function y(e){let{className:n,block:t,selectedValue:i,selectValue:r,tabValues:s}=e;const c=[],{blockElementScrollPositionUntilNextRender:l}=(0,o.a_)(),d=e=>{const n=e.currentTarget,t=c.indexOf(n),a=s[t].value;a!==i&&(l(n),r(a))},p=e=>{let n=null;switch(e.key){case"Enter":d(e);break;case"ArrowRight":{const t=c.indexOf(e.currentTarget)+1;n=c[t]??c[0];break}case"ArrowLeft":{const t=c.indexOf(e.currentTarget)-1;n=c[t]??c[c.length-1];break}}n?.focus()};return(0,b.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,a.A)("tabs",{"tabs--block":t},n),children:s.map((e=>{let{value:n,label:t,attributes:o}=e;return(0,b.jsx)("li",{role:"tab",tabIndex:i===n?0:-1,"aria-selected":i===n,ref:e=>{c.push(e)},onKeyDown:p,onClick:d,...o,className:(0,a.A)("tabs__item",_.tabItem,o?.className,{"tabs__item--active":i===n}),children:t??n},n)}))})}function x(e){let{lazy:n,children:t,selectedValue:o}=e;const r=(Array.isArray(t)?t:[t]).filter(Boolean);if(n){const e=r.find((e=>e.props.value===o));return e?(0,i.cloneElement)(e,{className:(0,a.A)("margin-top--md",e.props.className)}):null}return(0,b.jsx)("div",{className:"margin-top--md",children:r.map(((e,n)=>(0,i.cloneElement)(e,{key:n,hidden:e.props.value!==o})))})}function v(e){const n=g(e);return(0,b.jsxs)("div",{className:(0,a.A)("tabs-container",_.tabList),children:[(0,b.jsx)(y,{...n,...e}),(0,b.jsx)(x,{...n,...e})]})}function j(e){const n=(0,f.A)();return(0,b.jsx)(v,{...e,children:p(e.children)},String(n))}},78974:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/create_custom_template_1-42e79c0230fd08300b2610f81aa2ddbb.png"},79329:(e,n,t)=>{t.d(n,{A:()=>r});t(96540);var i=t(18215);const a={tabItem:"tabItem_Ymn6"};var o=t(74848);function r(e){let{children:n,hidden:t,className:r}=e;return(0,o.jsx)("div",{role:"tabpanel",className:(0,i.A)(a.tabItem,r),hidden:t,children:n})}}}]);
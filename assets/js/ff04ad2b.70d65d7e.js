"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[1984],{19259:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>t,default:()=>o,frontMatter:()=>r,metadata:()=>l,toc:()=>a});var i=s(74848),d=s(28453);const r={description:"Learn about the instance types we support",sidebar_position:4,pagination_next:null},t="Supported Cloud Instances",l={id:"portal-guide/compute-orchestration/cloud-instances",title:"Supported Cloud Instances",description:"Learn about the instance types we support",source:"@site/docs/portal-guide/compute-orchestration/cloud-instances.md",sourceDirName:"portal-guide/compute-orchestration",slug:"/portal-guide/compute-orchestration/cloud-instances",permalink:"/portal-guide/compute-orchestration/cloud-instances",draft:!1,unlisted:!1,editUrl:"https://github.com/Clarifai/docs/blob/main/docs/portal-guide/compute-orchestration/cloud-instances.md",tags:[],version:"current",sidebarPosition:4,frontMatter:{description:"Learn about the instance types we support",sidebar_position:4,pagination_next:null},sidebar:"tutorialSidebar",previous:{title:"Managing Your Compute",permalink:"/portal-guide/compute-orchestration/manage"}},c={},a=[{value:"AWS Cloud Instances",id:"aws-cloud-instances",level:2},{value:"t3a Instances",id:"t3a-instances",level:3},{value:"g4dn Instances",id:"g4dn-instances",level:3},{value:"g5 Instances",id:"g5-instances",level:3},{value:"g6 Instances",id:"g6-instances",level:3},{value:"GCP Cloud Instances",id:"gcp-cloud-instances",level:2},{value:"N2-Standard Instances",id:"n2-standard-instances",level:3},{value:"G2-Standard Instances",id:"g2-standard-instances",level:3},{value:"A2 &amp; A3 High-Performance Instances",id:"a2--a3-high-performance-instances",level:3}];function h(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,d.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h1,{id:"supported-cloud-instances",children:"Supported Cloud Instances"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Learn about the instance types we support"})}),"\n",(0,i.jsx)("hr",{}),"\n",(0,i.jsx)(n.p,{children:"We offer a range of Amazon Web Services (AWS) and Google Cloud Platform (GCP) instance types, designed to handle a variety of machine learning workloads."}),"\n",(0,i.jsx)(n.p,{children:"These instances vary in their CPU, RAM (Random Access Memory), and GPU configurations, which allow you to orchestrate the right balance of performance and cost for your use case."}),"\n",(0,i.jsx)(n.h2,{id:"aws-cloud-instances",children:"AWS Cloud Instances"}),"\n",(0,i.jsx)(n.h3,{id:"t3a-instances",children:"t3a Instances"}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"t3a"})," series is designed for cost-effective, general-purpose workloads that do not require GPU acceleration. It offers a balanced combination of CPU and memory, making it ideal for lightweight applications."]}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Instance Type"}),(0,i.jsx)(n.th,{children:"GPUs"}),(0,i.jsx)(n.th,{children:"Total GPU RAM"}),(0,i.jsx)(n.th,{children:"CPU"}),(0,i.jsx)(n.th,{children:"RAM"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"t3a.medium"})}),(0,i.jsx)(n.td,{children:"-"}),(0,i.jsx)(n.td,{children:"-"}),(0,i.jsx)(n.td,{children:"2x CPU"}),(0,i.jsx)(n.td,{children:"4GiB"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"t3a.large"})}),(0,i.jsx)(n.td,{children:"-"}),(0,i.jsx)(n.td,{children:"-"}),(0,i.jsx)(n.td,{children:"2x CPU"}),(0,i.jsx)(n.td,{children:"8GiB"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"t3a.xlarge"})}),(0,i.jsx)(n.td,{children:"-"}),(0,i.jsx)(n.td,{children:"-"}),(0,i.jsx)(n.td,{children:"4x CPU"}),(0,i.jsx)(n.td,{children:"16GiB"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"t3a.2xlarge"})}),(0,i.jsx)(n.td,{children:"-"}),(0,i.jsx)(n.td,{children:"-"}),(0,i.jsx)(n.td,{children:"8x CPU"}),(0,i.jsx)(n.td,{children:"32GiB"})]})]})]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Key Features"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["vCPUs (virtual CPUs) \u2014 Burstable performance for intermittent, compute-heavy tasks. Ideal for CPU-intensive operations like running traditional models or pre-processing pipelines.  For example, ",(0,i.jsx)(n.code,{children:"t3a.medium"})," offers two vCPUs, while ",(0,i.jsx)(n.code,{children:"t3a.2xlarge"})," offers eight vCPUs."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"RAM \u2014 Determines the capacity for handling data in memory. It ranges from 4 GiB to 32 GiB, allowing you to handle lightweight, data-intensive workloads without requiring GPU acceleration."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Example Use Case"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Running simple models for classification tasks."}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"g4dn-instances",children:"g4dn Instances"}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"g4dn"})," series is designed for moderate GPU-accelerated workloads, making it suitable for small-to-medium-scale machine learning tasks."]}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Instance Type"}),(0,i.jsx)(n.th,{children:"GPUs"}),(0,i.jsx)(n.th,{children:"Total GPU RAM"}),(0,i.jsx)(n.th,{children:"CPU"}),(0,i.jsx)(n.th,{children:"RAM"})]})}),(0,i.jsx)(n.tbody,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"g4dn.xlarge"})}),(0,i.jsx)(n.td,{children:"1x  NVIDIA-T4"}),(0,i.jsx)(n.td,{children:"16GiB"}),(0,i.jsx)(n.td,{children:"4x CPU"}),(0,i.jsx)(n.td,{children:"16GiB"})]})})]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Key Features"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"NVIDIA T4 GPUs \u2014 Optimized for inference and light model training, offering a balance of performance and cost."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"vCPUs and RAM \u2014 Includes four vCPUs and 16 GiB of RAM for data processing and workload orchestration."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Example Use Cases"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Inference workloads, such as running NLP models like BERT-base for text summarization and question answering."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Fine-tuning pre-trained models for specific tasks like object detection or sentiment analysis."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"g5-instances",children:"g5 Instances"}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"g5"})," series delivers enhanced GPU capabilities and is designed for tasks requiring higher memory and computational power, such as large-scale deep learning model training."]}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Instance Type"}),(0,i.jsx)(n.th,{children:"GPUs"}),(0,i.jsx)(n.th,{children:"Total GPU RAM"}),(0,i.jsx)(n.th,{children:"CPU"}),(0,i.jsx)(n.th,{children:"RAM"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"g5.xlarge"})}),(0,i.jsx)(n.td,{children:"1x  NVIDIA-A10G"}),(0,i.jsx)(n.td,{children:"24GiB"}),(0,i.jsx)(n.td,{children:"4x CPU"}),(0,i.jsx)(n.td,{children:"16GiB"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"g5.2xlarge"})}),(0,i.jsx)(n.td,{children:"1x  NVIDIA-A10G"}),(0,i.jsx)(n.td,{children:"24GiB"}),(0,i.jsx)(n.td,{children:"8x CPU"}),(0,i.jsx)(n.td,{children:"32GiB"})]})]})]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Key Features"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"NVIDIA A10G GPUs \u2014 High memory bandwidth and compute power for complex deep learning models and advanced workloads."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"vCPUs and RAM \u2014 Increased CPU and memory for tasks involving heavy data processing alongside GPU computation."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Example Use Cases"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Training mid-sized NLP models like GPT-2 or T5 for text generation, or training image segmentation models like UNet or Mask R-CNN for medical imaging."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Running object tracking or pose estimation workflows in real-time video analysis."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"g6-instances",children:"g6 Instances"}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"g6"})," series offers next-generation GPU technologies and is designed for the most demanding machine learning workloads, including large-scale model training and high-performance simulations. Each instance type in the ",(0,i.jsx)(n.code,{children:"g6"})," series is tailored to specific workloads."]}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Instance Type"}),(0,i.jsx)(n.th,{children:"GPUs"}),(0,i.jsx)(n.th,{children:"Total GPU RAM"}),(0,i.jsx)(n.th,{children:"CPU"}),(0,i.jsx)(n.th,{children:"RAM"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"g6.xlarge"})}),(0,i.jsx)(n.td,{children:"1x  NVIDIA-L4"}),(0,i.jsx)(n.td,{children:"24GiB"}),(0,i.jsx)(n.td,{children:"4x CPU"}),(0,i.jsx)(n.td,{children:"16GiB"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"g6.2xlarge"})}),(0,i.jsx)(n.td,{children:"1x  NVIDIA-L4"}),(0,i.jsx)(n.td,{children:"24GiB"}),(0,i.jsx)(n.td,{children:"8x CPU"}),(0,i.jsx)(n.td,{children:"32GiB"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"g6e.xlarge"})}),(0,i.jsx)(n.td,{children:"1x  NVIDIA-L40S"}),(0,i.jsx)(n.td,{children:"48GiB"}),(0,i.jsx)(n.td,{children:"4x CPU"}),(0,i.jsx)(n.td,{children:"32GiB"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"g6e.12xlarge"})}),(0,i.jsx)(n.td,{children:"4x  NVIDIA-L40S"}),(0,i.jsx)(n.td,{children:"192GiB"}),(0,i.jsx)(n.td,{children:"48x CPU"}),(0,i.jsx)(n.td,{children:"384GiB"})]})]})]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Key Features"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Next-Gen GPUs \u2014 NVIDIA L4 and L40S GPUs deliver exceptional performance for training and inference tasks, with GPU memory scaling from 24 GiB to 192 GiB."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"High vCPU & RAM Configurations \u2014  Ideal for handling massive datasets and parallel processing for complex workflows."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Example Use Cases"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"g6.xlarge"})," and ",(0,i.jsx)(n.code,{children:"g6.2xlarge"})," instances support mid-tier workloads, such as fine-tuning the BERT-large model or running computer vision tasks like text-to-image generation."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"g6e.xlarge"})," and ",(0,i.jsx)(n.code,{children:"g6e.12xlarge"})," instances support high-end workloads, such as training large-scale language models like GPT-4 or T5-XL for multi-modal tasks."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"gcp-cloud-instances",children:"GCP Cloud Instances"}),"\n",(0,i.jsx)(n.h3,{id:"n2-standard-instances",children:"N2-Standard Instances"}),"\n",(0,i.jsx)(n.p,{children:"The N2-Standard series is designed for cost-effective, general-purpose workloads that do not require GPU acceleration. These instances provide a balanced combination of CPU and memory, making them ideal for lightweight applications."}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Instance Type"}),(0,i.jsx)(n.th,{children:"GPUs"}),(0,i.jsx)(n.th,{children:"Total GPU RAM"}),(0,i.jsx)(n.th,{children:"CPU"}),(0,i.jsx)(n.th,{children:"RAM"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"n2-standard-2"}),(0,i.jsx)(n.td,{children:"-"}),(0,i.jsx)(n.td,{children:"-"}),(0,i.jsx)(n.td,{children:"2x CPU"}),(0,i.jsx)(n.td,{children:"8 GiB"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"n2-standard-4"}),(0,i.jsx)(n.td,{children:"-"}),(0,i.jsx)(n.td,{children:"-"}),(0,i.jsx)(n.td,{children:"4x CPU"}),(0,i.jsx)(n.td,{children:"16 GiB"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"n2-standard-8"}),(0,i.jsx)(n.td,{children:"-"}),(0,i.jsx)(n.td,{children:"-"}),(0,i.jsx)(n.td,{children:"8x CPU"}),(0,i.jsx)(n.td,{children:"32 GiB"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"n2-standard-16"}),(0,i.jsx)(n.td,{children:"-"}),(0,i.jsx)(n.td,{children:"-"}),(0,i.jsx)(n.td,{children:"16x CPU"}),(0,i.jsx)(n.td,{children:"64 GiB"})]})]})]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Key Features"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"vCPUs (Virtual CPUs) \u2014 Optimized for CPU-intensive operations like running traditional models or pre-processing pipelines."}),"\n",(0,i.jsx)(n.li,{children:"RAM \u2014 Ranges from 8 GiB to 64 GiB, allowing efficient handling of lightweight, data-intensive workloads."}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Example Use Case"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Running small-scale machine learning models or serving simple inference workloads."}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"g2-standard-instances",children:"G2-Standard Instances"}),"\n",(0,i.jsx)(n.p,{children:"The G2-Standard series is designed for moderate GPU-accelerated workloads, making it ideal for small-to-medium-scale machine learning tasks."}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Instance Type"}),(0,i.jsx)(n.th,{children:"GPUs"}),(0,i.jsx)(n.th,{children:"Total GPU RAM"}),(0,i.jsx)(n.th,{children:"CPU"}),(0,i.jsx)(n.th,{children:"RAM"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"g2-standard-4"}),(0,i.jsx)(n.td,{children:"1x NVIDIA-L4"}),(0,i.jsx)(n.td,{children:"24 GiB"}),(0,i.jsx)(n.td,{children:"4x CPU"}),(0,i.jsx)(n.td,{children:"16 GiB"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"g2-standard-8"}),(0,i.jsx)(n.td,{children:"1x NVIDIA-L4"}),(0,i.jsx)(n.td,{children:"24 GiB"}),(0,i.jsx)(n.td,{children:"8x CPU"}),(0,i.jsx)(n.td,{children:"32 GiB"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"g2-standard-12"}),(0,i.jsx)(n.td,{children:"1x NVIDIA-L4"}),(0,i.jsx)(n.td,{children:"24 GiB"}),(0,i.jsx)(n.td,{children:"12x CPU"}),(0,i.jsx)(n.td,{children:"48 GiB"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"g2-standard-16"}),(0,i.jsx)(n.td,{children:"1x NVIDIA-L4"}),(0,i.jsx)(n.td,{children:"24 GiB"}),(0,i.jsx)(n.td,{children:"16x CPU"}),(0,i.jsx)(n.td,{children:"64 GiB"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"g2-standard-32"}),(0,i.jsx)(n.td,{children:"1x NVIDIA-L4"}),(0,i.jsx)(n.td,{children:"24 GiB"}),(0,i.jsx)(n.td,{children:"32x CPU"}),(0,i.jsx)(n.td,{children:"128 GiB"})]})]})]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Key Features"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"NVIDIA L4 GPUs \u2014 Optimized for inference and light model training, offering a balance of performance and cost."}),"\n",(0,i.jsx)(n.li,{children:"Scalable vCPUs and RAM \u2014 Supports larger data processing and orchestration workloads."}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Example Use Cases"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Running NLP models like BERT-base for text summarization."}),"\n",(0,i.jsx)(n.li,{children:"Fine-tuning small vision models for object detection."}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"a2--a3-high-performance-instances",children:"A2 & A3 High-Performance Instances"}),"\n",(0,i.jsx)(n.p,{children:"For large-scale deep learning and AI workloads, the A2 and A3 series provide cutting-edge GPUs with high memory bandwidth."}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Instance Type"}),(0,i.jsx)(n.th,{children:"GPUs"}),(0,i.jsx)(n.th,{children:"Total GPU RAM"}),(0,i.jsx)(n.th,{children:"CPU"}),(0,i.jsx)(n.th,{children:"RAM"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"a2-ultragpu-1g"}),(0,i.jsx)(n.td,{children:"1x NVIDIA-A100"}),(0,i.jsx)(n.td,{children:"80 GiB"}),(0,i.jsx)(n.td,{children:"12x CPU"}),(0,i.jsx)(n.td,{children:"170 GiB"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"a3-highgpu-1g"}),(0,i.jsx)(n.td,{children:"1x NVIDIA-H100"}),(0,i.jsx)(n.td,{children:"80 GiB"}),(0,i.jsx)(n.td,{children:"26x CPU"}),(0,i.jsx)(n.td,{children:"234 GiB"})]})]})]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Key Features"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"NVIDIA A100 & H100 GPUs \u2014 Designed for high-end AI and deep learning tasks, including large-scale model training."}),"\n",(0,i.jsx)(n.li,{children:"High CPU & RAM Configurations \u2014 Enables parallel processing for massive datasets and complex workflows."}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Example Use Cases"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Training large language models like GPT-4 or T5-XL."}),"\n",(0,i.jsx)(n.li,{children:"Running real-time AI applications, such as video analytics or autonomous systems."}),"\n"]})]})}function o(e={}){const{wrapper:n}={...(0,d.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(h,{...e})}):h(e)}},28453:(e,n,s)=>{s.d(n,{R:()=>t,x:()=>l});var i=s(96540);const d={},r=i.createContext(d);function t(e){const n=i.useContext(r);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(d):e.components||d:t(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);
"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[5735],{11470:(e,t,i)=>{i.d(t,{A:()=>k});var n=i(96540),s=i(18215),r=i(23104),a=i(56347),o=i(205),d=i(57485),l=i(31682),c=i(70679);function h(e){return n.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,n.isValidElement)(e)&&function(e){const{props:t}=e;return!!t&&"object"==typeof t&&"value"in t}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function m(e){const{values:t,children:i}=e;return(0,n.useMemo)(()=>{const e=t??function(e){return h(e).map(({props:{value:e,label:t,attributes:i,default:n}})=>({value:e,label:t,attributes:i,default:n}))}(i);return function(e){const t=(0,l.XI)(e,(e,t)=>e.value===t.value);if(t.length>0)throw new Error(`Docusaurus error: Duplicate values "${t.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[t,i])}function g({value:e,tabValues:t}){return t.some(t=>t.value===e)}function p({queryString:e=!1,groupId:t}){const i=(0,a.W6)(),s=function({queryString:e=!1,groupId:t}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:e,groupId:t});return[(0,d.aZ)(s),(0,n.useCallback)(e=>{if(!s)return;const t=new URLSearchParams(i.location.search);t.set(s,e),i.replace({...i.location,search:t.toString()})},[s,i])]}function x(e){const{defaultValue:t,queryString:i=!1,groupId:s}=e,r=m(e),[a,d]=(0,n.useState)(()=>function({defaultValue:e,tabValues:t}){if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!g({value:e,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${t.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const i=t.find(e=>e.default)??t[0];if(!i)throw new Error("Unexpected error: 0 tabValues");return i.value}({defaultValue:t,tabValues:r})),[l,h]=p({queryString:i,groupId:s}),[x,f]=function({groupId:e}){const t=function(e){return e?`docusaurus.tab.${e}`:null}(e),[i,s]=(0,c.Dv)(t);return[i,(0,n.useCallback)(e=>{t&&s.set(e)},[t,s])]}({groupId:s}),u=(()=>{const e=l??x;return g({value:e,tabValues:r})?e:null})();(0,o.A)(()=>{u&&d(u)},[u]);return{selectedValue:a,selectValue:(0,n.useCallback)(e=>{if(!g({value:e,tabValues:r}))throw new Error(`Can't select invalid tab value=${e}`);d(e),h(e),f(e)},[h,f,r]),tabValues:r}}var f=i(92303);const u={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var j=i(74848);function b({className:e,block:t,selectedValue:i,selectValue:n,tabValues:a}){const o=[],{blockElementScrollPositionUntilNextRender:d}=(0,r.a_)(),l=e=>{const t=e.currentTarget,s=o.indexOf(t),r=a[s].value;r!==i&&(d(t),n(r))},c=e=>{let t=null;switch(e.key){case"Enter":l(e);break;case"ArrowRight":{const i=o.indexOf(e.currentTarget)+1;t=o[i]??o[0];break}case"ArrowLeft":{const i=o.indexOf(e.currentTarget)-1;t=o[i]??o[o.length-1];break}}t?.focus()};return(0,j.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,s.A)("tabs",{"tabs--block":t},e),children:a.map(({value:e,label:t,attributes:n})=>(0,j.jsx)("li",{role:"tab",tabIndex:i===e?0:-1,"aria-selected":i===e,ref:e=>{o.push(e)},onKeyDown:c,onClick:l,...n,className:(0,s.A)("tabs__item",u.tabItem,n?.className,{"tabs__item--active":i===e}),children:t??e},e))})}function w({lazy:e,children:t,selectedValue:i}){const r=(Array.isArray(t)?t:[t]).filter(Boolean);if(e){const e=r.find(e=>e.props.value===i);return e?(0,n.cloneElement)(e,{className:(0,s.A)("margin-top--md",e.props.className)}):null}return(0,j.jsx)("div",{className:"margin-top--md",children:r.map((e,t)=>(0,n.cloneElement)(e,{key:t,hidden:e.props.value!==i}))})}function y(e){const t=x(e);return(0,j.jsxs)("div",{className:(0,s.A)("tabs-container",u.tabList),children:[(0,j.jsx)(b,{...t,...e}),(0,j.jsx)(w,{...t,...e})]})}function k(e){const t=(0,f.A)();return(0,j.jsx)(y,{...e,children:h(e.children)},String(t))}},19365:(e,t,i)=>{i.d(t,{A:()=>a});i(96540);var n=i(18215);const s={tabItem:"tabItem_Ymn6"};var r=i(74848);function a({children:e,hidden:t,className:i}){return(0,r.jsx)("div",{role:"tabpanel",className:(0,n.A)(s.tabItem,i),hidden:t,children:e})}},73578:(e,t,i)=>{i.r(t),i.d(t,{assets:()=>u,contentTitle:()=>f,default:()=>w,frontMatter:()=>x,metadata:()=>n,toc:()=>j});const n=JSON.parse('{"id":"create/workflows/examples/yaml","title":"YAML-based Examples","description":"Simple examples of workflows defined in YAML","source":"@site/docs/create/workflows/examples/yaml.md","sourceDirName":"create/workflows/examples","slug":"/create/workflows/examples/yaml","permalink":"/create/workflows/examples/yaml","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"description":"Simple examples of workflows defined in YAML","sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Workflows Examples","permalink":"/create/workflows/examples/"},"next":{"title":"Auto-Annotation via UI","permalink":"/create/workflows/examples/auto-annotation-ui"}}');var s=i(74848),r=i(28453),a=(i(11470),i(19365),i(73748));const o="workflow:\n  id: asr-sentiment\n  nodes:\n    - id: audio-speech-recognition\n      model:\n          model_id: asr-wav2vec2-large-robust-ft-swbd-300h-english\n          user_id: facebook\n          app_id: asr\n\n    - id: text-sentiment-classification\n      model:\n          model_id: sentiment-analysis-twitter-roberta-base\n          user_id: erfan\n          app_id: text-classification\n\n      node_inputs:\n        - node_id: audio-speech-recognition",d="workflow:\n  id: Demographics\n  nodes:\n    - id: detect-concept\n      model:\n        model_id: face-detection\n        model_version_id: 45fb9a671625463fa646c3523a3087d5\n\n    - id: image-crop\n      model:\n        model_id: margin-110-image-crop\n        model_version_id: b9987421b40a46649566826ef9325303\n      node_inputs:\n        - node_id: detect-concept\n\n    - id: demographics-race\n      model:\n        model_id: ethnicity-demographics-recognition\n        model_version_id: b2897edbda314615856039fb0c489796\n      node_inputs:\n        - node_id: image-crop\n\n    - id: demographics-gender\n      model:\n        model_id: gender-demographics-recognition\n        model_version_id: ff83d5baac004aafbe6b372ffa6f8227\n      node_inputs:\n        - node_id: image-crop\n\n    - id: demographics-age\n      model:\n        model_id: age-demographics-recognition\n        model_version_id: fb9f10339ac14e23b8e960e74984401b\n      node_inputs:\n        - node_id: image-crop\n",l="workflow:\n  id: Face-Search\n  nodes:\n    - id: face-detect\n      model:\n        model_id: face-detection\n        model_version_id: fe995da8cb73490f8556416ecf25cea3\n\n    - id: crop\n      model:\n        model_id: margin-100-image-crop\n        model_version_id: 0af5cd8ad40e43ef92154e4f4bc76bef\n      node_inputs:\n        - node_id: face-detect\n\n    - id: face-landmarks\n      model:\n        model_id: face-landmarks\n        model_version_id: 98ace9ca45e64339be94b06011557e2a\n      node_inputs:\n        - node_id: crop\n\n    - id: face-alignment\n      model:\n        model_id: landmarks-align\n        model_version_id: 4bc8b83a327247829ec638c78cde5f8b\n      node_inputs:\n        - node_id: face-landmarks\n\n    - id: face-embed\n      model:\n        model_id: face-identification-transfer-learn\n        model_version_id: fc3b8814fbe54533a3d80a1896dc9884\n      node_inputs:\n        - node_id: face-alignment\n\n    - id: face-cluster\n      model:\n        model_id: face-clustering\n        model_version_id: 621d74074a5443d7ad9dc1503fba9ff0\n      node_inputs:\n        - node_id: face-embed\n",c="workflow:\n  id: Face-Sentiment\n  nodes:\n    - id: face-det\n      model:\n        model_id: face-detection\n        model_version_id: 6dc7e46bc9124c5c8824be4822abe105\n\n    - id: margin-110\n      model:\n        model_id: margin-110-image-crop\n        model_version_id: b9987421b40a46649566826ef9325303\n      node_inputs:\n        - node_id: face-det\n\n    - id: face-sentiment\n      model:\n        model_id: face-sentiment-recognition\n        model_version_id: a5d7776f0c064a41b48c3ce039049f65\n      node_inputs:\n        - node_id: margin-110\n",h="workflow:\n  id: General\n  nodes:\n    - id: general-v1.5-concept\n      model:\n          model_id: aaa03c23b3724a16a56b629203edc62c\n          model_version_id: aa7f35c01e0642fda5cf400f543e7c40\n\n    - id: general-v1.5-embed\n      model:\n          model_id: bbb5f41425b8468d9b7a554ff10f8581\n          model_version_id: bb186755eda04f9cbb6fe32e816be104\n\n    - id: general-v1.5-cluster\n      model:\n          model_id: cccbe437d6e54e2bb911c6aa292fb072\n          model_version_id: cc2074cff6dc4c02b6f4e1b8606dcb54\n      node_inputs:\n        - node_id: general-v1.5-embed\n",m='workflow:\n  id: wf-ocr\n  nodes:\n    - id: ocr-workflow\n      model:\n          model_id: language-aware-multilingual-ocr-multiplex\n\n    - id: text-aggregator\n      model:\n          model_id: text-aggregation\n          model_type_id: text-aggregation-operator\n          output_info:\n            params:\n              avg_word_width_window_factor: 2.0\n              avg_word_height_window_factor: 1.0\n\n      node_inputs:\n        - node_id: ocr-workflow\n\n    - id: language-id-operator\n      model:\n          model_id: language-id\n          model_type_id: language-id-operator\n          output_info:\n            params:\n              library: "fasttext"\n              topk: 1\n              threshold:  0.1\n              lowercase: true\n\n      node_inputs:\n        - node_id: text-aggregator\n',g="workflow:\n  id: wf-prompter-llm\n  nodes:\n    - id: prompter\n      model:\n          model_id: prompter\n          model_type_id: prompter\n          description: 'Prompter Model'\n          output_info:\n            params:\n              prompt_template: 'Classify sentiment between postive and negative for the text {data.text.raw}'\n\n    - id: llm\n      model:\n          user_id: mistralai\n          model_id: mistral-7B-Instruct\n          app_id: completion\n\n      node_inputs:\n        - node_id: prompter",p="workflow:\n  id: wf-prompter-llm\n  nodes:\n    - id: rag-prompter\n      model:\n          model_id: rag-prompter\n          model_type_id: rag-prompter\n          description: 'RAG Prompter Model'\n\n    - id: llm\n      model:\n          user_id: mistralai\n          model_id: mistral-7B-Instruct\n          app_id: completion\n\n      node_inputs:\n        - node_id: rag-prompter",x={description:"Simple examples of workflows defined in YAML",sidebar_position:1},f="YAML-based Examples",u={},j=[{value:"Assorted Examples",id:"assorted-examples",level:2},{value:"ASR Sentiment",id:"asr-sentiment",level:2},{value:"Demographics",id:"demographics",level:2},{value:"Face Search",id:"face-search",level:2},{value:"Face Sentiment",id:"face-sentiment",level:2},{value:"General",id:"general",level:2},{value:"Language Aware OCR",id:"language-aware-ocr",level:2},{value:"Prompter LLM",id:"prompter-llm",level:2},{value:"RAG Prompter LLM",id:"rag-prompter-llm",level:2}];function b(e){const t={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",header:"header",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.header,{children:(0,s.jsx)(t.h1,{id:"yaml-based-examples",children:"YAML-based Examples"})}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.strong,{children:"Simple examples of workflows defined in YAML"})}),"\n",(0,s.jsx)("hr",{}),"\n","\n","\n",(0,s.jsx)(t.h2,{id:"assorted-examples",children:"Assorted Examples"}),"\n",(0,s.jsxs)(t.table,{children:[(0,s.jsx)(t.thead,{children:(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.th,{children:"Node Name"}),(0,s.jsx)(t.th,{children:"Input & Output"}),(0,s.jsx)(t.th,{children:"Description"}),(0,s.jsx)(t.th,{style:{textAlign:"center"},children:"Example Usage"})]})}),(0,s.jsxs)(t.tbody,{children:[(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"audio-to-text"}),(0,s.jsx)(t.td,{children:"Audio -> Text"}),(0,s.jsx)(t.td,{children:"Classify audio signal into string of text."}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:(0,s.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Predict/audio-to-text.yml",children:[(0,s.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"barcode-operator"}),(0,s.jsx)(t.td,{children:"Image -> Text"}),(0,s.jsx)(t.td,{children:"Operator that detects and recognizes barcodes from the image. It assigns regions with barcode text for each detected barcode. Supports EAN/UPC, Code 128, Code 39, Interleaved 2 of 5 and QR Code."}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:(0,s.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Predict/barcode-operator.yml",children:[(0,s.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"Centroid Tracker"}),(0,s.jsx)(t.td,{children:"Frames -> Track ID"}),(0,s.jsx)(t.td,{children:"Centroid trackers rely on the Euclidean distance between centroids of regions in different video frames to assign the same track ID to detections of the same object."}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:(0,s.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Predict/centroid-tracker.yml",children:[(0,s.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"Clusterer"}),(0,s.jsx)(t.td,{children:"Embeddings -> Clusters"}),(0,s.jsx)(t.td,{children:"Cluster semantically similar images and video frames together in embedding space. This is the basis for good visual search within your app at scale or for grouping your data together without the need for annotated concepts"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:(0,s.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Predict/clusterer.yml",children:[(0,s.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"embeddings-classifier"}),(0,s.jsx)(t.td,{children:"Embeddings -> Concepts"}),(0,s.jsx)(t.td,{children:"Classify images or texts based on the embedding model that has indexed them in your app. Transfer learning leverages feature representations from a pre-trained model based on massive amounts of data, so you don't have to train a new model from scratch and can learn new things very quickly with minimal training data"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:(0,s.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Predict/embeddings-classifier.yml",children:[(0,s.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"image-color-recognizer"}),(0,s.jsx)(t.td,{children:"Image -> Colors"}),(0,s.jsx)(t.td,{children:"Recognize standard color formats and the proportion each color that covers an image"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:(0,s.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Predict/image-color-recognizer.yml",children:[(0,s.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"image-to-text"}),(0,s.jsx)(t.td,{children:"Image -> Text"}),(0,s.jsx)(t.td,{children:"Takes in cropped regions with text in them and returns the text it sees."}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:(0,s.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Predict/image-to-text.yml",children:[(0,s.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"kalman-filter-tracker"}),(0,s.jsx)(t.td,{children:"Frames -> Track ID"}),(0,s.jsx)(t.td,{children:"Kalman Filter trackers rely on the Kalman Filter algorithm to estimate the next position of an object based on its position and velocity in previous frames. Then detections are matched to predictions by using the Hungarian algorithm"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:(0,s.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Predict/kalman-filter-tracker.yml",children:[(0,s.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"kalman-reid-tracker"}),(0,s.jsx)(t.td,{children:"Frames -> Track ID"}),(0,s.jsx)(t.td,{children:"Kalman reid tracker is a kalman filter tracker that expects the Embedding proto field to be populated for detections, and reassigns track IDs based off of embedding distance"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:(0,s.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Predict/kalman-reid-tracker.yml",children:[(0,s.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"neural-lite-tracker"}),(0,s.jsx)(t.td,{children:"Frames -> Track ID"}),(0,s.jsx)(t.td,{children:"Neural Lite Tracker uses light-weight trainable graphical models to infer states of tracks and perform associations using hybrid similairty of lou and centroid distance"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:(0,s.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Predict/neural-lite-tracker.yml",children:[(0,s.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"neural-tracker"}),(0,s.jsx)(t.td,{children:"Frames -> Track ID"}),(0,s.jsx)(t.td,{children:"Neural Tracker uses neural probabilistic models to perform filtering and association"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:(0,s.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Predict/neural-tracker.yml",children:[(0,s.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"optical-character-recognizer"}),(0,s.jsx)(t.td,{children:"Image -> Text"}),(0,s.jsx)(t.td,{children:"Detect bounding box regions in images or video frames where text is present and then output the text read with the score"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:(0,s.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Predict/optical-character-recognizer.yml",children:[(0,s.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"tesseract-operator"}),(0,s.jsx)(t.td,{children:"Image -> Text"}),(0,s.jsx)(t.td,{children:"Operator for Optical Character Recognition using the Tesseract libraries"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:(0,s.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Predict/tesseract-operator.yml",children:[(0,s.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"text-classifier"}),(0,s.jsx)(t.td,{children:"Text -> Concepts"}),(0,s.jsx)(t.td,{children:"Classify text into a set of concepts"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:(0,s.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Predict/text-classifier.yml",children:[(0,s.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"text-embedder"}),(0,s.jsx)(t.td,{children:"Text -> Embeddings"}),(0,s.jsx)(t.td,{children:"Embed text into a vector representing a high level understanding from our Al models.  These embeddings enable similarity search and training on top of them"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:(0,s.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Predict/text-embedder.yml",children:[(0,s.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"text-token-classifier"}),(0,s.jsx)(t.td,{children:"Text -> Concepts"}),(0,s.jsx)(t.td,{children:"Classify tokens from a set of entity classes"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:(0,s.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Predict/text-token-classifier.yml",children:[(0,s.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"visual-classifier"}),(0,s.jsx)(t.td,{children:"Image -> Concepts"}),(0,s.jsx)(t.td,{children:"Classify images and videos frames into set of concepts"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:(0,s.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Predict/visual-classifier.yml",children:[(0,s.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"visual-detector"}),(0,s.jsx)(t.td,{children:"Image -> Bounding Box"}),(0,s.jsx)(t.td,{children:"Detect bounding box regions in images or video frames where things and then classify objects, descriptive words or topics within the boxes"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:(0,s.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Predict/visual-detector.yml",children:[(0,s.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"visual-embedder"}),(0,s.jsx)(t.td,{children:"Image -> Embeddings"}),(0,s.jsx)(t.td,{children:"Embed images and videos frames into a vector representing a high level understanding from our Al models. These embeddings enable visual search and training on top of them"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:(0,s.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Predict/visual-embedder.yml",children:[(0,s.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"visual-segmenter"}),(0,s.jsx)(t.td,{children:"Image -> Concepts"}),(0,s.jsx)(t.td,{children:"Segment a per-pixel mask in images where things are and then classify objects, descriptive words or topics within the masks"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:(0,s.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Predict/visual-segmenter.yml",children:[(0,s.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"concept-thresholder"}),(0,s.jsx)(t.td,{children:"Concepts -> Concpets"}),(0,s.jsx)(t.td,{children:'Threshold input concepts according to both a threshold and an operator (>, >=, =, <=, or <). For example, assume the " > " threshold type is set for the model, then if the input concept value is greater than the threshold for that concept, the input concept will be output from this model, otherwise it will not be output by the model'}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:(0,s.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Filter/concept-thresholder.yml",children:[(0,s.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"random-sample"}),(0,s.jsx)(t.td,{children:"Any -> Any"}),(0,s.jsx)(t.td,{children:"Randomly sample allowing the input to pass to the output. This is done with the conditional keep_fraction > rand() where keep_fraction is the fraction to allow through on average"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:(0,s.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Filter/random-sample.yml",children:[(0,s.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"region-thresholder"}),(0,s.jsx)(t.td,{children:"Concepts -> Concepts"}),(0,s.jsx)(t.td,{children:'Threshold regions based on the concepts that they contain using a threshold per concept and an overall operator (>, >=, =, <=, or <). For example, assume the " > " threshold type is set for the model, then if the input regions[...].data.concepts.value is greater than the threshold for that concept, the input concept will be output from this model, otherwise it will not be output by the model. If the entire list of concepts at regions[...].data.concepts is filtered out then the overall region will also be removed'}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:(0,s.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Filter/region-thresholder.yml",children:[(0,s.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"byte-tracker"}),(0,s.jsx)(t.td,{children:"Frame -> Track ID"}),(0,s.jsx)(t.td,{children:"Uses byte tracking algorithm for tracking objects"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:(0,s.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Edit/byte-tracker.yml",children:[(0,s.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"concept-synonym-mapper"}),(0,s.jsx)(t.td,{children:"Concept -> Concept"}),(0,s.jsx)(t.td,{children:"Map the input concepts to output concepts by following synonym concept relations in the knowledge graph of your app"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:(0,s.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Edit/concept-synonym-mapper.yml",children:[(0,s.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"image-align"}),(0,s.jsx)(t.td,{children:"Image -> Image"}),(0,s.jsx)(t.td,{children:"Aligns images using keypoints"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:(0,s.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Edit/image-align.yml",children:[(0,s.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"image-crop"}),(0,s.jsx)(t.td,{children:"Image -> Image"}),(0,s.jsx)(t.td,{children:"Crop the input image according to each input region that is present in the input. When used in a workflow this model can look back along the graph of the workflow to find the input image if the preceding model does not output an image itself so that you can do image -> detector -> cropper type of workflow easily"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:(0,s.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Edit/image-crop.yml",children:[(0,s.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"image-tiling-operator"}),(0,s.jsx)(t.td,{children:"Image -> Image"}),(0,s.jsx)(t.td,{children:"Operator for tiling images into a fixed number of equal sized images"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:(0,s.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Edit/image-tiling-operator.yml",children:[(0,s.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"image-to-image"}),(0,s.jsx)(t.td,{children:"Image -> Image"}),(0,s.jsx)(t.td,{children:"Given an image, apply a transformation on the input and return the post-processed image as output"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:(0,s.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Edit/image-to-image.yml",children:[(0,s.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"input-filter"}),(0,s.jsx)(t.td,{children:"Any -> Any"}),(0,s.jsx)(t.td,{children:"If the input going through this model does not match those we are filtering for, it will not be passed on in the workflow branch"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:(0,s.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Edit/input-filter.yml",children:[(0,s.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"input-searcher"}),(0,s.jsx)(t.td,{children:"Concepts,Images,Text -> Hits"}),(0,s.jsx)(t.td,{children:"Triggers a visual search in another app based on the model configs if concept(s) are found in images and returns the matched search hits as regions."}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:(0,s.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Edit/keyword-filter-operator.yml",children:[(0,s.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"keyword-filter-operator"}),(0,s.jsx)(t.td,{children:"Text -> Concepts"}),(0,s.jsx)(t.td,{children:"This operator is initialized with a set of words, and then determines which are found in the input text."}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:(0,s.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Edit/input-searcher.yml",children:[(0,s.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"language-id-operator"}),(0,s.jsx)(t.td,{children:"Text -> Concepts"}),(0,s.jsx)(t.td,{children:"Operator for language identification using the langdetect library"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:(0,s.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Edit/language-id-operator.yml",children:[(0,s.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"multimodal-embedder"}),(0,s.jsx)(t.td,{children:"Any -> Embeddings"}),(0,s.jsx)(t.td,{children:"Embed text or image into a vector representing a high level understanding from our Al models, e.g. CLIP. These embeddings enable similarity search and training on top of them."}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:(0,s.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Edit/multimodal-embedder.yml",children:[(0,s.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"multimodal-to-text"}),(0,s.jsx)(t.td,{children:"Any -> Text"}),(0,s.jsx)(t.td,{children:"Generate text from either text or images or both as input, allowing it to understand and respond to questions about those images"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:(0,s.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Edit/multimodal-to-text.yml",children:[(0,s.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"prompter"}),(0,s.jsx)(t.td,{children:"Text -> Text"}),(0,s.jsx)(t.td,{children:"Prompt template where inputted text will be inserted into placeholders marked with (data.text.raw)."}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:(0,s.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Edit/prompter.yml",children:[(0,s.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"rag-prompter"}),(0,s.jsx)(t.td,{children:"Text -> Text"}),(0,s.jsxs)(t.td,{children:["A prompt template where we will perform a semantic search in the app with the incoming text. The inputted text will be inserted into placeholders marked with '(data.text.raw)' and search results will be inserted into placeholders with '",(0,s.jsx)(t.code,{children:"{data.hits}"}),"', which will be new line separated"]}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:(0,s.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Edit/rag-prompter.yml",children:[(0,s.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"regex-based-classifier"}),(0,s.jsx)(t.td,{children:"Text -> Concepts"}),(0,s.jsx)(t.td,{children:"Classifies text using regex. If the regex matches, the text is classified as the provided concepts."}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:(0,s.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Edit/regex-based-classifier.yml",children:[(0,s.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"text-to-audio"}),(0,s.jsx)(t.td,{children:"Text -> Audio"}),(0,s.jsx)(t.td,{children:"Given text input, this model produces an audio file containing the spoken version of the input"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:(0,s.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Edit/text-to-audio.yml",children:[(0,s.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"text-to-image"}),(0,s.jsx)(t.td,{children:"Text -> Image"}),(0,s.jsx)(t.td,{children:"Takes in a prompt and generates an image"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:(0,s.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Edit/text-to-image.yml",children:[(0,s.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"tiling-region-aggregator-operator"}),(0,s.jsx)(t.td,{children:"Frames -> Concepts,Bounding Box"}),(0,s.jsx)(t.td,{children:"Operator to be used as a follow up to the image-tiling-operator and visual detector. This operator will transform the detections on each of tiles back to the original image and perform non-maximum suppression. Only the top class prediction for each box is considered"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:(0,s.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Edit/tiling-region-aggregator-operator.yml",children:[(0,s.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"visual-keypointer"}),(0,s.jsx)(t.td,{children:"Image -> Keypoints"}),(0,s.jsx)(t.td,{children:"This model detects keypoints in images or video frames."}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:(0,s.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Edit/visual-keypointer.yml",children:[(0,s.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"isolation-operator"}),(0,s.jsx)(t.td,{children:"Concepts,BoundingBox -> Concepts,BoundingBox"}),(0,s.jsx)(t.td,{children:"Operator that computes distance between detections and assigns isolation label"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:(0,s.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Aggregate/isolation-operator.yml",children:[(0,s.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"object-counter"}),(0,s.jsx)(t.td,{children:"Concepts -> Metadata"}),(0,s.jsx)(t.td,{children:"count number of regions that match this model's active concepts frame by frame"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:(0,s.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Aggregate/object-counter.yml",children:[(0,s.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"text-aggregation-operator"}),(0,s.jsx)(t.td,{children:"Text -> Text"}),(0,s.jsx)(t.td,{children:"Operator that combines text detections into text body for the whole image. Detections are sorted from left to right first and then top to bottom, using the top-left corner of the bounding box as reference"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:(0,s.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Aggregate/text-aggregation-operator.yml",children:[(0,s.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"tokens-to-entity-operator"}),(0,s.jsx)(t.td,{children:"Text,Concepts -> Text,Concepts"}),(0,s.jsx)(t.td,{children:"Operator that combines text tokens into entities, e.g. New' + 'York' -> New York"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:(0,s.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Aggregate/tokens-to-entity-operator.yml",children:[(0,s.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"annotation-writer"}),(0,s.jsx)(t.td,{children:"Any -> Any"}),(0,s.jsx)(t.td,{children:"Write the input data to the database in the form of an annotation with a specified status as if a specific user created the annotation"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:(0,s.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Action/annotation-writer.yml",children:[(0,s.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"aws-lambda"}),(0,s.jsx)(t.td,{children:"Any -> Any"}),(0,s.jsx)(t.td,{children:"This model sends data to an AWS lambda function so you can implement any arbitrary logic to be handled within a model predict or workflow. The request our API sends is a PostModelOutputsRequest in the 'request' field and the response we expect is a MultiOutputResponse response in the 'response' field"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:(0,s.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Action/aws-lambda.yml",children:[(0,s.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"email"}),(0,s.jsx)(t.td,{children:"Any -> Any"}),(0,s.jsx)(t.td,{children:"Email alert model will send an email if there are any data fields input to this model"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:(0,s.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Action/email.yml",children:[(0,s.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"results-push"}),(0,s.jsx)(t.td,{children:"Any -> Any"}),(0,s.jsx)(t.td,{children:"This model pushes clarifai prediction results in an external format"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:(0,s.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Action/results-push.yml",children:[(0,s.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"sms"}),(0,s.jsx)(t.td,{children:"Any -> Any"}),(0,s.jsx)(t.td,{children:"SMS alert model will send a SMS if there are any data fields input to this model"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:(0,s.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Action/sms.yml",children:[(0,s.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"status-push"}),(0,s.jsx)(t.td,{children:"Any -> Any"}),(0,s.jsx)(t.td,{children:"This model pushes processing status of a batch of inputs ingested through vendor/inputs endpoint in one request"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:(0,s.jsxs)("a",{href:"https://github.com/Clarifai/examples/blob/main/workflows/configs/Nodes/Action/status-push.yml",children:[(0,s.jsx)("img",{src:"/img/python-sdk/yaml.jpeg",width:"50",height:"50"})," "]})})]})]})]}),"\n",(0,s.jsx)(t.h2,{id:"asr-sentiment",children:"ASR Sentiment"}),"\n",(0,s.jsx)(t.p,{children:"Automatic Speech Recognition (ASR) sentiment analysis is the process of detecting the emotional tone or sentiment in spoken language by first transcribing speech using an ASR model and then analyzing the resulting text."}),"\n",(0,s.jsx)(a.A,{className:"language-text",children:o}),"\n",(0,s.jsx)(t.h2,{id:"demographics",children:"Demographics"}),"\n",(0,s.jsx)(t.p,{children:"This is a multi-model workflow designed to detect faces, crop them, and recognize key demographic characteristics. It visually classifies attributes such as age, gender, and cultural appearance."}),"\n",(0,s.jsx)(a.A,{className:"language-text",children:d}),"\n",(0,s.jsx)(t.h2,{id:"face-search",children:"Face Search"}),"\n",(0,s.jsx)(t.p,{children:"A workflow that combines face detection, recognition, and embedding to generate facial landmarks and enable visual search based on the embeddings of detected faces."}),"\n",(0,s.jsx)(a.A,{className:"language-text",children:l}),"\n",(0,s.jsx)(t.h2,{id:"face-sentiment",children:"Face Sentiment"}),"\n",(0,s.jsx)(t.p,{children:"A multi-model workflow that combines face detection with sentiment classification to recognize seven emotional expressions: anger, disgust, fear, neutral, happiness, sadness, and contempt."}),"\n",(0,s.jsx)(a.A,{className:"language-text",children:c}),"\n",(0,s.jsx)(t.h2,{id:"general",children:"General"}),"\n",(0,s.jsx)(t.p,{children:"A general-purpose image detection workflow that identifies a wide range of common objects and enables visual search using embeddings generated from the detected regions."}),"\n",(0,s.jsx)(a.A,{className:"language-text",children:h}),"\n",(0,s.jsx)(t.h2,{id:"language-aware-ocr",children:"Language Aware OCR"}),"\n",(0,s.jsx)(t.p,{children:"A workflow that performs Optical Character Recognition (OCR) across multiple languages, automatically adapting to the language present in the input text."}),"\n",(0,s.jsx)(a.A,{className:"language-text",children:m}),"\n",(0,s.jsx)(t.h2,{id:"prompter-llm",children:"Prompter LLM"}),"\n",(0,s.jsx)(t.p,{children:"A workflow that utilizes a prompt template to interact with a Large Language Model (LLM), enabling dynamic and context-aware text generation based on input data."}),"\n",(0,s.jsx)(a.A,{className:"language-text",children:g}),"\n",(0,s.jsx)(t.h2,{id:"rag-prompter-llm",children:"RAG Prompter LLM"}),"\n",(0,s.jsx)(t.p,{children:"This workflow combines a Large Language Model (LLM) with a Retrieval-Augmented Generation (RAG) prompter template to generate responses informed by relevant external knowledge."}),"\n",(0,s.jsx)(a.A,{className:"language-text",children:p}),"\n",(0,s.jsx)(t.admonition,{type:"tip",children:(0,s.jsxs)(t.p,{children:["Click ",(0,s.jsx)(t.a,{href:"https://github.com/Clarifai/examples/tree/main/workflows/configs",children:"here"})," to view more YAML-based workflows examples."]})})]})}function w(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(b,{...e})}):b(e)}}}]);
"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[1991],{16626:(e,i,t)=>{t.r(i),t.d(i,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>s,metadata:()=>o,toc:()=>c});var n=t(74848),a=t(28453);const s={description:"Learn about our visual classification templates",sidebar_position:1,keywords:["visual classification templates","deep learning visual classification","AI visual classification","image classification templates","deep training visual classification","machine learning visual classification","custom visual classification models","pre-trained visual classification templates"]},r="Visual Classification Templates",o={id:"portal-guide/model/deep-training/visual-classification-templates",title:"Visual Classification Templates",description:"Learn about our visual classification templates",source:"@site/docs/portal-guide/model/deep-training/visual-classification-templates.md",sourceDirName:"portal-guide/model/deep-training",slug:"/portal-guide/model/deep-training/visual-classification-templates",permalink:"/portal-guide/model/deep-training/visual-classification-templates",draft:!1,unlisted:!1,editUrl:"https://github.com/Clarifai/docs/blob/main/docs/portal-guide/model/deep-training/visual-classification-templates.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{description:"Learn about our visual classification templates",sidebar_position:1,keywords:["visual classification templates","deep learning visual classification","AI visual classification","image classification templates","deep training visual classification","machine learning visual classification","custom visual classification models","pre-trained visual classification templates"]},sidebar:"tutorialSidebar",previous:{title:"Deep Fine-Tuning",permalink:"/portal-guide/model/deep-training/"},next:{title:"Visual Detection Templates",permalink:"/portal-guide/model/deep-training/visual-detection-templates"}},l={},c=[{value:"MMClassification_ResNet_50_RSB_A1",id:"mmclassification_resnet_50_rsb_a1",level:2},{value:"Clarifai_InceptionBatchNorm",id:"clarifai_inceptionbatchnorm",level:2},{value:"Clarifai_InceptionV2",id:"clarifai_inceptionv2",level:2},{value:"Clarifai_ResNext",id:"clarifai_resnext",level:2},{value:"Clarifai_InceptionTransferEmbedNorm",id:"clarifai_inceptiontransferembednorm",level:2}];function h(e){const i={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",li:"li",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(i.h1,{id:"visual-classification-templates",children:"Visual Classification Templates"}),"\n",(0,n.jsx)(i.p,{children:(0,n.jsx)(i.strong,{children:"Learn about our visual classification templates"})}),"\n",(0,n.jsx)("hr",{}),"\n",(0,n.jsx)(i.p,{children:"Clarifai visual classification templates let you train a model to classify objects in your image inputs. Each template comes with its own hyperparameters, which you can tune to influence \u201chow\u201d your model learns. With hyperparameters, you can customize and fine-tune a template to suit your specific tasks and achieve better performance."}),"\n",(0,n.jsxs)(i.admonition,{type:"tip",children:[(0,n.jsx)(i.p,{children:"You can customize most hyperparameters by specifying the following values:"}),(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.code,{children:"minimum"}),"\u2014the minimum value a given parameter can take."]}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.code,{children:"maximum"}),"\u2014the maximum value a given parameter can take."]}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.code,{children:"step"}),"\u2014determines how much you can increment or decrement the minimum or maximum value in a single click/change."]}),"\n"]})]}),"\n",(0,n.jsx)(i.h2,{id:"mmclassification_resnet_50_rsb_a1",children:"MMClassification_ResNet_50_RSB_A1"}),"\n",(0,n.jsx)(i.p,{children:"This template is a customized variant of the ResNet-50 architecture for multimodal classification tasks. Let\u2019s break down the components in the naming of the deep learning model architecture:"}),"\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsxs)(i.li,{children:["\n",(0,n.jsxs)(i.p,{children:[(0,n.jsx)(i.strong,{children:"MMClassification:"})," This refers to the ",(0,n.jsx)(i.a,{href:"https://github.com/open-mmlab/mmpretrain/tree/main",children:"MMClassification toolkit"})," that is designed for image classification tasks."]}),"\n"]}),"\n",(0,n.jsxs)(i.li,{children:["\n",(0,n.jsxs)(i.p,{children:[(0,n.jsx)(i.strong,{children:"ResNet_50:"}),' This refers to a specific variant of the Residual Network (ResNet) architecture. ResNet is a popular deep neural network architecture known for its skip connections that help alleviate the vanishing gradient problem. The number "50" typically denotes the depth or number of layers in the ResNet model.']}),"\n"]}),"\n",(0,n.jsxs)(i.li,{children:["\n",(0,n.jsxs)(i.p,{children:[(0,n.jsx)(i.strong,{children:"RSB_A1:"})," This refers to a particular modification, adaptation, or variant of the ResNet architecture."]}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(i.admonition,{type:"info",children:(0,n.jsx)(i.p,{children:"We currently support MMClassification v2.1.0."})}),"\n",(0,n.jsxs)(i.p,{children:["The ",(0,n.jsx)(i.strong,{children:"MMClassification_ResNet_50_RSB_A1"})," template supports the following hyperparameters:"]}),"\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.strong,{children:"Image size"}),"\u2014This is the image size for training and inference. ResNet uses square images. Images are scaled for efficient processing, and a lower number will take up less memory and run faster. A higher number will have more pixel information to train on and will increase accuracy."]}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.strong,{children:"Batch size"}),"\u2014The number of images used during training. Increased batch size allows for a better approximation of gradient over those samples. Batches allow for stochastic gradient descent, by choosing a random set of X images for each training update. You may want to increase the batch size if the model is large and takes a long time to train. You may also want to increase the batch size if your total number of model concepts is larger than the batch size (you may want to increase it to around 2x the category count). The minimum value it supports for customization is ",(0,n.jsx)(i.code,{children:"1"}),", while the maximum is ",(0,n.jsx)(i.code,{children:"256"}),"\u2014with an incremental or decremental step of ",(0,n.jsx)(i.code,{children:"1"}),"."]}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.strong,{children:"Num epochs"}),"\u2014This is the total number of epochs to train for. An epoch is defined as one-pass over the entire dataset. If you increase it, the model will take longer to train, but it could make the model more robust. The minimum value it supports for customization is ",(0,n.jsx)(i.code,{children:"1"}),", while the maximum is ",(0,n.jsx)(i.code,{children:"600"}),"\u2014with an incremental or decremental step of ",(0,n.jsx)(i.code,{children:"1"}),"."]}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.strong,{children:"Per item_lrate"}),"\u2014This is the initial learning rate per item; it's the rate that the model weights are changed per item. The ",(0,n.jsx)(i.strong,{children:"lrate"})," (learning rate) is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function. The overall learning rate (per step) is calculated by ",(0,n.jsx)(i.code,{children:"lrate = batch_size * per_item_lrate"}),". The minimum value it supports for customization is ",(0,n.jsx)(i.code,{children:"0.0"}),"."]}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.strong,{children:"Weight decay"}),"\u2014This is the weight decay value. It is used to prevent overfitting by penalizing large weights in the model. The minimum value it supports for customization is ",(0,n.jsx)(i.code,{children:"0.0"}),", while the maximum is ",(0,n.jsx)(i.code,{children:"1"}),"."]}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.strong,{children:"Per item_min_lrate"}),"\u2014This is the minimum learning (per item) at the end of training using the cosine schedule. The minimum value it supports for customization is ",(0,n.jsx)(i.code,{children:"0.0"}),"."]}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.strong,{children:"Warmup iters"}),"\u2014This is the number of steps in the warmup phase, during which the learning rate gradually increases before reaching its specified value.  The minimum value it supports for customization is ",(0,n.jsx)(i.code,{children:"0.0"}),"."]}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.strong,{children:"Warmup ratio"}),"\u2014The warmup phase learning rate multiplier, which scales the learning rate during the warmup phase."]}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.strong,{children:"Pretrained weights"}),"\u2014This specifies whether to init the model with pre-trained weights. You can choose either ",(0,n.jsx)(i.code,{children:"None"})," or ",(0,n.jsx)(i.code,{children:"ImageNet-1k"})," (default) for this parameter."]}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.strong,{children:"Flip probability"}),"\u2014This is the probability that an image will be randomly flipped during training. Flipping images horizontally or vertically can augment the dataset and improve model generalization. The minimum value it supports for customization is ",(0,n.jsx)(i.code,{children:"0.0"}),", while the maximum is ",(0,n.jsx)(i.code,{children:"1"}),"."]}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.strong,{children:"Flip direction"}),"\u2014This is the direction to randomly flip during training. You can choose either ",(0,n.jsx)(i.code,{children:"horizontal"})," (default) or ",(0,n.jsx)(i.code,{children:"vertical"})," for this parameter."]}),"\n"]}),"\n",(0,n.jsx)(i.h2,{id:"clarifai_inceptionbatchnorm",children:"Clarifai_InceptionBatchNorm"}),"\n",(0,n.jsx)(i.p,{children:"This is an image classifier template based on the Inception architecture, which has been pre-trained on a combination of the ImageNet-21K dataset and additional image classification data."}),"\n",(0,n.jsx)(i.p,{children:"The Inception architecture, initially introduced by Google, is known for its effectiveness in image classification tasks. It utilizes various convolutional layers and pooling operations to extract hierarchical features from images, enabling accurate classification."}),"\n",(0,n.jsx)(i.p,{children:"By leveraging transfer learning, the pretrained Inception model can be used as a starting point for training an image classifier on a specific dataset or task."}),"\n",(0,n.jsx)(i.p,{children:"In this case, the model has been pre-trained on the ImageNet-21K dataset, which consists of millions of labeled images from a wide range of categories. This dataset serves as a general-purpose pretraining source, providing the model with a foundation of knowledge about various visual concepts and features."}),"\n",(0,n.jsx)(i.p,{children:"Additionally, the model has been further trained or fine-tuned on additional image classification data. This suggests that specific image datasets related to the intended classification task or domain have been utilized to enhance the model's performance and adapt it to the specific context."}),"\n",(0,n.jsx)(i.p,{children:"The template is implemented using the Batch Normalization technique. The Batch Normalization method is a normalization technique that helps accelerate training and improve model performance by reducing internal covariate shift."}),"\n",(0,n.jsxs)(i.p,{children:["By incorporating Batch Normalization into the Inception architecture, the ",(0,n.jsx)(i.strong,{children:"Clarifai_InceptionBatchNorm"})," classifier achieves better generalization and stability during training. It allows for efficient and accurate classification of images, leveraging the rich pretraining on the ImageNet-21K dataset and the additional image classification data."]}),"\n",(0,n.jsxs)(i.p,{children:["The ",(0,n.jsx)(i.strong,{children:"Clarifai_InceptionBatchNorm"})," template supports the following hyperparameters:"]}),"\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.strong,{children:"Logreg"}),'\u2014This specifies whether to use sigmoid units (logreg=1) or softmax (logreg=0); you can choose either "Logistic Regression" or "Softmax" as the activation function of the output layer. The default setting, 1, corresponds to Logistic Regression and will allow for multiple True concepts (i.e., P > 0.5) to be predicted for a given input. Conversely, specify a value of 0 to implement Softmax if your concepts should be treated as "mutually exclusive" (i.e., only one concept could be correctly assigned to a given input). This will result in each prediction output representing a discrete probability distribution (i.e., all predicted values sum to 1). The minimum value it supports for customization is ',(0,n.jsx)(i.code,{children:"0"}),", while the maximum is ",(0,n.jsx)(i.code,{children:"1"}),"\u2014with an incremental or decremental step of ",(0,n.jsx)(i.code,{children:"1"}),"."]}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.strong,{children:"Image size"}),"\u2014This is the input image size, specifically the minimum side dimension. Images are scaled for efficient processing, and a lower number will take up less memory and run faster. A higher number will have more pixel information to train on and will increase accuracy. The minimum value it supports for customization is ",(0,n.jsx)(i.code,{children:"32"}),", while the maximum is ",(0,n.jsx)(i.code,{children:"1024"}),"\u2014with an incremental or decremental step of ",(0,n.jsx)(i.code,{children:"16"}),"."]}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.strong,{children:"Batch size"}),"\u2014The number of images used during training. Increased batch size allows for a better approximation of gradient over those samples. Batches allow for stochastic gradient descent, by choosing a random set of X images for each training update. You may want to increase the batch size if the model is large and takes a long time to train. You may also want to increase the batch size if your total number of model concepts is larger than the batch size (you may want to increase it to around 2x the category count). The minimum value it supports for customization is ",(0,n.jsx)(i.code,{children:"1"}),", while the maximum is ",(0,n.jsx)(i.code,{children:"128"}),"\u2014with an incremental or decremental step of ",(0,n.jsx)(i.code,{children:"1"}),"."]}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.strong,{children:"Init epochs"}),"\u2014This is the number of epochs to run at the initial learning rate before the first step/change in the learning rate. An epoch is defined as one-pass over the entire dataset. If you increase it, the model will take longer to train, but it could make the model more robust. The minimum value it supports for customization is ",(0,n.jsx)(i.code,{children:"1"}),", while the maximum is ",(0,n.jsx)(i.code,{children:"200"}),"\u2014with an incremental or decremental step of ",(0,n.jsx)(i.code,{children:"1"}),"."]}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.strong,{children:"Step epochs"}),'\u2014This is the number of epochs between learning rate decreases. The learning rate will be adjusted after each "Step epochs" number of epochs. The minimum value it supports for customization is ',(0,n.jsx)(i.code,{children:"1"}),", while the maximum is ",(0,n.jsx)(i.code,{children:"200"}),"\u2014with an incremental or decremental step of ",(0,n.jsx)(i.code,{children:"1"}),"."]}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.strong,{children:"Num epochs"}),"\u2014This is the total number of epochs to train the model. It determines how many passes the model makes over the entire dataset during training. The minimum value it supports for customization is ",(0,n.jsx)(i.code,{children:"1"}),", while the maximum is ",(0,n.jsx)(i.code,{children:"200"}),"\u2014with an incremental or decremental step of ",(0,n.jsx)(i.code,{children:"1"}),"."]}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.strong,{children:"Per item_lrate"}),"\u2014This is the initial learning rate per item; it's the rate that the model weights are changed per item. The overall learning rate (per step) is calculated by ",(0,n.jsx)(i.code,{children:"lrate = batch_size * per_item_lrate"}),". The minimum value it supports for customization is ",(0,n.jsx)(i.code,{children:"0.0"}),"."]}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.strong,{children:"Num items_per_epoch"}),'\u2014This is the number of input images per "epoch." The default value is the number of images in the dataset.']}),"\n"]}),"\n",(0,n.jsx)(i.h2,{id:"clarifai_inceptionv2",children:"Clarifai_InceptionV2"}),"\n",(0,n.jsx)(i.p,{children:"This template is an implementation of the InceptionV2 architecture without any modifications, starting with randomly initialized weights. This means that the model does not utilize any pretraining on large-scale datasets like ImageNet or any other specific initialization method."}),"\n",(0,n.jsx)(i.p,{children:"Instead, it begins with random parameter values for all the layers in the InceptionV2 network. This allows for training the model from scratch or adapting it to a specific task or dataset by optimizing the weights based on the provided training data."}),"\n",(0,n.jsx)(i.p,{children:"The InceptionV2 architecture is a variant of the Inception architecture, which was introduced by researchers at Google as a deep convolutional neural network (CNN) for image classification tasks. InceptionV2 is an improvement upon the original Inception architecture, also known as InceptionV1."}),"\n",(0,n.jsx)(i.p,{children:"The main goal of the InceptionV2 architecture, like its predecessor, is to efficiently capture multi-scale information from images by utilizing various convolutional layers with different receptive field sizes. This allows the network to handle objects of different scales and capture both fine-grained and high-level features simultaneously."}),"\n",(0,n.jsxs)(i.p,{children:["The ",(0,n.jsx)(i.strong,{children:"Clarifai_InceptionV2"})," template supports the following hyperparameters:"]}),"\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.strong,{children:"Logreg"}),'\u2014This specifies whether to use sigmoid units (logreg=1) or softmax (logreg=0); you can choose either "Logistic Regression" or "Softmax" as the activation function of the output layer. The default setting, 1, corresponds to Logistic Regression and will allow for multiple True concepts (i.e., P > 0.5) to be predicted for a given input. Conversely, specify a value of 0 to implement Softmax if your concepts should be treated as "mutually exclusive" (i.e., only one concept could be correctly assigned to a given input). This will result in each prediction output representing a discrete probability distribution (i.e., all predicted values sum to 1). The minimum value it supports for customization is ',(0,n.jsx)(i.code,{children:"0"}),", while the maximum is ",(0,n.jsx)(i.code,{children:"1"}),"\u2014with an incremental or decremental step of ",(0,n.jsx)(i.code,{children:"1"}),"."]}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.strong,{children:"Image size"}),"\u2014This is the input image size, specifically the minimum side dimension. Images are scaled for efficient processing, and a lower number will take up less memory and run faster. A higher number will have more pixel information to train on and will increase accuracy. The minimum value it supports for customization is ",(0,n.jsx)(i.code,{children:"32"}),", while the maximum is ",(0,n.jsx)(i.code,{children:"1024"}),"\u2014with an incremental or decremental step of ",(0,n.jsx)(i.code,{children:"16"}),"."]}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.strong,{children:"Batch size"}),"\u2014The number of images used during training. Increased batch size allows for a better approximation of gradient over those samples. Batches allow for stochastic gradient descent, by choosing a random set of X images for each training update. You may want to increase the batch size if the model is large and takes a long time to train. You may also want to increase the batch size if your total number of model concepts is larger than the batch size (you may want to increase it to around 2x the category count). The minimum value it supports for customization is ",(0,n.jsx)(i.code,{children:"1"}),", while the maximum is ",(0,n.jsx)(i.code,{children:"128"}),"\u2014with an incremental or decremental step of ",(0,n.jsx)(i.code,{children:"1"}),"."]}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.strong,{children:"Init epochs"}),"\u2014This is the number of epochs to run at the initial learning rate before the first step/change in the learning rate. An epoch is defined as one-pass over the entire dataset. If you increase it, the model will take longer to train, but it could make the model more robust. The minimum value it supports for customization is ",(0,n.jsx)(i.code,{children:"1"}),", while the maximum is ",(0,n.jsx)(i.code,{children:"200"}),"\u2014with an incremental or decremental step of ",(0,n.jsx)(i.code,{children:"1"}),"."]}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.strong,{children:"Step epochs"}),'\u2014This is the number of epochs between learning rate decreases. The learning rate will be adjusted after each "Step epochs" number of epochs. The minimum value it supports for customization is ',(0,n.jsx)(i.code,{children:"1"}),", while the maximum is ",(0,n.jsx)(i.code,{children:"200"}),"\u2014with an incremental or decremental step of ",(0,n.jsx)(i.code,{children:"1"}),"."]}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.strong,{children:"Num epochs"}),"\u2014This is the total number of epochs to train the model. It determines how many passes the model makes over the entire dataset during training. The minimum value it supports for customization is ",(0,n.jsx)(i.code,{children:"1"}),", while the maximum is ",(0,n.jsx)(i.code,{children:"200"}),"\u2014with an incremental or decremental step of ",(0,n.jsx)(i.code,{children:"1"}),"."]}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.strong,{children:"Per item_lrate"}),"\u2014This is the initial learning rate per item; it's the rate that the model weights are changed per item. The overall learning rate (per step) is calculated by ",(0,n.jsx)(i.code,{children:"lrate = batch_size * per_item_lrate"}),". The minimum value it supports for customization is ",(0,n.jsx)(i.code,{children:"0.0"}),"."]}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.strong,{children:"Num items_per_epoch"}),'\u2014This is the number of input images per "epoch." The default value is the number of images in the dataset.']}),"\n"]}),"\n",(0,n.jsx)(i.h2,{id:"clarifai_resnext",children:"Clarifai_ResNext"}),"\n",(0,n.jsx)(i.p,{children:"This template combines the power of the ResNeXt architecture, pre-trained on ImageNet-21K, with fine-tuning on domain-specific image classification data, and tailored modifications to meet Clarifai's unique requirements."}),"\n",(0,n.jsxs)(i.p,{children:[(0,n.jsx)(i.a,{href:"https://github.com/open-mmlab/mmpretrain/tree/main/configs/resnext",children:"ResNeXt"}),', short for "Residual Next," is a deep convolutional neural network (CNN) architecture that extends the ResNet (Residual Network) architecture. It was introduced by researchers at Facebook AI Research (FAIR) as an advancement in the field of computer vision.']}),"\n",(0,n.jsx)(i.p,{children:'ResNeXt introduces the concept of "cardinality" to enhance the representational power of the network. The cardinality represents the number of parallel paths within each network block, and it captures different types of feature interactions. Unlike the original ResNet architecture, which focuses on increasing depth or width,\nResNeXt achieves higher model capacity by increasing the number of parallel branches, thus allowing for richer and more diverse feature representations.'}),"\n",(0,n.jsx)(i.p,{children:"The main idea behind ResNeXt is to provide a flexible and scalable architecture that can be easily adjusted based on available computational resources and requirements. By varying the cardinality parameter, ResNeXt can be customized to balance model complexity and performance."}),"\n",(0,n.jsx)(i.p,{children:"ResNeXt architectures have demonstrated superior performance on various computer vision tasks, particularly image classification, by leveraging the power of deep residual connections, which enable efficient training of very deep networks. These networks have achieved state-of-the-art results on benchmark datasets, such as ImageNet."}),"\n",(0,n.jsxs)(i.p,{children:["This implementation is pre-trained on the ImageNet-21K dataset, which encompasses millions of labeled images across a diverse range of categories. By leveraging this large-scale pretraining, ",(0,n.jsx)(i.strong,{children:"Clarifai_ResNext"})," benefits from learning rich and generalizable visual representations from the vast and diverse ImageNet-21K dataset."]}),"\n",(0,n.jsxs)(i.p,{children:["Additionally, ",(0,n.jsx)(i.strong,{children:"Clarifai_ResNext"})," has been further trained or fine-tuned on additional image classification data specific to the target domain or task. This additional training ensures that the model is adapted to the nuances and characteristics of the specific image classification problem, further improving its performance and accuracy within the desired context."]}),"\n",(0,n.jsxs)(i.p,{children:["The ",(0,n.jsx)(i.strong,{children:"Clarifai_ResNext"})," template supports the following hyperparameters:"]}),"\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.strong,{children:"Logreg"}),'\u2014This specifies whether to use sigmoid units (logreg=1) or softmax (logreg=0); you can choose either "Logistic Regression" or "Softmax" as the activation function of the output layer. The default setting, 1, corresponds to Logistic Regression and will allow for multiple True concepts (i.e., P > 0.5) to be predicted for a given input. Conversely, specify a value of 0 to implement Softmax if your concepts should be treated as "mutually exclusive" (i.e., only one concept could be correctly assigned to a given input). This will result in each prediction output representing a discrete probability distribution (i.e., all predicted values sum to 1). The minimum value it supports for customization is ',(0,n.jsx)(i.code,{children:"0"}),", while the maximum is ",(0,n.jsx)(i.code,{children:"1"}),"\u2014with an incremental or decremental step of ",(0,n.jsx)(i.code,{children:"1"}),"."]}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.strong,{children:"Image size"}),"\u2014This is the input image size, specifically the minimum side dimension. Images are scaled for efficient processing, and a lower number will take up less memory and run faster. A higher number will have more pixel information to train on and will increase accuracy. The minimum value it supports for customization is ",(0,n.jsx)(i.code,{children:"32"}),", while the maximum is ",(0,n.jsx)(i.code,{children:"1024"}),"\u2014with an incremental or decremental step of ",(0,n.jsx)(i.code,{children:"16"}),"."]}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.strong,{children:"Batch size"}),"\u2014The number of images used during training. Increased batch size allows for a better approximation of gradient over those samples. Batches allow for stochastic gradient descent, by choosing a random set of X images for each training update. You may want to increase the batch size if the model is large and takes a long time to train. You may also want to increase the batch size if your total number of model concepts is larger than the batch size (you may want to increase it to around 2x the category count). The minimum value it supports for customization is ",(0,n.jsx)(i.code,{children:"1"}),", while the maximum is ",(0,n.jsx)(i.code,{children:"128"}),"\u2014with an incremental or decremental step of ",(0,n.jsx)(i.code,{children:"1"}),"."]}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.strong,{children:"Init epochs"}),"\u2014This is the number of epochs to run at the initial learning rate before the first step/change in the learning rate. An epoch is defined as one-pass over the entire dataset. If you increase it, the model will take longer to train, but it could make the model more robust. The minimum value it supports for customization is ",(0,n.jsx)(i.code,{children:"1"}),", while the maximum is ",(0,n.jsx)(i.code,{children:"200"}),"\u2014with an incremental or decremental step of ",(0,n.jsx)(i.code,{children:"1"}),"."]}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.strong,{children:"Step epochs"}),'\u2014This is the number of epochs between learning rate decreases. The learning rate will be adjusted after each "Step epochs" number of epochs. The minimum value it supports for customization is ',(0,n.jsx)(i.code,{children:"1"}),", while the maximum is ",(0,n.jsx)(i.code,{children:"200"}),"\u2014with an incremental or decremental step of ",(0,n.jsx)(i.code,{children:"1"}),"."]}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.strong,{children:"Num epochs"}),"\u2014This is the total number of epochs to train the model. It determines how many passes the model makes over the entire dataset during training. The minimum value it supports for customization is ",(0,n.jsx)(i.code,{children:"1"}),", while the maximum is ",(0,n.jsx)(i.code,{children:"200"}),"\u2014with an incremental or decremental step of ",(0,n.jsx)(i.code,{children:"1"}),"."]}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.strong,{children:"Per item_lrate"}),"\u2014This is the initial learning rate per item; it's the rate that the model weights are changed per item. The overall learning rate (per step) is calculated by ",(0,n.jsx)(i.code,{children:"lrate = batch_size * per_item_lrate"}),". The minimum value it supports for customization is ",(0,n.jsx)(i.code,{children:"0.0"}),"."]}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.strong,{children:"Num items_per_epoch"}),'\u2014This is the number of input images per "epoch." The default value is the number of images in the dataset.']}),"\n"]}),"\n",(0,n.jsx)(i.h2,{id:"clarifai_inceptiontransferembednorm",children:"Clarifai_InceptionTransferEmbedNorm"}),"\n",(0,n.jsx)(i.p,{children:"This template is an advanced image classifier that leverages the power of the Inception architecture as its foundation. It has been pre-trained on the vast and diverse ImageNet-21K dataset, which provides a comprehensive understanding of various visual concepts. Additionally, to enhance its capabilities further, the model has been exposed to additional image classification data, enabling it to handle a broader range of tasks."}),"\n",(0,n.jsxs)(i.p,{children:["To adapt the pretrained model for transfer learning, the classification head and hyperparameters have undergone careful modifications and tuning. The classification head refers to the top layers of the network responsible for mapping the learned representations to specific classes or categories. By customizing this component, ",(0,n.jsx)(i.strong,{children:"Clarifai_InceptionTransferEmbedNorm"})," can effectively transfer its knowledge from the source domain (ImageNet-21K) to new, target domains with different sets of classes."]}),"\n",(0,n.jsxs)(i.p,{children:["Furthermore, the hyperparameters of the model have been fine-tuned to optimize its performance for transfer learning tasks. Hyperparameters are adjustable settings that govern the learning process, such as learning rate, batch size, and regularization parameters. Through meticulous experimentation and validation, the hyperparameters of ",(0,n.jsx)(i.strong,{children:"Clarifai_InceptionTransferEmbedNorm"})," have been carefully chosen to strike a balance between preserving the general knowledge from the source domain and adapting to the unique characteristics of the target domain."]}),"\n",(0,n.jsxs)(i.p,{children:["By combining the powerful Inception architecture, pre-trained on ImageNet-21K, with the tailored modifications and hyperparameter tuning for transfer learning, ",(0,n.jsx)(i.strong,{children:"Clarifai_InceptionTransferEmbedNorm"})," offers an effective and efficient solution for various image classification tasks, providing accurate predictions and insights."]}),"\n",(0,n.jsxs)(i.p,{children:["The ",(0,n.jsx)(i.strong,{children:"Clarifai_InceptionTransferEmbedNorm"})," template supports the following hyperparameters:"]}),"\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.strong,{children:"Logreg"}),'\u2014This specifies whether to use sigmoid units (logreg=1) or softmax (logreg=0); you can choose either "Logistic Regression" or "Softmax" as the activation function of the output layer. The default setting, 1, corresponds to Logistic Regression and will allow for multiple True concepts (i.e., P > 0.5) to be predicted for a given input. Conversely, specify a value of 0 to implement Softmax if your concepts should be treated as "mutually exclusive" (i.e., only one concept could be correctly assigned to a given input). This will result in each prediction output representing a discrete probability distribution (i.e., all predicted values sum to 1). The minimum value it supports for customization is ',(0,n.jsx)(i.code,{children:"0"}),", while the maximum is ",(0,n.jsx)(i.code,{children:"1"}),"\u2014with an incremental or decremental step of ",(0,n.jsx)(i.code,{children:"1"}),"."]}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.strong,{children:"Image size"}),"\u2014This is the input image size, specifically the minimum side dimension. Images are scaled for efficient processing, and a lower number will take up less memory and run faster. A higher number will have more pixel information to train on and will increase accuracy. The minimum value it supports for customization is ",(0,n.jsx)(i.code,{children:"32"}),", while the maximum is ",(0,n.jsx)(i.code,{children:"1024"}),"\u2014with an incremental or decremental step of ",(0,n.jsx)(i.code,{children:"16"}),"."]}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.strong,{children:"Batch size"}),"\u2014The number of images used during training. Increased batch size allows for a better approximation of gradient over those samples. Batches allow for stochastic gradient descent, by choosing a random set of X images for each training update. You may want to increase the batch size if the model is large and takes a long time to train. You may also want to increase the batch size if your total number of model concepts is larger than the batch size (you may want to increase it to around 2x the category count). The minimum value it supports for customization is ",(0,n.jsx)(i.code,{children:"1"}),", while the maximum is ",(0,n.jsx)(i.code,{children:"128"}),"\u2014with an incremental or decremental step of ",(0,n.jsx)(i.code,{children:"1"}),"."]}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.strong,{children:"lrate"}),"\u2014This is the learning rate per minibatch. It is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function. The minimum value it supports for customization is ",(0,n.jsx)(i.code,{children:"0.0"}),"."]}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.strong,{children:"Base gradient_multiplier"}),'\u2014This sets the learning rate of the pre-initialized base (also sometimes called "backbone") model that generates embeddings. Learning rate controls how the weights of our network are adjusted with respect to the loss gradient. The lower the value, the slower the trip along the downward slope. A low learning rate can help ensure that local minima are not missed, but can take a long time to converge, especially if the model gets stuck on a plateau region.']}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.strong,{children:"Num epochs"}),"\u2014This is the total number of epochs to train the model. It determines how many passes the model makes over the entire dataset during training. The minimum value it supports for customization is ",(0,n.jsx)(i.code,{children:"1"}),", while the maximum is ",(0,n.jsx)(i.code,{children:"200"}),"\u2014with an incremental or decremental step of ",(0,n.jsx)(i.code,{children:"1"}),"."]}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.strong,{children:"Num items_per_epoch"}),'\u2014This is the number of input images per "epoch." The default value is the number of images in the dataset.']}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.strong,{children:"Average horizontal_flips"}),"\u2014If set to true, the template will average the embeddings from the original image and a horizontal flip of the image to get the final embedding vectors to output."]}),"\n"]})]})}function d(e={}){const{wrapper:i}={...(0,a.R)(),...e.components};return i?(0,n.jsx)(i,{...e,children:(0,n.jsx)(h,{...e})}):h(e)}},28453:(e,i,t)=>{t.d(i,{R:()=>r,x:()=>o});var n=t(96540);const a={},s=n.createContext(a);function r(e){const i=n.useContext(s);return n.useMemo((function(){return"function"==typeof e?e(i):{...i,...e}}),[i,e])}function o(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),n.createElement(s.Provider,{value:i},e.children)}}}]);
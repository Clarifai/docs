"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[9723],{11470:(e,n,o)=>{o.d(n,{A:()=>I});var t=o(96540),i=o(18215),r=o(23104),l=o(56347),a=o(205),s=o(57485),d=o(31682),c=o(70679);function h(e){return t.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,t.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function u(e){const{values:n,children:o}=e;return(0,t.useMemo)(()=>{const e=n??function(e){return h(e).map(({props:{value:e,label:n,attributes:o,default:t}})=>({value:e,label:n,attributes:o,default:t}))}(o);return function(e){const n=(0,d.XI)(e,(e,n)=>e.value===n.value);if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[n,o])}function p({value:e,tabValues:n}){return n.some(n=>n.value===e)}function m({queryString:e=!1,groupId:n}){const o=(0,l.W6)(),i=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,s.aZ)(i),(0,t.useCallback)(e=>{if(!i)return;const n=new URLSearchParams(o.location.search);n.set(i,e),o.replace({...o.location,search:n.toString()})},[i,o])]}function f(e){const{defaultValue:n,queryString:o=!1,groupId:i}=e,r=u(e),[l,s]=(0,t.useState)(()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!p({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const o=n.find(e=>e.default)??n[0];if(!o)throw new Error("Unexpected error: 0 tabValues");return o.value}({defaultValue:n,tabValues:r})),[d,h]=m({queryString:o,groupId:i}),[f,x]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[o,i]=(0,c.Dv)(n);return[o,(0,t.useCallback)(e=>{n&&i.set(e)},[n,i])]}({groupId:i}),g=(()=>{const e=d??f;return p({value:e,tabValues:r})?e:null})();(0,a.A)(()=>{g&&s(g)},[g]);return{selectedValue:l,selectValue:(0,t.useCallback)(e=>{if(!p({value:e,tabValues:r}))throw new Error(`Can't select invalid tab value=${e}`);s(e),h(e),x(e)},[h,x,r]),tabValues:r}}var x=o(92303);const g={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var y=o(74848);function j({className:e,block:n,selectedValue:o,selectValue:t,tabValues:l}){const a=[],{blockElementScrollPositionUntilNextRender:s}=(0,r.a_)(),d=e=>{const n=e.currentTarget,i=a.indexOf(n),r=l[i].value;r!==o&&(s(n),t(r))},c=e=>{let n=null;switch(e.key){case"Enter":d(e);break;case"ArrowRight":{const o=a.indexOf(e.currentTarget)+1;n=a[o]??a[0];break}case"ArrowLeft":{const o=a.indexOf(e.currentTarget)-1;n=a[o]??a[a.length-1];break}}n?.focus()};return(0,y.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,i.A)("tabs",{"tabs--block":n},e),children:l.map(({value:e,label:n,attributes:t})=>(0,y.jsx)("li",{role:"tab",tabIndex:o===e?0:-1,"aria-selected":o===e,ref:e=>{a.push(e)},onKeyDown:c,onClick:d,...t,className:(0,i.A)("tabs__item",g.tabItem,t?.className,{"tabs__item--active":o===e}),children:n??e},e))})}function b({lazy:e,children:n,selectedValue:o}){const r=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=r.find(e=>e.props.value===o);return e?(0,t.cloneElement)(e,{className:(0,i.A)("margin-top--md",e.props.className)}):null}return(0,y.jsx)("div",{className:"margin-top--md",children:r.map((e,n)=>(0,t.cloneElement)(e,{key:n,hidden:e.props.value!==o}))})}function v(e){const n=f(e);return(0,y.jsxs)("div",{className:(0,i.A)("tabs-container",g.tabList),children:[(0,y.jsx)(j,{...n,...e}),(0,y.jsx)(b,{...n,...e})]})}function I(e){const n=(0,x.A)();return(0,y.jsx)(v,{...e,children:h(e.children)},String(n))}},19365:(e,n,o)=>{o.d(n,{A:()=>l});o(96540);var t=o(18215);const i={tabItem:"tabItem_Ymn6"};var r=o(74848);function l({children:e,hidden:n,className:o}){return(0,r.jsx)("div",{role:"tabpanel",className:(0,t.A)(i.tabItem,o),hidden:n,children:e})}},44946:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>w,contentTitle:()=>I,default:()=>k,frontMatter:()=>v,metadata:()=>t,toc:()=>A});const t=JSON.parse('{"id":"compute/upload/README","title":"Build and Upload Models","description":"Build and import models, including from external sources like Hugging Face","source":"@site/docs/compute/upload/README.mdx","sourceDirName":"compute/upload","slug":"/compute/upload/","permalink":"/compute/upload/","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"description":"Build and import models, including from external sources like Hugging Face","toc_min_heading_level":2,"toc_max_heading_level":5},"sidebar":"tutorialSidebar","previous":{"title":"Advanced Inference Options","permalink":"/compute/inference/advanced"},"next":{"title":"Test Models Locally","permalink":"/compute/upload/test-locally"}}');var i=o(74848),r=o(28453),l=o(11470),a=o(19365),s=o(73748);const d='from clarifai.runners.models.model_class import ModelClass\nfrom typing import Iterator\n\n\nclass MyModel(ModelClass):\n  """A custom runner that adds "Hello World" to the end of the text."""\n\n  def load_model(self):\n    """Load the model here."""\n\n  @ModelClass.method\n  def predict(self, text1: str = "") -> str:\n    """This is the method that will be called when the runner is run. It takes in an input and\n    returns an output.\n    """\n\n    output_text = text1 + " Hello World!"\n\n    return output_text\n\n  @ModelClass.method\n  def generate(self, text1: str = "") -> Iterator[str]:\n    """Example yielding a whole batch of streamed stuff back."""\n\n    for i in range(10):  # fake something iterating generating 10 times.\n      output_text = text1 + f"Generate Hello World {i}"\n      yield output_text\n\n  @ModelClass.method\n  def stream(self, input_iterator: Iterator[str]) -> Iterator[str]:\n    """Example yielding a whole batch of streamed stuff back."""\n\n    for i, input in enumerate(input_iterator):\n      output_text = input + f"Stream Hello World {i}"\n      yield output_text',c='model:\n  id: "my-uploaded-model"\n  user_id: "YOUR_USER_ID_HERE"\n  app_id: "YOUR_APP_ID_HERE"\n  model_type_id: "text-to-text"\n\nbuild_info:\n  python_version: "3.11"\n\ninference_compute_info:\n  cpu_limit: "1"\n  cpu_requests: "1"\n  cpu_memory: "13Gi"\n  cpu_memory_requests: "2Gi"\n  num_accelerators: 1\n  accelerator_type: ["NVIDIA-*"]\n  accelerator_memory: "15Gi"\n',h='checkpoints:\n  type: "huggingface"\n  repo_id: "meta-llama/Meta-Llama-3-8B-Instruct"\n  when: "runtime"\n  hf_token: "your_hf_token" # Required for private models',u="concepts:\n  - id: '0'\n    name: bus\n  - id: '1'\n    name: person\n  - id: '2'\n    name: bicycle\n  - id: '3'\n    name: car",p='clarifai model upload\n[INFO] 16:53:31.153832 No checkpoints specified in the config file |  thread=2540\n[INFO] 16:53:31.169705 Setup: Using Python version 3.11 from the config file to build the Dockerfile |  thread=2540\n[INFO] 16:53:31.179960 Setup: Validating requirements.txt file at C:\\Users\\Alfrick\\Desktop\\upload-model\\new2\\requirements.txt using uv pip compile |  thread=2540\n[INFO] 16:53:32.919408 Setup: Requirements.txt file validated successfully |  thread=2540\n[INFO] 16:53:32.932616 Setup: Linting Python files: [\'C:\\\\Users\\\\Alfrick\\\\Desktop\\\\upload-model\\\\new2\\\\1\\\\model.py\'] |  thread=2540\n[INFO] 16:53:33.054929 Setup: Python code linted successfully, no errors found. |  thread=2540\n[INFO] 16:53:33.056731 Setup: Using NVIDIA base image to build the Docker image and upload the model |  thread=2540\n[INFO] 16:53:33.343749 New model will be created at https://clarifai.com/alfrick/upload-models-2/models/new2 with it\'s first version. |  thread=2540\nPress Enter to continue...\n[INFO] 16:53:36.893686 Uploading file... |  thread=22676\n[INFO] 16:53:36.897061 Upload complete! |  thread=22676\nStatus: Upload in progress, Progress: 0% - Starting upload.  request_id: sdk-python-11.5.4-8151d7189a23425da945528977ea9Status: Upload done, Progress: 0% - Completed upload of files, initiating model version image build..  request_id: sdk-pStatus: Model image is currently being built., Progress: 0% - Model version image is being built.  request_id: sdk-pytho[INFO] 16:53:37.541441 Created Model Version ID: cd24a0369c4443e1b494ddddc3b42ef9 |  thread=2540\n[INFO] 16:53:37.541441 Full url to that version is: https://clarifai.com/alfrick/upload-models-2/models/new2 |  thread=2540\n[INFO] 16:53:43.222157 2025-06-27 13:53:36.865379 INFO: Downloading uploaded model from storage... |  thread=2540\n[INFO] 16:53:48.357261 2025-06-27 13:53:42.654003 INFO: Done downloading model\n2025-06-27 13:53:42.657878 INFO: Extracting upload...\n2025-06-27 13:53:42.662686 INFO: Done extracting upload\n2025-06-27 13:53:42.665555 INFO: Parsing requirements file for model version ID ****ddddc3b42ef9\n2025-06-27 13:53:42.691445 INFO: Dockerfile found at /shared/context/Dockerfile\ncat: /shared/context/downloader/hf_token: No such file or directory\n2025-06-27 13:53:43.397865 INFO: Setting up credentials\namazon-ecr-credential-helper\nVersion:    0.8.0\nGit commit: ********\n2025-06-27 13:53:43.402700 INFO: Building image...\n#1 \\[internal] load build definition from Dockerfile\n#1 transferring dockerfile: 2.72kB done\n#1 DONE 0.0s\n\n#2 resolve image config for docker-image://docker.io/docker/dockerfile:1.13-labs\n#2 DONE 0.1s\n\n#3 docker-image://docker.io/docker/dockerfile:1.13-labs@sha256:************18b8\n#3 resolve docker.io/docker/dockerfile:1.13-labs@sha256:************18b8 done\n#3 CACHED\n\n#4 \\[internal] load metadata for public.ecr.aws/clarifai-models/python-base:3.11-********\n#4 DONE 0.1s\n\n#5 \\[internal] load .dockerignore\n#5 transferring context: 2B done\n#5 DONE 0.0s\n\n#6 \\[final 1/8] FROM public.ecr.aws/clarifai-models/python-base:3.11-********@sha256:************0579\n#6 resolve public.ecr.aws/clarifai-models/python-base:3.11-********@sha256:************0579 done\n#6 DONE 0.0s\n\n#7 \\[internal] load build context\n#7 transferring context: 2.73kB done\n#7 DONE 0.0s\n\n#8 \\[final 5/8] COPY --chown=nonroot:nonroot downloader/unused.yaml /home/nonroot/main/1/checkpoints/.cache/unused.yaml\n#8 CACHED\n\n#9 \\[final 4/8] RUN ["uv", "pip", "show", "--no-cache-dir", "clarifai"]\n#9 CACHED\n\n#10 \\[final 2/8] COPY --link requirements.txt /home/nonroot/requirements.txt\n#10 CACHED\n\n#11 \\[final 3/8] RUN ["uv", "pip", "install", "--no-cache-dir", "-r", "/home/nonroot/requirements.txt"]\n#11 CACHED\n\n#12 \\[final 6/8] RUN  ["python", "-m", "clarifai.cli", "model", "download-checkpoints", "/home/nonroot/main", "--out_path", "/home/nonroot/main/1/checkpoints", "--stage", "build"]\n#12 CACHED\n\n#13 \\[final 7/8] COPY --link=true 1 /home/nonroot/main/1\n#13 DONE 0.0s\n\n#14 \\[final 8/8] COPY --link=true requirements.txt config.yaml /home/nonroot/main/\n#14 DONE 0.0s\n\n#15 exporting to image\n#15 exporting layers done\n#15 exporting manifest sha256:************2afa done\n#15 exporting config sha256:************79d5 done\n#15 pushing layers\n#15 ...\n\n#16 \\[auth] sharing credentials for 891377382885.dkr.ecr.us-east-1.amazonaws.com\n#16 DONE 0.0s\n\n#15 exporting to image\n#15 pushing layers 1.1s done\n#15 pushing manifest for ****/prod/pytorch:****ddddc3b42ef9@sha256:************2afa\n#15 pushing manifest for ****/prod/pytorch:****ddddc3b42ef9@sha256:************2afa 0.4s done\n#15 DONE 1.5s\n2025-06-27 13:53:45.299206 INFO: Done building image!!! |  thread=2540\n[INFO] 16:53:50.090215 Model build complete! |  thread=2540\n[INFO] 16:53:50.096221 Build time elapsed 12.6s) |  thread=2540\n[INFO] 16:53:50.096221 Check out the model at https://clarifai.com/alfrick/upload-models-2/models/new2 version: cd24a0369c4443e1b494ddddc3b42ef9 |  thread=2540\n[INFO] 16:53:50.106292\n\nXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n# Here is a code snippet to use this model:\nXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n                 |  thread=2540\n[INFO] 16:53:50.108847\n# Clarifai Model Client Script\n# Set the environment variables `CLARIFAI_DEPLOYMENT_ID` and `CLARIFAI_PAT` to run this script.\n# Example usage:\nimport os\n\nfrom clarifai.client import Model\nfrom clarifai.runners.utils import data_types\n\nmodel = Model("https://clarifai.com/alfrick/upload-models-2/models/new2",\n    deployment_id = os.environ[\'CLARIFAI_DEPLOYMENT_ID\'], # Only needed for dedicated deployed models\n )\n\n\n# Example model prediction from different model methods:\n\nresponse = model.predict(text1="What is the future of AI?")\nprint(response)\n\nresponse = model.generate(text1="What is the future of AI?")\nfor res in response:\n    print(res)\n\nresponse = model.stream(input_iterator=iter([What is the future of AI?]))\n\n |  thread=2540\nDo you want to deploy the model? (y/n): y\n[INFO] 16:53:59.722256 Checking for available compute clusters... |  thread=2540\n[INFO] 16:54:01.175314 Available compute clusters: |  thread=2540\n[INFO] 16:54:01.175314 1. new-cluster () |  thread=2540\n[INFO] 16:54:01.175314 2. local-dev-compute-cluster (Default Local Dev Compute Cluster) |  thread=2540\nChoose a compute cluster (1-2) or \'n\' to create a new one: n\n[INFO] 16:54:23.170187 Please create a new compute cluster by visiting: https://clarifai.com/settings/compute/new |  thread=2540\nDo you want to open the compute cluster creation page in your browser? (y/n): y\nAfter creating the compute cluster, press Enter to continue...\n[INFO] 16:56:17.225197 Re-checking for available compute clusters... |  thread=2540\n[INFO] 16:58:37.998573 Available compute clusters: |  thread=2540\n[INFO] 16:58:38.002603 1. new-cluster2 () |  thread=2540\n[INFO] 16:58:38.002603 2. new-cluster () |  thread=2540\n[INFO] 16:58:38.002603 3. local-dev-compute-cluster (Default Local Dev Compute Cluster) |  thread=2540\nChoose a compute cluster (1-3): 1\n[INFO] 16:58:52.048631 Checking for available nodepools in compute cluster \'new-cluster2\'... |  thread=2540\n[INFO] 16:58:53.611561 Available nodepools: |  thread=2540\n[INFO] 16:58:53.613665 1. new-nodepool2 () |  thread=2540\nChoose a nodepool (1-1) or \'n\' to create a new one: n\n[INFO] 16:58:59.948120 Please create a new nodepool by visiting: https://clarifai.com/settings/compute/new-cluster2/nodepools/new |  thread=2540\nDo you want to open the nodepool creation page in your browser? (y/n): y\nAfter creating the nodepool, press Enter to continue...\n[INFO] 17:00:27.299095 Re-checking for available nodepools in compute cluster \'new-cluster2\'... |  thread=2540\n[INFO] 17:00:28.105393 Available nodepools: |  thread=2540\n[INFO] 17:00:28.110061 1. new-nodepool3 () |  thread=2540\n[INFO] 17:00:28.112077 2. new-nodepool2 () |  thread=2540\nChoose a nodepool (1-2): 1\n[INFO] 17:00:33.084285 Please create a new deployment by visiting: https://clarifai.com/settings/compute/deployments/new?computeClusterId=new-cluster2&nodePoolId=new-nodepool3 |  thread=2540\nDo you want to open the deployment creation page in your browser? (y/n): y\n[INFO] 17:00:38.341840 After creating the deployment, your model will be ready for inference! |  thread=2540\n[INFO] 17:00:38.343856 You can always return to view your deployments at: https://clarifai.com/settings/compute/deployments/new?computeClusterId=new-cluster2&nodePoolId=new-nodepool3 |  thread=2540\n',m='from clarifai.client import Model\n\n# Set PAT as an environment variable\n#   export CLARIFAI_PAT=YOUR_PAT_HERE # Unix-Like Systems\n#   set CLARIFAI_PAT=YOUR_PAT_HERE  # Windows\n\n# Initialize with model URL\nmodel = Model(url="https://clarifai.com/alfrick/docs-demos/models/my-uploaded-model")\n\nresponse = model.predict("Yes, I uploaded it!")\n\nprint(response)',f='from clarifai.client import Model\n\n# Set PAT as an environment variable\n#   export CLARIFAI_PAT=YOUR_PAT_HERE # Unix-Like Systems\n#   set CLARIFAI_PAT=YOUR_PAT_HERE  # Windows\n\n# Initialize with model URL\nmodel = Model(url="https://clarifai.com/alfrick/docs-demos/models/my-uploaded-model")\n\nfor response in model.generate("Yes, I uploaded it! "):\n    print(response.text)',x='from clarifai.client import Model\nfrom clarifai.runners.utils.data_types import Text\n\n# Set PAT as an environment variable\n#   export CLARIFAI_PAT=YOUR_PAT_HERE # Unix-Like Systems\n#   set CLARIFAI_PAT=YOUR_PAT_HERE  # Windows\n\n# Initialize with model URL\nmodel = Model(url="https://clarifai.com/alfrick/docs-demos/models/my-uploaded-model", deployment_id="YOUR_DEPLOYMENT_ID_HERE")\n\n# Create a list of input Texts to simulate a stream\ninput_texts = iter([\n    Text(text="First input."),\n    Text(text="Second input."),\n    Text(text="Third input.")\n])\n\n# Call the stream method and process outputs\nresponse_iterator = model.stream(input_texts)\n\n# Print streamed results\nprint("Streaming output:\\n")\nfor response in response_iterator:\n    print(response.text)\n',g="Text(text='Yes, I uploaded it! Hello World!', url=None)",y="Yes, I uploaded it! Generate Hello World 0\nYes, I uploaded it! Generate Hello World 1\nYes, I uploaded it! Generate Hello World 2\nYes, I uploaded it! Generate Hello World 3\nYes, I uploaded it! Generate Hello World 4\nYes, I uploaded it! Generate Hello World 5\nYes, I uploaded it! Generate Hello World 6\nYes, I uploaded it! Generate Hello World 7\nYes, I uploaded it! Generate Hello World 8\nYes, I uploaded it! Generate Hello World 9",j="Streaming output:\n\nFirst input.Stream Hello World 0\nSecond input.Stream Hello World 1\nThird input.Stream Hello World 2",b='# syntax=docker/dockerfile:1.13-labs\nFROM --platform=$TARGETPLATFORM public.ecr.aws/clarifai-models/python-base:3.11-42938da8e33b0f37ee7db16b83631da94c2348b9 as final\n\nCOPY --link requirements.txt /home/nonroot/requirements.txt\n\nENV VIRTUAL_ENV=/venv\nENV PATH="/home/nonroot/.local/bin:$VIRTUAL_ENV/bin:$PATH"\n\n\n# Update clarifai package so we always have latest protocol to the API. Everything should land in /venv\nRUN ["uv", "pip", "install", "--no-cache-dir", "-r", "/home/nonroot/requirements.txt"]\nRUN ["uv", "pip", "show", "--no-cache-dir", "clarifai"]\n\n# Set the NUMBA cache dir to /tmp\n# Set the TORCHINDUCTOR cache dir to /tmp\n# The CLARIFAI* will be set by the templaing system.\nENV NUMBA_CACHE_DIR=/tmp/numba_cache \\\n    TORCHINDUCTOR_CACHE_DIR=/tmp/torchinductor_cache \\\n    HOME=/tmp \\\n    DEBIAN_FRONTEND=noninteractive\n\n#####\n# Copy the files needed to download\n#####\n# This creates the directory that HF downloader will populate and with nonroot:nonroot permissions up.\nCOPY --chown=nonroot:nonroot downloader/unused.yaml /home/nonroot/main/1/checkpoints/.cache/unused.yaml\n\n#####\n# Download checkpoints if config.yaml has checkpoints.when = "build"\nCOPY --link=true config.yaml /home/nonroot/main/\nRUN ["python", "-m", "clarifai.cli", "model", "download-checkpoints", "/home/nonroot/main", "--out_path", "/home/nonroot/main/1/checkpoints", "--stage", "build"]\n#####\n\n# Copy in the actual files like config.yaml, requirements.txt, and most importantly 1/model.py\n# for the actual model.\n# If checkpoints aren\'t downloaded since a checkpoints: block is not provided, then they will\n# be in the build context and copied here as well.\nCOPY --link=true 1 /home/nonroot/main/1\n# At this point we only need these for validation in the SDK.\nCOPY --link=true requirements.txt config.yaml /home/nonroot/main/\n\n# Add the model directory to the python path.\nENV PYTHONPATH=${PYTHONPATH}:/home/nonroot/main \\\n    CLARIFAI_PAT=${CLARIFAI_PAT} \\\n    CLARIFAI_USER_ID=${CLARIFAI_USER_ID} \\\n    CLARIFAI_RUNNER_ID=${CLARIFAI_RUNNER_ID} \\\n    CLARIFAI_NODEPOOL_ID=${CLARIFAI_NODEPOOL_ID} \\\n    CLARIFAI_COMPUTE_CLUSTER_ID=${CLARIFAI_COMPUTE_CLUSTER_ID} \\\n    CLARIFAI_API_BASE=${CLARIFAI_API_BASE:-https://api.clarifai.com}\n\n# Finally run the clarifai entrypoint to start the runner loop and local runner server.\n# Note(zeiler): we may want to make this a clarifai CLI call.\nENTRYPOINT ["python", "-m", "clarifai.runners.server"]\nCMD ["--model_path", "/home/nonroot/main"]\n#############################\n',v={description:"Build and import models, including from external sources like Hugging Face",toc_min_heading_level:2,toc_max_heading_level:5},I="Build and Upload Models",w={},A=[{value:"Step 1: Perform Prerequisites",id:"step-1-perform-prerequisites",level:2},{value:"Sign Up or Log In",id:"sign-up-or-log-in",level:3},{value:"Install Clarifai Package",id:"install-clarifai-package",level:3},{value:"Set up Docker or a Virtual Environment",id:"set-up-docker-or-a-virtual-environment",level:3},{value:"Create Project Directory",id:"create-project-directory",level:3},{value:"Step 2: Build a Model",id:"step-2-build-a-model",level:2},{value:"Prepare <code>model.py</code>",id:"prepare-modelpy",level:3},{value:"a. <code>load_model</code> Method",id:"a-load_model-method",level:4},{value:"b. Prediction Methods",id:"b-prediction-methods",level:4},{value:"Prepare <code>config.yaml</code>",id:"prepare-configyaml",level:3},{value:"Model Info",id:"model-info",level:4},{value:"Build Info",id:"build-info",level:4},{value:"Compute Resources",id:"compute-resources",level:4},{value:"Hugging Face Model Checkpoints",id:"hugging-face-model-checkpoints",level:4},{value:"Model Concepts or Labels",id:"model-concepts-or-labels",level:4},{value:"Prepare <code>requirements.txt</code>",id:"prepare-requirementstxt",level:3},{value:"Step 3: Test the Model Locally",id:"step-3-test-the-model-locally",level:2},{value:"Step 4: Upload the Model to Clarifai",id:"step-4-upload-the-model-to-clarifai",level:2},{value:"Step 5: Deploy the Model",id:"step-5-deploy-the-model",level:2},{value:"Step 6: Predict With Model",id:"step-6-predict-with-model",level:2},{value:"Unary-Unary Predict Call",id:"unary-unary-predict-call",level:3},{value:"Unary-Stream Predict Call",id:"unary-stream-predict-call",level:3},{value:"Stream-Stream Predict Call",id:"stream-stream-predict-call",level:3},{value:"Additional Examples",id:"additional-examples",level:2}];function C(e){const n={a:"a",admonition:"admonition",blockquote:"blockquote",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",mdxAdmonitionTitle:"mdxAdmonitionTitle",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components},{Details:o}=n;return o||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"build-and-upload-models",children:"Build and Upload Models"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Build and import models, including from sources like Hugging Face"})}),"\n",(0,i.jsx)("hr",{}),"\n",(0,i.jsx)("div",{style:{position:"relative",width:"100%",overflow:"hidden","padding-top":"56.25%"},children:(0,i.jsx)("iframe",{width:"900",height:"500",style:{position:"absolute",top:"0",left:"0",bottom:"0",right:"0",width:"100%",height:"100%"},src:"https://www.youtube.com/embed/SpIDmDtf7UE",title:"Upload Custom Models to Clarifai Platform Using Python SDK",frameborder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share",allowfullscreen:!0})}),"\n",(0,i.jsx)("br",{}),"\n",(0,i.jsx)("br",{}),"\n",(0,i.jsx)(n.p,{children:"The Clarifai Python SDK allows you to upload custom models easily. Whether you're working with a pre-trained model from an external source like Hugging Face, or one you've built from scratch, Clarifai allows seamless integration of your models, enabling you to take advantage of the platform\u2019s powerful capabilities."}),"\n",(0,i.jsx)(n.p,{children:"Once imported to our platform, your model can be utilized alongside Clarifai's vast suite of AI tools. It will be automatically deployed and ready to be evaluated, combined with other models and agent operators in a workflow, or used to serve inference requests as it is."}),"\n",(0,i.jsx)(n.admonition,{title:"Objective",type:"info",children:(0,i.jsxs)(n.p,{children:["Let\u2019s walk through how to build and upload a custom model to the Clarifai platform. This example model appends the phrase ",(0,i.jsx)(n.code,{children:"Hello World"})," to any input text and also supports streaming responses."]})}),"\n",(0,i.jsx)(n.admonition,{type:"tip",children:(0,i.jsxs)(n.p,{children:["You can explore ",(0,i.jsx)(n.a,{href:"https://github.com/Clarifai/runners-examples",children:"this repository"})," for examples on uploading different model types."]})}),"\n","\n","\n","\n","\n","\n",(0,i.jsx)(n.h2,{id:"step-1-perform-prerequisites",children:"Step 1: Perform Prerequisites"}),"\n",(0,i.jsx)(n.h3,{id:"sign-up-or-log-in",children:"Sign Up or Log In"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.a,{href:"https://clarifai.com/login",children:"Log in to"})," your existing Clarifai account, or ",(0,i.jsx)(n.a,{href:"https://clarifai.com/signup",children:"sign up"})," for a new one. If you're creating a new account, a default application will be provided for you."]}),"\n",(0,i.jsx)(n.p,{children:"Next, retrieve the following credentials:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"App ID"})," \u2013 Go to your application\u2019s page and select the ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/create/applications/manage#app-overview",children:(0,i.jsx)(n.strong,{children:"Overview"})})," option in the collapsible left sidebar. Get the app ID from there."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"User ID"})," \u2013 In the collapsible left sidebar, select ",(0,i.jsx)(n.strong,{children:"Settings"})," and choose ",(0,i.jsx)(n.strong,{children:"Account"})," from the dropdown list. Then, locate your user ID."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Personal Access Token (PAT)"})," \u2013 From the same ",(0,i.jsx)(n.strong,{children:"Settings"})," option, choose ",(0,i.jsx)(n.strong,{children:"Secrets"})," to generate or copy your ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/control/authentication/pat",children:"PAT"}),". This token is used to authenticate your connection with the Clarifai platform."]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["You need to set the ",(0,i.jsx)(n.code,{children:"CLARIFAI_PAT"})," you've retrieved as an environment variable to enhance its security."]}),"\n",(0,i.jsxs)(l.A,{groupId:"code",children:[(0,i.jsx)(a.A,{value:"bash",label:"Unix-Like Systems",children:(0,i.jsx)(s.A,{className:"language-bash",children:" export CLARIFAI_PAT=YOUR_PERSONAL_ACCESS_TOKEN_HERE "})}),(0,i.jsx)(a.A,{value:"bash2",label:"Windows",children:(0,i.jsx)(s.A,{className:"language-bash",children:" set CLARIFAI_PAT=YOUR_PERSONAL_ACCESS_TOKEN_HERE "})})]}),"\n",(0,i.jsx)(n.h3,{id:"install-clarifai-package",children:"Install Clarifai Package"}),"\n",(0,i.jsxs)(n.p,{children:["Install the latest version of the ",(0,i.jsx)(n.code,{children:"clarifai"})," Python package. This will also install the Clarifai ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/additional-resources/api-overview/cli/",children:"Command Line Interface"})," (CLI), which we'll use for testing and uploading the model."]}),"\n",(0,i.jsx)(l.A,{groupId:"code",children:(0,i.jsx)(a.A,{value:"bash",label:"Bash",children:(0,i.jsx)(s.A,{className:"language-bash",children:" pip install --upgrade clarifai "})})}),"\n",(0,i.jsx)(n.h3,{id:"set-up-docker-or-a-virtual-environment",children:"Set up Docker or a Virtual Environment"}),"\n",(0,i.jsx)(n.p,{children:"To test, run, and upload your model, you need to set up either a Docker container or a Python virtual environment. This ensures proper dependency management and prevents conflicts in your project."}),"\n",(0,i.jsxs)(n.p,{children:["Both options allow you to work with different Python versions. For example, you can use Python 3.11 for uploading one model and Python 3.12 for another \u2014 configured via the ",(0,i.jsx)(n.a,{href:"#build-info",children:(0,i.jsx)(n.code,{children:"config.yaml"})})," file."]}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsx)(n.p,{children:"If Docker is installed on your system, it is highly recommended to use it for running the model. Docker provides better isolation and a fully portable environment, including for Python and system libraries."}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["You should ensure your local environment has sufficient memory and compute resources to handle model loading and execution, especially during ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/sdk/compute-orchestration/test-models-locally",children:"testing"}),"."]}),"\n",(0,i.jsx)(n.h3,{id:"create-project-directory",children:"Create Project Directory"}),"\n",(0,i.jsx)(n.admonition,{type:"tip",children:(0,i.jsxs)(n.p,{children:["You can automatically generate the required files by running the ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/resources/api-overview/cli#clarifai-model-init",children:(0,i.jsx)(n.code,{children:"clarifai model init"})})," command in the terminal from your current directory. After the files are created, you can modify them as needed."]})}),"\n",(0,i.jsx)(n.p,{children:"Create a project directory and organize your files as indicated below to fit the requirements of uploading models to the Clarifai platform."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",children:"your_model_directory/\n\u251c\u2500\u2500 1/\n\u2502   \u2514\u2500\u2500 model.py\n\u251c\u2500\u2500 requirements.txt\n\u2514\u2500\u2500 config.yaml\n"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"your_model_directory/"})," \u2013 The root directory containing all files related to your custom model.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"1/"})," \u2013 A subdirectory that holds the model file (",(0,i.jsxs)(n.em,{children:["Note that the folder is named as ",(0,i.jsx)(n.strong,{children:"1"})]}),").","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"model.py"})," \u2013 Contains the code that defines your model, including loading the model and running inference."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"requirements.txt"})," \u2013 Lists the Python dependencies required to run your model."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"config.yaml"})," \u2013 Contains model metadata and configuration details necessary for building the model, defining compute resources, and more."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"step-2-build-a-model",children:"Step 2: Build a Model"}),"\n",(0,i.jsx)(n.p,{children:"Let's talk about the general steps you'd follow to upload any type of model to the Clarifai platform."}),"\n",(0,i.jsxs)(n.h3,{id:"prepare-modelpy",children:["Prepare ",(0,i.jsx)(n.code,{children:"model.py"})]}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"model.py"})," file contains the core logic for your model, including how the model is loaded and how predictions are made. This file must define a custom class that inherits from ",(0,i.jsx)(n.code,{children:"ModelClass"})," and implements the required methods."]}),"\n",(0,i.jsxs)(n.p,{children:["This is the ",(0,i.jsx)(n.code,{children:"model.py"})," file for the custom model we want to upload:"]}),"\n",(0,i.jsx)(l.A,{groupId:"code",children:(0,i.jsx)(a.A,{value:"python",label:"Python",children:(0,i.jsx)(s.A,{className:"language-python",children:d})})}),"\n",(0,i.jsx)(n.p,{children:"Let\u2019s break down what each part of the file does."}),"\n",(0,i.jsxs)(n.h4,{id:"a-load_model-method",children:["a. ",(0,i.jsx)(n.code,{children:"load_model"})," Method"]}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"load_model"})," method is optional but recommended, as it prepares the model for inference by handling resource-heavy initializations. It is particularly useful for:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"One-time setup of heavy resources, such as loading trained models or initializing data transformations."}),"\n",(0,i.jsx)(n.li,{children:"Executing tasks during model container startup to reduce runtime latency."}),"\n",(0,i.jsx)(n.li,{children:"Loading essential components like tokenizers, pipelines, and other model-related assets."}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Here is an example:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def load_model(self):\n  self.tokenizer = AutoTokenizer.from_pretrained("model/")\n  self.pipeline = transformers.pipeline(...)\n'})}),"\n",(0,i.jsx)(n.h4,{id:"b-prediction-methods",children:"b. Prediction Methods"}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"model.py"})," file must include at least one method decorated with ",(0,i.jsx)(n.code,{children:"@ModelClass.method"})," to define the prediction endpoints."]}),"\n",(0,i.jsxs)(n.p,{children:["In the example model we want to upload, we defined a method that appends the phrase ",(0,i.jsx)(n.code,{children:"Hello World"})," to any input text and added support for different types of ",(0,i.jsx)(n.a,{href:"#step-6-predict-with-model",children:"streaming responses"}),"."]}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Note:"})," The structure of prediction methods on the client side directly mirrors the method signatures defined in your ",(0,i.jsx)(n.code,{children:"model.py"})," file. This one-to-one mapping provides flexibility in defining prediction methods with varying names and arguments."]}),"\n"]}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsx)(n.p,{children:"Here are some examples of method mapping:"}),"\n"]}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsxs)(n.th,{children:[(0,i.jsx)(n.code,{children:"model.py"})," Model Implementation"]}),(0,i.jsx)(n.th,{children:"Client-Side Usage Pattern"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"@ModelClass.method def predict(...)"})}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"model.predict(...)"})})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"@ModelClass.method def generate(...)"})}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"model.generate(...)"})})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"@ModelClass.method def stream(...)"})}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"model.stream(...)"})})]})]})]}),"\n"]}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:["You can learn more about the structure of prediction methods ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/compute/models/inference/#structure-of-prediction-methods",children:"here"}),"."]}),"\n"]}),"\n",(0,i.jsxs)(n.admonition,{type:"warning",children:[(0,i.jsx)(n.mdxAdmonitionTitle,{children:(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/compute/models/model-upload/data-types/",children:"Supported Input and Output Data Types"})}),(0,i.jsxs)(n.p,{children:["Each parameter in the class methods must be annotated with a type, and the return type must also be specified. Clarifai's model framework supports rich data typing for both inputs and outputs. Supported types include ",(0,i.jsx)(n.code,{children:"Text"}),", ",(0,i.jsx)(n.code,{children:"Image"}),", ",(0,i.jsx)(n.code,{children:"Audio"}),", ",(0,i.jsx)(n.code,{children:"Video"}),", and more."]})]}),"\n",(0,i.jsxs)(n.h3,{id:"prepare-configyaml",children:["Prepare ",(0,i.jsx)(n.code,{children:"config.yaml"})]}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"config.yaml"})," file is essential for specifying the model\u2019s metadata, compute resource requirements, and model checkpoints."]}),"\n",(0,i.jsxs)(n.p,{children:["This is the ",(0,i.jsx)(n.code,{children:"config.yaml"})," file for the custom model we want to upload:"]}),"\n",(0,i.jsx)(l.A,{groupId:"code",children:(0,i.jsx)(a.A,{value:"yaml",label:"YAML",children:(0,i.jsx)(s.A,{className:"language-yaml",children:c})})}),"\n",(0,i.jsx)(n.p,{children:"Let\u2019s break down what each part of the file does."}),"\n",(0,i.jsx)(n.h4,{id:"model-info",children:"Model Info"}),"\n",(0,i.jsxs)(n.p,{children:["This section defines your unique model ID (any arbitrary name you choose), along with the Clarifai user ID and app ID you retrieved ",(0,i.jsx)(n.a,{href:"#sign-up-or-log-in",children:"earlier"}),". These values will determine where the model is uploaded on the Clarifai platform."]}),"\n",(0,i.jsx)(n.h4,{id:"build-info",children:"Build Info"}),"\n",(0,i.jsxs)(n.p,{children:["This section specifies details about the environment used to build or run the model. You can include the ",(0,i.jsx)(n.code,{children:"python_version"}),", which is useful for ensuring compatibility between the model and its runtime environment, as different Python versions may have varying dependencies, library support, and performance characteristics."]}),"\n",(0,i.jsx)(n.admonition,{type:"note",children:(0,i.jsx)(n.p,{children:"We currently support Python 3.11 and Python 3.12 (default)."})}),"\n",(0,i.jsx)(n.h4,{id:"compute-resources",children:"Compute Resources"}),"\n",(0,i.jsx)(n.p,{children:"You must define the minimum compute resources required for running your model, including CPU, memory, and optional GPU specifications."}),"\n",(0,i.jsx)(n.p,{children:"These are some parameters you can define:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"cpu_limit"})})," \u2013 Number of CPUs allocated for the model (follows ",(0,i.jsx)(n.a,{href:"https://kubernetes.io/docs/reference/kubernetes-api/common-definitions/quantity/",children:"Kubernetes notation"}),', e.g., "1", "2").']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"cpu_requests"})})," (default: ",(0,i.jsx)(n.code,{children:"500m"}),' (500 millicores)) \u2013 Specifies the minimum amount of CPU resources to request. Follows Kubernetes notation (e.g., "100m", "1", "4.5"), where 1 equals one full CPU core.']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"cpu_memory"})}),' \u2013 Minimum memory required for the CPU (uses Kubernetes notation, e.g., "1Gi", "1500Mi", "3Gi").']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"cpu_memory_requests"})})," (default: ",(0,i.jsx)(n.code,{children:"500Mi"})," (500 mebibytes)) \u2013 Specifies the minimum amount of memory to request for the CPU. Also follows Kubernetes notation, such as 1Ki (1 kibibyte), 1500Mi (1500 mebibytes), 3Gi(3 gibibytes), and 4Ti (4 tebibytes)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"num_accelerators"})})," \u2013 Number of GPUs or TPUs to use for inference."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"accelerator_type"})})," \u2013 Specifies the type of hardware ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/compute/deployments/cloud-instances/",children:"accelerators"}),' (e.g., GPU or TPU) supported by the model (e.g., "NVIDIA-A10G"). ',(0,i.jsxs)(n.em,{children:["Note that instead of specifying an exact accelerator type, you can use a wildcard ",(0,i.jsx)(n.code,{children:"(*)"})," to automatically match all available accelerators that fit your use case. For example, using ",(0,i.jsx)(n.code,{children:'["NVIDIA-*"]'})," will enable the system to choose from all NVIDIA options compatible with your model."]})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"accelerator_memory"})})," \u2013 Minimum memory required for the GPU or TPU."]}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"hugging-face-model-checkpoints",children:"Hugging Face Model Checkpoints"}),"\n",(0,i.jsx)(n.p,{children:"If you're using a model from Hugging Face, you can automatically download its checkpoints by specifying the appropriate configuration in this section."}),"\n",(0,i.jsxs)(n.p,{children:["For private or restricted Hugging Face repositories, make sure to include an access token. Learn how to generate one ",(0,i.jsx)(n.a,{href:"https://huggingface.co/docs/hub/en/security-tokens",children:"here"}),"."]}),"\n",(0,i.jsxs)(n.p,{children:["See the ",(0,i.jsx)(n.a,{href:"#additional-examples",children:"additional examples"})," below for how to define Hugging Face checkpoints."]}),"\n",(0,i.jsx)(l.A,{groupId:"code",children:(0,i.jsx)(a.A,{value:"yaml",label:"YAML",children:(0,i.jsx)(s.A,{className:"language-yaml",children:h})})}),"\n",(0,i.jsxs)(n.admonition,{type:"note",children:[(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"when"})," parameter in the ",(0,i.jsx)(n.code,{children:"checkpoints"})," section determines when model checkpoints should be downloaded and stored. It must be set to one of the following options:"]}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"runtime"})," (",(0,i.jsx)(n.em,{children:"default"}),") \u2013 Downloads checkpoints when loading the model in the ",(0,i.jsx)(n.code,{children:"load_model"})," method."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"build"})," \u2013 Downloads checkpoints during the image build process."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"upload"})," \u2013 Downloads checkpoints before uploading the model."]}),"\n"]}),(0,i.jsxs)(n.p,{children:["For larger models, we highly recommend downloading checkpoints at ",(0,i.jsx)(n.code,{children:"runtime"}),". Doing so prevents unnecessary increases in Docker image size, which has some advantages:"]}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Smaller image sizes"}),"\n",(0,i.jsx)(n.li,{children:"Faster build times"}),"\n",(0,i.jsx)(n.li,{children:"Quicker uploads and inference on the Clarifai platform"}),"\n"]}),(0,i.jsxs)(n.p,{children:["Downloading checkpoints at ",(0,i.jsx)(n.code,{children:"build"})," or ",(0,i.jsx)(n.code,{children:"upload"})," time can significantly increase image size, resulting in longer upload times and increased cold start latency."]})]}),"\n",(0,i.jsx)(n.h4,{id:"model-concepts-or-labels",children:"Model Concepts or Labels"}),"\n",(0,i.jsxs)(n.p,{children:["This section is required if your model outputs concepts or labels and is not being directly loaded from Hugging Face. So, you must define a ",(0,i.jsx)(n.code,{children:"concepts"})," section in the ",(0,i.jsx)(n.code,{children:"config.yaml"})," file."]}),"\n",(0,i.jsx)(n.p,{children:"The following model types output concepts or labels:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"visual-classifier"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"visual-detector"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"visual-segmenter"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"text-classifier"})}),"\n"]}),"\n",(0,i.jsx)(l.A,{groupId:"code",children:(0,i.jsx)(a.A,{value:"yaml",label:"YAML",children:(0,i.jsx)(s.A,{className:"language-yaml",children:u})})}),"\n",(0,i.jsxs)(n.admonition,{type:"note",children:[(0,i.jsx)(n.mdxAdmonitionTitle,{}),(0,i.jsxs)(n.p,{children:["If you're using a model from Hugging Face and the ",(0,i.jsx)(n.code,{children:"checkpoints"})," section is defined, the Clarifai platform will automatically infer concepts. In this case, you don\u2019t need to manually specify them."]})]}),"\n",(0,i.jsxs)(n.h3,{id:"prepare-requirementstxt",children:["Prepare ",(0,i.jsx)(n.code,{children:"requirements.txt"})]}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"requirements.txt"})," file lists all the Python dependencies your model needs."]}),"\n",(0,i.jsxs)(n.p,{children:["This is the ",(0,i.jsx)(n.code,{children:"requirements.txt"})," file for the custom model we want to upload:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",children:"clarifai>=11.8.2\n"})}),"\n",(0,i.jsx)(n.p,{children:"If your model requires Torch, we provide optimized pre-built Torch images as the base for machine learning and inference tasks."}),"\n",(0,i.jsx)(n.p,{children:"These images include all necessary dependencies, ensuring efficient execution. The available pre-built Torch images are:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"2.4.1-py3.11-cuda124"})," \u2014 Based on PyTorch 2.4.1, Python 3.11, and CUDA 12.4."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"2.5.1-py3.11-cuda124"})," \u2014 Based on PyTorch 2.5.1, Python 3.11, and CUDA 12.4."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"2.4.1-py3.12-cuda124"})," \u2014 Based on PyTorch 2.4.1, Python 3.12, and CUDA 12.4."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"2.5.1-py3.12-cuda124"})," \u2014 Based on PyTorch 2.5.1, Python 3.12, and CUDA 12.4."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"To use a specific Torch version, define it in your requirements.txt file like this:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",children:"torch==2.5.1\n"})}),"\n",(0,i.jsx)(n.p,{children:"This ensures the correct pre-built image is pulled from Clarifai's container registry, ensuring the correct environment is used. This minimizes cold start times and speeds up model uploads and runtime execution \u2014 avoiding the overhead of building images from scratch or pulling and configuring them from external sources."}),"\n",(0,i.jsxs)(n.p,{children:["We recommend using either ",(0,i.jsx)(n.code,{children:"torch==2.5.1"})," or ",(0,i.jsx)(n.code,{children:"torch==2.4.1"}),". If your model requires a different Torch version, you can specify it in requirements.txt, but this may slightly increase the model upload time."]}),"\n",(0,i.jsx)(n.h2,{id:"step-3-test-the-model-locally",children:"Step 3: Test the Model Locally"}),"\n",(0,i.jsx)(n.p,{children:"Before uploading your model to the Clarifai platform, it's important to test it locally to catch any typos or misconfigurations in the code."}),"\n",(0,i.jsxs)(n.p,{children:["Learn how to  run and test your models locally ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/compute/local-runners/",children:"here"}),"."]}),"\n",(0,i.jsx)(n.h2,{id:"step-4-upload-the-model-to-clarifai",children:"Step 4: Upload the Model to Clarifai"}),"\n",(0,i.jsx)(n.p,{children:"Once your model is ready, you can upload it to the platform using Clarifai CLI."}),"\n",(0,i.jsx)(n.p,{children:"To upload your model, run the following command in your terminal:"}),"\n",(0,i.jsx)(l.A,{groupId:"code",children:(0,i.jsx)(a.A,{value:"bash",label:"Bash",children:(0,i.jsx)(s.A,{className:"language-bash",children:" clarifai model upload ./your/model/path/here "})})}),"\n",(0,i.jsx)(n.p,{children:"Alternatively, navigate to the directory containing your custom model and run the command without specifying the directory path:"}),"\n",(0,i.jsx)(l.A,{groupId:"code",children:(0,i.jsx)(a.A,{value:"bash",label:"Bash",children:(0,i.jsx)(s.A,{className:"language-bash",children:" clarifai model upload "})})}),"\n",(0,i.jsx)(n.p,{children:"This command builds the model\u2019s Docker image using the defined compute resources and uploads it to Clarifai, where it can be served in production.\nThe build logs will be displayed in your terminal, which helps you troubleshoot any upload issues."}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Note"}),": If you make any changes to your model and upload it again to the Clarifai platform, a new version of the model will be created automatically."]}),"\n"]}),"\n",(0,i.jsxs)(n.admonition,{title:"Skip Dockerfile",type:"note",children:[(0,i.jsxs)(n.p,{children:["By default, when you upload a model, the CLI automatically generates a ",(0,i.jsx)(n.code,{children:"Dockerfile"})," in the root of your model directory. This ensures your model can be built and deployed with the correct environment."]}),(0,i.jsxs)(n.p,{children:["In some cases, though, you may prefer to provide your own custom ",(0,i.jsx)(n.a,{href:"https://docs.docker.com/reference/dockerfile/",children:"Dockerfile"}),", such as when a specific base image is required for model inference."]}),(0,i.jsxs)(n.p,{children:["To do this, use the ",(0,i.jsx)(n.code,{children:"--skip_dockerfile"})," flag. This tells the CLI to skip automatic Dockerfile generation and instead rely on the one you\u2019ve created."]}),(0,i.jsx)(l.A,{groupId:"code",children:(0,i.jsx)(a.A,{value:"bash",label:"Bash",children:(0,i.jsx)(s.A,{className:"language-bash",children:"clarifai model upload --skip_dockerfile"})})}),(0,i.jsxs)(o,{children:[(0,i.jsx)("summary",{children:"Automatically Generated Dockerfile Example"}),(0,i.jsx)(s.A,{className:"language-dockerfile",children:b})]})]}),"\n",(0,i.jsx)(n.h2,{id:"step-5-deploy-the-model",children:"Step 5: Deploy the Model"}),"\n",(0,i.jsx)(n.p,{children:"After you've successfully uploaded your model to the Clarifai platform, the terminal will guide you through the deployment process, getting your model ready for inference."}),"\n",(0,i.jsx)(n.p,{children:"You can follow the prompts to:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/compute/deployments/clusters-nodepools",children:"Set up a cluster"})})," \u2013 This serves as the overarching computational environment."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Create a nodepool"})," \u2013 A nodepool is a group of compute nodes within your cluster that provides the resources needed to run your model."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/compute/deployments/deploy-model",children:"Deploy your model"})})," \u2013 Once your nodepool is set up, you can deploy your model, making it available to process prediction requests."]}),"\n"]}),"\n",(0,i.jsxs)(o,{children:[(0,i.jsx)("summary",{children:"Build Logs Example"}),(0,i.jsx)(s.A,{className:"language-text",children:p})]}),"\n",(0,i.jsx)(n.h2,{id:"step-6-predict-with-model",children:"Step 6: Predict With Model"}),"\n",(0,i.jsxs)(n.p,{children:["Once the model is successfully deployed, you can generate predictions either programmatically or through the ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/getting-started/quickstart-playground",children:"UI Playground"}),"."]}),"\n",(0,i.jsx)(n.h3,{id:"unary-unary-predict-call",children:"Unary-Unary Predict Call"}),"\n",(0,i.jsxs)(n.p,{children:["You can make a ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/compute/models/inference/api/#unary-unary-predict-call",children:"unary-unary"})," predict call using the model."]}),"\n",(0,i.jsx)(l.A,{groupId:"code",children:(0,i.jsx)(a.A,{value:"python",label:"Python",children:(0,i.jsx)(s.A,{className:"language-python",children:m})})}),"\n",(0,i.jsxs)(o,{children:[(0,i.jsx)("summary",{children:"Example Output"}),(0,i.jsx)(s.A,{className:"language-text",children:g})]}),"\n",(0,i.jsx)(n.h3,{id:"unary-stream-predict-call",children:"Unary-Stream Predict Call"}),"\n",(0,i.jsxs)(n.p,{children:["You can make a ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/compute/models/inference/api/#unary-stream-predict-call",children:"unary-stream"})," predict call using the model."]}),"\n",(0,i.jsx)(l.A,{groupId:"code",children:(0,i.jsx)(a.A,{value:"python",label:"Python",children:(0,i.jsx)(s.A,{className:"language-python",children:f})})}),"\n",(0,i.jsxs)(o,{children:[(0,i.jsx)("summary",{children:"Example Output"}),(0,i.jsx)(s.A,{className:"language-text",children:y})]}),"\n",(0,i.jsx)(n.h3,{id:"stream-stream-predict-call",children:"Stream-Stream Predict Call"}),"\n",(0,i.jsxs)(n.p,{children:["You can make a ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/compute/models/inference/api/#stream-stream-predict-call",children:"stream-stream"})," predict call using the model."]}),"\n",(0,i.jsx)(l.A,{groupId:"code",children:(0,i.jsx)(a.A,{value:"python",label:"Python",children:(0,i.jsx)(s.A,{className:"language-python",children:x})})}),"\n",(0,i.jsxs)(o,{children:[(0,i.jsx)("summary",{children:"Example Output"}),(0,i.jsx)(s.A,{className:"language-text",children:j})]}),"\n",(0,i.jsx)(n.h2,{id:"additional-examples",children:"Additional Examples"}),"\n",(0,i.jsx)(n.admonition,{type:"tip",children:(0,i.jsxs)(n.p,{children:["You can find various up-to-date model upload examples ",(0,i.jsx)(n.a,{href:"https://github.com/Clarifai/runners-examples",children:"here"}),", which demonstrate different use cases and optimizations.\nHere is an example of how to ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/resources/api-overview/cli#initialize-with-github-template",children:"download"})," a model: ",(0,i.jsx)(n.code,{children:"clarifai model init --github-url https://github.com/Clarifai/runners-examples/tree/main/local-runners/ollama-model-upload"}),"."]})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://github.com/Clarifai/runners-examples/tree/main/llm/vllm-gemma-3-1b-it",children:"An OpenAI-compatible model"})," built with Clarifai\u2019s ",(0,i.jsx)(n.code,{children:"OpenAIModelClass"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://github.com/Clarifai/runners-examples/tree/main/llm/hf-llama-3_2-1b-instruct",children:"A Llama-3.2-1B-Instruct model"})," built with Clarifai\u2019s ",(0,i.jsx)(n.code,{children:"ModelClass"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://github.com/Clarifai/runners-examples/tree/main/image-classifier/nsfw-image-classifier",children:"An NSFW Image Classifier model"})," built with Clarifai\u2019s ",(0,i.jsx)(n.code,{children:"VisualClassifierClass"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://github.com/Clarifai/runners-examples/tree/main/image-detector/detr-resnet-image-detection",children:"A DETR ResNet Image Detector model"})," built with Clarifai\u2019s ",(0,i.jsx)(n.code,{children:"VisualDetectorClass"})]}),"\n",(0,i.jsxs)(n.li,{children:["Sandboxed execution with ",(0,i.jsx)(n.a,{href:"https://www.daytona.io/",children:"Daytona"})," and Clarifai","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://www.youtube.com/watch?v=S9pMcLzqbgo",children:"A YouTube video"})," on how to upload and run a custom model on Clarifai, start a local runner, and execute the model within a Daytona Sandbox. The model\u2019s repository is ",(0,i.jsx)(n.a,{href:"https://github.com/Clarifai/runners-examples/tree/daytona-code-generator-model/code-generation/daytona-code-generator",children:"here"}),"."]}),"\n"]}),"\n"]}),"\n"]})]})}function k(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(C,{...e})}):C(e)}}}]);
"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[1832],{11470:(e,n,t)=>{t.d(n,{A:()=>_});var a=t(96540),i=t(18215),o=t(17559),l=t(23104),s=t(56347),r=t(205),c=t(57485),u=t(31682),d=t(70679);function p(e){return a.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,a.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function h(e){const{values:n,children:t}=e;return(0,a.useMemo)(()=>{const e=n??function(e){return p(e).map(({props:{value:e,label:n,attributes:t,default:a}})=>({value:e,label:n,attributes:t,default:a}))}(t);return function(e){const n=(0,u.XI)(e,(e,n)=>e.value===n.value);if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[n,t])}function m({value:e,tabValues:n}){return n.some(n=>n.value===e)}function f({queryString:e=!1,groupId:n}){const t=(0,s.W6)(),i=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,c.aZ)(i),(0,a.useCallback)(e=>{if(!i)return;const n=new URLSearchParams(t.location.search);n.set(i,e),t.replace({...t.location,search:n.toString()})},[i,t])]}function g(e){const{defaultValue:n,queryString:t=!1,groupId:i}=e,o=h(e),[l,s]=(0,a.useState)(()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!m({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const t=n.find(e=>e.default)??n[0];if(!t)throw new Error("Unexpected error: 0 tabValues");return t.value}({defaultValue:n,tabValues:o})),[c,u]=f({queryString:t,groupId:i}),[p,g]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[t,i]=(0,d.Dv)(n);return[t,(0,a.useCallback)(e=>{n&&i.set(e)},[n,i])]}({groupId:i}),b=(()=>{const e=c??p;return m({value:e,tabValues:o})?e:null})();(0,r.A)(()=>{b&&s(b)},[b]);return{selectedValue:l,selectValue:(0,a.useCallback)(e=>{if(!m({value:e,tabValues:o}))throw new Error(`Can't select invalid tab value=${e}`);s(e),u(e),g(e)},[u,g,o]),tabValues:o}}var b=t(92303);const v={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var x=t(74848);function y({className:e,block:n,selectedValue:t,selectValue:a,tabValues:o}){const s=[],{blockElementScrollPositionUntilNextRender:r}=(0,l.a_)(),c=e=>{const n=e.currentTarget,i=s.indexOf(n),l=o[i].value;l!==t&&(r(n),a(l))},u=e=>{let n=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const t=s.indexOf(e.currentTarget)+1;n=s[t]??s[0];break}case"ArrowLeft":{const t=s.indexOf(e.currentTarget)-1;n=s[t]??s[s.length-1];break}}n?.focus()};return(0,x.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,i.A)("tabs",{"tabs--block":n},e),children:o.map(({value:e,label:n,attributes:a})=>(0,x.jsx)("li",{role:"tab",tabIndex:t===e?0:-1,"aria-selected":t===e,ref:e=>{s.push(e)},onKeyDown:u,onClick:c,...a,className:(0,i.A)("tabs__item",v.tabItem,a?.className,{"tabs__item--active":t===e}),children:n??e},e))})}function A({lazy:e,children:n,selectedValue:t}){const o=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=o.find(e=>e.props.value===t);return e?(0,a.cloneElement)(e,{className:(0,i.A)("margin-top--md",e.props.className)}):null}return(0,x.jsx)("div",{className:"margin-top--md",children:o.map((e,n)=>(0,a.cloneElement)(e,{key:n,hidden:e.props.value!==t}))})}function j(e){const n=g(e);return(0,x.jsxs)("div",{className:(0,i.A)(o.G.tabs.container,"tabs-container",v.tabList),children:[(0,x.jsx)(y,{...n,...e}),(0,x.jsx)(A,{...n,...e})]})}function _(e){const n=(0,b.A)();return(0,x.jsx)(j,{...e,children:p(e.children)},String(n))}},19365:(e,n,t)=>{t.d(n,{A:()=>l});t(96540);var a=t(18215);const i={tabItem:"tabItem_Ymn6"};var o=t(74848);function l({children:e,hidden:n,className:t}){return(0,o.jsx)("div",{role:"tabpanel",className:(0,a.A)(i.tabItem,t),hidden:n,children:e})}},55778:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>f,contentTitle:()=>m,default:()=>v,frontMatter:()=>h,metadata:()=>a,toc:()=>g});const a=JSON.parse('{"id":"compute/inference/litellm","title":"LiteLLM","description":"Run inferences on Clarifai models using LiteLLM","source":"@site/docs/compute/inference/litellm.md","sourceDirName":"compute/inference","slug":"/compute/inference/litellm","permalink":"/compute/inference/litellm","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"description":"Run inferences on Clarifai models using LiteLLM","sidebar_position":3,"toc_max_heading_level":4},"sidebar":"tutorialSidebar","previous":{"title":"OpenAI","permalink":"/compute/inference/open-ai"},"next":{"title":"Vercel AI SDK","permalink":"/compute/inference/vercel"}}');var i=t(74848),o=t(28453),l=t(11470),s=t(19365),r=t(88149);const c='import os\nimport litellm\n\nresponse = litellm.completion(\n    model="openai/https://clarifai.com/anthropic/completion/models/claude-sonnet-4",\n    api_key=os.environ["CLARIFAI_PAT"],  # Ensure CLARIFAI_PAT is set as an environment variable\n    api_base="https://api.clarifai.com/v2/ext/openai/v1",\n    # Message formatting is consistent with OpenAI\'s schema ({"role": ..., "content": ...}).\n    messages=[\n        {"role": "system", "content": "You are a friendly assistant."},\n        {"role": "user", "content": "Hey, how\'s it going?"}\n    ],\n    # You can add OpenAI-compatible parameters here\n    temperature=0.7,         # Optional: controls randomness\n    max_tokens=100           # Optional: limits response length\n)\n\n# Print the assistant\'s reply\nprint(response[\'choices\'][0][\'message\'][\'content\'])',u='import os\nimport litellm\n\n# Define the tool (function) the model can call\ntools = [\n    {\n        "type": "function",\n        "function": {\n            "name": "get_weather",\n            "description": "Retrieve the current temperature for a given location.",\n            "parameters": {\n                "type": "object",\n                "properties": {\n                    "location": {\n                        "type": "string",\n                        "description": "City and country, e.g., \'Tokyo, Japan\'"\n                    }\n                },\n                "required": ["location"],\n                "additionalProperties": False\n            }\n        }\n    }\n]\n\n# Send the request to a Clarifai-hosted model using LiteLLM\nresponse = litellm.completion(\n    model="openai/https://clarifai.com/openai/chat-completion/models/o4-mini",\n    api_key=os.environ["CLARIFAI_PAT"],  # Ensure CLARIFAI_PAT is set as an environment variable\n    api_base="https://api.clarifai.com/v2/ext/openai/v1",\n    messages=[\n        {"role": "user", "content": "What is the weather in Paris today?"}\n    ],\n    tools=tools\n)\n\n# Output the tool call suggested by the model (if any)\nprint(response.choices[0].message.tool_calls)\n',d='import os\nimport litellm\n\n# Enable streaming\nresponse_stream = litellm.completion(\n    model="openai/https://clarifai.com/anthropic/completion/models/claude-sonnet-4",\n    api_key=os.environ["CLARIFAI_PAT"],  # Ensure CLARIFAI_PAT is set as an environment variable\n    api_base="https://api.clarifai.com/v2/ext/openai/v1",\n    messages=[\n        {"role": "system", "content": "You are a friendly assistant."},\n        {"role": "user", "content": "Hey, how\'s it going? Tell me a short story about a space-faring cat."}\n    ],\n    stream=True  # Enable streaming\n)\n\n# Print the streamed output in real time\nfor chunk in response_stream:\n    content = chunk.get("choices", [{}])[0].get("delta", {}).get("content")\n    if content:\n        print(content, end="", flush=True)\n',p='import os\nimport json\nimport litellm\n\n# Step 1: Define the tool schema\ntools = [\n    {\n        "type": "function",\n        "function": {\n            "name": "get_weather",\n            "description": "Retrieve the current temperature for a given location.",\n            "parameters": {\n                "type": "object",\n                "properties": {\n                    "location": {\n                        "type": "string",\n                        "description": "City and country, e.g., \'Tokyo, Japan\'"\n                    }\n                },\n                "required": ["location"],\n                "additionalProperties": False\n            }\n        }\n    }\n]\n\n# Step 2: Define a function that simulates tool execution\ndef get_weather(location: str) -> str:\n    # In a real app, you\'d query a weather API here\n    return f"The current temperature in {location} is 22\xb0C."\n\n# Step 3: Make the initial request to trigger the tool\nresponse = litellm.completion(\n    model="openai/https://clarifai.com/openai/chat-completion/models/o4-mini",\n    api_key=os.environ["CLARIFAI_PAT"],\n    api_base="https://api.clarifai.com/v2/ext/openai/v1",\n    messages=[\n        {"role": "user", "content": "What is the weather in Paris today?"}\n    ],\n    tools=tools\n)\n\ntool_calls = response.choices[0].message.tool_calls\n\n# Step 4: Parse the tool call and run the function\nif tool_calls:\n    for tool_call in tool_calls:\n        tool_name = tool_call.function.name\n        arguments = json.loads(tool_call.function.arguments)\n        \n        if tool_name == "get_weather":\n            result = get_weather(arguments["location"])\n            \n            # Step 5: Send the function result back to the model\n            follow_up = litellm.completion(\n                model="openai/https://clarifai.com/openai/chat-completion/models/o4-mini",\n                api_key=os.environ["CLARIFAI_PAT"], # Ensure CLARIFAI_PAT is set as an environment variable\n                api_base="https://api.clarifai.com/v2/ext/openai/v1",\n                messages=[\n                    {"role": "user", "content": "What is the weather in Paris today?"},\n                    {"role": "assistant", "tool_calls": [tool_call]},\n                    {"role": "tool", "tool_call_id": tool_call.id, "content": result}\n                ]\n            )\n            \n            # Print the final assistant message\n            print(follow_up.choices[0].message.content)\nelse:\n    print("No tool was called.")\n',h={description:"Run inferences on Clarifai models using LiteLLM",sidebar_position:3,toc_max_heading_level:4},m="LiteLLM",f={},g=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Install LiteLLM",id:"install-litellm",level:3},{value:"Get a PAT Key",id:"get-a-pat-key",level:3},{value:"Get a Clarifai Model",id:"get-a-clarifai-model",level:3},{value:"Chat Completions",id:"chat-completions",level:2},{value:"Streaming",id:"streaming",level:2},{value:"Tool Calling",id:"tool-calling",level:2}];function b(e){const n={a:"a",admonition:"admonition",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",p:"p",strong:"strong",...(0,o.R)(),...e.components},{Details:t}=n;return t||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"litellm",children:"LiteLLM"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Run inferences on Clarifai models using LiteLLM"})}),"\n",(0,i.jsx)("hr",{}),"\n",(0,i.jsx)(n.p,{children:"LiteLLM provides a universal interface that simplifies working with LLMs across multiple providers. It offers a single, consistent API for making inferences, allowing you to interact with a wide range of models using the same method, regardless of the underlying provider."}),"\n",(0,i.jsxs)(n.p,{children:["LiteLLM natively supports OpenAI-compatible APIs, making it easy to run inferences on ",(0,i.jsx)(n.a,{href:"https://docs.litellm.ai/docs/providers/clarifai",children:"Clarifai-hosted"})," models with minimal setup."]}),"\n",(0,i.jsx)(n.admonition,{type:"tip",children:(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.a,{href:"https://github.com/Clarifai/examples/tree/main/models/model_predict",children:"Click here"})," for additional examples on how to perform model predictions using various SDKs \u2014 such as the Clarifai SDK, OpenAI client, and LiteLLM. The examples demonstrate various model types and include both streaming and non-streaming modes, as well as tool calling capabilities."]})}),"\n","\n","\n",(0,i.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsx)(n.h3,{id:"install-litellm",children:"Install LiteLLM"}),"\n",(0,i.jsxs)(n.p,{children:["Install the ",(0,i.jsx)(n.code,{children:"litellm"})," package."]}),"\n",(0,i.jsx)(l.A,{groupId:"code",children:(0,i.jsx)(s.A,{value:"bash",label:"Python",children:(0,i.jsx)(r.A,{className:"language-bash",children:" pip install litellm "})})}),"\n",(0,i.jsx)(n.h3,{id:"get-a-pat-key",children:"Get a PAT Key"}),"\n",(0,i.jsxs)(n.p,{children:["You need a ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/control/authentication/pat",children:"PAT"})," key to authenticate your connection to the Clarifai platform. You can get one by navigating to ",(0,i.jsx)(n.strong,{children:"Settings"})," in the collapsible left sidebar, selecting ",(0,i.jsx)(n.strong,{children:"Secrets"}),", and creating or copying an existing token from there."]}),"\n",(0,i.jsxs)(n.p,{children:["You can then set the PAT as an environment variable using ",(0,i.jsx)(n.code,{children:"CLARIFAI_PAT"}),":"]}),"\n",(0,i.jsxs)(l.A,{groupId:"code",children:[(0,i.jsx)(s.A,{value:"bash",label:"Unix-Like Systems",children:(0,i.jsx)(r.A,{className:"language-bash",children:" export CLARIFAI_PAT=YOUR_PERSONAL_ACCESS_TOKEN_HERE "})}),(0,i.jsx)(s.A,{value:"bash2",label:"Windows",children:(0,i.jsx)(r.A,{className:"language-bash",children:" set CLARIFAI_PAT=YOUR_PERSONAL_ACCESS_TOKEN_HERE "})})]}),"\n",(0,i.jsx)(n.h3,{id:"get-a-clarifai-model",children:"Get a Clarifai Model"}),"\n",(0,i.jsxs)(n.p,{children:["Go to the Clarifai ",(0,i.jsx)(n.a,{href:"https://clarifai.com/explore",children:"Community"})," platform and select the model you want to use for making predictions."]}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Note:"})," When specifying a Clarifai model in LiteLLM, use the model path prefixed with ",(0,i.jsx)(n.code,{children:"openai/"}),", followed by the full Clarifai model URL.\nFor example: ",(0,i.jsx)(n.code,{children:"openai/https://clarifai.com/openai/chat-completion/models/o4-mini"}),"."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"chat-completions",children:"Chat Completions"}),"\n",(0,i.jsxs)(n.p,{children:["In LiteLLM, the ",(0,i.jsx)(n.a,{href:"https://docs.litellm.ai/docs/completion",children:(0,i.jsx)(n.code,{children:"completion()"})})," function is the primary method for interacting with language models that follow the OpenAI Chat API format. It supports both traditional completions and chat-based interactions by accepting a list of messages \u2014 similar to OpenAI\u2019s ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/compute/providers/open-ai#chat-completions",children:(0,i.jsx)(n.code,{children:"chat.completions.create()"})}),"."]}),"\n",(0,i.jsx)(l.A,{children:(0,i.jsx)(s.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(r.A,{className:"language-python",children:c})})}),"\n",(0,i.jsxs)(t,{children:[(0,i.jsx)("summary",{children:"Example Output"}),(0,i.jsx)(r.A,{className:"language-text",children:"Hey there! I'm doing well, thanks for asking! How are you doing today? Is there anything I can help you with or would you like to chat about something?"})]}),"\n",(0,i.jsx)(n.h2,{id:"streaming",children:"Streaming"}),"\n",(0,i.jsxs)(n.p,{children:["When ",(0,i.jsx)(n.a,{href:"https://docs.litellm.ai/docs/completion/stream",children:"streaming"})," is enabled by setting ",(0,i.jsx)(n.code,{children:"stream=True"}),", the ",(0,i.jsx)(n.code,{children:"completion"})," method returns an iterator that yields partial responses in real time as the model generates them, instead of a single complete dictionary."]}),"\n",(0,i.jsx)(l.A,{children:(0,i.jsx)(s.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(r.A,{className:"language-python",children:d})})}),"\n",(0,i.jsx)(n.h2,{id:"tool-calling",children:"Tool Calling"}),"\n",(0,i.jsxs)(n.p,{children:["Clarifai models accessed via LiteLLM fully support ",(0,i.jsx)(n.a,{href:"https://docs.clarifai.com/compute/models/inference/api#tool-calling",children:"tool calling"}),", enabling advanced interactions such as function execution during a conversation."]}),"\n",(0,i.jsx)(l.A,{children:(0,i.jsx)(s.A,{value:"python",label:"Python SDK",children:(0,i.jsx)(r.A,{className:"language-python",children:u})})}),"\n",(0,i.jsxs)(t,{children:[(0,i.jsx)("summary",{children:" Tool Calling Implementation Example"}),(0,i.jsx)(r.A,{className:"language-python",children:p})]})]})}function v(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(b,{...e})}):b(e)}}}]);
"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[7457],{58215:function(e,n,t){var a=t(67294);n.Z=function(e){var n=e.children,t=e.hidden,o=e.className;return a.createElement("div",{role:"tabpanel",hidden:t,className:o},n)}},26396:function(e,n,t){t.d(n,{Z:function(){return c}});var a=t(87462),o=t(67294),s=t(72389),r=t(79443);var i=function(){var e=(0,o.useContext)(r.Z);if(null==e)throw new Error('"useUserPreferencesContext" is used outside of "Layout" component.');return e},l=t(63616),u=t(86010),d="tabItem_vU9c";function p(e){var n,t,s,r=e.lazy,p=e.block,c=e.defaultValue,_=e.values,m=e.groupId,f=e.className,h=o.Children.map(e.children,(function(e){if((0,o.isValidElement)(e)&&void 0!==e.props.value)return e;throw new Error("Docusaurus error: Bad <Tabs> child <"+("string"==typeof e.type?e.type:e.type.name)+'>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.')})),g=null!=_?_:h.map((function(e){var n=e.props;return{value:n.value,label:n.label,attributes:n.attributes}})),b=(0,l.lx)(g,(function(e,n){return e.value===n.value}));if(b.length>0)throw new Error('Docusaurus error: Duplicate values "'+b.map((function(e){return e.value})).join(", ")+'" found in <Tabs>. Every value needs to be unique.');var E=null===c?c:null!=(n=null!=c?c:null==(t=h.find((function(e){return e.props.default})))?void 0:t.props.value)?n:null==(s=h[0])?void 0:s.props.value;if(null!==E&&!g.some((function(e){return e.value===E})))throw new Error('Docusaurus error: The <Tabs> has a defaultValue "'+E+'" but none of its children has the corresponding value. Available values are: '+g.map((function(e){return e.value})).join(", ")+". If you intend to show no default tab, use defaultValue={null} instead.");var I=i(),w=I.tabGroupChoices,D=I.setTabGroupChoices,T=(0,o.useState)(E),v=T[0],O=T[1],k=[],C=(0,l.o5)().blockElementScrollPositionUntilNextRender;if(null!=m){var N=w[m];null!=N&&N!==v&&g.some((function(e){return e.value===N}))&&O(N)}var y=function(e){var n=e.currentTarget,t=k.indexOf(n),a=g[t].value;a!==v&&(C(n),O(a),null!=m&&D(m,a))},P=function(e){var n,t=null;switch(e.key){case"ArrowRight":var a=k.indexOf(e.currentTarget)+1;t=k[a]||k[0];break;case"ArrowLeft":var o=k.indexOf(e.currentTarget)-1;t=k[o]||k[k.length-1]}null==(n=t)||n.focus()};return o.createElement("div",{className:"tabs-container"},o.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,u.Z)("tabs",{"tabs--block":p},f)},g.map((function(e){var n=e.value,t=e.label,s=e.attributes;return o.createElement("li",(0,a.Z)({role:"tab",tabIndex:v===n?0:-1,"aria-selected":v===n,key:n,ref:function(e){return k.push(e)},onKeyDown:P,onFocus:y,onClick:y},s,{className:(0,u.Z)("tabs__item",d,null==s?void 0:s.className,{"tabs__item--active":v===n})}),null!=t?t:n)}))),r?(0,o.cloneElement)(h.filter((function(e){return e.props.value===v}))[0],{className:"margin-vert--md"}):o.createElement("div",{className:"margin-vert--md"},h.map((function(e,n){return(0,o.cloneElement)(e,{key:n,hidden:e.props.value!==v})}))))}function c(e){var n=(0,s.Z)();return o.createElement(p,(0,a.Z)({key:String(n)},e))}},82583:function(e,n,t){t.r(n),t.d(n,{contentTitle:function(){return p},default:function(){return f},frontMatter:function(){return d},metadata:function(){return c},toc:function(){return _}});var a=t(87462),o=t(63366),s=(t(67294),t(3905)),r=t(26396),i=t(58215),l=t(19055),u=["components"],d={description:"Train the complete graph for your model.",sidebar_position:6},p="Deep Training",c={unversionedId:"api-guide/model/deep-training",id:"api-guide/model/deep-training",title:"Deep Training",description:"Train the complete graph for your model.",source:"@site/docs/api-guide/model/deep-training.md",sourceDirName:"api-guide/model",slug:"/api-guide/model/deep-training",permalink:"/api-guide/model/deep-training",tags:[],version:"current",sidebarPosition:6,frontMatter:{description:"Train the complete graph for your model.",sidebar_position:6},sidebar:"tutorialSidebar",previous:{title:"Models: Create, Update, Get, Delete",permalink:"/api-guide/model/create-get-update-and-delete"},next:{title:"Evaluating Models",permalink:"/api-guide/evaluate/"}},_=[{value:"Template Types",id:"template-types",children:[{value:"Visual Classifier",id:"visual-classifier",children:[],level:3},{value:"Visual Detector",id:"visual-detector",children:[],level:3},{value:"Visual Embedder",id:"visual-embedder",children:[],level:3}],level:2},{value:"Hyperparameters",id:"hyperparameters",children:[],level:2},{value:"Create",id:"create",children:[{value:"Create a Visual Classifier",id:"create-a-visual-classifier",children:[],level:3},{value:"Create a Visual Detector",id:"create-a-visual-detector",children:[],level:3},{value:"Create a Visual Embedder",id:"create-a-visual-embedder",children:[],level:3},{value:"Create a Workflow",id:"create-a-workflow",children:[],level:3}],level:2},{value:"Update",id:"update",children:[{value:"Update Your Default Workflow",id:"update-your-default-workflow",children:[],level:3}],level:2}],m={toc:_};function f(e){var n=e.components,t=(0,o.Z)(e,u);return(0,s.kt)("wrapper",(0,a.Z)({},m,t,{components:n,mdxType:"MDXLayout"}),(0,s.kt)("h1",{id:"deep-training"},"Deep Training"),(0,s.kt)("p",null,(0,s.kt)("strong",{parentName:"p"},"Train the complete graph for your model")),(0,s.kt)("hr",null),(0,s.kt)("p",null,'Clarifai offers a variety of prebuilt models that are designed to help you build AI solutions quickly and efficiently. Clarifai Models are the recommended starting points for many users because they offer incredibly fast training times when you customize them using the "Context-Based Classifier" type in Portal\'s Model Mode.'),(0,s.kt)("p",null,'But there are many cases where accuracy and the ability to carefully target solutions takes priority over speed and ease of use. Additionally, you may need a model to learn new features, not recognized by existing Clarifai Models. For these cases, it is possible to "deep train" your custom models and integrate them directly within your workflows.'),(0,s.kt)("p",null,"In general, deep trained models need more data than ones trained on top of Clarifai Models. For most applications you\u2019ll need at least 1000 training inputs, but it could be much more than this depending on your specific use case."),(0,s.kt)("p",null,"You might consider deep training if you have:"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"A custom tailored dataset")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Accurate labels")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Expertise and time to fine tune models"))),(0,s.kt)("p",null,"Deep training is in early access preview. To request access, ",(0,s.kt)("a",{parentName:"p",href:"https://www.clarifai.com/contact"},"contact us"),"."),(0,s.kt)("h2",{id:"template-types"},"Template Types"),(0,s.kt)("p",null,"You can take advantage of a variety of templates when building your deep trained models. Templates give you the control to choose the specific architecture used by your neural network, and also define a set of hyperparameters that you can use to fine-tune the way that your model learns."),(0,s.kt)("h3",{id:"visual-classifier"},"Visual Classifier"),(0,s.kt)("p",null,"Classification templates let you classify what is in your images or videos."),(0,s.kt)("h3",{id:"visual-detector"},"Visual Detector"),(0,s.kt)("p",null,"Detection templates make it easy to build models that can identify objects within a region of your images or videos. Detection models return concepts and bounding boxes."),(0,s.kt)("h3",{id:"visual-embedder"},"Visual Embedder"),(0,s.kt)("p",null,"Embedding models can be useful in their own right ","(","for applications like clustering and visual search",")",', or as an input to a machine learning model for a supervised task. In effect, embedding templates enable you to create your own "base models" that you can then use in your workflows.'),(0,s.kt)("h2",{id:"hyperparameters"},"Hyperparameters"),(0,s.kt)("p",null,"Deep training gives you the power to tune the hyperparameters that affect \u201chow\u201d your model learns. Model Mode dynamically changes the available hyperparameters based on the template selected."),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"average","_","horizontal","_","flips"),"\u2060\u2014Provides basic data augmentation for your dataset. If set to true, there is a 0.5 probability that current image and associated ground truth will flip horizontally."),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"base","_","gradient","_","multiplier"),"\u2060\u2014This sets the learning rate of the pre-initialized base ","(",'also sometimes called "backbone"',")"," model that generates embeddings. Learning rate controls how the weights of our network are adjusted with respect to the loss gradient. The lower the value, the slower the trip along the downward slope. A low learning rate can help ensure that local minima are not missed, but can take a long time to converge, especially if the model gets stuck on a plateau region."),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"batch","_","size"),"\u2014The number of images used to make updates to the model. Increased batch size allows for a better approximation of gradient over those samples. Batches allow for stochastic gradient descent, by choosing a random set of X images for each training update. You may want to increase batch size if the model is large and taking a long time to train. You may also want to increase the batch size if the total number of model concepts is larger than the batch size ","(","you may want to increase it to around 2x the category count",")","."),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"detection","_","score","_","threshold"),"\u2014Only bounding boxes with a detection score above this threshold will be returned."),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"image","_","size"),"\u2014The size of images used for training. Images are scaled for efficient processing, and a lower number will take up less memory and run faster. A higher number will have more pixel information to train on and will increase accuracy."),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"init","_","epochs"),"\u2014The initial number of epochs before the first step/change in the ",(0,s.kt)("strong",{parentName:"li"},"lrate"),"."),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"logreg"),"\u2014Set to True to use ",(0,s.kt)("strong",{parentName:"li"},"logistic regression"),", set to False to use ",(0,s.kt)("strong",{parentName:"li"},"softmax")," ","(","for binary classification",")","."),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"lrate"),"\u2014The learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function."),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"num","_","epochs"),"\u2014An epoch is defined as one-pass over the entire dataset. If you increase it, it will take longer to train but it could make the model more robust."),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"num","_","items","_","per","_","epoch"),'\u2014The number of training examples per "epoch". An epoch would be defined as one-pass over this amount of examples.'),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"per","_","128","_","lrate"),"\u2014Total change in ",(0,s.kt)("strong",{parentName:"li"},"lrate")," after 128 images processed. This is calculated as lrate = per","_","128","_","lrate ","*"," ","(","batch","_","size / 128",")","."),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"per","_","item","_","lrate"),"\u2014The rate that model weights are changed per item."),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"step","_","epochs"),"\u2014The number of epochs between applications of the step/change in ",(0,s.kt)("strong",{parentName:"li"},"lrate")," scheduler."),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"test","_","freq"),"\u2014The number of epochs should you run before evaluation of the test set. Increased frequency can allow for more granular testing but will extend processing time."),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"use","_","perclass","_","regression"),"\u2014Enables box coordinate local regression on a per-class basis. When set to True there will be ",(0,s.kt)("inlineCode",{parentName:"li"},"num_classes")," sets of regressors for each anchor location. When set to False, there will be one coordinate regressor for each anchor location.")),(0,s.kt)("div",{className:"admonition admonition-info alert alert--info"},(0,s.kt)("div",{parentName:"div",className:"admonition-heading"},(0,s.kt)("h5",{parentName:"div"},(0,s.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,s.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,s.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"}))),"info")),(0,s.kt)("div",{parentName:"div",className:"admonition-content"},(0,s.kt)("p",{parentName:"div"},"The initialization code used in the following examples is outlined in detail on the ",(0,s.kt)("a",{parentName:"p",href:"../api-overview/api-clients#client-installation-instructions"},"client installation page.")))),(0,s.kt)("h2",{id:"create"},"Create"),(0,s.kt)("h3",{id:"create-a-visual-classifier"},"Create a Visual Classifier"),(0,s.kt)("p",null,"Use a visual classifier model if you would like to classify images and videos frames into set of concepts."),(0,s.kt)(r.Z,{mdxType:"Tabs"},(0,s.kt)(i.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,s.kt)(l.Z,{className:"language-python",mdxType:"CodeBlock"},"##########################################################################\n# In this section, we set the user authentication, app ID, model ID, and\n# concept IDs. Change these strings to run your own example.\n##########################################################################\n\nUSER_ID = 'YOUR_USER_ID_HERE'\n# Your PAT (Personal Access Token) can be found in the portal under Authentification\nPAT = 'YOUR_PAT_HERE'\nAPP_ID = 'YOUR_APP_ID_HERE'\n# Change these to create your own visual classifier\nMODEL_ID = 'lawrence-1591638385' \nCONCEPT_ID_1 = 'ferrari23' \nCONCEPT_ID_2 = 'outdoors23' \n\n##########################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n##########################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\nfrom google.protobuf.struct_pb2 import Struct\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (('authorization', 'Key ' + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\ntrain_params = Struct()\ntrain_params.update(\n{\n  \"template\": \"classification_cifar10_v1\",\n  \"num_epochs\": 2\n}\n)\n\npost_models_response = stub.PostModels(\nservice_pb2.PostModelsRequest(\n  user_app_id=userDataObject,\n  models=[\n    resources_pb2.Model(\n      id=MODEL_ID,\n      model_type_id=\"visual-classifier\",\n      train_info=resources_pb2.TrainInfo(params=train_params),\n      output_info=resources_pb2.OutputInfo(\n        data=resources_pb2.Data(\n          concepts=[\n            resources_pb2.Concept(id=CONCEPT_ID_1),\n            resources_pb2.Concept(id=CONCEPT_ID_2)\n          ]\n        ),\n        output_config=resources_pb2.OutputConfig(closed_environment=True)\n      )\n    )\n  ]\n),\nmetadata=metadata\n)\n\nif post_models_response.status.code != status_code_pb2.SUCCESS:\n    print(post_models_response.status)\n    raise Exception(\"Post models failed, status: \" + post_models_response.status.description)\n")),(0,s.kt)(i.Z,{value:"nodejs",label:"NodeJS",mdxType:"TabItem"},(0,s.kt)(l.Z,{className:"language-javascript",mdxType:"CodeBlock"},'//index.js file\n\n////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, app ID, model ID, and\n// concept IDs. Change these strings to run your own example.\n////////////////////////////////////////////////////////////////////////////\n\nconst USER_ID = \'YOUR_USER_ID_HERE\';\n// Your PAT (Personal Access Token) can be found in the portal under Authentification\nconst PAT = \'YOUR_PAT_HERE\';\nconst APP_ID = \'YOUR_APP_ID_HERE\';\n// Change these to create your own visual classifier\nconst MODEL_ID = \'lawrence-1591638385\';\nconst CONCEPT_ID_1 = \'ferrari23\';\nconst CONCEPT_ID_2 = \'outdoors23\';\n\n/////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n/////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\nstub.PostModels(\n  {\n    user_app_id: {\n      user_id: USER_ID,\n      app_id: APP_ID\n    },\n    models: [\n      {\n        id: MODEL_ID,\n        model_type_id: "visual-classifier",\n        train_info: {\n          params: {\n            num_epoch: 2,\n            template: "classification_cifar10_v1"\n          }\n        },\n        output_info: {\n          data: {\n            concepts: [\n              { id: CONCEPT_ID_1 },\n              { id: CONCEPT_ID_2 }\n            ]\n          },\n          output_config: {\n            closed_environment: true\n          }\n        }\n      }\n    ]\n  },\n  metadata,\n  (err, response) => {\n    if (err) {\n      throw new Error(err);\n    }\n\n    if (response.status.code !== 10000) {\n      throw new Error("Received status: " + response.status.description + "\\n" + response.status.details);\n    }\n  }\n);')),(0,s.kt)(i.Z,{value:"java",label:"Java",mdxType:"TabItem"},(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-java"},'import com.clarifai.grpc.api.*;\nimport com.clarifai.grpc.api.status.*;\n\n// Insert here the initialization code as outlined on this page:\n// https://docs.clarifai.com/api-guide/api-overview/api-clients#client-installation-instructions\n\nStruct.Builder trainInfoParams = Struct.newBuilder()\n    .putFields(\n        "num_epochs", Value.newBuilder().setNumberValue(2).build()\n\n    )\n    .putFields(\n        "template", Value.newBuilder().setStringValue("classification_cifar10_v1").build()\n    );\n\nSingleModelResponse postModelsResponse = stub.postModels(\n    PostModelsRequest.newBuilder()\n        .addModels(\n            Model.newBuilder()\n                .setId("lawrence-1591638385")\n                .setModelTypeId("visual-classifier")\n                .setTrainInfo(TrainInfo.newBuilder().setParams(trainInfoParams))\n                .setOutputInfo(\n                    OutputInfo.newBuilder()\n                        .setData(\n                            Data.newBuilder()\n                                .addConcepts(Concept.newBuilder().setId("ferrari23"))\n                                .addConcepts(Concept.newBuilder().setId("outdoors23"))\n                        )\n                        .setOutputConfig(\n                            OutputConfig.newBuilder()\n                                .setClosedEnvironment(true)\n                        )\n                )\n        )\n        .build()\n);\n\nif (postModelsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n    throw new RuntimeException("Post models failed, status: " + postModelsResponse.getStatus());\n}\n'))),(0,s.kt)(i.Z,{value:"curl",label:"cURL",mdxType:"TabItem"},(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},'curl -X POST \'https://api.clarifai.com/v2/models\' \\\n    -H \'Authorization: Key YOUR_API_KEY\' \\\n    -H \'Content-Type: application/json\' \\\n    --data-raw \'{\n        "model": {\n            "id": "lawrence-1591638385",\n            "model_type_id": "visual-classifier",\n            "train_info": {\n                "params": {\n                    "template": "classification_cifar10_v1",\n                    "num_epochs": 2\n                }\n            },\n            "output_info": {\n                "data": {\n                    "concepts": [\n                        {"id":"ferrari23"},\n                        {"id":"outdoors23"}\n                    ]\n                },\n                "output_config": {\n                  "closed_environment" : true\n                }\n            }\n        }\n    }\'\n')))),(0,s.kt)("h3",{id:"create-a-visual-detector"},"Create a Visual Detector"),(0,s.kt)("p",null,"Create a visual detector to detect bounding box regions in images or video frames and then classify the detected images. You can also send the image regions to an image cropper model to create a new cropped image."),(0,s.kt)(r.Z,{mdxType:"Tabs"},(0,s.kt)(i.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,s.kt)(l.Z,{className:"language-python",mdxType:"CodeBlock"},"##########################################################################\n# In this section, we set the user authentication, app ID, model ID, and\n# concept IDs. Change these strings to run your own example.\n##########################################################################\n\nUSER_ID = 'YOUR_USER_ID_HERE'\n# Your PAT (Personal Access Token) can be found in the portal under Authentification\nPAT = 'YOUR_PAT_HERE'\nAPP_ID = 'YOUR_APP_ID_HERE'\n# Change these to create your own visual detector\nMODEL_ID = 'detection-test-1591638385' \nCONCEPT_ID_1 = 'ferrari23' \nCONCEPT_ID_2 = 'outdoors23' \n\n##########################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n##########################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\nfrom google.protobuf.struct_pb2 import Struct\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (('authorization', 'Key ' + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\ntrain_params = Struct()\ntrain_params.update(\n    {\n        \"template\": \"Clarifai_InceptionV2\",\n        \"num_epochs\": 2\n    }\n)\n\npost_models_response = stub.PostModels(\n    service_pb2.PostModelsRequest(\n        user_app_id=userDataObject,\n        models=[\n            resources_pb2.Model(\n                id=MODEL_ID,\n                model_type_id=\"visual-detector\",\n                train_info=resources_pb2.TrainInfo(params=train_params),\n                output_info=resources_pb2.OutputInfo(\n                    data=resources_pb2.Data(\n                        concepts=[\n                            resources_pb2.Concept(id=CONCEPT_ID_1),\n                            resources_pb2.Concept(id=CONCEPT_ID_2)\n                        ]\n                    ),\n                    output_config=resources_pb2.OutputConfig(closed_environment=True)\n                )\n            )\n        ]\n    ),\n    metadata=metadata\n)\n\nif post_models_response.status.code != status_code_pb2.SUCCESS:\n    print(post_models_response.status)\n    raise Exception(\"Post models failed, status: \" + post_models_response.status.description)\n")),(0,s.kt)(i.Z,{value:"nodejs",label:"NodeJS",mdxType:"TabItem"},(0,s.kt)(l.Z,{className:"language-javascript",mdxType:"CodeBlock"},'//index.js file\n\n////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, app ID, model ID, and\n// concept IDs. Change these strings to run your own example.\n////////////////////////////////////////////////////////////////////////////\n\nconst USER_ID = \'YOUR_USER_ID_HERE\';\n// Your PAT (Personal Access Token) can be found in the portal under Authentification\nconst PAT = \'YOUR_PAT_HERE\';\nconst APP_ID = \'YOUR_APP_ID_HERE\';\n// Change these to create your own visual detector\nconst MODEL_ID = \'detection-test-1591638385\';\nconst CONCEPT_ID_1 = \'ferrari23\';\nconst CONCEPT_ID_2 = \'outdoors23\';\n\n/////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n/////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\nstub.PostModels(\n    {\n        user_app_id: {\n            user_id: USER_ID,\n            app_id: APP_ID\n        },\n        models: [\n            {\n                id: MODEL_ID,\n                model_type_id: "visual-detector",\n                train_info: {\n                    params: {\n                        num_epoch: 2,\n                        template: "Clarifai-InceptionV2"\n                    }\n                },\n                output_info: {\n                    data: {\n                        concepts: [\n                            { id: CONCEPT_ID_1 },\n                            { id: CONCEPT_ID_2 }\n                        ]\n                    },\n                    output_config: {\n                        closed_environment: true\n                    }\n                }\n            }\n        ]\n    },\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            throw new Error("Received status: " + response.status.description + "\\n" + response.status.details);\n        }\n    }\n);')),(0,s.kt)(i.Z,{value:"java",label:"Java",mdxType:"TabItem"},(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-java"},'import com.clarifai.grpc.api.*;\nimport com.clarifai.grpc.api.status.*;\n\n// Insert here the initialization code as outlined on this page:\n// https://docs.clarifai.com/api-guide/api-overview/api-clients#client-installation-instructions\n\nStruct.Builder trainInfoParams = Struct.newBuilder()\n    .putFields(\n        "num_epochs", Value.newBuilder().setNumberValue(2).build()\n\n    )\n    .putFields(\n        "template", Value.newBuilder().setStringValue("Clarifai-InceptionV2").build()\n    );\n\nSingleModelResponse postModelsResponse = stub.postModels(\n    PostModelsRequest.newBuilder()\n        .addModels(\n            Model.newBuilder()\n                .setId("detection-test-1591638385")\n                .setModelTypeId("visual-detector")\n                .setTrainInfo(TrainInfo.newBuilder().setParams(trainInfoParams))\n                .setOutputInfo(\n                    OutputInfo.newBuilder()\n                        .setData(\n                            Data.newBuilder()\n                                .addConcepts(Concept.newBuilder().setId("ferrari23"))\n                                .addConcepts(Concept.newBuilder().setId("outdoors23"))\n                        )\n                        .setOutputConfig(\n                            OutputConfig.newBuilder()\n                                .setClosedEnvironment(true)\n                        )\n                )\n        )\n        .build()\n);\n\nif (postModelsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n    throw new RuntimeException("Post models failed, status: " + postModelsResponse.getStatus());\n}\n'))),(0,s.kt)(i.Z,{value:"curl",label:"cURL",mdxType:"TabItem"},(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},'curl -X POST \'https://api.clarifai.com/v2/models\' \\\n    -H \'Authorization: Key YOUR_API_KEY\' \\\n    -H \'Content-Type: application/json\' \\\n    --data-raw \'{\n        "model": {\n            "id": "detection-test-1591638385",\n            "model_type_id": "visual-detector",\n            "train_info": {\n                "params": {\n                    "template": "Clarifai-InceptionV2",\n                    "num_epochs": 2\n                }\n            },\n            "output_info": {\n                "data": {\n                    "concepts": [\n                        {"id":"ferrari23"},\n                        {"id":"outdoors23"}\n                    ]\n                },\n                "output_config": {\n                  "closed_environment" : true\n                }\n            }\n        }\n    }\'\n')))),(0,s.kt)("h3",{id:"create-a-visual-embedder"},"Create a Visual Embedder"),(0,s.kt)("p",null,'Create a visual embedding model to transform images and videos frames into "high level" vector representation understood by our AI models. These embeddings enable visual search and can be used as base models to train other models.'),(0,s.kt)(r.Z,{mdxType:"Tabs"},(0,s.kt)(i.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,s.kt)(l.Z,{className:"language-python",mdxType:"CodeBlock"},"##########################################################################\n# In this section, we set the user authentication, app ID, model ID, and\n# concept IDs. Change these strings to run your own example.\n##########################################################################\n\nUSER_ID = 'YOUR_USER_ID_HERE'\n# Your PAT (Personal Access Token) can be found in the portal under Authentification\nPAT = 'YOUR_PAT_HERE'\nAPP_ID = 'YOUR_APP_ID_HERE'\n# Change these to create your own visual embedder\nMODEL_ID = 'embed-test-1591638385' \nCONCEPT_ID_1 = 'ferrari23' \nCONCEPT_ID_2 = 'outdoors23' \n\n##########################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n##########################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\nfrom google.protobuf.struct_pb2 import Struct\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (('authorization', 'Key ' + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\ntrain_params = Struct()\ntrain_params.update(\n    {\n        \"template\": \"classification_basemodel_v1_embed\",\n        \"num_epochs\": 2\n    }\n)\n\npost_models_response = stub.PostModels(\n    service_pb2.PostModelsRequest(\n        user_app_id=userDataObject,\n        models=[\n            resources_pb2.Model(\n                id=MODEL_ID,\n                model_type_id=\"visual-embedder\",\n                train_info=resources_pb2.TrainInfo(params=train_params),\n                output_info=resources_pb2.OutputInfo(\n                    data=resources_pb2.Data(\n                        concepts=[\n                            resources_pb2.Concept(id=CONCEPT_ID_1),\n                            resources_pb2.Concept(id=CONCEPT_ID_2)\n                        ]\n                    ),\n                    output_config=resources_pb2.OutputConfig(closed_environment=True)\n                )\n            )\n        ]\n      ),\n      metadata=metadata\n)\n\nif post_models_response.status.code != status_code_pb2.SUCCESS:\n    print(post_models_response.status)\n    raise Exception(\"Post models failed, status: \" + post_models_response.status.description)\n")),(0,s.kt)(i.Z,{value:"nodejs",label:"NodeJS",mdxType:"TabItem"},(0,s.kt)(l.Z,{className:"language-javascript",mdxType:"CodeBlock"},'//index.js file\n\n////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, app ID, model ID, and\n// concept IDs. Change these strings to run your own example.\n////////////////////////////////////////////////////////////////////////////\n\nconst USER_ID = \'YOUR_USER_ID_HERE\';\n// Your PAT (Personal Access Token) can be found in the portal under Authentification\nconst PAT = \'YOUR_PAT_HERE\';\nconst APP_ID = \'YOUR_APP_ID_HERE\';\n// Change these to create your own visual embedder\nconst MODEL_ID = \'embed-test-1591638385\';\nconst CONCEPT_ID_1 = \'ferrari23\';\nconst CONCEPT_ID_2 = \'outdoors23\';\n\n/////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n/////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\nstub.PostModels(\n    {\n        user_app_id: {\n            user_id: USER_ID,\n            app_id: APP_ID\n        },\n        models: [\n            {\n                id: MODEL_ID,\n                model_type_id: "visual-embedder",\n                train_info: {\n                    params: {\n                        num_epoch: 2,\n                        template: "classification_basemodel_v1_embed"\n                    }\n                },\n                output_info: {\n                    data: {\n                        concepts: [\n                            { id: CONCEPT_ID_1 },\n                            { id: CONCEPT_ID_2 }\n                        ]\n                    },\n                    output_config: {\n                        closed_environment: true\n                    }\n                }\n            }\n        ]\n    },\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            throw new Error("Received status: " + response.status.description + "\\n" + response.status.details);\n        }\n    }\n);')),(0,s.kt)(i.Z,{value:"java",label:"Java",mdxType:"TabItem"},(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-java"},'import com.clarifai.grpc.api.*;\nimport com.clarifai.grpc.api.status.*;\n\n// Insert here the initialization code as outlined on this page:\n// https://docs.clarifai.com/api-guide/api-overview/api-clients#client-installation-instructions\n\nStruct.Builder trainInfoParams = Struct.newBuilder()\n    .putFields(\n        "num_epochs", Value.newBuilder().setNumberValue(2).build()\n\n    )\n    .putFields(\n        "template", Value.newBuilder().setStringValue("classification_basemodel_v1_embed").build()\n    );\n\nSingleModelResponse postModelsResponse = stub.postModels(\n    PostModelsRequest.newBuilder()\n        .addModels(\n            Model.newBuilder()\n                .setId("embed-test-1591638385")\n                .setModelTypeId("visual-embedder")\n                .setTrainInfo(TrainInfo.newBuilder().setParams(trainInfoParams))\n                .setOutputInfo(\n                    OutputInfo.newBuilder()\n                        .setData(\n                            Data.newBuilder()\n                                .addConcepts(Concept.newBuilder().setId("ferrari23"))\n                                .addConcepts(Concept.newBuilder().setId("outdoors23"))\n                        )\n                        .setOutputConfig(\n                            OutputConfig.newBuilder()\n                                .setClosedEnvironment(true)\n                        )\n                )\n        )\n        .build()\n);\n\nif (postModelsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n    throw new RuntimeException("Post models failed, status: " + postModelsResponse.getStatus());\n}\n'))),(0,s.kt)(i.Z,{value:"curl",label:"cURL",mdxType:"TabItem"},(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},'curl -X POST \'https://api.clarifai.com/v2/models\' \\\n    -H \'Authorization: Key YOUR_API_KEY\' \\\n    -H \'Content-Type: application/json\' \\\n    --data-raw \'{\n        "model": {\n            "id": "embed-test-1591638385",\n            "model_type_id": "visual-embedder",\n            "train_info": {\n                "params": {\n                    "template": "classification_basemodel_v1_embed",\n                    "num_epochs": 2\n                }\n            },\n            "output_info": {\n                "data": {\n                    "concepts": [\n                        {"id":"ferrari23"},\n                        {"id":"outdoors23"}\n                    ]\n                },\n                "output_config": {\n                  "closed_environment" : true\n                }\n            }\n        }\n    }\'\n')))),(0,s.kt)("h3",{id:"create-a-workflow"},"Create a Workflow"),(0,s.kt)("p",null,"Put your new deep-trained model to work by adding it to a workflow. Below is an example of how to create a workflow with a deep trained model."),(0,s.kt)(r.Z,{mdxType:"Tabs"},(0,s.kt)(i.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,s.kt)(l.Z,{className:"language-python",mdxType:"CodeBlock"},"###################################################################################\n# In this section, we set the user authentication, app ID, and the details we want\n# to use to create a workflow. Change these strings to run your own example.\n###################################################################################\n\nUSER_ID = 'YOUR_USER_ID_HERE'\n# Your PAT (Personal Access Token) can be found in the portal under Authentification\nPAT = 'YOUR_PAT_HERE'\nAPP_ID = 'YOUR_APP_ID_HERE'\n# Change these to create your own workflow\nWORKFLOW_ID = 'my-new-workflow-id' \nEMBED_MODEL_ID = 'YOUR_EMBED_MODEL_ID'\nEMBED_MODEL_VERSION_ID = 'YOUR_EMBED_MODEL_VERSION_ID'\nWORKFLOWNODE_ID = 'my-custom-model' \nCUSTOM_MODEL_ID = 'YOUR_CUSTOM_MODEL_ID'\nCUSTOM_MODEL_VERSION_ID = 'YOUR_CUSTOM_MODEL_VERSION_ID'\n\n##########################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n##########################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (('authorization', 'Key ' + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\npost_workflows_response = stub.PostWorkflows(\n    service_pb2.PostWorkflowsRequest(\n        user_app_id=userDataObject,\n        workflows=[\n            resources_pb2.Workflow(\n                id=WORKFLOW_ID,\n                nodes=[\n                    resources_pb2.WorkflowNode(\n                        id=\"embed\",\n                        model=resources_pb2.Model(\n                            id=EMBED_MODEL_ID,\n                            model_version=resources_pb2.ModelVersion(\n                                id=EMBED_MODEL_VERSION_ID\n                            )\n                        )\n                    ),\n                    resources_pb2.WorkflowNode(\n                        id=WORKFLOWNODE_ID,\n                        model=resources_pb2.Model(\n                            id=CUSTOM_MODEL_ID,\n                            model_version=resources_pb2.ModelVersion(\n                                id=CUSTOM_MODEL_VERSION_ID\n                            )\n                        ),\n                        node_inputs=[\n                            resources_pb2.NodeInput(node_id=\"embed\")\n                        ]\n                    ),\n                ]\n            )\n        ]\n    ),\n    metadata=metadata\n)\n\nif  post_workflows_response.status.code != status_code_pb2.SUCCESS:\n    print(post_workflows_response.status)\n    raise Exception(\"Post workflows failed, status: \" +  post_workflows_response.status.description)")),(0,s.kt)(i.Z,{value:"nodejs",label:"NodeJS",mdxType:"TabItem"},(0,s.kt)(l.Z,{className:"language-javascript",mdxType:"CodeBlock"},"//index.js file\n\n//////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, app ID, and the details we want\n// to use to create a workflow. Change these strings to run your own example.\n/////////////////////////////////////////////////////////////////////////////////////\n\nconst USER_ID = 'YOUR_USER_ID_HERE';\n// Your PAT (Personal Access Token) can be found in the portal under Authentification\nconst PAT = 'YOUR_PAT_HERE';\nconst APP_ID = 'YOUR_APP_ID_HERE';\n// Change these to create your own workflow\nconst WORKFLOW_ID = 'my-new-workflow-id';\nconst EMBED_MODEL_ID = 'YOUR_EMBED_MODEL_ID';\nconst EMBED_MODEL_VERSION_ID = 'YOUR_EMBED_MODEL_VERSION_ID';\nconst WORKFLOWNODE_ID = 'my-custom-model';\nconst CUSTOM_MODEL_ID = 'YOUR_CUSTOM_MODEL_ID';\nconst CUSTOM_MODEL_VERSION_ID = 'YOUR_CUSTOM_MODEL_VERSION_ID';\n\n/////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n/////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require(\"clarifai-nodejs-grpc\");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set(\"authorization\", \"Key \" + PAT);\n\nstub.PostWorkflows(\n    {\n        user_app_id: {\n            \"user_id\": USER_ID,\n            \"app_id\": APP_ID\n        },\n        workflows: [\n            {\n                id: WORKFLOW_ID,\n                nodes: [\n                    {\n                        id: \"embed\",\n                        model: {\n                            id: EMBED_MODEL_ID,\n                            model_version: {\n                                id: EMBED_MODEL_VERSION_ID\n                            }\n                        }\n                    },\n                    {\n                        id: WORKFLOWNODE_ID,\n                        model: {\n                            id: CUSTOM_MODEL_ID,\n                            model_version: {\n                                id: CUSTOM_MODEL_VERSION_ID\n                            }\n                        },\n                        node_inputs: [\n                            { node_id: \"embed\" }\n                        ]\n                    }\n                ]\n            }\n        ]\n    },\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            console.log(response.status);\n            throw new Error(\"Post workflows failed, status: \" + response.status.description);\n        }\n    }\n);")),(0,s.kt)(i.Z,{value:"java",label:"Java",mdxType:"TabItem"},(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-java"},'import com.clarifai.grpc.api.*;\nimport com.clarifai.grpc.api.status.*;\n\n// Insert here the initialization code as outlined on this page:\n// https://docs.clarifai.com/api-guide/api-overview/api-clients#client-installation-instructions\n\nMultiWorkflowResponse postWorkflowsResponse = stub.postWorkflows(\n  PostWorkflowsRequest.newBuilder()\n      .addWorkflows(\n          Workflow.newBuilder()\n              .setId("my-new-workflow-id")\n              .addNodes(\n                  WorkflowNode.newBuilder()\n                      .setId("embed")\n                      .setModel(\n                          Model.newBuilder()\n                              .setId("{YOUR_EMBED_MODEL_ID}")\n                              .setModelVersion(\n                                  ModelVersion.newBuilder()\n                                      .setId("{YOUR_EMBED_MODEL_VERSION_ID}")\n                              )\n                      )\n              )\n              .addNodes(\n                  WorkflowNode.newBuilder()\n                      .setId("my-custom-model")\n                      .setModel(\n                          Model.newBuilder()\n                              .setId("{YOUR_CUSTOM_MODEL_ID}")\n                              .setModelVersion(\n                                  ModelVersion.newBuilder()\n                                      .setId("{YOUR_CUSTOM_MODEL_MODEL_VERSION_ID}")\n                              )\n                      )\n                      .addNodeInputs(NodeInput.newBuilder().setNodeId("embed"))\n              )\n      )\n      .build()\n);\n\nif (postWorkflowsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n    throw new RuntimeException("Post workflows failed, status: " + postWorkflowsResponse.getStatus());\n}\n'))),(0,s.kt)(i.Z,{value:"curl",label:"cURL",mdxType:"TabItem"},(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},'curl -X POST \'https://api.clarifai.com/v2/workflows\' \\\n    -H \'Authorization: Key YOUR_API_KEY\' \\\n    -H \'Content-Type: application/json\' \\\n    --data-raw \'{\n        "workflows": [\n            {\n                "id": "my-new-workflow-id",\n                "nodes": [\n                    {\n                        "id": "embed",\n                        "model": {\n                            "id": "{YOUR_EMBED_MODEL_ID}",\n                            "model_version": {\n                                "id": "{YOUR_EMBED_MODEL_VERSION_ID}"\n                            }\n                        }\n                    },\n                    {\n                        "id": "my-custom-model",\n                        "model": {\n                            "id": "{YOUR_CUSTOM_MODEL_ID}",\n                            "model_version": {\n                                "id": "{YOUR_CUSTOM_MODEL_VERSION_ID}"\n                            }\n                        },\n                        "node_inputs": [\n                            {\n                                "node_id": "embed"\n                            }\n                        ]\n                    }\n                ]\n            }\n        ]\n    }\'\n')))),(0,s.kt)("h2",{id:"update"},"Update"),(0,s.kt)("h3",{id:"update-your-default-workflow"},"Update Your Default Workflow"),(0,s.kt)("p",null,"Index your inputs with a deep trained model by updating your default workflow. You can also use your deep trained embeddings as the basis for clustering and search."),(0,s.kt)("p",null,"Below is an example of how to update your default workflow with a deep trained model."),(0,s.kt)(r.Z,{mdxType:"Tabs"},(0,s.kt)(i.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,s.kt)(l.Z,{className:"language-python",mdxType:"CodeBlock"},"########################################################################\n# In this section, we set the user authentication, app ID, and default\n# workflow ID. Change these strings to run your own example.\n########################################################################\n\nUSER_ID = 'YOUR_USER_ID_HERE'\n# Your PAT (Personal Access Token) can be found in the portal under Authentification\nPAT = 'YOUR_PAT_HERE'\nAPP_ID = 'YOUR_APP_ID_HERE'\n# Change this to update your default workflow\nDEFAULT_WORKFlOW_ID = 'auto-annotation-workflow-id'\n\n##########################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n##########################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (('authorization', 'Key ' + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\npatch_apps_response = stub.PatchApps(\n    service_pb2.PatchAppsRequest(\n        user_app_id=userDataObject,\n        action=\"overwrite\",\n        apps=[\n            resources_pb2.App(\n                id=APP_ID,\n                default_workflow_id=DEFAULT_WORKFlOW_ID\n            )\n        ]\n    ),\n    metadata=metadata\n)\n\nif patch_apps_response.status.code != status_code_pb2.SUCCESS:\n    print(patch_apps_response.status)\n    raise Exception(\"Patch apps failed, status: \" + patch_apps_response.status.description) \n")),(0,s.kt)(i.Z,{value:"nodejs",label:"NodeJS",mdxType:"TabItem"},(0,s.kt)(l.Z,{className:"language-javascript",mdxType:"CodeBlock"},'//index.js file\n\n/////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, app ID, and default\n// workflow ID. Change these strings to run your own example.\n/////////////////////////////////////////////////////////////////////////////\n\nconst USER_ID = \'YOUR_USER_ID_HERE\';\n// Your PAT (Personal Access Token) can be found in the portal under Authentification\nconst PAT = \'YOUR_PAT_HERE\';\nconst APP_ID = \'YOUR_APP_ID_HERE\';\n// Change this to update your default workflow\nconst DEFAULT_WORKFlOW_ID = \'auto-annotation-workflow-id\';\n\n/////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n/////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\nstub.PatchApps(\n    {\n        user_app_id: {\n            "user_id": USER_ID,\n            "app_id": APP_ID\n        },\n        action: "overwrite",\n        apps: [\n            {\n                id: APP_ID,\n                default_workflow_id: DEFAULT_WORKFlOW_ID\n            }\n        ]\n    },\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            console.log(response.status);\n            throw new Error("Patch apps failed, status: " + response.status.description);\n        }\n    }\n);')),(0,s.kt)(i.Z,{value:"java",label:"Java",mdxType:"TabItem"},(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-java"},'import com.clarifai.grpc.api.*;\nimport com.clarifai.grpc.api.status.*;\n\n// Insert here the initialization code as outlined on this page:\n// https://docs.clarifai.com/api-guide/api-overview/api-clients#client-installation-instructions\n\nMultiAppResponse patchAppsResponse = stub.patchApps(\n    PatchAppsRequest.newBuilder()\n        .setAction("overwrite")\n        .addApps(\n            App.newBuilder()\n                .setId("{YOUR_APP_ID}")\n                .setDefaultWorkflowId("auto-annotation-workflow-id")\n        ).build()\n);\n\nif (patchAppsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n    throw new RuntimeException("Patch apps failed, status: " + patchAppsResponse.getStatus());\n}\n'))),(0,s.kt)(i.Z,{value:"curl",label:"cURL",mdxType:"TabItem"},(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},'curl -X PATCH \'https://api.clarifai.com/v2/users/me/apps\' \\\n    -H \'Authorization: Key {{PAT}}\' \\\n    -H \'Content-Type: application/json\' \\\n    --data-raw \'{\n        "action": "overwrite",\n        "apps": [\n            {\n                "id": "{{app}}",\n                "default_workflow_id": "auto-annotation-workflow-ID"\n            }\n        ]\n    }\'\n')))))}f.isMDXComponent=!0}}]);
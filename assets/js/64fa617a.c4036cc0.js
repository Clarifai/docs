"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[6021],{69553:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>k,contentTitle:()=>v,default:()=>I,frontMatter:()=>b,metadata:()=>j,toc:()=>w});var o=t(74848),i=t(28453),r=t(11470),s=t(19365),a=t(21432);const l='# Model to be uploaded: https://huggingface.co/Falconsai/nsfw_image_detection\n\nimport os\nfrom io import BytesIO\nfrom typing import Iterator\n\nimport requests\nimport torch\nfrom clarifai.runners.models.model_runner import ModelRunner\nfrom clarifai.utils.logging import logger\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\nfrom PIL import Image\nfrom transformers import AutoModelForImageClassification, ViTImageProcessor\n\n\ndef preprocess_image(image_url=None, image_base64=None):\n  if image_base64:\n    img = Image.open(BytesIO(image_base64))\n  elif image_url:\n    img = Image.open(BytesIO(requests.get(image_url).content))\n  return img\n\n\nclass MyRunner(ModelRunner):\n  """A custom runner that loads the model and classifies images using it.\n  """\n\n  def load_model(self):\n    """Load the model here."""\n\n    self.device = \'cuda\' if torch.cuda.is_available() else \'cpu\'\n    logger.info(f"Running on device: {self.device}")\n\n    # if checkpoints section is in config.yaml file then checkpoints will be downloaded at this path during model upload time.\n    checkpoint_path = os.path.join(os.path.dirname(__file__), "checkpoints")\n\n    self.model = AutoModelForImageClassification.from_pretrained(checkpoint_path,).to(self.device)\n    self.processor = ViTImageProcessor.from_pretrained(checkpoint_path)\n    logger.info("Done loading!")\n\n  def predict(self, request: service_pb2.PostModelOutputsRequest\n             ) -> Iterator[service_pb2.MultiOutputResponse]:\n    """This is the method that will be called when the runner is run. It takes in an input and\n    returns an output.\n    """\n\n    # Get the concept protos from the model.\n    concept_protos = request.model.model_version.output_info.data.concepts\n\n    outputs = []\n    # TODO: parallelize this over inputs in a single request.\n    for inp in request.inputs:\n      output = resources_pb2.Output()\n\n      data = inp.data\n\n      output_concepts = []\n\n      if data.image.base64 != b"":\n        img = preprocess_image(image_base64=data.image.base64)\n      elif data.image.url != "":\n        img = preprocess_image(image_url=data.image.url)\n\n      with torch.no_grad():\n        inputs = self.processor(images=img, return_tensors="pt").to(self.device)\n        model_output = self.model(**inputs)\n        logits = model_output.logits\n\n      probs = torch.softmax(logits, dim=-1)[0]\n      sorted_indices = torch.argsort(probs, dim=-1, descending=True)\n      for idx in sorted_indices:\n        concept_protos[idx.item()].value = probs[idx.item()].item()\n        output_concepts.append(concept_protos[idx.item()])\n\n      output.data.concepts.extend(output_concepts)\n\n      output.status.code = status_code_pb2.SUCCESS\n      outputs.append(output)\n    return service_pb2.MultiOutputResponse(outputs=outputs,)\n\n  def generate(self, request: service_pb2.PostModelOutputsRequest\n              ) -> Iterator[service_pb2.MultiOutputResponse]:\n    raise NotImplementedError("Stream method is not implemented for image classification models.")\n\n  def stream(self, request_iterator: Iterator[service_pb2.PostModelOutputsRequest]\n            ) -> Iterator[service_pb2.MultiOutputResponse]:\n    ## raise NotImplementedError\n    raise NotImplementedError("Stream method is not implemented for image classification models.")',d="torch==2.4.1\ntokenizers==0.19.1\ntransformers==4.44.1\npillow==10.4.0\nrequests==2.32.3",u='# This is the sample config file for the image-classifier model\n\nmodel:\n  id: "model_id"\n  user_id: "user_id"\n  app_id: "app_id"\n  model_type_id: "visual-classifier"\n\nbuild_info:\n  python_version: "3.10"\n\ninference_compute_info:\n  cpu_limit: "1"\n  cpu_memory: "2Gi"\n  num_accelerators: 1\n  accelerator_type: ["NVIDIA-A10G"]\n  accelerator_memory: "3Gi"\n\ncheckpoints:\n  type: "huggingface"\n  repo_id: "Falconsai/nsfw_image_detection"\n  hf_token: "hf_token"',c='# Model to be uploaded: https://huggingface.co/facebook/detr-resnet-50\n\nimport os\nfrom io import BytesIO\nfrom typing import Iterator\n\nimport requests\nimport torch\nfrom clarifai.runners.models.model_runner import ModelRunner\nfrom clarifai.utils.logging import logger\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\nfrom PIL import Image\nfrom transformers import DetrForObjectDetection, DetrImageProcessor\n\n\ndef preprocess_image(image_url=None, image_base64=None):\n  if image_base64:\n    img = Image.open(BytesIO(image_base64))\n  elif image_url:\n    img = Image.open(BytesIO(requests.get(image_url).content))\n  return img\n\n\nclass MyRunner(ModelRunner):\n  """A custom runner that adds "Hello World" to the end of the text and replaces the domain of the\n  image URL as an example.\n  """\n\n  def load_model(self):\n    """Load the model here."""\n    checkpoint_path = os.path.join(os.path.dirname(__file__), "checkpoints")\n    self.device = \'cuda\' if torch.cuda.is_available() else \'cpu\'\n    logger.info(f"Running on device: {self.device}")\n\n    self.model = DetrForObjectDetection.from_pretrained(\n        checkpoint_path, revision="no_timm").to(self.device)\n    self.processor = DetrImageProcessor.from_pretrained(checkpoint_path, revision="no_timm")\n    logger.info("Done loading!")\n\n  def predict(self, request: service_pb2.PostModelOutputsRequest\n             ) -> Iterator[service_pb2.MultiOutputResponse]:\n    """This is the method that will be called when the runner is run. It takes in an input and\n    returns an output.\n    """\n\n    # Get the concept protos from the model.\n    concept_protos = request.model.model_version.output_info.data.concepts\n\n    outputs = []\n    # TODO: parallelize this over inputs in a single request.\n    for inp in request.inputs:\n      output = resources_pb2.Output()\n\n      data = inp.data\n\n      output_regions = []\n\n      if data.image.base64 != b"":\n        img = preprocess_image(image_base64=data.image.base64)\n      elif data.image.url != "":\n        img = preprocess_image(image_url=data.image.url)\n\n      with torch.no_grad():\n        inputs = self.processor(images=img, return_tensors="pt").to(self.device)\n        model_output = self.model(**inputs)\n\n      # convert outputs (bounding boxes and class logits) to COCO API\n      # let\'s only keep detections with score > 0.7 (You can set it to any other value)\n      target_sizes = torch.tensor([img.size[::-1]])\n      results = self.processor.post_process_object_detection(\n          model_output, target_sizes=target_sizes, threshold=0.7)[0]\n\n      width, height = img.size\n      for score, label_idx, box in zip(results["scores"], results["labels"], results["boxes"]):\n        # Normalize bounding box\n        x_min, y_min, x_max, y_max = box\n\n        top_row = round(y_min.item() / height, 2)\n        left_col = round(x_min.item() / width, 2)\n        bottom_row = round(y_max.item() / height, 2)\n        right_col = round(x_max.item() / width, 2)\n\n        output_region = resources_pb2.Region()\n        output_region.id = str(label_idx.item())\n        output_region.value = score.item()\n\n        concept_protos[label_idx.item()].value = score.item()\n        output_region.data.concepts.add(concept_protos[label_idx.item()])\n\n        output_region.region_info.bounding_box.top_row = top_row\n        output_region.region_info.bounding_box.left_col = left_col\n        output_region.region_info.bounding_box.bottom_row = bottom_row\n        output_region.region_info.bounding_box.right_col = right_col\n\n        output_regions.append(output_region)\n\n      output.data.regions.extend(output_regions)\n\n      output.status.code = status_code_pb2.SUCCESS\n      outputs.append(output)\n    return service_pb2.MultiOutputResponse(outputs=outputs,)\n\n  def generate(self, request: service_pb2.PostModelOutputsRequest\n              ) -> Iterator[service_pb2.MultiOutputResponse]:\n    raise NotImplementedError("Stream method is not implemented for image detection models.")\n\n  def stream(self, request_iterator: Iterator[service_pb2.PostModelOutputsRequest]\n            ) -> Iterator[service_pb2.MultiOutputResponse]:\n    raise NotImplementedError("Stream method is not implemented for image detection models.")',p="torch==2.4.1\ntokenizers==0.19.1\ntransformers==4.44.2\npillow==10.4.0\nrequests==2.32.3",m='# This is the sample config file for the image-detection model\n\nmodel:\n  id: "model_id"\n  user_id: "user_id"\n  app_id: "app_id"\n  model_type_id: "visual-detector"\n\nbuild_info:\n  python_version: "3.10"\n\ninference_compute_info:\n  cpu_limit: "1"\n  cpu_memory: "2Gi"\n  num_accelerators: 1\n  accelerator_type: ["NVIDIA-A10G"]\n  accelerator_memory: "5Gi"\n\n\ncheckpoints:\n  type: "huggingface"\n  repo_id: "facebook/detr-resnet-50"\n  hf_token: "hf_token"',h='# Model to be uploaded: https://huggingface.co/casperhansen/llama-3-8b-instruct-awq\n\nimport os\nfrom threading import Thread\nfrom typing import Iterator\n\nimport torch\nfrom clarifai.runners.models.model_runner import ModelRunner\nfrom clarifai.utils.logging import logger\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2\nfrom clarifai_grpc.grpc.api.status import status_code_pb2, status_pb2\nfrom google.protobuf import json_format\nfrom transformers import (AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer)\n\n\nclass MyRunner(ModelRunner):\n  """A custom runner that loads the Llama model and generates text using it.\n  """\n\n  def load_model(self):\n    """Load the model here."""\n    self.device = \'cuda\' if torch.cuda.is_available() else \'cpu\'\n    logger.info(f"Running on device: {self.device}")\n\n    # if checkpoints section is in config.yaml file then checkpoints will be downloaded at this path during model upload time.\n    checkpoints = os.path.join(os.path.dirname(__file__), "checkpoints")\n\n    self.tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n    self.model = AutoModelForCausalLM.from_pretrained(\n        checkpoints,\n        low_cpu_mem_usage=True,\n        device_map=self.device,\n        torch_dtype=torch.bfloat16,\n    )\n    # Create a streamer for streaming the output of the model\n    self.streamer = TextIteratorStreamer(\n        self.tokenizer, skip_prompt=True, skip_special_tokens=True)\n    logger.info("Done loading!")\n\n  def predict(self,\n              request: service_pb2.PostModelOutputsRequest) -> service_pb2.MultiOutputResponse:\n    """This is the method that will be called when the runner is run. It takes in an input and\n    returns an outputs the response using llama model.\n    """\n\n    # TODO: Could cache the model and this conversion if the hash is the same.\n    model = request.model\n    output_info = {}\n    if request.model.model_version.id != "":\n      output_info = json_format.MessageToDict(\n          model.model_version.output_info, preserving_proto_field_name=True)\n\n    outputs = []\n    # TODO: parallelize this over inputs in a single request.\n    for inp in request.inputs:\n      data = inp.data\n\n      # Optional use of output_info\n      inference_params = {}\n      if "params" in output_info:\n        inference_params = output_info["params"]\n\n      temperature = inference_params.get("temperature", 0.7)\n      max_tokens = inference_params.get("max_tokens", 100)\n      max_tokens = int(max_tokens)\n\n      top_k = inference_params.get("top_k", 40)\n      top_k = int(top_k)\n      top_p = inference_params.get("top_p", 1.0)\n\n      if data.text.raw != "":\n        prompt = data.text.raw\n\n        inputs = self.tokenizer([prompt], return_tensors="pt").to(self.device)\n        output_tokens = self.model.generate(\n            **inputs,\n            eos_token_id=self.tokenizer.eos_token_id,\n            do_sample=True,\n            temperature=temperature,\n            max_new_tokens=max_tokens,\n            top_p=top_p,\n            top_k=top_k,\n        )\n        llm_outputs = self.tokenizer.batch_decode(\n            output_tokens[:, inputs[\'input_ids\'].shape[1]:], skip_special_tokens=True)\n\n        output = resources_pb2.Output()\n        output.data.text.raw = llm_outputs[0]\n\n      output.status.code = status_code_pb2.SUCCESS\n      outputs.append(output)\n    return service_pb2.MultiOutputResponse(outputs=outputs,)\n\n  def generate(self, request: service_pb2.PostModelOutputsRequest\n              ) -> Iterator[service_pb2.MultiOutputResponse]:\n    """Example yielding a whole batch of streamed stuff back."""\n\n    # TODO: Could cache the model and this conversion if the hash is the same.\n    model = request.model\n    output_info = {}\n    if request.model.model_version.id != "":\n      output_info = json_format.MessageToDict(\n          model.model_version.output_info, preserving_proto_field_name=True)\n\n    # TODO: parallelize this over inputs in a single request.\n    for inp in request.inputs:\n      data = inp.data\n\n      # Optional use of output_info\n      inference_params = {}\n      if "params" in output_info:\n        inference_params = output_info["params"]\n\n      temperature = inference_params.get("temperature", 0.7)\n      max_tokens = inference_params.get("max_tokens", 100)\n      max_tokens = int(max_tokens)\n      top_p = inference_params.get("top_p", 1.0)\n\n      top_k = inference_params.get("top_k", 40)\n      top_k = int(top_k)\n\n      kwargs = dict(temperature=temperature, top_p=top_p, max_new_tokens=max_tokens, top_k=top_k)\n\n      if data.text.raw != "":\n        prompt = data.text.raw\n\n        inputs = self.tokenizer(prompt, return_tensors="pt").input_ids.cuda()\n        generation_kwargs = dict(input_ids=inputs, streamer=self.streamer, **kwargs)\n        thread = Thread(target=self.model.generate, kwargs=generation_kwargs)\n        thread.start()\n\n        for new_text in self.streamer:\n          output = resources_pb2.Output()\n\n          output.data.text.raw = new_text\n          output.status.code = status_code_pb2.SUCCESS\n          result = service_pb2.MultiOutputResponse(\n              status=status_pb2.Status(\n                  code=status_code_pb2.SUCCESS,\n                  description="Success",\n              ),\n              outputs=[output],\n          )\n          yield result\n        thread.join()\n\n  def stream(self, request_iterator: Iterator[service_pb2.PostModelOutputsRequest]\n            ) -> Iterator[service_pb2.MultiOutputResponse]:\n    """Example yielding a whole batch of streamed stuff back."""\n    output_info = {}\n    for ri, request in enumerate(request_iterator):\n      if ri == 0:  # only first request has model information.\n        model = request.model\n        if request.model.model_version.id != "":\n          output_info = json_format.MessageToDict(\n              model.model_version.output_info, preserving_proto_field_name=True)\n          # Optional use of output_info\n          inference_params = {}\n          if "params" in output_info:\n            inference_params = output_info["params"]\n      # TODO: parallelize this over inputs in a single request.\n      for inp in request.inputs:\n        data = inp.data\n\n        # Optional use of output_info\n        inference_params = {}\n        if "params" in output_info:\n          inference_params = output_info["params"]\n\n        temperature = inference_params.get("temperature", 0.7)\n        max_tokens = inference_params.get("max_tokens", 100)\n        max_tokens = int(max_tokens)\n        top_p = inference_params.get("top_p", 1.0)\n\n        top_k = inference_params.get("top_k", 40)\n        top_k = int(top_k)\n\n        kwargs = dict(temperature=temperature, top_p=top_p, max_new_tokens=max_tokens, top_k=top_k)\n\n        if data.text.raw != "":\n          prompt = data.text.raw\n\n          inputs = self.tokenizer(prompt, return_tensors="pt").input_ids.cuda()\n          generation_kwargs = dict(input_ids=inputs, streamer=self.streamer, **kwargs)\n          thread = Thread(target=self.model.generate, kwargs=generation_kwargs)\n          thread.start()\n\n          for new_text in self.streamer:\n            output = resources_pb2.Output()\n\n            output.data.text.raw = new_text\n            output.status.code = status_code_pb2.SUCCESS\n            result = service_pb2.MultiOutputResponse(\n                status=status_pb2.Status(\n                    code=status_code_pb2.SUCCESS,\n                    description="Success",\n                ),\n                outputs=[output],\n            )\n            yield result\n          thread.join()',f="torch==2.3.1\ntokenizers==0.19.1\ntransformers==4.44.2\naccelerate==0.34.2\nscipy==1.10.1\noptimum==1.22.0\nxformers==0.0.27\nprotobuf==5.27.3\neinops==0.8.0\nrequests==2.32.2\nsentence_transformers==2.2.0\nsentencepiece==0.2.0\nautoawq==0.2.6",_='# This is the sample config file for the Llama model\n\nmodel:\n  id: "llama-3-8b-instruct"\n  user_id: "user_id"\n  app_id: "app_id"\n  model_type_id: "text-to-text"\n\nbuild_info:\n  python_version: "3.10"\n\ninference_compute_info:\n  cpu_limit: "1"\n  cpu_memory: "8Gi"\n  num_accelerators: 1\n  accelerator_type: ["NVIDIA-A10G"]\n  accelerator_memory: "12Gi"\n\ncheckpoints:\n  type: "huggingface"\n  repo_id: "casperhansen/llama-3-8b-instruct-awq"\n  hf_token: "hf_token"',g='# Model to be uploaded: https://platform.openai.com/docs/guides/speech-to-text/quickstart\n\nimport copy\nimport io\nfrom typing import Iterator\n\nimport requests\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2\nfrom clarifai_grpc.grpc.api.status import status_code_pb2, status_pb2\nfrom openai import OpenAI\n\nfrom clarifai.runners.models.model_runner import ModelRunner\n\n\ndef bytes_to_audio_file(audio_bytes):\n  """Convert bytes data into a file-like object."""\n  if not audio_bytes:\n    raise ValueError("Audio bytes cannot be empty.")\n  audio_file = io.BytesIO(audio_bytes)\n  audio_file.name = "audio.mp3"  # This name is used for the API\n  return audio_file\n\n\ndef preprocess_audio(audio_url=None, audio_bytes=None, chunk_size=1024, stream=False):\n  """\n  Fetch and preprocess audio data from a URL or bytes.\n\n  Parameters:\n    url (str): URL to fetch audio from (if provided).\n    bytes (bytes): Audio data in bytes (if provided).\n    chunk_size (int): Size of chunks for streaming.\n    stream (bool): Whether to stream the audio in chunks.\n\n  Returns:\n    Generator or file-like object containing audio data.\n  """\n\n  if audio_bytes:\n    if stream:\n      # Stream the audio in chunks (generator)\n      def audio_stream_generator():\n        for i in range(0, len(audio_bytes), chunk_size):\n          yield audio_bytes[i : i + chunk_size]\n\n      return audio_stream_generator()\n    else:\n      # Return a single chunk of audio\n      return audio_bytes\n  elif audio_url:\n    response = requests.get(audio_url, stream=stream)\n    if response.status_code != 200:\n      raise Exception(f"Failed to fetch audio. Status code: {response.status_code}")\n\n    if stream:\n      # Stream the audio in chunks (generator)\n      def audio_stream_generator():\n        for chunk in response.iter_content(chunk_size=chunk_size):\n          if chunk:  # Filter out keep-alive new chunks\n            yield chunk\n\n      return audio_stream_generator()\n    else:\n      # Return a single chunk of audio\n      return response.content\n\n  else:\n    raise ValueError("Either \'url\' or \'audio_bytes\' must be provided")\n\n\nOPENAI_API_KEY = "API_KEY"\n\n\nclass MyRunner(ModelRunner):\n  """A custom runner that used for transcribing audio."""\n\n  def load_model(self):\n    """Load the model here."""\n    self.client = OpenAI(api_key=OPENAI_API_KEY)\n    self.modelname = "whisper-1"\n    self.language = None\n\n    # reset the task in set_translate_task\n    self.task = "transcribe"\n\n  def predict(\n    self, request: service_pb2.PostModelOutputsRequest\n  ) -> service_pb2.MultiOutputResponse:\n    """This is the method that will be called when the runner is run. It takes in an input and\n    returns an output.\n    """\n\n    outputs = []\n    # TODO: parallelize this over inputs in a single request.\n    for inp in request.inputs:\n      output = resources_pb2.Output()\n\n      data = inp.data\n      audio_bytes = None\n      if data.audio.base64:\n        audio_bytes = preprocess_audio(audio_bytes=data.audio.base64, stream=False)\n\n      elif data.audio.url:\n        audio_bytes = preprocess_audio(\n          audio_url=data.audio.url,\n          stream=False,\n        )\n\n      # Send audio bytes to Whisper for transcription\n      transcription = self.client.audio.transcriptions.create(\n        model=self.modelname, language=self.language, file=bytes_to_audio_file(audio_bytes)\n      )\n\n      # Set the output data\n      output.data.text.raw = transcription.text\n      output.status.code = status_code_pb2.SUCCESS\n      outputs.append(output)\n    return service_pb2.MultiOutputResponse(\n      outputs=outputs,\n    )\n\n  def generate(\n    self, request: service_pb2.PostModelOutputsRequest\n  ) -> Iterator[service_pb2.MultiOutputResponse]:\n    def request_iterator(request, chunk_size=1024):\n      request_copy = copy.deepcopy(request)\n      for inp in request_copy.inputs:\n        data = inp.data\n\n        audio_chunks = None\n        if data.audio.base64:\n          audio_chunks = preprocess_audio(\n            audio_bytes=data.audio.base64, stream=True, chunk_size=chunk_size\n          )\n        elif data.audio.url:\n          audio_chunks = preprocess_audio(\n            audio_url=data.audio.url,\n            stream=True,\n            chunk_size=chunk_size,\n          )\n\n        for chunk in audio_chunks:\n          inp.data.audio.base64 = chunk\n          yield request_copy\n\n    chunk_size = 1024 * 1024\n    return self.stream(request_iterator(request, chunk_size=chunk_size))\n\n  def stream(\n    self, request_iterator: Iterator[service_pb2.PostModelOutputsRequest]\n  ) -> Iterator[service_pb2.MultiOutputResponse]:\n    for request in request_iterator:\n      for inp in request.inputs:\n        output = resources_pb2.Output()\n\n        data = inp.data\n        chunk_size = 10 * 1024 * 1024\n        if data.image.base64 != b"":\n          audio_chunks = preprocess_audio(\n            audio_bytes=data.audio.base64, stream=True, chunk_size=chunk_size\n          )\n        elif data.audio.url != "":\n          audio_chunks = preprocess_audio(\n            audio_url=data.audio.url, stream=True, chunk_size=chunk_size\n          )\n\n        for chunk in audio_chunks:\n          transcription = self.client.audio.transcriptions.create(\n            model=self.modelname, language=self.language, file=bytes_to_audio_file(chunk)\n          )\n          # Set the output data\n          output.data.text.raw = transcription.text\n\n          output.status.code = status_code_pb2.SUCCESS\n          result = service_pb2.MultiOutputResponse(\n            status=status_pb2.Status(\n              code=status_code_pb2.SUCCESS,\n              description="Success",\n            ),\n            outputs=[output],\n          )\n          yield result',x="openai\nrequests",y='# This is the sample config file for the Openai Whisper model\n\nmodel:\n  id: "model_id"\n  user_id: "user_id"\n  app_id: "app_id"\n  model_type_id: "audio-to-text"\n\nbuild_info:\n  python_version: "3.10"\n\ninference_compute_info:\n  cpu_limit: "1"\n  cpu_memory: "500m"\n  num_accelerators: 0',b={sidebar_position:1},v="Model Upload",j={id:"sdk/advance-model-operations/model-upload",title:"Model Upload",description:"Learn how to upload a model using Clarifai SDKs",source:"@site/docs/sdk/advance-model-operations/model-upload.md",sourceDirName:"sdk/advance-model-operations",slug:"/sdk/advance-model-operations/model-upload",permalink:"/sdk/advance-model-operations/model-upload",draft:!1,unlisted:!1,editUrl:"https://github.com/Clarifai/docs/blob/main/docs/sdk/advance-model-operations/model-upload.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"tutorialSidebar",previous:{title:"Advanced Model Operations",permalink:"/sdk/advance-model-operations/"},next:{title:"Model Export",permalink:"/sdk/advance-model-operations/model-export"}},k={},w=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Installation",id:"installation",level:3},{value:"Environment Set Up",id:"environment-set-up",level:3},{value:"Create Project Directory",id:"create-project-directory",level:3},{value:"How to Upload a Model",id:"how-to-upload-a-model",level:2},{value:"Step 1: Define the <code>config.yaml</code> File",id:"step-1-define-the-configyaml-file",level:3},{value:"Model Info",id:"model-info",level:4},{value:"Compute Resources",id:"compute-resources",level:4},{value:"Model Checkpoints",id:"model-checkpoints",level:4},{value:"Model Concepts or Labels",id:"model-concepts-or-labels",level:4},{value:"Step 2: Define Dependencies in <code>requirements.txt</code>",id:"step-2-define-dependencies-in-requirementstxt",level:3},{value:"Step 3: Prepare the <code>model.py</code> File",id:"step-3-prepare-the-modelpy-file",level:3},{value:"Step 4: Test the Model Locally",id:"step-4-test-the-model-locally",level:3},{value:"Step 5: Upload the Model to Clarifai",id:"step-5-upload-the-model-to-clarifai",level:3},{value:"Examples",id:"examples",level:2},{value:"Image Classifier",id:"image-classifier",level:3},{value:"<code>model.py</code>",id:"modelpy",level:4},{value:"<code>requirements.txt</code>",id:"requirementstxt",level:4},{value:"<code>config.yaml</code>",id:"configyaml",level:4},{value:"Image Detector",id:"image-detector",level:3},{value:"<code>model.py</code>",id:"modelpy-1",level:4},{value:"<code>requirements.txt</code>",id:"requirementstxt-1",level:4},{value:"<code>config.yaml</code>",id:"configyaml-1",level:4},{value:"Large Language Models (LLMs)",id:"large-language-models-llms",level:3},{value:"<code>model.py</code>",id:"modelpy-2",level:4},{value:"<code>requirements.txt</code>",id:"requirementstxt-2",level:4},{value:"<code>config.yaml</code>",id:"configyaml-2",level:4},{value:"Speech Recognition Model",id:"speech-recognition-model",level:3},{value:"<code>model.py</code>",id:"modelpy-3",level:4},{value:"<code>requirements.txt</code>",id:"requirementstxt-3",level:4},{value:"<code>config.yaml</code>",id:"configyaml-3",level:4}];function q(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",li:"li",mdxAdmonitionTitle:"mdxAdmonitionTitle",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h1,{id:"model-upload",children:"Model Upload"}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Learn how to upload a model using Clarifai SDKs"})}),"\n",(0,o.jsx)("hr",{}),"\n",(0,o.jsx)(n.p,{children:"The Clarifai SDKs allow you to upload custom models easily. Whether you're working with a pre-trained model from an external source or one you've built from scratch, Clarifai allows seamless integration of your models, enabling you to take advantage of the platform\u2019s powerful capabilities."}),"\n",(0,o.jsx)(n.p,{children:"Once uploaded, your model can be utilized alongside Clarifai's vast suite of AI tools. It will be automatically deployed and ready to be evaluated, combined with other models and agent operators in a workflow, or used to serve inference requests as it is."}),"\n",(0,o.jsx)(n.p,{children:"Let\u2019s demonstrate how you can successfully upload different types of models to the Clarifai platform."}),"\n",(0,o.jsx)(n.admonition,{type:"info",children:(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:["This new feature is still in ",(0,o.jsx)(n.a,{href:"https://docs.clarifai.com/product-updates/changelog/release-types",children:"private preview"}),". If you'd like to test it out and provide feedback, please request access ",(0,o.jsx)(n.a,{href:"https://forms.gle/MSx7QNxmug2oFZYD6",children:"here"}),"."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:["This new upload experience is compatible with the latest ",(0,o.jsx)(n.a,{href:"https://github.com/Clarifai/clarifai-python",children:(0,o.jsx)(n.code,{children:"clarifai"})})," Python package, starting from version 10.9.1."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:["If you prefer the previous upload method, which is supported up to version 10.8.4, you can refer to the documentation ",(0,o.jsx)(n.a,{target:"_blank","data-noBrokenLinkCheck":!0,href:t(28175).A+"",children:"here"}),"."]}),"\n"]}),"\n"]})}),"\n",(0,o.jsx)(n.admonition,{type:"tip",children:(0,o.jsxs)(n.p,{children:["You can run the following command to clone the repository containing examples of how to upload various model types and follow along with this documentation:\n",(0,o.jsx)(n.code,{children:"git clone https://github.com/Clarifai/examples.git"}),". After cloning it, go to the ",(0,o.jsx)(n.code,{children:"models/model_upload"})," folder."]})}),"\n",(0,o.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,o.jsx)(n.h3,{id:"installation",children:"Installation"}),"\n",(0,o.jsxs)(n.p,{children:["To begin, install the latest version of the ",(0,o.jsx)(n.code,{children:"clarifai"})," Python package."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-text",children:"pip install --upgrade clarifai\n"})}),"\n",(0,o.jsx)(n.h3,{id:"environment-set-up",children:"Environment Set Up"}),"\n",(0,o.jsxs)(n.p,{children:["Authenticate your connection to the Clarifai platform by setting your ",(0,o.jsx)(n.code,{children:"CLARIFAI_PAT "}),"(Personal Access Token) as an environment variable."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-text",children:"export CLARIFAI_PAT=YOUR_PERSONAL_ACCESS_TOKEN_HERE\n"})}),"\n",(0,o.jsx)(n.h3,{id:"create-project-directory",children:"Create Project Directory"}),"\n",(0,o.jsx)(n.p,{children:"Create a project directory and organize your files as indicated below to fit the requirements of uploading models to the Clarifai platform."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-text",children:"your_model_directory/\n\u251c\u2500\u2500 1/\n\u2502   \u2514\u2500\u2500 model.py\n\u251c\u2500\u2500 requirements.txt\n\u2514\u2500\u2500 config.yaml\n"})}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"your_model_directory/"})," \u2013 The main directory containing your model files.","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"1/"})," \u2013 A subdirectory that holds the model file (",(0,o.jsxs)(n.em,{children:["Note that the folder is named as ",(0,o.jsx)(n.strong,{children:"1"})]}),").","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"model.py"})," \u2013 Contains the code that defines your model, including loading the model and running inference."]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"requirements.txt"})," \u2013 Lists the Python libraries and dependencies required to run your model."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"config.yaml"})," \u2013 Contains model metadata and configuration details necessary for building the Docker image, defining compute resources, and uploading the model to Clarifai."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"how-to-upload-a-model",children:"How to Upload a Model"}),"\n",(0,o.jsx)(n.p,{children:"Let's talk about the common steps you'd follow to upload any type of model to the Clarifai platform."}),"\n",(0,o.jsxs)(n.h3,{id:"step-1-define-the-configyaml-file",children:["Step 1: Define the ",(0,o.jsx)(n.code,{children:"config.yaml"})," File"]}),"\n",(0,o.jsxs)(n.p,{children:["The ",(0,o.jsx)(n.code,{children:"config.yaml"})," file is essential for specifying the model\u2019s metadata, compute resource requirements, and model checkpoints."]}),"\n",(0,o.jsx)(n.p,{children:"Here\u2019s a breakdown of the key sections in the file."}),"\n",(0,o.jsx)(n.h4,{id:"model-info",children:"Model Info"}),"\n",(0,o.jsx)(n.p,{children:"This section defines your model ID, Clarifai user ID, and Clarifai app ID, which will determine where the model is uploaded on the Clarifai platform."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-yaml",children:'model:\n  id: "model_id"\n  user_id: "user_id"\n  app_id: "app_id"\n  model_type_id: "text-to-text" # Change this based on your model type (e.g., image-classifier, text-to-text)\n'})}),"\n",(0,o.jsx)(n.h4,{id:"compute-resources",children:"Compute Resources"}),"\n",(0,o.jsx)(n.p,{children:"Here, you define the minimum compute resources required for running your model, including CPU, memory, and optional GPU specifications."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-yaml",children:'inference_compute_info:\n  cpu_limit: "2"\n  cpu_memory: "13Gi"\n  num_accelerators: 1\n  accelerator_type: ["NVIDIA-A10G"] # Specify the GPU type if needed\n  accelerator_memory: "15Gi"\n'})}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:(0,o.jsx)(n.code,{children:"cpu_limit"})}),' \u2013 Number of CPUs allocated for the model (follows Kubernetes notation, e.g., "1", "2").']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:(0,o.jsx)(n.code,{children:"cpu_memory"})}),' \u2013 Minimum memory required for the CPU (uses Kubernetes notation, e.g., "1Gi", "1500Mi", "3Gi").']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:(0,o.jsx)(n.code,{children:"num_accelerators"})})," \u2013 Number of GPUs or TPUs to use for inference."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:(0,o.jsx)(n.code,{children:"accelerator_type"})}),' \u2013 Specifies the type of accelerators (e.g., GPU or TPU) supported by the model (e.g., "NVIDIA-A10G").']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:(0,o.jsx)(n.code,{children:"accelerator_memory"})})," \u2013 Minimum memory required for the GPU or TPU."]}),"\n"]}),"\n",(0,o.jsx)(n.h4,{id:"model-checkpoints",children:"Model Checkpoints"}),"\n",(0,o.jsx)(n.p,{children:"If you're using a model from Hugging Face, you can automatically download its checkpoints by specifying the appropriate configuration in this section. For private or restricted Hugging Face repositories, include an access token."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-yaml",children:'checkpoints:\n  type: "huggingface"\n  repo_id: "meta-llama/Meta-Llama-3-8B-Instruct"\n  hf_token: "your_hf_token" # Required for private models\n'})}),"\n",(0,o.jsx)(n.h4,{id:"model-concepts-or-labels",children:"Model Concepts or Labels"}),"\n",(0,o.jsx)(n.admonition,{type:"important",children:(0,o.jsx)(n.p,{children:"This section is required if your model outputs concepts or labels and is not being directly loaded from Hugging Face."})}),"\n",(0,o.jsxs)(n.p,{children:["For models that output concepts or labels, such as classification or detection models, you must define a ",(0,o.jsx)(n.code,{children:"concepts"})," section in the ",(0,o.jsx)(n.code,{children:"config.yaml"})," file:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-yaml",children:"concepts:\n  - id: '0'\n    name: bus\n  - id: '1'\n    name: person\n  - id: '2'\n    name: bicycle\n  - id: '3'\n    name: car\n"})}),"\n",(0,o.jsxs)(n.admonition,{type:"note",children:[(0,o.jsx)(n.mdxAdmonitionTitle,{}),(0,o.jsxs)(n.p,{children:["If you're using a model from Hugging Face and the ",(0,o.jsx)(n.code,{children:"checkpoints"})," section is defined, the Clarifai platform will automatically infer concepts. In this case, you don\u2019t need to manually specify them."]})]}),"\n",(0,o.jsxs)(n.h3,{id:"step-2-define-dependencies-in-requirementstxt",children:["Step 2: Define Dependencies in ",(0,o.jsx)(n.code,{children:"requirements.txt"})]}),"\n",(0,o.jsxs)(n.p,{children:["The ",(0,o.jsx)(n.code,{children:"requirements.txt"})," file lists all the Python dependencies your model needs. This ensures that the necessary libraries are installed in the runtime environment."]}),"\n",(0,o.jsxs)(n.h3,{id:"step-3-prepare-the-modelpy-file",children:["Step 3: Prepare the ",(0,o.jsx)(n.code,{children:"model.py"})," File"]}),"\n",(0,o.jsxs)(n.p,{children:["The ",(0,o.jsx)(n.code,{children:"model.py"})," file contains the logic for your model, including how it loads and handles predictions. This file must implement a class that inherits from ",(0,o.jsx)(n.code,{children:"ModelRunner"})," and defines the following methods:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:(0,o.jsx)(n.code,{children:"load_model()"})})," \u2013 Initializes and loads the model, preparing it for inference."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:(0,o.jsx)(n.code,{children:"predict(input_data)"})})," \u2013 Handles the core logic for making predictions. It processes the input data and returns the output response."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:(0,o.jsx)(n.code,{children:"generate(input_data)"})})," \u2013 Provides output in a streaming manner, if applicable to the model's use case."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:(0,o.jsx)(n.code,{children:"stream(input_data)"})})," \u2013 Manages both streaming input and output, primarily for more advanced use cases where data is processed continuously."]}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"from clarifai.runners.models.model_runner import ModelRunner\n\nclass YourCustomModelRunner(ModelRunner):\n    def load_model(self):\n        # Initialize and load the model here\n        pass\n\n    def predict(self, request):\n        # Handle input and return the model's predictions\n        return output_data\n\n    def generate(self, request):\n        # Handle streaming output (if applicable)\n        pass\n\n    def stream(self, request):\n        # Handle both streaming input and output\n        pass\n"})}),"\n",(0,o.jsx)(n.h3,{id:"step-4-test-the-model-locally",children:"Step 4: Test the Model Locally"}),"\n",(0,o.jsxs)(n.p,{children:["Before uploading your model to the Clarifai platform, it's important to test it locally to catch any typos or misconfigurations in the code. This can prevent upload failures due to issues in the ",(0,o.jsx)(n.code,{children:"model.py"})," or incorrect model implementation."]}),"\n",(0,o.jsx)(n.p,{children:"It also ensures the model runs smoothly and that all dependencies are correctly configured."}),"\n",(0,o.jsx)(n.p,{children:"To run your model locally, use the following command:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"python -m clarifai.runners.models.model_run_locally --model_path <model_directory_path>\n"})}),"\n",(0,o.jsx)(n.admonition,{type:"tip",children:(0,o.jsxs)(n.p,{children:["You can add a dot (",(0,o.jsx)(n.code,{children:"."}),") to the model path like this: ",(0,o.jsx)(n.code,{children:"python -m clarifai.runners.models.model_run_locally --model_path ."})," This means you're specifying the current directory as the model path \u2013 the command will look for the model files (such as ",(0,o.jsx)(n.code,{children:"model.py"}),", ",(0,o.jsx)(n.code,{children:"requirements.txt"}),", and ",(0,o.jsx)(n.code,{children:"config.yaml"}),") within the directory where you're executing the command."]})}),"\n",(0,o.jsx)(n.admonition,{type:"warning",children:(0,o.jsx)(n.p,{children:"Ensure your local environment has sufficient memory and compute resources to load and run the model for testing."})}),"\n",(0,o.jsx)(n.h3,{id:"step-5-upload-the-model-to-clarifai",children:"Step 5: Upload the Model to Clarifai"}),"\n",(0,o.jsx)(n.p,{children:"Once your model is ready, upload it to the Clarifai platform by running the following command:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"python -m clarifai.runners.models.model_upload --model_path <model_directory_path>\n"})}),"\n",(0,o.jsx)(n.p,{children:"This command builds the model\u2019s Docker image using the defined compute resources and uploads it to Clarifai, where it can be served in production."}),"\n","\n","\n","\n","\n","\n",(0,o.jsx)(n.h2,{id:"examples",children:"Examples"}),"\n",(0,o.jsx)(n.h3,{id:"image-classifier",children:"Image Classifier"}),"\n",(0,o.jsx)(n.h4,{id:"modelpy",children:(0,o.jsx)(n.code,{children:"model.py"})}),"\n",(0,o.jsx)(r.A,{children:(0,o.jsx)(s.A,{value:"python",label:"Python",children:(0,o.jsx)(a.A,{className:"language-python",children:l})})}),"\n",(0,o.jsx)(n.h4,{id:"requirementstxt",children:(0,o.jsx)(n.code,{children:"requirements.txt"})}),"\n",(0,o.jsx)(r.A,{children:(0,o.jsx)(s.A,{value:"text",label:"Text",children:(0,o.jsx)(a.A,{className:"language-text",children:d})})}),"\n",(0,o.jsx)(n.h4,{id:"configyaml",children:(0,o.jsx)(n.code,{children:"config.yaml"})}),"\n",(0,o.jsx)(r.A,{children:(0,o.jsx)(s.A,{value:"yaml",label:"YAML",children:(0,o.jsx)(a.A,{className:"language-yaml",children:u})})}),"\n",(0,o.jsx)(n.h3,{id:"image-detector",children:"Image Detector"}),"\n",(0,o.jsx)(n.h4,{id:"modelpy-1",children:(0,o.jsx)(n.code,{children:"model.py"})}),"\n",(0,o.jsx)(r.A,{children:(0,o.jsx)(s.A,{value:"python",label:"Python",children:(0,o.jsx)(a.A,{className:"language-python",children:c})})}),"\n",(0,o.jsx)(n.h4,{id:"requirementstxt-1",children:(0,o.jsx)(n.code,{children:"requirements.txt"})}),"\n",(0,o.jsx)(r.A,{children:(0,o.jsx)(s.A,{value:"text",label:"Text",children:(0,o.jsx)(a.A,{className:"language-text",children:p})})}),"\n",(0,o.jsx)(n.h4,{id:"configyaml-1",children:(0,o.jsx)(n.code,{children:"config.yaml"})}),"\n",(0,o.jsx)(r.A,{children:(0,o.jsx)(s.A,{value:"yaml",label:"YAML",children:(0,o.jsx)(a.A,{className:"language-yaml",children:m})})}),"\n",(0,o.jsx)(n.h3,{id:"large-language-models-llms",children:"Large Language Models (LLMs)"}),"\n",(0,o.jsx)(n.h4,{id:"modelpy-2",children:(0,o.jsx)(n.code,{children:"model.py"})}),"\n",(0,o.jsx)(r.A,{children:(0,o.jsx)(s.A,{value:"python",label:"Python",children:(0,o.jsx)(a.A,{className:"language-python",children:h})})}),"\n",(0,o.jsx)(n.h4,{id:"requirementstxt-2",children:(0,o.jsx)(n.code,{children:"requirements.txt"})}),"\n",(0,o.jsx)(r.A,{children:(0,o.jsx)(s.A,{value:"text",label:"Text",children:(0,o.jsx)(a.A,{className:"language-text",children:f})})}),"\n",(0,o.jsx)(n.h4,{id:"configyaml-2",children:(0,o.jsx)(n.code,{children:"config.yaml"})}),"\n",(0,o.jsx)(r.A,{children:(0,o.jsx)(s.A,{value:"yaml",label:"YAML",children:(0,o.jsx)(a.A,{className:"language-yaml",children:_})})}),"\n",(0,o.jsx)(n.admonition,{type:"tip",children:(0,o.jsx)(n.p,{children:"You can refer to the examples repository mentioned above for additional examples of uploading other large language models (LLMs)."})}),"\n",(0,o.jsx)(n.h3,{id:"speech-recognition-model",children:"Speech Recognition Model"}),"\n",(0,o.jsx)(n.h4,{id:"modelpy-3",children:(0,o.jsx)(n.code,{children:"model.py"})}),"\n",(0,o.jsx)(r.A,{children:(0,o.jsx)(s.A,{value:"python",label:"Python",children:(0,o.jsx)(a.A,{className:"language-python",children:g})})}),"\n",(0,o.jsx)(n.h4,{id:"requirementstxt-3",children:(0,o.jsx)(n.code,{children:"requirements.txt"})}),"\n",(0,o.jsx)(r.A,{children:(0,o.jsx)(s.A,{value:"text",label:"Text",children:(0,o.jsx)(a.A,{className:"language-text",children:x})})}),"\n",(0,o.jsx)(n.h4,{id:"configyaml-3",children:(0,o.jsx)(n.code,{children:"config.yaml"})}),"\n",(0,o.jsx)(r.A,{children:(0,o.jsx)(s.A,{value:"yaml",label:"YAML",children:(0,o.jsx)(a.A,{className:"language-yaml",children:y})})})]})}function I(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(q,{...e})}):q(e)}},19365:(e,n,t)=>{t.d(n,{A:()=>s});t(96540);var o=t(18215);const i={tabItem:"tabItem_Ymn6"};var r=t(74848);function s(e){let{children:n,hidden:t,className:s}=e;return(0,r.jsx)("div",{role:"tabpanel",className:(0,o.A)(i.tabItem,s),hidden:t,children:n})}},11470:(e,n,t)=>{t.d(n,{A:()=>j});var o=t(96540),i=t(18215),r=t(23104),s=t(56347),a=t(205),l=t(57485),d=t(31682),u=t(70679);function c(e){return o.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,o.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function p(e){const{values:n,children:t}=e;return(0,o.useMemo)((()=>{const e=n??function(e){return c(e).map((e=>{let{props:{value:n,label:t,attributes:o,default:i}}=e;return{value:n,label:t,attributes:o,default:i}}))}(t);return function(e){const n=(0,d.X)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,t])}function m(e){let{value:n,tabValues:t}=e;return t.some((e=>e.value===n))}function h(e){let{queryString:n=!1,groupId:t}=e;const i=(0,s.W6)(),r=function(e){let{queryString:n=!1,groupId:t}=e;if("string"==typeof n)return n;if(!1===n)return null;if(!0===n&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:n,groupId:t});return[(0,l.aZ)(r),(0,o.useCallback)((e=>{if(!r)return;const n=new URLSearchParams(i.location.search);n.set(r,e),i.replace({...i.location,search:n.toString()})}),[r,i])]}function f(e){const{defaultValue:n,queryString:t=!1,groupId:i}=e,r=p(e),[s,l]=(0,o.useState)((()=>function(e){let{defaultValue:n,tabValues:t}=e;if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(n){if(!m({value:n,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${n}" but none of its children has the corresponding value. Available values are: ${t.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return n}const o=t.find((e=>e.default))??t[0];if(!o)throw new Error("Unexpected error: 0 tabValues");return o.value}({defaultValue:n,tabValues:r}))),[d,c]=h({queryString:t,groupId:i}),[f,_]=function(e){let{groupId:n}=e;const t=function(e){return e?`docusaurus.tab.${e}`:null}(n),[i,r]=(0,u.Dv)(t);return[i,(0,o.useCallback)((e=>{t&&r.set(e)}),[t,r])]}({groupId:i}),g=(()=>{const e=d??f;return m({value:e,tabValues:r})?e:null})();(0,a.A)((()=>{g&&l(g)}),[g]);return{selectedValue:s,selectValue:(0,o.useCallback)((e=>{if(!m({value:e,tabValues:r}))throw new Error(`Can't select invalid tab value=${e}`);l(e),c(e),_(e)}),[c,_,r]),tabValues:r}}var _=t(92303);const g={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var x=t(74848);function y(e){let{className:n,block:t,selectedValue:o,selectValue:s,tabValues:a}=e;const l=[],{blockElementScrollPositionUntilNextRender:d}=(0,r.a_)(),u=e=>{const n=e.currentTarget,t=l.indexOf(n),i=a[t].value;i!==o&&(d(n),s(i))},c=e=>{let n=null;switch(e.key){case"Enter":u(e);break;case"ArrowRight":{const t=l.indexOf(e.currentTarget)+1;n=l[t]??l[0];break}case"ArrowLeft":{const t=l.indexOf(e.currentTarget)-1;n=l[t]??l[l.length-1];break}}n?.focus()};return(0,x.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,i.A)("tabs",{"tabs--block":t},n),children:a.map((e=>{let{value:n,label:t,attributes:r}=e;return(0,x.jsx)("li",{role:"tab",tabIndex:o===n?0:-1,"aria-selected":o===n,ref:e=>l.push(e),onKeyDown:c,onClick:u,...r,className:(0,i.A)("tabs__item",g.tabItem,r?.className,{"tabs__item--active":o===n}),children:t??n},n)}))})}function b(e){let{lazy:n,children:t,selectedValue:i}=e;const r=(Array.isArray(t)?t:[t]).filter(Boolean);if(n){const e=r.find((e=>e.props.value===i));return e?(0,o.cloneElement)(e,{className:"margin-top--md"}):null}return(0,x.jsx)("div",{className:"margin-top--md",children:r.map(((e,n)=>(0,o.cloneElement)(e,{key:n,hidden:e.props.value!==i})))})}function v(e){const n=f(e);return(0,x.jsxs)("div",{className:(0,i.A)("tabs-container",g.tabList),children:[(0,x.jsx)(y,{...n,...e}),(0,x.jsx)(b,{...n,...e})]})}function j(e){const n=(0,_.A)();return(0,x.jsx)(v,{...e,children:c(e.children)},String(n))}},28175:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/files/old_model_upload_method-9985c40b5c542a77c426504974559818.pdf"}}]);
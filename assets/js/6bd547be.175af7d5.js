"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[7086],{11470:(e,n,t)=>{t.d(n,{A:()=>I});var o=t(96540),r=t(18215),a=t(23104),s=t(56347),l=t(205),i=t(57485),c=t(31682),d=t(70679);function u(e){return o.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,o.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function h(e){const{values:n,children:t}=e;return(0,o.useMemo)(()=>{const e=n??function(e){return u(e).map(({props:{value:e,label:n,attributes:t,default:o}})=>({value:e,label:n,attributes:t,default:o}))}(t);return function(e){const n=(0,c.XI)(e,(e,n)=>e.value===n.value);if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[n,t])}function p({value:e,tabValues:n}){return n.some(n=>n.value===e)}function m({queryString:e=!1,groupId:n}){const t=(0,s.W6)(),r=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,i.aZ)(r),(0,o.useCallback)(e=>{if(!r)return;const n=new URLSearchParams(t.location.search);n.set(r,e),t.replace({...t.location,search:n.toString()})},[r,t])]}function f(e){const{defaultValue:n,queryString:t=!1,groupId:r}=e,a=h(e),[s,i]=(0,o.useState)(()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!p({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const t=n.find(e=>e.default)??n[0];if(!t)throw new Error("Unexpected error: 0 tabValues");return t.value}({defaultValue:n,tabValues:a})),[c,u]=m({queryString:t,groupId:r}),[f,g]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[t,r]=(0,d.Dv)(n);return[t,(0,o.useCallback)(e=>{n&&r.set(e)},[n,r])]}({groupId:r}),v=(()=>{const e=c??f;return p({value:e,tabValues:a})?e:null})();(0,l.A)(()=>{v&&i(v)},[v]);return{selectedValue:s,selectValue:(0,o.useCallback)(e=>{if(!p({value:e,tabValues:a}))throw new Error(`Can't select invalid tab value=${e}`);i(e),u(e),g(e)},[u,g,a]),tabValues:a}}var g=t(92303);const v={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var x=t(74848);function y({className:e,block:n,selectedValue:t,selectValue:o,tabValues:s}){const l=[],{blockElementScrollPositionUntilNextRender:i}=(0,a.a_)(),c=e=>{const n=e.currentTarget,r=l.indexOf(n),a=s[r].value;a!==t&&(i(n),o(a))},d=e=>{let n=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const t=l.indexOf(e.currentTarget)+1;n=l[t]??l[0];break}case"ArrowLeft":{const t=l.indexOf(e.currentTarget)-1;n=l[t]??l[l.length-1];break}}n?.focus()};return(0,x.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,r.A)("tabs",{"tabs--block":n},e),children:s.map(({value:e,label:n,attributes:o})=>(0,x.jsx)("li",{role:"tab",tabIndex:t===e?0:-1,"aria-selected":t===e,ref:e=>{l.push(e)},onKeyDown:d,onClick:c,...o,className:(0,r.A)("tabs__item",v.tabItem,o?.className,{"tabs__item--active":t===e}),children:n??e},e))})}function _({lazy:e,children:n,selectedValue:t}){const a=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=a.find(e=>e.props.value===t);return e?(0,o.cloneElement)(e,{className:(0,r.A)("margin-top--md",e.props.className)}):null}return(0,x.jsx)("div",{className:"margin-top--md",children:a.map((e,n)=>(0,o.cloneElement)(e,{key:n,hidden:e.props.value!==t}))})}function b(e){const n=f(e);return(0,x.jsxs)("div",{className:(0,r.A)("tabs-container",v.tabList),children:[(0,x.jsx)(y,{...n,...e}),(0,x.jsx)(_,{...n,...e})]})}function I(e){const n=(0,g.A)();return(0,x.jsx)(b,{...e,children:u(e.children)},String(n))}},19365:(e,n,t)=>{t.d(n,{A:()=>s});t(96540);var o=t(18215);const r={tabItem:"tabItem_Ymn6"};var a=t(74848);function s({children:e,hidden:n,className:t}){return(0,a.jsx)("div",{role:"tabpanel",className:(0,o.A)(r.tabItem,t),hidden:n,children:e})}},68899:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>x,contentTitle:()=>v,default:()=>b,frontMatter:()=>g,metadata:()=>o,toc:()=>y});const o=JSON.parse('{"id":"compute/local-runners/vllm","title":"vLLM","description":"Download and serve vLLM models locally and expose them via a public API","source":"@site/docs/compute/local-runners/vllm.md","sourceDirName":"compute/local-runners","slug":"/compute/local-runners/vllm","permalink":"/compute/local-runners/vllm","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"description":"Download and serve vLLM models locally and expose them via a public API","sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"LM Studio","permalink":"/compute/local-runners/lmstudio"},"next":{"title":"Agents","permalink":"/compute/agents/"}}');var r=t(74848),a=t(28453),s=t(11470),l=t(19365),i=t(73748);const c="clarifai model init --toolkit vllm --model-name HuggingFaceH4/zephyr-7b-beta\n[INFO] 12:37:30.485152 Parsed GitHub repository: owner=Clarifai, repo=runners-examples, branch=vllm, folder_path= |  thread=8309383360 \n[INFO] 12:37:38.228774 Files to be downloaded are:\n1. 1/model.py\n2. config.yaml\n3. requirements.txt |  thread=8309383360 \nPress Enter to continue...\n[INFO] 12:37:40.819580 Initializing model from GitHub repository: https://github.com/Clarifai/runners-examples |  thread=8309383360 \n[INFO] 12:40:28.485625 Successfully cloned repository from https://github.com/Clarifai/runners-examples (branch: vllm) |  thread=8309383360 \n[INFO] 12:40:28.494056 Updated Hugging Face model repo_id to: HuggingFaceH4/zephyr-7b-beta |  thread=8309383360 \n[INFO] 12:40:28.494107 Model initialization complete with GitHub repository |  thread=8309383360 \n[INFO] 12:40:28.494133 Next steps: |  thread=8309383360 \n[INFO] 12:40:28.494152 1. Review the model configuration |  thread=8309383360 \n[INFO] 12:40:28.494169 2. Install any required dependencies manually |  thread=8309383360 \n[INFO] 12:40:28.494186 3. Test the model locally using 'clarifai model local-test' |  thread=8309383360 ",d="clarifai login\nEnter your Clarifai user ID: alfrick\n> To authenticate, you'll need a Personal Access Token (PAT).\n> You can create one from your account settings: https://clarifai.com/alfrick/settings/security\n\nEnter your Personal Access Token (PAT) value (or type \"ENVVAR\" to use an environment variable): d6570db0fe964ce7a96c357ce84803b1\n\n> Verifying token...\n[INFO] 11:15:43.091990 Validating the Context Credentials... |  thread=8309383360 \n[INFO] 11:15:46.647300 \u2705 Context is valid |  thread=8309383360 \n\n> Let's save these credentials to a new context.\n> You can have multiple contexts to easily switch between accounts or projects.\n\nEnter a name for this context [default]:  \n\u2705 Success! You are now logged in.\nCredentials saved to the 'default' context.\n\n\ud83d\udca1 To switch contexts later, use `clarifai config use-context <name>`.\n[INFO] 11:15:54.361216 Login successful for user 'alfrick' in context 'default' |  thread=8309383360 ",u="clarifai model local-runner    \n[INFO] 12:10:30.600393 Hugging Face repo access validated |  thread=8309383360 \n[INFO] 12:10:31.857533 Detected OpenAI chat completions for Clarifai model streaming - validating stream_options... |  thread=8309383360 \n[ERROR] 12:10:31.857843 Missing configuration to track usage for OpenAI chat completion calls. Go to your model scripts and make sure to set both: 1) stream_options={'include_usage': True}2) set_output_context |  thread=8309383360 \n[INFO] 12:10:31.858162 > Checking local runner requirements... |  thread=8309383360 \n[INFO] 12:10:31.881003 Checking 19 dependencies... |  thread=8309383360 \n[INFO] 12:10:31.882531 \u2705 All 19 dependencies are installed! |  thread=8309383360 \n[INFO] 12:10:31.883637 > Verifying local runner setup... |  thread=8309383360 \n[INFO] 12:10:31.883679 Current context: default |  thread=8309383360 \n[INFO] 12:10:31.883712 Current user_id: alfrick |  thread=8309383360 \n[INFO] 12:10:31.883740 Current PAT: d6570**** |  thread=8309383360 \n[INFO] 12:10:31.886325 Current compute_cluster_id: local-runner-compute-cluster |  thread=8309383360 \n[WARNING] 12:10:32.878933 Failed to get compute cluster with ID 'local-runner-compute-cluster':\ncode: CONN_DOES_NOT_EXIST\ndescription: \"Resource does not exist\"\ndetails: \"ComputeCluster with ID \\'local-runner-compute-cluster\\' not found. Check your request fields.\"\nreq_id: \"sdk-python-11.8.2-475bf6a34c264ab2910fff508c10fe93\"\n |  thread=8309383360 \nCompute cluster not found. Do you want to create a new compute cluster alfrick/local-runner-compute-cluster? (y/n): y\n[INFO] 12:11:00.503967 Compute Cluster with ID 'local-runner-compute-cluster' is created:\ncode: SUCCESS\ndescription: \"Ok\"\nreq_id: \"sdk-python-11.8.2-1eb39e52ebb2407d822d96a17f60ba79\"\n |  thread=8309383360 \n[INFO] 12:11:00.514530 Current nodepool_id: local-runner-nodepool |  thread=8309383360 \n[WARNING] 12:11:01.457525 Failed to get nodepool with ID 'local-runner-nodepool':\ncode: CONN_DOES_NOT_EXIST\ndescription: \"Resource does not exist\"\ndetails: \"Nodepool not found. Check your request fields.\"\nreq_id: \"sdk-python-11.8.2-42bb02d8ae984f08af1620c548219830\"\n |  thread=8309383360 \nNodepool not found. Do you want to create a new nodepool alfrick/local-runner-compute-cluster/local-runner-nodepool? (y/n): y\n[INFO] 12:11:05.019304 Nodepool with ID 'local-runner-nodepool' is created:\ncode: SUCCESS\ndescription: \"Ok\"\nreq_id: \"sdk-python-11.8.2-9eba74dedf464e5d9c55309596745d01\"\n |  thread=8309383360 \n[INFO] 12:11:05.032966 Current app_id: local-runner-app |  thread=8309383360 \n[INFO] 12:11:05.308616 Current model_id: local-runner-model |  thread=8309383360 \n[WARNING] 12:11:07.729547 Attempting to patch latest version: d2ce23ed22144da1b683161fafa2d5d0 |  thread=8309383360 \n[INFO] 12:11:08.986955 Successfully patched version d2ce23ed22144da1b683161fafa2d5d0 |  thread=8309383360 \n[INFO] 12:11:08.992377 Current model version d2ce23ed22144da1b683161fafa2d5d0 |  thread=8309383360 \n[INFO] 12:11:08.992504 Creating the local runner tying this 'alfrick/local-runner-app/models/local-runner-model' model (version: d2ce23ed22144da1b683161fafa2d5d0) to the 'alfrick/local-runner-compute-cluster/local-runner-nodepool' nodepool. |  thread=8309383360 \n[INFO] 12:11:09.973825 Runner with ID '8d5c067f543847e8aa6741623d7c84e4' is created:\ncode: SUCCESS\ndescription: \"Ok\"\nreq_id: \"sdk-python-11.8.2-35702721f9324bbe8d862226c7dc2e9a\"\n |  thread=8309383360 \n[INFO] 12:11:09.985978 Current runner_id: 8d5c067f543847e8aa6741623d7c84e4 |  thread=8309383360 \n[WARNING] 12:11:10.250570 Failed to get deployment with ID local-runner-deployment:\ncode: CONN_DOES_NOT_EXIST\ndescription: \"Resource does not exist\"\ndetails: \"Deployment with ID \\'local-runner-deployment\\' not found. Check your request fields.\"\nreq_id: \"sdk-python-11.8.2-971bef87a5e948b198acd29d338d0ec8\"\n |  thread=8309383360 \nDeployment not found. Do you want to create a new deployment alfrick/local-runner-compute-cluster/local-runner-nodepool/local-runner-deployment? (y/n): y\n[INFO] 12:11:14.749509 Deployment with ID 'local-runner-deployment' is created:\ncode: SUCCESS\ndescription: \"Ok\"\nreq_id: \"sdk-python-11.8.2-7720d23dc2b947ffb719d3062b92579f\"\n |  thread=8309383360 \n[INFO] 12:11:14.761115 Current deployment_id: local-runner-deployment |  thread=8309383360 \n[INFO] 12:11:14.763204 Current model section of config.yaml: {'id': 'MODEL_ID2332', 'user_id': 'alfrick', 'app_id': 'items-app', 'model_type_id': 'text-to-text'} |  thread=8309383360 \nDo you want to backup config.yaml to config.yaml.bk then update the config.yaml with the new model information? (y/n): y\n[INFO] 12:11:17.610493 Checking 19 dependencies... |  thread=8309383360 \n[INFO] 12:11:17.612804 \u2705 All 19 dependencies are installed! |  thread=8309383360 \n[INFO] 12:11:17.612887 \u2705 Starting local runner... |  thread=8309383360 \n[INFO] 12:11:17.612959 No secrets path configured, running without secrets |  thread=8309383360 \n[INFO] 12:11:18.154539 Hugging Face repo access validated |  thread=8309383360 \n[INFO] 12:11:19.116921 Detected OpenAI chat completions for Clarifai model streaming - validating stream_options... |  thread=8309383360 \n[ERROR] 12:11:19.117170 Missing configuration to track usage for OpenAI chat completion calls. Go to your model scripts and make sure to set both: 1) stream_options={'include_usage': True}2) set_output_context |  thread=8309383360 \n[INFO] 12:11:19.729580 Hugging Face repo access validated |  thread=8309383360 \n[INFO] 12:11:20.675478 Detected OpenAI chat completions for Clarifai model streaming - validating stream_options... |  thread=8309383360 \n[ERROR] 12:11:20.675945 Missing configuration to track usage for OpenAI chat completion calls. Go to your model scripts and make sure to set both: 1) stream_options={'include_usage': True}2) set_output_context |  thread=8309383360 \n[INFO] 12:11:21.238145 Hugging Face token validated |  thread=8309383360 \n[INFO] 12:11:22.887162 Total download size: 13815.09 MB |  thread=8309383360 \n[INFO] 12:11:22.887555 Downloading model checkpoints... |  thread=8309383360 \n/Users/macbookpro/Desktop/code/venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:982: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\nFor more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n  warnings.warn(\nconfig.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 638/638 [00:00<00:00, 4.34MB/s]\neval_results.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 553/553 [00:00<00:00, 5.22MB/s]\nREADME.md: 25.6kB [00:00, 31.3MB/s]                                                                             | 0.00/638 [00:00<?, ?B/s]\nadded_tokens.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 42.0/42.0 [00:00<00:00, 506kB/s]\ngeneration_config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 111/111 [00:00<00:00, 1.07MB/s]\nall_results.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 728/728 [00:00<00:00, 11.9MB/s]\n.gitattributes: 1.52kB [00:00, 13.8MB/s]                                                                        | 0.00/111 [00:00<?, ?B/s]\nFetching 23 files:   4%|\u2588\u2588\u2588\u258c                                                                               | 1/23 [00:01<00:25,  1.17s/it]\n.gitattributes: 0.00B [00:00, ?B/s]\nmodel-00003-of-00008.safetensors:   0%|                                                                       | 0.00/1.98G [00:00<?, ?B/s]\nmodel-00004-of-00008.safetensors:   0%|                                                                       | 0.00/1.95G [00:00<?, ?B/s]\nmodel-00005-of-00008.safetensors:   0%|                                                                       | 0.00/1.98G [00:00<?, ?B/s]\nmodel-00006-of-00008.safetensors:   0%|                                                                       | 0.00/1.95G [00:00<?, ?B/s]\nmodel-00008-of-00008.safetensors:   0%|                                                                        | 0.00/816M [00:00<?, ?B/s]\nmodel-00002-of-00008.safetensors:   0%|                                                                       | 0.00/1.95G [00:00<?, ?B/s]\nmodel-00007-of-00008.safetensors:   0%|                                                                       | 0.00/1.98G [00:00<?, ?B/s]",h='import os\nfrom openai import OpenAI\n\n# Initialize the OpenAI client, pointing to Clarifai\'s API\nclient = OpenAI(     \n    base_url="https://api.clarifai.com/v2/ext/openai/v1",  # Clarifai\'s OpenAI-compatible API endpoint\n    api_key=os.environ["CLARIFAI_PAT"]  # Ensure CLARIFAI_PAT is set as an environment variable\n)\n\n# Make a chat completion request to a Clarifai-hosted model\nresponse = client.chat.completions.create(    \n    model="https://clarifai.com/<user-id>/local-runner-app/models/local-runner-model",    \n    messages=[\n        {"role": "system", "content": "You are a helpful assistant."},\n        {"role": "user", "content": "What is the future of AI?"}\n    ],  \n)\n\n# Print the model\'s response\nprint(response.choices[0].message.content)',p='import os\nimport sys\n\nfrom typing import List, Iterator\n\nfrom clarifai.runners.models.model_builder import ModelBuilder\nfrom clarifai.runners.models.openai_class import OpenAIModelClass\nfrom clarifai.runners.utils.openai_convertor import build_openai_messages\nfrom clarifai.runners.utils.data_utils import Param\nfrom clarifai.utils.logging import logger\n\nfrom openai import OpenAI\n\nPYTHON_EXEC = sys.executable\n\ndef vllm_openai_server(checkpoints, **kwargs):\n    """Start vLLM OpenAI compatible server."""\n\n    from clarifai.runners.utils.model_utils import execute_shell_command, wait_for_server, terminate_process\n    # Start building the command\n    cmds = [\n        PYTHON_EXEC, \'-m\', \'vllm.entrypoints.openai.api_server\', \'--model\', checkpoints,\n    ]\n    # Add all parameters from kwargs to the command\n    for key, value in kwargs.items():\n        if value is None:  # Skip None values\n            continue\n        param_name = key.replace(\'_\', \'-\')\n        if isinstance(value, bool):\n            if value:  # Only add the flag if True\n                cmds.append(f\'--{param_name}\')\n        else:\n            cmds.extend([f\'--{param_name}\', str(value)])\n    # Create server instance\n    server = type(\'Server\', (), {\n        \'host\': kwargs.get(\'host\', \'0.0.0.0\'),\n        \'port\': kwargs.get(\'port\', 23333),\n        \'backend\': "vllm",\n        \'process\': None\n    })()\n\n    try:\n        server.process = execute_shell_command(" ".join(cmds))\n        logger.info("Waiting for " + f"http://{server.host}:{server.port}")\n        wait_for_server(f"http://{server.host}:{server.port}")\n        logger.info("Server started successfully at " + f"http://{server.host}:{server.port}")\n    except Exception as e:\n        logger.error(f"Failed to start vllm server: {str(e)}")\n        if server.process:\n            terminate_process(server.process)\n        raise RuntimeError(f"Failed to start vllm server: {str(e)}")\n\n    return server\n\nclass VLLMModel(OpenAIModelClass):\n    """\n    A Model that integrates with the Clarifai platform and uses vLLM framework for inference to run the Llama 3.1 8B model with tool calling capabilities.\n    """\n    client = True  # This will be set in load_model method\n    model = True  # This will be set in load_model method\n\n    def load_model(self):\n        """Load the model here and start the  server."""\n        os.path.join(os.path.dirname(__file__))\n        # This is the path to the chat template file and you can get this chat template from vLLM repo(https://github.com/vllm-project/vllm/blob/main/examples/tool_chat_template_llama3.1_json.jinja)\n        chat_template = \'examples/tool_chat_template_llama3.1_json.jinja\'\n\n        server_args = {\n            \'max_model_len\': 2048,\n            \'dtype\': \'auto\',\n            \'task\': \'auto\',\n            \'kv_cache_dtype\': \'auto\',\n            \'tensor_parallel_size\': 1,\n            \'quantization\': None,\n            \'cpu_offload_gb\': 5.0,\n            \'chat_template\': None,\n            \'port\': 23333,\n            \'host\': \'localhost\',\n        }\n\n        model_path = os.path.dirname(os.path.dirname(__file__))\n        builder = ModelBuilder(model_path, download_validation_only=True)\n        model_config = builder.config\n\n        stage = model_config["checkpoints"][\'when\']\n        checkpoints = builder.config["checkpoints"][\'repo_id\']\n        if stage in ["build", "runtime"]:\n            checkpoints = builder.download_checkpoints(stage=stage)\n\n        # Start server\n        self.server = vllm_openai_server(checkpoints, **server_args)\n        # CLIent initialization\n        self.client = OpenAI(\n                api_key="notset",\n                base_url=f\'http://{self.server.host}:{self.server.port}/v1\')\n        self.model = self.client.models.list().data[0].id\n\n    @OpenAIModelClass.method\n    def predict(self,\n                prompt: str,\n                chat_history: List[dict] = None,\n                tools: List[dict] = None,\n                tool_choice: str = None,\n                max_tokens: int = Param(default=512, description="The maximum number of tokens to generate. Shorter token lengths will provide faster performance.", ),\n                temperature: float = Param(default=0.7, description="A decimal number that determines the degree of randomness in the response", ),\n                top_p: float = Param(default=0.95, description="An alternative to sampling with temperature, where the model considers the results of the tokens with top_p probability mass."),\n                ) -> str:\n        """\n        This method is used to predict the response for the given prompt and chat history using the model and tools.\n        """\n        if tools is not None and tool_choice is None:\n            tool_choice = "auto"\n\n        messages = build_openai_messages(prompt=prompt, messages=chat_history)\n        response = self.client.chat.completions.create(\n            model=self.model,\n            messages=messages,\n            tools=tools,\n            tool_choice=tool_choice,\n            max_completion_tokens=max_tokens,\n            temperature=temperature,\n            top_p=top_p)\n\n        if response.choices[0] and response.choices[0].message.tool_calls:\n            import json\n            # If the response contains tool calls, return as a string\n\n            tool_calls = response.choices[0].message.tool_calls\n            tool_calls_json = json.dumps([tc.to_dict() for tc in tool_calls], indent=2)\n            return tool_calls_json\n        else:\n            # Otherwise, return the content of the first choice\n            return response.choices[0].message.content\n\n\n    @OpenAIModelClass.method\n    def generate(self,\n                prompt: str,\n                chat_history: List[dict] = None,\n                tools: List[dict] = None,\n                tool_choice: str = None,\n                max_tokens: int = Param(default=512, description="The maximum number of tokens to generate. Shorter token lengths will provide faster performance.", ),\n                temperature: float = Param(default=0.7, description="A decimal number that determines the degree of randomness in the response", ),\n                top_p: float = Param(default=0.95, description="An alternative to sampling with temperature, where the model considers the results of the tokens with top_p probability mass.")) -> Iterator[str]:\n        """\n        This method is used to stream generated text tokens from a prompt + optional chat history and tools.\n        """\n        messages = build_openai_messages(prompt=prompt, messages=chat_history)\n        response = self.client.chat.completions.create(\n            model=self.model,\n            messages=messages,\n            tools=tools,\n            tool_choice=tool_choice,\n            max_completion_tokens=max_tokens,\n            temperature=temperature,\n            top_p=top_p,\n            stream=True)\n\n        for chunk in response:\n            if chunk.choices:\n                if chunk.choices[0].delta.tool_calls:\n                    # If the response contains tool calls, return the first one as a string\n                    import json\n                    tool_calls = chunk.choices[0].delta.tool_calls\n                    tool_calls_json = [tc.to_dict() for tc in tool_calls]\n                    # Convert to JSON string\n                    json_string = json.dumps(tool_calls_json, indent=2)\n                    # Yield the JSON string\n                    yield json_string\n                else:\n                    # Otherwise, return the content of the first choice\n                    text = (chunk.choices[0].delta.content\n                            if (chunk and chunk.choices[0].delta.content) is not None else \'\')\n                    yield text',m="build_info:\n  python_version: '3.12'\ncheckpoints:\n  hf_token: hf_token\n  repo_id: HuggingFaceH4/zephyr-7b-beta\n  type: huggingface\n  when: runtime\ninference_compute_info:\n  accelerator_memory: 5Gi\n  accelerator_type:\n  - NVIDIA-*\n  cpu_limit: '1'\n  cpu_memory: 5Gi\n  num_accelerators: 1\nmodel:\n  app_id: APP_ID\n  id: MODEL_ID\n  model_type_id: text-to-text\n  user_id: USER_ID\n",f="# Core dependencies\ntorch==2.5.1\nvllm==0.8.0\ntransformers==4.50.1\naccelerate==1.2.0\noptimum==1.23.3\neinops==0.8.0\ntokenizers==0.21.0\npackaging>=24.0\nninja>=1.11.1\n\n# Qwen and vision-language utilities\nqwen-vl-utils==0.0.8\ntimm>=1.0.9\n\n# Audio and video support\nsoundfile>=0.13.1\nlibrosa>=0.10.2\n\nscipy==1.15.2\n\n# Additional utilities\npsutil>=5.9.0\nbackoff==2.2.1\npeft>=0.13.2\nopenai>=1.14.0\nclarifai>=10.0.0\n",g={description:"Download and serve vLLM models locally and expose them via a public API",sidebar_position:4},v="vLLM",x={},y=[{value:"Step 1: Perform Prerequisites",id:"step-1-perform-prerequisites",level:2},{value:"Sign Up or Log In",id:"sign-up-or-log-in",level:3},{value:"Install the Clarifai CLI",id:"install-the-clarifai-cli",level:3},{value:"Install vLLM",id:"install-vllm",level:3},{value:"Install the OpenAI Package",id:"install-the-openai-package",level:3},{value:"Step 2: Initialize a Model",id:"step-2-initialize-a-model",level:2},{value:"<code>model.py</code>",id:"modelpy",level:3},{value:"<code>config.yaml</code>",id:"configyaml",level:3},{value:"<code>requirements.txt</code>",id:"requirementstxt",level:3},{value:"Step 3: Log In to Clarifai",id:"step-3-log-in-to-clarifai",level:2},{value:"Step 4: Start the Local Runner",id:"step-4-start-the-local-runner",level:2},{value:"Step 5: Test Your Runner",id:"step-5-test-your-runner",level:2}];function _(e){const n={a:"a",admonition:"admonition",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components},{Details:t}=n;return t||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"vllm",children:"vLLM"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Download and serve vLLM models locally and expose them via a public API"})}),"\n",(0,r.jsx)("hr",{}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"https://docs.vllm.ai/en/latest/",children:"vLLM"})," is an open-source, high-performance inference engine that allows you to serve large language models (LLMs) locally with remarkable speed and efficiency. It supports OpenAI-compatible APIs, making it easy to integrate with the Clarifai platform."]}),"\n",(0,r.jsxs)(n.p,{children:["With Clarifai\u2019s ",(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/compute/local-runners/",children:"Local Runners"}),", you can seamlessly deploy vLLM-powered models on your own machine, expose them through a secure public URL, and take full advantage of Clarifai\u2019s AI capabilities \u2014 while retaining control, privacy, and performance."]}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Note:"})," After downloading the model using the vLLM toolkit, you can ",(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/compute/upload/#step-4-upload-the-model-to-clarifai",children:"upload"})," it to Clarifai to leverage the platform\u2019s capabilities."]}),"\n"]}),"\n","\n","\n",(0,r.jsx)(n.h2,{id:"step-1-perform-prerequisites",children:"Step 1: Perform Prerequisites"}),"\n",(0,r.jsx)(n.h3,{id:"sign-up-or-log-in",children:"Sign Up or Log In"}),"\n",(0,r.jsxs)(n.p,{children:["First, either ",(0,r.jsx)(n.a,{href:"https://clarifai.com/login",children:"log in"})," to your existing Clarifai account or ",(0,r.jsx)(n.a,{href:"https://clarifai.com/signup",children:"sign up"})," for a new one. Once logged in, you'll need these credentials to set up your project:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"App ID"})," \u2013 Navigate to the application you'll use for your model. In the collapsible left sidebar, select the ",(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/create/applications/manage/#app-overview",children:"Overview"})," option. Get the app ID from there."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"User ID"})," \u2013 In the collapsible left sidebar, go to ",(0,r.jsx)(n.strong,{children:"Settings"})," and select the ",(0,r.jsx)(n.strong,{children:"Account"})," option. Then, find your user ID."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Personal Access Token (PAT)"})," \u2013 This token is essential to authenticate your connection with the Clarifai platform. To create or copy your ",(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/control/authentication/pat",children:"PAT"}),", go to ",(0,r.jsx)(n.strong,{children:"Settings"})," and choose the ",(0,r.jsx)(n.strong,{children:"Secrets"})," option."]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Then, store it as an environment variable for secure authentication:"}),"\n",(0,r.jsxs)(s.A,{groupId:"code",children:[(0,r.jsx)(l.A,{value:"bash",label:"Unix-like Systems",children:(0,r.jsx)(i.A,{className:"language-bash",children:"export CLARIFAI_PAT=YOUR_PERSONAL_ACCESS_TOKEN_HERE"})}),(0,r.jsx)(l.A,{value:"bash2",label:"Windows",children:(0,r.jsx)(i.A,{className:"language-bash",children:"set CLARIFAI_PAT=YOUR_PERSONAL_ACCESS_TOKEN_HERE"})})]}),"\n",(0,r.jsx)(n.h3,{id:"install-the-clarifai-cli",children:"Install the Clarifai CLI"}),"\n",(0,r.jsxs)(n.p,{children:["Install the ",(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/sdk/cli",children:"Clarifai CLI"})," to access Local Runners and manage your deployments."]}),"\n",(0,r.jsx)(s.A,{groupId:"code",children:(0,r.jsx)(l.A,{value:"bash",label:"Bash",children:(0,r.jsx)(i.A,{className:"language-bash",children:"pip install --upgrade clarifai"})})}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Note:"})," Ensure you have ",(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/resources/api-overview/python-sdk#python-requirements",children:"Python 3.11 or 3.12"})," installed to run Local Runners successfully."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"install-vllm",children:"Install vLLM"}),"\n",(0,r.jsx)(n.p,{children:"Install the vLLM package."}),"\n",(0,r.jsx)(s.A,{groupId:"code",children:(0,r.jsx)(l.A,{value:"bash",label:"Bash",children:(0,r.jsx)(i.A,{className:"language-bash",children:"pip install vllm"})})}),"\n",(0,r.jsxs)(n.p,{children:["vLLM supports models from the Hugging Face Hub (e.g., LLaMA, Mistral, Falcon, etc.) and serves them via a local ",(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/compute/inference/#predict-with-openai-compatible-format",children:"OpenAI-compatible API"}),"."]}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Note"}),": You need a Hugging Face access token to download models from private or restricted repositories. You can learn how to get it from ",(0,r.jsx)(n.a,{href:"https://huggingface.co/docs/hub/en/security-tokens",children:"here"}),"."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"install-the-openai-package",children:"Install the OpenAI Package"}),"\n",(0,r.jsxs)(n.p,{children:["Install the ",(0,r.jsx)(n.code,{children:"openai"})," client library \u2014 it will be used to send requests to your vLLM server."]}),"\n",(0,r.jsx)(s.A,{groupId:"code",children:(0,r.jsx)(l.A,{value:"bash",label:"Bash",children:(0,r.jsx)(i.A,{className:"language-bash",children:"pip install openai"})})}),"\n",(0,r.jsx)(n.h2,{id:"step-2-initialize-a-model",children:"Step 2: Initialize a Model"}),"\n",(0,r.jsx)(n.p,{children:"Use the Clarifai CLI to initialize a vLLM-based model directory. This setup prepares all required files for local execution and Clarifai integration."}),"\n",(0,r.jsx)(n.p,{children:"You can further customize or optimize the model by modifying the generated files as necessary."}),"\n",(0,r.jsx)(s.A,{groupId:"code",children:(0,r.jsx)(l.A,{value:"bash",label:"Bash",children:(0,r.jsx)(i.A,{className:"language-bash",children:"clarifai model init --toolkit vllm"})})}),"\n",(0,r.jsxs)(n.admonition,{type:"tip",children:[(0,r.jsxs)(n.p,{children:["You can initialize any model ",(0,r.jsx)(n.a,{href:"https://docs.vllm.ai/en/v0.9.2/models/supported_models.html",children:"supported"})," by vLLM. If you want to initialize a specific vLLM model, use the ",(0,r.jsx)(n.code,{children:"--model-name"})," flag."]}),(0,r.jsx)(s.A,{groupId:"code",children:(0,r.jsx)(l.A,{value:"bash",label:"Bash",children:(0,r.jsx)(i.A,{className:"language-bash",children:"clarifai model init --toolkit vllm --model-name HuggingFaceH4/zephyr-7b-beta"})})})]}),"\n",(0,r.jsxs)(t,{children:[(0,r.jsx)("summary",{children:"Example Output"}),(0,r.jsx)(i.A,{className:"language-text",children:c})]}),"\n",(0,r.jsx)(n.p,{children:"You\u2019ll get a folder structure similar to:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"\u251c\u2500\u2500 1/\n\u2502   \u2514\u2500\u2500 model.py\n\u251c\u2500\u2500 requirements.txt\n\u2514\u2500\u2500 config.yaml\n"})}),"\n",(0,r.jsx)(n.h3,{id:"modelpy",children:(0,r.jsx)(n.code,{children:"model.py"})}),"\n",(0,r.jsxs)(t,{children:[(0,r.jsx)("summary",{children:"Example: model.py"}),(0,r.jsx)(i.A,{className:"language-text",children:p})]}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/compute/upload/#prepare-modelpy",children:(0,r.jsx)(n.code,{children:"model.py"})})," file inside the ",(0,r.jsx)(n.code,{children:"1/"})," directory defines how your model performs inference through the vLLM runtime, using the OpenAI-compatible API endpoint served locally."]}),"\n",(0,r.jsx)(n.h3,{id:"configyaml",children:(0,r.jsx)(n.code,{children:"config.yaml"})}),"\n",(0,r.jsxs)(t,{children:[(0,r.jsx)("summary",{children:"Example: config.yaml"}),(0,r.jsx)(i.A,{className:"language-text",children:m})]}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.code,{children:"config.yaml"})," file defines the model\u2019s configuration \u2014 including compute requirements, checkpoint sources, and other essential runtime settings."]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["In the ",(0,r.jsx)(n.code,{children:"model"})," section, provide a unique model ID (any name you prefer), along with your Clarifai user ID and app ID. These values specify where your model will be deployed within the Clarifai platform."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.a,{href:"/compute/local-runners/hf#configyaml",children:(0,r.jsx)(n.code,{children:"checkpoints"})})," section defines how to retrieve the model\u2019s weights from Hugging Face. If you\u2019re using a private or restricted repository, be sure to include your Hugging Face access token to enable secure downloading."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"requirementstxt",children:(0,r.jsx)(n.code,{children:"requirements.txt"})}),"\n",(0,r.jsxs)(t,{children:[(0,r.jsx)("summary",{children:"Example: requirements.txt"}),(0,r.jsx)(i.A,{className:"language-text",children:f})]}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.code,{children:"requirements.txt"})," file lists Python dependencies required by your model. If you haven\u2019t installed them yet, run the following command to install the dependencies:"]}),"\n",(0,r.jsx)(s.A,{groupId:"code",children:(0,r.jsx)(l.A,{value:"bash",label:"Bash",children:(0,r.jsx)(i.A,{className:"language-bash",children:"pip install -r requirements.txt"})})}),"\n",(0,r.jsx)(n.h2,{id:"step-3-log-in-to-clarifai",children:"Step 3: Log In to Clarifai"}),"\n",(0,r.jsx)(n.p,{children:"Run the following command to authenticate your local environment with Clarifai."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"clarifai login\n"})}),"\n",(0,r.jsxs)(n.p,{children:["You\u2019ll be prompted for your user ID, PAT, and an optional ",(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/compute/local-runners/#step-2-create-a-context-optional",children:"context name"}),"."]}),"\n",(0,r.jsxs)(t,{children:[(0,r.jsx)("summary",{children:"Example Output"}),(0,r.jsx)(i.A,{className:"language-text",children:d})]}),"\n",(0,r.jsx)(n.h2,{id:"step-4-start-the-local-runner",children:"Step 4: Start the Local Runner"}),"\n",(0,r.jsx)(n.p,{children:"Launch the Local Runner to start serving your vLLM model locally."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"clarifai model local-runner\n"})}),"\n",(0,r.jsx)(n.p,{children:"If any configuration is missing, the CLI will prompt you to define or confirm it."}),"\n",(0,r.jsx)(n.p,{children:"This runner will use vLLM\u2019s backend to serve model predictions and make them accessible via a Clarifai-managed public API endpoint."}),"\n",(0,r.jsxs)(t,{children:[(0,r.jsx)("summary",{children:"Example Output"}),(0,r.jsx)(i.A,{className:"language-text",children:u})]}),"\n",(0,r.jsx)(n.h2,{id:"step-5-test-your-runner",children:"Step 5: Test Your Runner"}),"\n",(0,r.jsx)(n.p,{children:"Once your Local Runner is running and the model has finished downloading, you can test it using the OpenAI-compatible API format."}),"\n",(0,r.jsx)(s.A,{groupId:"code",children:(0,r.jsx)(l.A,{value:"python",label:"Python (OpenAI)",children:(0,r.jsx)(i.A,{className:"language-python",children:h})})}),"\n",(0,r.jsxs)(n.p,{children:["The script sends a sample prompt to your locally running vLLM model and prints the response. Your model will now be serving ",(0,r.jsx)(n.a,{href:"https://docs.clarifai.com/compute/inference/clarifai/api",children:"predictions"})," through Clarifai\u2019s local inference layer."]})]})}function b(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(_,{...e})}):_(e)}}}]);
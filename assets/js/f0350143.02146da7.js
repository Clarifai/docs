"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[3747],{84166:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>O,contentTitle:()=>S,default:()=>R,frontMatter:()=>w,metadata:()=>T,toc:()=>D});var a=t(74848),s=t(28453),i=t(11470),o=t(19365),r=t(21432);const c="#########################################################################################################\n# In this section, we set the user authentication, user and app ID, model details, URL of the image\n# we want as an input, and prediction language. Change these strings to run your own example.\n#########################################################################################################\n\n# Your PAT (Personal Access Token) can be found in the Account's Security section\nPAT = 'YOUR_PAT_HERE'\n# Specify the correct user_id/app_id pairings\n# Since you're making inferences outside your app's scope\nUSER_ID = 'clarifai'\nAPP_ID = 'main'\n# Change these to whatever you want to process\nMODEL_ID = 'general-image-recognition'\nIMAGE_URL = 'https://samples.clarifai.com/metro-north.jpg'\nPREDICT_LANGUAGE = \"zh\"  # Chinese\n\n############################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n############################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (('authorization', 'Key ' + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\npost_model_outputs_response = stub.PostModelOutputs(\n    service_pb2.PostModelOutputsRequest(\n        user_app_id=userDataObject,  # The userDataObject is created in the overview and is required when using a PAT\n        model_id=MODEL_ID,  \n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    image=resources_pb2.Image(\n                        url=IMAGE_URL\n                    )\n                )\n            )\n        ],\n        model=resources_pb2.Model(\n            output_info=resources_pb2.OutputInfo(\n                output_config=resources_pb2.OutputConfig(\n                    language=PREDICT_LANGUAGE\n                )\n            )\n        )\n    ),\n    metadata=metadata\n)\n\nif post_model_outputs_response.status.code != status_code_pb2.SUCCESS:\n    print(post_model_outputs_response.status)\n    raise Exception(\"Post model outputs failed, status: \" + post_model_outputs_response.status.description)\n\n# Since we have one input, one output will exist here.\noutput = post_model_outputs_response.outputs[0]\n\nprint(\"Predicted concepts:\")\nfor concept in output.data.concepts:\n    print(\"\\t%s %.2f\" % (concept.name, concept.value))\n\n# Uncomment this line to print the raw output\n#print(output)",u="##########################################################################################\n# In this section, we set the user authentication, app ID, and concept name and language.\n# Change these strings to run your own example.\n##########################################################################################\n\nUSER_ID = 'YOUR_USER_ID_HERE'\n# Your PAT (Personal Access Token) can be found in the Account's Security section\nPAT = 'YOUR_PAT_HERE'\nAPP_ID = 'YOUR_APP_ID_HERE'\n# Change these to whatever you want to process\nCONCEPT_NAME = '\u4eba'\nCONCEPT_LANGUAGE = \"zh\"  # Chinese\n\n############################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n############################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (('authorization', 'Key ' + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\npost_concepts_searches_response = stub.PostConceptsSearches(\n    service_pb2.PostConceptsSearchesRequest(\n        user_app_id=userDataObject,  # The userDataObject is created in the overview and is required when using a PAT\n        concept_query=resources_pb2.ConceptQuery(\n            name=CONCEPT_NAME,\n            language=CONCEPT_LANGUAGE\n        )\n    ),\n    metadata=metadata\n)\n\nif post_concepts_searches_response.status.code != status_code_pb2.SUCCESS:\n    print(post_concepts_searches_response.status)\n    raise Exception(\"Post concepts searches failed, status: \" + post_concepts_searches_response.status.description)\n\nprint(\"Found concepts:\")\nfor concept in post_concepts_searches_response.concepts:\n    print(\"\\t%s %.2f\" % (concept.name, concept.value))\n\n# Uncomment this line to print the raw output\n#print(post_concepts_searches_response)",l='\x3c!--index.html file--\x3e\n\n<script>\n    ////////////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, URL of the image\n    // we want as an input, and prediction language. Change these strings to run your own example.\n    ////////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n    // Your PAT (Personal Access Token) can be found in the Account\'s Security section\n    const PAT = \'YOUR_PAT_HERE\';\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    const USER_ID = \'clarifai\';   \n    const APP_ID = \'main\';   \n    // Change these to whatever you want to process\n    const MODEL_ID = \'general-image-recognition\';\n    const MODEL_VERSION_ID = "aa7f35c01e0642fda5cf400f543e7c40";\n    const IMAGE_URL = \'https://samples.clarifai.com/metro-north.jpg\';\n    const PREDICT_LANGUAGE = "zh";  // Chinese\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    const raw = JSON.stringify({\n        "user_app_id": {\n            "user_id": USER_ID,\n            "app_id": APP_ID\n        },\n        "inputs": [\n            {\n                "data": {\n                    "image": {\n                        "url": IMAGE_URL\n                    }\n                }\n            }\n        ],\n        "model": {\n            "output_info": {\n                "output_config": {\n                    "language": PREDICT_LANGUAGE\n                }\n            }\n        }\n    });\n\n    const requestOptions = {\n        method: \'POST\',\n        headers: {\n            \'Accept\': \'application/json\',\n            \'Authorization\': \'Key \' + PAT\n        },\n        body: raw\n    };\n\n    // NOTE: MODEL_VERSION_ID is optional, you can also call prediction with the MODEL_ID only\n    // https://api.clarifai.com/v2/models/{YOUR_MODEL_ID}/outputs\n    // this will default to the latest version_id\n\n    fetch("https://api.clarifai.com/v2/models/" + MODEL_ID + "/versions/" + MODEL_VERSION_ID + "/outputs", requestOptions)\n        .then(response => response.text())\n        .then(result => console.log(result))\n        .catch(error => console.log(\'error\', error));\n<\/script>',p='\x3c!--index.html file--\x3e\n\n<script>\n    ///////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, app ID, and concept name and language.\n    // Change these strings to run your own example.\n    ///////////////////////////////////////////////////////////////////////////////////////////////\n\n    const USER_ID = \'YOUR_USER_ID_HERE\';\n    // Your PAT (Personal Access Token) can be found in the Account\'s Security section\n    const PAT = \'YOUR_PAT_HERE\';\n    const APP_ID = \'YOUR_APP_ID_HERE\';\n    // Change these to whatever you want to process\n    const CONCEPT_NAME = \'\u4eba\';\n    const CONCEPT_LANGUAGE = "zh"; // Chinese\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    const raw = JSON.stringify({\n        "user_app_id": {\n            "user_id": USER_ID,\n            "app_id": APP_ID\n        },\n        "query": {\n            "ands": [\n                {\n                    "output": {\n                        "data": {\n                            "concepts": [\n                                {\n                                    "name": CONCEPT_NAME\n                                }\n                            ]\n                        }\n                    }\n                }\n            ],\n            "language": CONCEPT_LANGUAGE\n        }\n    });\n\n    const requestOptions = {\n        method: \'POST\',\n        headers: {\n            \'Accept\': \'application/json\',\n            \'Authorization\': \'Key \' + PAT\n        },\n        body: raw\n    };\n\n    fetch("https://api.clarifai.com/v2/searches", requestOptions)\n        .then(response => response.text())\n        .then(result => console.log(result))\n        .catch(error => console.log(\'error\', error));\n<\/script>',d='//index.js file\n\n////////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, URL of the image\n// we want as an input, and prediction language. Change these strings to run your own example.\n///////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\nconst PAT = \'YOUR_PAT_HERE\';\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\nconst USER_ID = \'clarifai\';\nconst APP_ID = \'main\';\n// Change these to whatever you want to process\nconst MODEL_ID = \'general-image-recognition\';\nconst IMAGE_URL = \'https://samples.clarifai.com/metro-north.jpg\';\nconst PREDICT_LANGUAGE = "zh";  // Chinese\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\nstub.PostModelOutputs(\n    {\n        user_app_id: {\n            "user_id": USER_ID,\n            "app_id": APP_ID\n        },\n        model_id: MODEL_ID, \n        inputs: [\n            { data: { image: { url: IMAGE_URL, allow_duplicate_url: true } } }\n        ],\n        model: { output_info: { output_config: { language: PREDICT_LANGUAGE } } }\n    },\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            throw new Error("Post model outputs failed, status: " + response.status.description);\n        }\n\n        // Since we have one input, one output will exist here.\n        const output = response.outputs[0];\n\n        console.log("Predicted concepts:");\n        for (const concept of output.data.concepts) {\n            console.log(concept.name + " " + concept.value);\n        }\n    }\n\n);',h='//index.js file\n\n///////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, app ID, and concept name and language.\n// Change these strings to run your own example.\n///////////////////////////////////////////////////////////////////////////////////////////////\n\nconst USER_ID = \'YOUR_USER_ID_HERE\';\n// Your PAT (Personal Access Token) can be found in the Account\'s Security section\nconst PAT = \'YOUR_PAT_HERE\';\nconst APP_ID = \'YOUR_APP_ID_HERE\';\n// Change these to whatever you want to process\nconst CONCEPT_NAME = \'\u4eba\';\nconst CONCEPT_LANGUAGE = "zh"; // Chinese\n\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\nstub.PostConceptsSearches(\n    {\n        user_app_id: {\n            "user_id": USER_ID,\n            "app_id": APP_ID\n        },\n        concept_query: { name: CONCEPT_NAME, language: CONCEPT_LANGUAGE }\n    },\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            throw new Error("Post concepts searches failed, status: " + response.status.description);\n        }\n\n        console.log("Found concepts:");\n        for (const concept of response.concepts) {\n            console.log("\\t" + concept.name + " " + concept.value);\n        }\n    }\n\n);',g='package com.clarifai.example;\n\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.status.StatusCode;\n\npublic class ClarifaiExample {\n\n    //////////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, URL of the image\n    // we want as an input, and prediction language. Change these strings to run your own example.\n    //////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n    //Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    static final String USER_ID = "clarifai";    \n    static final String APP_ID = "main";\n    // Change these to whatever you want to process\n    static final String MODEL_ID = "general-image-recognition";\n    static final String IMAGE_URL = "https://samples.clarifai.com/metro-north.jpg";\n    static final String PREDICT_LANGUAGE = "zh"; // Chinese\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\t\n\n    public static void main(String[] args) {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n            .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        MultiOutputResponse postModelOutputsResponse = stub.postModelOutputs(\n            PostModelOutputsRequest.newBuilder()\n            .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n            .setModelId(MODEL_ID) \n            .addInputs(\n                Input.newBuilder().setData(\n                    Data.newBuilder().setImage(\n                        Image.newBuilder().setUrl(IMAGE_URL)\n                    )\n                )\n            )\n            .setModel(\n                Model.newBuilder().setOutputInfo(\n                    OutputInfo.newBuilder().setOutputConfig(\n                        OutputConfig.newBuilder().setLanguage(PREDICT_LANGUAGE)\n                    )\n                )\n            )\n            .build()\n        );\n\n        if (postModelOutputsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("Post model outputs failed, status: " + postModelOutputsResponse.getStatus());\n        }\n\n        // Since we have one input, one output will exist here\n        Output output = postModelOutputsResponse.getOutputs(0);\n\n        System.out.println("Predicted concepts:");\n        for (Concept concept: output.getData().getConceptsList()) {\n            System.out.printf("%s %.2f%n", concept.getName(), concept.getValue());\n        }\n\n    }\n\n}',m='package com.clarifai.example;\n\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.status.StatusCode;\n\npublic class ClarifaiExample {\n\n    //////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, app ID, and concept name and language.\n    // Change these strings to run your own example.\n    /////////////////////////////////////////////////////////////////////////////////////////////\t\n\n    static final String USER_ID = "YOUR_USER_ID_HERE";\n    //Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    static final String APP_ID = "YOUR_APP_ID_HERE";\n    // Change these to whatever you want to process\n    static final String CONCEPT_NAME = "\u4eba";\n    static final String CONCEPT_LANGUAGE = "zh"; // Chinese\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\t\n\n    public static void main(String[] args) {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n            .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        MultiConceptResponse postConceptsSearchesResponse = stub.postConceptsSearches(\n            PostConceptsSearchesRequest.newBuilder()\n            .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n            .setConceptQuery(\n                ConceptQuery.newBuilder()\n                .setName(CONCEPT_NAME)\n                .setLanguage(CONCEPT_LANGUAGE)\n            )\n            .build()\n        );\n\n        if (postConceptsSearchesResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("Post concepts searches failed, status: " + postConceptsSearchesResponse.getStatus());\n        }\n\n        System.out.println("Found concepts:");\n        for (Concept concept: postConceptsSearchesResponse.getConceptsList()) {\n            System.out.printf("\\t%s %.2f%n", concept.getName(), concept.getValue());\n        }\n\n    }\n\n}',_="<?php\n\nrequire __DIR__ . '/vendor/autoload.php';\n\n/////////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, URL of the image\n// we want as an input, and prediction language. Change these strings to run your own example.\n///////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the Account's Security section\n$PAT = \"YOUR_PAT_HERE\";\n// Specify the correct user_id/app_id pairings\n// Since you're making inferences outside your app's scope\n$USER_ID = \"clarifai\";\n$APP_ID = \"main\";\n// Change these to whatever you want to process\n$MODEL_ID = 'general-image-recognition';\n$MODEL_VERSION_ID = \"aa7f35c01e0642fda5cf400f543e7c40\";\n$IMAGE_URL = 'https://samples.clarifai.com/metro-north.jpg';\n$PREDICT_LANGUAGE = \"zh\";  // Chinese\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Image;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\Model;\nuse Clarifai\\Api\\OutputInfo;\nuse Clarifai\\Api\\OutputConfig;\nuse Clarifai\\Api\\PostModelOutputsRequest;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ['Authorization' => ['Key ' . $PAT ]];\n\n$userDataObject = new UserAppIDSet([\n    'user_id' => $USER_ID, \n    'app_id' => $APP_ID \n]);\n\n// Let's make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client->PostModelOutputs(\n    // The request object carries the request along with the request status and other metadata related to the request itself\n    new PostModelOutputsRequest([\n        'user_app_id' => $userDataObject,\n        'model_id' => $MODEL_ID, \n        'inputs' => [\n            new Input([ // The Input object wraps the Data object in order to meet the API specification                \n                'data' => new Data([ // The Data object is constructed around the Image object. It offers a container that has additional image independent\n                                    // metadata. In this particular use case, no other metadata is needed to be specified\n                    'image' => new Image([ // In the Clarifai platform, an image is defined by a special Image object\n                        'url' => $IMAGE_URL\n                    ])\n                ])\n            ])\n        ],\n        // The model object is a wrapper around the OutputInfo object\n        \"model\" => new Model([\n            //The OutputInfo object is a wrapper around the OutputConfig object\n            \"output_info\" => new OutputInfo([\n                // Output configuration can be specified by the OutputConfig object. Here we specify a predict language \n                // for what we want to narrow down to in the results.\n                \"output_config\" => new OutputConfig([\n                    \"language\" => $PREDICT_LANGUAGE        \n                ])\n            ])\n        ])\n\n    ]),\n    $metadata\n)->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception(\"Error: {$status->details}\");\n}\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure \n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    throw new Exception(\"Failure response: \" . $response->getStatus()->getDescription() . \" \" .\n        $response->getStatus()->getDetails());\n}\n\n// The output of a successful call can be used in many ways. In this example, we loop through all of the predicted concepts \n// and print them out along with their numerical prediction value (confidence)\necho \"Predicted concepts: </br>\";\nforeach ($response->getOutputs()[0]->getData()->getConcepts() as $concept) {\n    echo $concept->getName() . \": \" . number_format($concept->getValue(), 2) . \"</br>\";\n}\n\n?>",f="<?php\n\nrequire __DIR__ . '/vendor/autoload.php';\n\n///////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, app ID, and concept name and language.\n// Change these strings to run your own example.\n///////////////////////////////////////////////////////////////////////////////////////////////\n\n$USER_ID = \"YOUR_USER_ID_HERE\";\n// Your PAT (Personal Access Token) can be found in the Account's Security section\n$PAT = \"YOUR_PAT_HERE\";\n$APP_ID = \"YOUR_APP_ID_HERE\";\n// Change these to whatever you want to process\n$CONCEPT_NAME = '\u4eba';\n$CONCEPT_LANGUAGE = \"zh\"; // Chinese\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\ConceptQuery;\nuse Clarifai\\Api\\PostConceptsSearchesRequest;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ['Authorization' => ['Key ' . $PAT ]];\n\n$userDataObject = new UserAppIDSet([\n    'user_id' => $USER_ID, \n    'app_id' => $APP_ID \n]);\n\n// Let's make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client->PostConceptsSearches(\n    // The request object carries the request along with the request status and other metadata related to the request itself\n    new PostConceptsSearchesRequest([\n        'user_app_id' => $userDataObject,\n        // The ConceptQuery object contains the concept restrictions for the search\n        'concept_query' => new ConceptQuery([\n            'name' => $CONCEPT_NAME,\n            'language' => $CONCEPT_LANGUAGE\n        ])          \n    ]),\n    $metadata\n)->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception(\"Error: {$status->details}\");\n}\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure \n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    throw new Exception(\"Failure response: \" . $response->getStatus()->getDescription() . \" \" .\n        $response->getStatus()->getDetails());\n}\n\n// The output of a successful call can be used in many ways. In this example, we loop through all of the predicted concepts \n// and print them out along with their numerical prediction value (confidence)\necho \"Found concepts: </br>\";\nforeach ($response->getConcepts() as $concept) {\n    echo $concept->getName() . \": \" . number_format($concept->getValue(), 2) . \"</br>\";\n}\n\n?>",E='curl -X POST "https://api.clarifai.com/v2/users/clarifai/apps/main/models/general-image-recognition/outputs" \\\n  -H "Authorization: Key YOUR_PAT_HERE" \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n  "inputs": [\n    {\n      "data": {\n        "image": {\n          "url": "https://samples.clarifai.com/metro-north.jpg"\n        }\n      }\n    }\n  ],\n  "model":{\n    "output_info":{\n      "output_config":{\n        "language":"zh"\n      }\n    }\n  }\n}\'\n',A='curl -X POST "https://api.clarifai.com/v2/users/YOUR_USER_ID_HERE/apps/YOUR_APP_ID_HERE/searches" \\\n  -H "Authorization: Key YOUR_PAT_HERE" \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "query": {\n      "ands": [\n        {\n          "output": {\n            "data": {\n              "concepts": [\n                {\n                  "name": "\u4eba"\n                }\n              ]\n            }\n          }\n        }\n      ],\n      "language": "zh"\n    }\n  }\'\n  ',C="Predicted concepts:\n    \u94c1\u8def\u5217\u8f66 1.00\n    \u94c1\u8def 1.00\n    \u5730\u94c1 1.00\n    \u7ad9 1.00\n    \u706b\u8f66 1.00\n    \u8fd0\u8f93\u7cfb\u7edf 1.00\n    \u65c5\u6e38 0.99\n    \u901a\u52e4 0.98\n    \u5e73\u53f0 0.98\n    \u5149 0.97\n    \u94c1\u8def\u8f66\u7ad9 0.97\n    \u6a21\u7173 0.97\n    \u57ce\u5e02 0.96\n    \u9a6c\u8def 0.96\n    \u57ce\u5e02\u7684 0.96\n    \u4ea4\u901a 0.96\n    \u8857\u9053 0.95\n    \u516c\u5171 0.93\n    \u6709\u8f68\u7535\u8f66\uff08\u5de5\u4e1a\uff09 0.93\n    \u5546\u4e1a 0.93",I='id: "1b2bda0911ed4a58a70198f495aaf8bc"\nstatus {\n  code: SUCCESS\n  description: "Ok"\n}\ncreated_at {\n  seconds: 1701799513\n  nanos: 885844210\n}\nmodel {\n  id: "general-image-recognition"\n  name: "Image Recognition"\n  created_at {\n    seconds: 1457543499\n    nanos: 608845000\n  }\n  app_id: "main"\n  model_version {\n    id: "aa7f35c01e0642fda5cf400f543e7c40"\n    created_at {\n      seconds: 1520370624\n      nanos: 454834000\n    }\n    status {\n      code: MODEL_TRAINED\n      description: "Model is trained and ready"\n    }\n    visibility {\n      gettable: PUBLIC\n    }\n    app_id: "main"\n    user_id: "clarifai"\n    metadata {\n    }\n  }\n  user_id: "clarifai"\n  model_type_id: "visual-classifier"\n  visibility {\n    gettable: PUBLIC\n  }\n  modified_at {\n    seconds: 1694180313\n    nanos: 148401000\n  }\n  workflow_recommended {\n  }\n}\ninput {\n  id: "d89a929f60584d5a84854ec60f2243ab"\n  data {\n    image {\n      url: "https://samples.clarifai.com/metro-north.jpg"\n    }\n  }\n}\ndata {\n  concepts {\n    id: "ai_HLmqFqBf"\n    name: "\\351\\223\\201\\350\\267\\257\\345\\210\\227\\350\\275\\246"\n    value: 0.9996053576469421\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_fvlBqXZR"\n    name: "\\351\\223\\201\\350\\267\\257"\n    value: 0.9992986917495728\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_SHNDcmJ3"\n    name: "\\345\\234\\260\\351\\223\\201"\n    value: 0.9982585310935974\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_6kTjGfF6"\n    name: "\\347\\253\\231"\n    value: 0.9980133771896362\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_RRXLczch"\n    name: "\\347\\201\\253\\350\\275\\246"\n    value: 0.9972604513168335\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_Xxjc3MhT"\n    name: "\\350\\277\\220\\350\\276\\223\\347\\263\\273\\347\\273\\237"\n    value: 0.9969792366027832\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_VRmbGVWh"\n    name: "\\346\\227\\205\\346\\270\\270"\n    value: 0.9889689683914185\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_jlb9q33b"\n    name: "\\351\\200\\232\\345\\213\\244"\n    value: 0.9809139370918274\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_2gkfMDsM"\n    name: "\\345\\271\\263\\345\\217\\260"\n    value: 0.9806650876998901\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_n9vjC1jB"\n    name: "\\345\\205\\211"\n    value: 0.9741945266723633\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_sQQj52KZ"\n    name: "\\351\\223\\201\\350\\267\\257\\350\\275\\246\\347\\253\\231"\n    value: 0.9688410758972168\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_l4WckcJN"\n    name: "\\346\\250\\241\\347\\205\\263"\n    value: 0.9673133492469788\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_WBQfVV0p"\n    name: "\\345\\237\\216\\345\\270\\202"\n    value: 0.9615091681480408\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_TZ3C79C6"\n    name: "\\351\\251\\254\\350\\267\\257"\n    value: 0.9613693356513977\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_CpFBRWzD"\n    name: "\\345\\237\\216\\345\\270\\202\\347\\232\\204"\n    value: 0.960391640663147\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_tr0MBp64"\n    name: "\\344\\272\\244\\351\\200\\232"\n    value: 0.9599775075912476\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_GjVpxXrs"\n    name: "\\350\\241\\227\\351\\201\\223"\n    value: 0.9475197196006775\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_mcSHVRfS"\n    name: "\\345\\205\\254\\345\\205\\261"\n    value: 0.934360921382904\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_J6d1kV8t"\n    name: "\\346\\234\\211\\350\\275\\250\\347\\224\\265\\350\\275\\246\\357\\274\\210\\345\\267\\245\\344\\270\\232\\357\\274\\211"\n    value: 0.9320586323738098\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_6lhccv44"\n    name: "\\345\\225\\206\\344\\270\\232"\n    value: 0.9294787645339966\n    app_id: "main"\n  }\n}\n',b="Found concepts:\n    \u4eba 1.00\n    \u4eba 1.00",P='status {\n  code: SUCCESS\n  description: "Ok"\n  req_id: "ca65f42148166781ce557b825945ec60"\n}\nconcepts {\n  id: "ai_ZKJ48TFz"\n  name: "\\344\\272\\272"\n  value: 1.0\n  created_at {\n    seconds: 1458214981\n    nanos: 223962000\n  }\n  language: "zh"\n  app_id: "main"\n  visibility {\n    gettable: PUBLIC\n  }\n  user_id: "clarifai"\n}\nconcepts {\n  id: "ai_l8TKp2h5"\n  name: "\\344\\272\\272"\n  value: 1.0\n  created_at {\n    seconds: 1458214981\n    nanos: 223962000\n  }\n  language: "zh"\n  app_id: "main"\n  visibility {\n    gettable: PUBLIC\n  }\n  user_id: "clarifai"\n}\n',w={description:"Multilingual predictions.",sidebar_position:9},S="Multilingual Classification",T={id:"api-guide/predict/multilingual-classification",title:"Multilingual Classification",description:"Multilingual predictions.",source:"@site/docs/api-guide/predict/multilingual-classification.md",sourceDirName:"api-guide/predict",slug:"/api-guide/predict/multilingual-classification",permalink:"/api-guide/predict/multilingual-classification",draft:!1,unlisted:!1,editUrl:"https://github.com/Clarifai/docs/blob/main/docs/api-guide/predict/multilingual-classification.md",tags:[],version:"current",sidebarPosition:9,frontMatter:{description:"Multilingual predictions.",sidebar_position:9},sidebar:"tutorialSidebar",previous:{title:"Prediction Parameters",permalink:"/api-guide/predict/prediction-parameters"},next:{title:"Creating and Managing Concepts",permalink:"/api-guide/concepts/"}},O={},D=[{value:"Predict By Specific Language",id:"predict-by-specific-language",level:2},{value:"Search Concepts in Languages",id:"search-concepts-in-languages",level:2}];function N(n){const e={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",p:"p",strong:"strong",...(0,s.R)(),...n.components},{Details:t}=e;return t||function(n,e){throw new Error("Expected "+(e?"component":"object")+" `"+n+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.h1,{id:"multilingual-classification",children:"Multilingual Classification"}),"\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Make multilingual predictions"})}),"\n",(0,a.jsx)("hr",{}),"\n",(0,a.jsxs)(e.p,{children:["The Clarifai API supports ",(0,a.jsx)(e.a,{href:"https://docs.clarifai.com/api-guide/concepts/languages/",children:"many languages in addition to English"}),". When making a ",(0,a.jsx)(e.a,{href:"https://docs.clarifai.com/api-guide/predict/",children:"predict API request"}),", you can pass in the language you would like the concepts returned in."]}),"\n",(0,a.jsx)(e.p,{children:"When you create a new Application, you must specify a default language, which will be the language of the returned concepts, if not specified in the predict request."}),"\n","\n","\n","\n","\n","\n","\n","\n",(0,a.jsx)(e.h2,{id:"predict-by-specific-language",children:"Predict By Specific Language"}),"\n",(0,a.jsx)(e.p,{children:"You can predict concepts in a language other than the Application's default, by explicitly passing in the language."}),"\n",(0,a.jsxs)(e.p,{children:["Below is an example of how you would predict concepts in Chinese using Clarifai's ",(0,a.jsx)(e.a,{href:"https://clarifai.com/clarifai/main/models/general-image-recognition",children:(0,a.jsx)(e.code,{children:"general-image-recognition"})})," model."]}),"\n",(0,a.jsx)(e.admonition,{type:"info",children:(0,a.jsxs)(e.p,{children:["The initialization code used in the following examples is outlined in detail on the ",(0,a.jsx)(e.a,{href:"https://docs.clarifai.com/api-guide/api-overview/api-clients/#client-installation-instructions",children:"client installation page."})]})}),"\n",(0,a.jsxs)(i.A,{children:[(0,a.jsx)(o.A,{value:"python",label:"Python",children:(0,a.jsx)(r.A,{className:"language-python",children:c})}),(0,a.jsx)(o.A,{value:"js_rest",label:"JavaScript (REST)",children:(0,a.jsx)(r.A,{className:"language-javascript",children:l})}),(0,a.jsx)(o.A,{value:"nodejs",label:"NodeJS",children:(0,a.jsx)(r.A,{className:"language-javascript",children:d})}),(0,a.jsx)(o.A,{value:"java",label:"Java",children:(0,a.jsx)(r.A,{className:"language-java",children:g})}),(0,a.jsx)(o.A,{value:"php",label:"PHP",children:(0,a.jsx)(r.A,{className:"language-php",children:_})}),(0,a.jsx)(o.A,{value:"curl",label:"cURL",children:(0,a.jsx)(r.A,{className:"language-bash",children:E})})]}),"\n",(0,a.jsxs)(t,{children:[(0,a.jsx)("summary",{children:"Text Output Example"}),(0,a.jsx)(r.A,{className:"language-text",children:C})]}),"\n",(0,a.jsxs)(t,{children:[(0,a.jsx)("summary",{children:"Raw Output Example"}),(0,a.jsx)(r.A,{className:"language-javascript",children:I})]}),"\n",(0,a.jsx)(e.h2,{id:"search-concepts-in-languages",children:"Search Concepts in Languages"}),"\n",(0,a.jsx)(e.p,{children:"You can search for concepts in other languages even if the default language of your application is English. When you add inputs to your application, concepts are predicted for every language."}),"\n",(0,a.jsx)(e.p,{children:"Below is an example of how your would search for '\u4eba', which is simplified Chinese for 'people'."}),"\n",(0,a.jsxs)(i.A,{children:[(0,a.jsx)(o.A,{value:"python",label:"Python",children:(0,a.jsx)(r.A,{className:"language-python",children:u})}),(0,a.jsx)(o.A,{value:"js_rest",label:"JavaScript (REST)",children:(0,a.jsx)(r.A,{className:"language-javascript",children:p})}),(0,a.jsx)(o.A,{value:"nodejs",label:"NodeJS",children:(0,a.jsx)(r.A,{className:"language-javascript",children:h})}),(0,a.jsx)(o.A,{value:"java",label:"Java",children:(0,a.jsx)(r.A,{className:"language-java",children:m})}),(0,a.jsx)(o.A,{value:"php",label:"PHP",children:(0,a.jsx)(r.A,{className:"language-php",children:f})}),(0,a.jsx)(o.A,{value:"curl",label:"cURL",children:(0,a.jsx)(r.A,{className:"language-bash",children:A})})]}),"\n",(0,a.jsxs)(t,{children:[(0,a.jsx)("summary",{children:"Text Output Example"}),(0,a.jsx)(r.A,{className:"language-text",children:b})]}),"\n",(0,a.jsxs)(t,{children:[(0,a.jsx)("summary",{children:"Raw Output Example"}),(0,a.jsx)(r.A,{className:"language-javascript",children:P})]})]})}function R(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(N,{...n})}):N(n)}},19365:(n,e,t)=>{t.d(e,{A:()=>o});t(96540);var a=t(18215);const s={tabItem:"tabItem_Ymn6"};var i=t(74848);function o(n){let{children:e,hidden:t,className:o}=n;return(0,i.jsx)("div",{role:"tabpanel",className:(0,a.A)(s.tabItem,o),hidden:t,children:e})}},11470:(n,e,t)=>{t.d(e,{A:()=>b});var a=t(96540),s=t(18215),i=t(23104),o=t(56347),r=t(205),c=t(57485),u=t(31682),l=t(70679);function p(n){return a.Children.toArray(n).filter((n=>"\n"!==n)).map((n=>{if(!n||(0,a.isValidElement)(n)&&function(n){const{props:e}=n;return!!e&&"object"==typeof e&&"value"in e}(n))return n;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof n.type?n.type:n.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function d(n){const{values:e,children:t}=n;return(0,a.useMemo)((()=>{const n=e??function(n){return p(n).map((n=>{let{props:{value:e,label:t,attributes:a,default:s}}=n;return{value:e,label:t,attributes:a,default:s}}))}(t);return function(n){const e=(0,u.X)(n,((n,e)=>n.value===e.value));if(e.length>0)throw new Error(`Docusaurus error: Duplicate values "${e.map((n=>n.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(n),n}),[e,t])}function h(n){let{value:e,tabValues:t}=n;return t.some((n=>n.value===e))}function g(n){let{queryString:e=!1,groupId:t}=n;const s=(0,o.W6)(),i=function(n){let{queryString:e=!1,groupId:t}=n;if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:e,groupId:t});return[(0,c.aZ)(i),(0,a.useCallback)((n=>{if(!i)return;const e=new URLSearchParams(s.location.search);e.set(i,n),s.replace({...s.location,search:e.toString()})}),[i,s])]}function m(n){const{defaultValue:e,queryString:t=!1,groupId:s}=n,i=d(n),[o,c]=(0,a.useState)((()=>function(n){let{defaultValue:e,tabValues:t}=n;if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!h({value:e,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${t.map((n=>n.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const a=t.find((n=>n.default))??t[0];if(!a)throw new Error("Unexpected error: 0 tabValues");return a.value}({defaultValue:e,tabValues:i}))),[u,p]=g({queryString:t,groupId:s}),[m,_]=function(n){let{groupId:e}=n;const t=function(n){return n?`docusaurus.tab.${n}`:null}(e),[s,i]=(0,l.Dv)(t);return[s,(0,a.useCallback)((n=>{t&&i.set(n)}),[t,i])]}({groupId:s}),f=(()=>{const n=u??m;return h({value:n,tabValues:i})?n:null})();(0,r.A)((()=>{f&&c(f)}),[f]);return{selectedValue:o,selectValue:(0,a.useCallback)((n=>{if(!h({value:n,tabValues:i}))throw new Error(`Can't select invalid tab value=${n}`);c(n),p(n),_(n)}),[p,_,i]),tabValues:i}}var _=t(92303);const f={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var E=t(74848);function A(n){let{className:e,block:t,selectedValue:a,selectValue:o,tabValues:r}=n;const c=[],{blockElementScrollPositionUntilNextRender:u}=(0,i.a_)(),l=n=>{const e=n.currentTarget,t=c.indexOf(e),s=r[t].value;s!==a&&(u(e),o(s))},p=n=>{let e=null;switch(n.key){case"Enter":l(n);break;case"ArrowRight":{const t=c.indexOf(n.currentTarget)+1;e=c[t]??c[0];break}case"ArrowLeft":{const t=c.indexOf(n.currentTarget)-1;e=c[t]??c[c.length-1];break}}e?.focus()};return(0,E.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,s.A)("tabs",{"tabs--block":t},e),children:r.map((n=>{let{value:e,label:t,attributes:i}=n;return(0,E.jsx)("li",{role:"tab",tabIndex:a===e?0:-1,"aria-selected":a===e,ref:n=>c.push(n),onKeyDown:p,onClick:l,...i,className:(0,s.A)("tabs__item",f.tabItem,i?.className,{"tabs__item--active":a===e}),children:t??e},e)}))})}function C(n){let{lazy:e,children:t,selectedValue:s}=n;const i=(Array.isArray(t)?t:[t]).filter(Boolean);if(e){const n=i.find((n=>n.props.value===s));return n?(0,a.cloneElement)(n,{className:"margin-top--md"}):null}return(0,E.jsx)("div",{className:"margin-top--md",children:i.map(((n,e)=>(0,a.cloneElement)(n,{key:e,hidden:n.props.value!==s})))})}function I(n){const e=m(n);return(0,E.jsxs)("div",{className:(0,s.A)("tabs-container",f.tabList),children:[(0,E.jsx)(A,{...e,...n}),(0,E.jsx)(C,{...e,...n})]})}function b(n){const e=(0,_.A)();return(0,E.jsx)(I,{...n,children:p(n.children)},String(e))}}}]);
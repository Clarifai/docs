"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[6852],{85162:(n,e,t)=>{t.d(e,{Z:()=>s});var a=t(67294),i=t(86010);const o={tabItem:"tabItem_Ymn6"};function s(n){let{children:e,hidden:t,className:s}=n;return a.createElement("div",{role:"tabpanel",className:(0,i.Z)(o.tabItem,s),hidden:t},e)}},74866:(n,e,t)=>{t.d(e,{Z:()=>w});var a=t(87462),i=t(67294),o=t(86010),s=t(12466),r=t(16550),c=t(91980),p=t(67392),u=t(50012);function l(n){return function(n){return i.Children.map(n,(n=>{if(!n||(0,i.isValidElement)(n)&&function(n){const{props:e}=n;return!!e&&"object"==typeof e&&"value"in e}(n))return n;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof n.type?n.type:n.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}(n).map((n=>{let{props:{value:e,label:t,attributes:a,default:i}}=n;return{value:e,label:t,attributes:a,default:i}}))}function d(n){const{values:e,children:t}=n;return(0,i.useMemo)((()=>{const n=e??l(t);return function(n){const e=(0,p.l)(n,((n,e)=>n.value===e.value));if(e.length>0)throw new Error(`Docusaurus error: Duplicate values "${e.map((n=>n.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(n),n}),[e,t])}function m(n){let{value:e,tabValues:t}=n;return t.some((n=>n.value===e))}function f(n){let{queryString:e=!1,groupId:t}=n;const a=(0,r.k6)(),o=function(n){let{queryString:e=!1,groupId:t}=n;if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:e,groupId:t});return[(0,c._X)(o),(0,i.useCallback)((n=>{if(!o)return;const e=new URLSearchParams(a.location.search);e.set(o,n),a.replace({...a.location,search:e.toString()})}),[o,a])]}function h(n){const{defaultValue:e,queryString:t=!1,groupId:a}=n,o=d(n),[s,r]=(0,i.useState)((()=>function(n){let{defaultValue:e,tabValues:t}=n;if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!m({value:e,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${t.map((n=>n.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const a=t.find((n=>n.default))??t[0];if(!a)throw new Error("Unexpected error: 0 tabValues");return a.value}({defaultValue:e,tabValues:o}))),[c,p]=f({queryString:t,groupId:a}),[l,h]=function(n){let{groupId:e}=n;const t=function(n){return n?`docusaurus.tab.${n}`:null}(e),[a,o]=(0,u.Nk)(t);return[a,(0,i.useCallback)((n=>{t&&o.set(n)}),[t,o])]}({groupId:a}),_=(()=>{const n=c??l;return m({value:n,tabValues:o})?n:null})();(0,i.useLayoutEffect)((()=>{_&&r(_)}),[_]);return{selectedValue:s,selectValue:(0,i.useCallback)((n=>{if(!m({value:n,tabValues:o}))throw new Error(`Can't select invalid tab value=${n}`);r(n),p(n),h(n)}),[p,h,o]),tabValues:o}}var _=t(72389);const g={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};function I(n){let{className:e,block:t,selectedValue:r,selectValue:c,tabValues:p}=n;const u=[],{blockElementScrollPositionUntilNextRender:l}=(0,s.o5)(),d=n=>{const e=n.currentTarget,t=u.indexOf(e),a=p[t].value;a!==r&&(l(e),c(a))},m=n=>{let e=null;switch(n.key){case"Enter":d(n);break;case"ArrowRight":{const t=u.indexOf(n.currentTarget)+1;e=u[t]??u[0];break}case"ArrowLeft":{const t=u.indexOf(n.currentTarget)-1;e=u[t]??u[u.length-1];break}}e?.focus()};return i.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,o.Z)("tabs",{"tabs--block":t},e)},p.map((n=>{let{value:e,label:t,attributes:s}=n;return i.createElement("li",(0,a.Z)({role:"tab",tabIndex:r===e?0:-1,"aria-selected":r===e,key:e,ref:n=>u.push(n),onKeyDown:m,onClick:d},s,{className:(0,o.Z)("tabs__item",g.tabItem,s?.className,{"tabs__item--active":r===e})}),t??e)})))}function E(n){let{lazy:e,children:t,selectedValue:a}=n;const o=(Array.isArray(t)?t:[t]).filter(Boolean);if(e){const n=o.find((n=>n.props.value===a));return n?(0,i.cloneElement)(n,{className:"margin-top--md"}):null}return i.createElement("div",{className:"margin-top--md"},o.map(((n,e)=>(0,i.cloneElement)(n,{key:e,hidden:n.props.value!==a}))))}function O(n){const e=h(n);return i.createElement("div",{className:(0,o.Z)("tabs-container",g.tabList)},i.createElement(I,(0,a.Z)({},n,e)),i.createElement(E,(0,a.Z)({},n,e)))}function w(n){const e=(0,_.Z)();return i.createElement(O,(0,a.Z)({key:String(e)},n))}},57708:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>G,contentTitle:()=>B,default:()=>X,frontMatter:()=>H,metadata:()=>V,toc:()=>Y});var a=t(87462),i=(t(67294),t(3905)),o=t(74866),s=t(85162),r=t(90814);const c="#######################################################################################################\n# In this section, we set the user authentication, user and app ID, model ID, model version ID, and\n# URL of the image we want as an input. Change these strings to run your own example.\n#######################################################################################################\n\n# Your PAT (Personal Access Token) can be found in the portal under Authentification\nPAT = 'YOUR_PAT_HERE'\n# Specify the correct user_id/app_id pairings\n# Since you're making inferences outside your app's scope\nUSER_ID = 'clarifai'\nAPP_ID = 'main'\n# Change these to whatever you want to process\nMODEL_ID = 'general-image-recognition'\nMODEL_VERSION_ID = 'aa7f35c01e0642fda5cf400f543e7c40'\nIMAGE_URL = 'https://samples.clarifai.com/metro-north.jpg'\n\n############################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n############################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (('authorization', 'Key ' + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\npost_model_outputs_response = stub.PostModelOutputs(\n    service_pb2.PostModelOutputsRequest(\n        user_app_id=userDataObject,  # The userDataObject is created in the overview and is required when using a PAT\n        model_id=MODEL_ID,\n        version_id=MODEL_VERSION_ID,  \n        inputs=[\n            \n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    image=resources_pb2.Image(\n                        url=IMAGE_URL\n                    )\n                )\n            )\n        ]\n    ),\n    metadata=metadata\n)\nif post_model_outputs_response.status.code != status_code_pb2.SUCCESS:\n    print(post_model_outputs_response.status)\n    raise Exception(\"Post model outputs failed, status: \" + post_model_outputs_response.status.description)\n\n# Since we have one input, one output will exist here\noutput = post_model_outputs_response.outputs[0]\n\nprint(\"Predicted concepts:\")\nfor concept in output.data.concepts:\n    print(\"%s %.2f\" % (concept.name, concept.value))\n\n# Uncomment this line to print the full Response JSON\n#print(output)",p="#######################################################################################################\n# In this section, we set the user authentication, user and app ID, model details, URL of the image\n# we want as an input, and max concepts. Change these strings to run your own example.\n#######################################################################################################\n\n# Your PAT (Personal Access Token) can be found in the portal under Authentification\nPAT = 'YOUR_PAT_HERE'\n# Specify the correct user_id/app_id pairings\n# Since you're making inferences outside your app's scope\nUSER_ID = 'clarifai'\nAPP_ID = 'main'\n# Change these to whatever you want to process\nMODEL_ID = 'general-image-recognition'\nMODEL_VERSION_ID = 'aa7f35c01e0642fda5cf400f543e7c40'\nIMAGE_URL = 'https://samples.clarifai.com/metro-north.jpg'\nMAX_CONCEPTS = 3\n\n############################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n############################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (('authorization', 'Key ' + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\npost_model_outputs_response = stub.PostModelOutputs(\n    service_pb2.PostModelOutputsRequest(\n        user_app_id=userDataObject,  # The userDataObject is created in the overview and is required when using a PAT\n        model_id=MODEL_ID,\n        version_id=MODEL_VERSION_ID,  # This is optional. Defaults to the latest model version  \n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    image=resources_pb2.Image(\n                        url=IMAGE_URL\n                    )\n                )\n            )\n        ],\n        model=resources_pb2.Model(\n            output_info=resources_pb2.OutputInfo(\n                output_config=resources_pb2.OutputConfig(\n                    max_concepts=MAX_CONCEPTS\n                )\n            )\n        )\n    ),\n    metadata=metadata\n)\nif post_model_outputs_response.status.code != status_code_pb2.SUCCESS:\n    print(post_model_outputs_response.status)\n    raise Exception(\"Post model outputs failed, status: \" + post_model_outputs_response.status.description)\n\n# Since we have one input, one output will exist here.\noutput = post_model_outputs_response.outputs[0]\n\nprint(\"Predicted concepts:\")\nfor concept in output.data.concepts:\n    print(\"%s %.2f\" % (concept.name, concept.value))\n\n# Uncomment this line to print the full Response JSON\n#print(output)",u="#########################################################################################################\n# In this section, we set the user authentication, user and app ID, model details, URL of the image\n# we want as an input, and minimum value. Change these strings to run your own example.\n#########################################################################################################\n\n# Your PAT (Personal Access Token) can be found in the portal under Authentification\nPAT = 'YOUR_PAT_HERE'\n# Specify the correct user_id/app_id pairings\n# Since you're making inferences outside your app's scope\nUSER_ID = 'clarifai'\nAPP_ID = 'main'\n# Change these to whatever you want to process\nMODEL_ID = 'general-image-recognition'\nMODEL_VERSION_ID = 'aa7f35c01e0642fda5cf400f543e7c40'\nIMAGE_URL = 'https://samples.clarifai.com/metro-north.jpg'\nMINIMUM_VALUE = 0.95\n\n############################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n############################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (('authorization', 'Key ' + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\npost_model_outputs_response = stub.PostModelOutputs(\n    service_pb2.PostModelOutputsRequest(\n        user_app_id=userDataObject,  # The userDataObject is created in the overview and is required when using a PAT\n        model_id=MODEL_ID,  \n        version_id=MODEL_VERSION_ID,  # This is optional. Defaults to the latest model version\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    image=resources_pb2.Image(\n                        url=IMAGE_URL\n                    )\n                )\n            )\n        ],\n        model=resources_pb2.Model(\n            output_info=resources_pb2.OutputInfo(\n                output_config=resources_pb2.OutputConfig(\n                    min_value=MINIMUM_VALUE\n                )\n            )\n        )\n    ),\n    metadata=metadata\n)\nif post_model_outputs_response.status.code != status_code_pb2.SUCCESS:\n    print(post_model_outputs_response.status)\n    raise Exception(\"Post model outputs failed, status: \" + post_model_outputs_response.status.description)\n\n# Since we have one input, one output will exist here.\noutput = post_model_outputs_response.outputs[0]\n\nprint(\"Predicted concepts:\")\nfor concept in output.data.concepts:\n    print(\"%s %.2f\" % (concept.name, concept.value))\n\n# Uncomment this line to print the full Response JSON\n#print(output)",l="#########################################################################################################\n# In this section, we set the user authentication, user and app ID, model details, URL of the image\n# we want as an input, and concept name and ID. Change these strings to run your own example.\n########################################################################################################\n\n# Your PAT (Personal Access Token) can be found in the portal under Authentification\nPAT = 'YOUR_PAT_HERE'\n# Specify the correct user_id/app_id pairings\n# Since you're making inferences outside your app's scope\nUSER_ID = 'clarifai'\nAPP_ID = 'main'\n# Change these to whatever you want to process\nMODEL_ID = 'general-image-recognition'\nMODEL_VERSION_ID = 'aa7f35c01e0642fda5cf400f543e7c40'\nIMAGE_URL = 'https://samples.clarifai.com/metro-north.jpg'\nCONCEPT_NAME = \"train\"\nCONCEPT_ID = \"ai_6kTjGfF6\"\n\n############################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n############################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (('authorization', 'Key ' + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\npost_model_outputs_response = stub.PostModelOutputs(\n    service_pb2.PostModelOutputsRequest(\n        user_app_id=userDataObject,  # The userDataObject is created in the overview and is required when using a PAT\n        model_id=MODEL_ID,  \n        version_id=MODEL_VERSION_ID,  # This is optional. Defaults to the latest model version\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    image=resources_pb2.Image(\n                        url=IMAGE_URL\n                    )\n                )\n            )\n        ],\n        model=resources_pb2.Model(\n            output_info=resources_pb2.OutputInfo(\n                output_config=resources_pb2.OutputConfig(\n                    select_concepts=[\n                        # When selecting concepts, value is ignored, so no need to specify it\n                        resources_pb2.Concept(name=CONCEPT_NAME),\n                        resources_pb2.Concept(id=CONCEPT_ID)\n                    ]\n                )\n            )\n        )\n    ),\n    metadata=metadata\n)\nif post_model_outputs_response.status.code != status_code_pb2.SUCCESS:\n    print(post_model_outputs_response.status)\n    raise Exception(\"Post model outputs failed, status: \" + post_model_outputs_response.status.description)\n\n# Since we have one input, one output will exist here.\noutput = post_model_outputs_response.outputs[0]\n\nprint(\"Predicted concepts:\")\nfor concept in output.data.concepts:\n    print(\"%s %.2f\" % (concept.name, concept.value))\n\n# Uncomment this line to print the full Response JSON\n#print(output)",d="\x3c!--index.html file--\x3e\n\n<script>\n    ///////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model ID, model version ID, and\n    // URL of the image we want as an input. Change these strings to run your own example.\n    //////////////////////////////////////////////////////////////////////////////////////////////////////\n\n    // Your PAT (Personal Access Token) can be found in the portal under Authentification\n    const PAT = 'YOUR_PAT_HERE';\n    // Specify the correct user_id/app_id pairings\n    // Since you're making inferences outside your app's scope\n    const USER_ID = 'clarifai';    \n    const APP_ID = 'main';\n    // Change these to whatever you want to process\n    const MODEL_ID = 'general-image-recognition';\n    const MODEL_VERSION_ID = 'aa7f35c01e0642fda5cf400f543e7c40';\n    const IMAGE_URL = 'https://samples.clarifai.com/metro-north.jpg';    \n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    const raw = JSON.stringify({\n        \"user_app_id\": {\n            \"user_id\": USER_ID,\n            \"app_id\": APP_ID\n        },\n        \"inputs\": [\n            {\n                \"data\": {\n                    \"image\": {\n                        \"url\": IMAGE_URL\n                    }\n                }\n            }\n        ]\n    });\n\n    const requestOptions = {\n        method: 'POST',\n        headers: {\n            'Accept': 'application/json',\n            'Authorization': 'Key ' + PAT\n        },\n        body: raw\n    };\n\n    // NOTE: MODEL_VERSION_ID is optional, you can also call prediction with the MODEL_ID only\n    // https://api.clarifai.com/v2/models/{YOUR_MODEL_ID}/outputs\n    // this will default to the latest version_id\n\n    fetch(\"https://api.clarifai.com/v2/models/\" + MODEL_ID + \"/versions/\" + MODEL_VERSION_ID + \"/outputs\", requestOptions)\n        .then(response => response.text())\n        .then(result => console.log(result))\n        .catch(error => console.log('error', error));\n<\/script>",m='\x3c!--index.html file--\x3e\n\n<script>\n    ////////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, URL of the image\n    // we want as an input, and max concepts. Change these strings to run your own example.\n    ///////////////////////////////////////////////////////////////////////////////////////////////////////\n\n    // Your PAT (Personal Access Token) can be found in the portal under Authentification\n    const PAT = \'YOUR_PAT_HERE\';\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    const USER_ID = \'clarifai\';    \n    const APP_ID = \'main\';\n    // Change these to whatever you want to process\n    const MODEL_ID = \'general-image-recognition\';\n    const MODEL_VERSION_ID = \'aa7f35c01e0642fda5cf400f543e7c40\';\n    const IMAGE_URL = \'https://samples.clarifai.com/metro-north.jpg\';\n    const MAX_CONCEPTS = 3;\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    const raw = JSON.stringify({\n        "user_app_id": {\n            "user_id": USER_ID,\n            "app_id": APP_ID\n        },\n        "inputs": [\n            {\n                "data": {\n                    "image": {\n                        "url": IMAGE_URL\n                    }\n                }\n            }\n        ],\n        "model": {\n            "output_info": {\n                "output_config": {\n                    "max_concepts": MAX_CONCEPTS\n                }\n            }\n        }\n    });\n\n    const requestOptions = {\n        method: \'POST\',\n        headers: {\n            \'Accept\': \'application/json\',\n            \'Authorization\': \'Key \' + PAT\n        },\n        body: raw\n    };\n\n    // NOTE: MODEL_VERSION_ID is optional, you can also call prediction with the MODEL_ID only\n    // https://api.clarifai.com/v2/models/{YOUR_MODEL_ID}/outputs\n    // this will default to the latest version_id\n\n    fetch("https://api.clarifai.com/v2/models/" + MODEL_ID + "/versions/" + MODEL_VERSION_ID + "/outputs", requestOptions)\n        .then(response => response.text())\n        .then(result => console.log(result))\n        .catch(error => console.log(\'error\', error));\n<\/script>',f='\x3c!--index.html file--\x3e\n\n<script>\n    /////////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, URL of the image\n    // we want as an input, and minimum value. Change these strings to run your own example.\n    ////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n    // Your PAT (Personal Access Token) can be found in the portal under Authentification\n    const PAT = \'YOUR_PAT_HERE\';\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    const USER_ID = \'clarifai\';    \n    const APP_ID = \'main\';\n    // Change these to whatever you want to process\n    const MODEL_ID = \'general-image-recognition\';   \n    const MODEL_VERSION_ID = \'aa7f35c01e0642fda5cf400f543e7c40\';\n    const IMAGE_URL = \'https://samples.clarifai.com/metro-north.jpg\';\n    const MINIMUM_VALUE = 0.95;\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    const raw = JSON.stringify({\n        "user_app_id": {\n            "user_id": USER_ID,\n            "app_id": APP_ID\n        },\n        "inputs": [\n            {\n                "data": {\n                    "image": {\n                        "url": IMAGE_URL\n                    }\n                }\n            }\n        ],\n        "model": {\n            "output_info": {\n                "output_config": {\n                    "min_value": MINIMUM_VALUE\n                }\n            }\n        }\n    });\n\n    const requestOptions = {\n        method: \'POST\',\n        headers: {\n            \'Accept\': \'application/json\',\n            \'Authorization\': \'Key \' + PAT\n        },\n        body: raw\n    };\n\n    // NOTE: MODEL_VERSION_ID is optional, you can also call prediction with the MODEL_ID only\n    // https://api.clarifai.com/v2/models/{YOUR_MODEL_ID}/outputs\n    // this will default to the latest version_id\n\n    fetch("https://api.clarifai.com/v2/models/" + MODEL_ID + "/versions/" + MODEL_VERSION_ID + "/outputs", requestOptions)\n        .then(response => response.text())\n        .then(result => console.log(result))\n        .catch(error => console.log(\'error\', error));\n<\/script>',h='\x3c!--index.html file--\x3e\n\n<script>\n    /////////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, URL of the image\n    // we want as an input, and concept name and ID. Change these strings to run your own example.\n    ////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n    // Your PAT (Personal Access Token) can be found in the portal under Authentification\n    const PAT = \'YOUR_PAT_HERE\';\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    const USER_ID = \'clarifai\';    \n    const APP_ID = \'main\';\n    // Change these to whatever you want to process\n    const MODEL_ID = \'general-image-recognition\';   \n    const MODEL_VERSION_ID = \'aa7f35c01e0642fda5cf400f543e7c40\'; \n    const IMAGE_URL = \'https://samples.clarifai.com/metro-north.jpg\';\n    const CONCEPT_NAME = "train";\n    const CONCEPT_ID = "ai_6kTjGfF6";\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    const raw = JSON.stringify({\n        "user_app_id": {\n            "user_id": USER_ID,\n            "app_id": APP_ID\n        },\n        "inputs": [\n            {\n                "data": {\n                    "image": {\n                        "url": IMAGE_URL\n                    }\n                }\n            }\n        ],\n        "model": {\n            "output_info": {\n                "output_config": {\n                    "select_concepts": [\n                        { "name": CONCEPT_NAME },\n                        { "id": CONCEPT_ID }\n                    ]\n                }\n            }\n        }\n    });\n\n    const requestOptions = {\n        method: \'POST\',\n        headers: {\n            \'Accept\': \'application/json\',\n            \'Authorization\': \'Key \' + PAT\n        },\n        body: raw\n    };\n\n    // NOTE: MODEL_VERSION_ID is optional, you can also call prediction with the MODEL_ID only\n    // https://api.clarifai.com/v2/models/{YOUR_MODEL_ID}/outputs\n    // this will default to the latest version_id\n\n    fetch("https://api.clarifai.com/v2/models/" + MODEL_ID + "/versions/" + MODEL_VERSION_ID + "/outputs", requestOptions)\n        .then(response => response.text())\n        .then(result => console.log(result))\n        .catch(error => console.log(\'error\', error));\n<\/script>',_='//index.js file\n\n////////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model ID, model version ID, and \n// URL of the image we want as an input. Change these strings to run your own example.\n//////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the portal under Authentification\nconst PAT = \'YOUR_PAT_HERE\';\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\nconst USER_ID = \'clarifai\';\nconst APP_ID = \'main\';\n// Change these to whatever you want to process\nconst MODEL_ID = \'general-image-recognition\';\nconst MODEL_VERSION_ID = \'aa7f35c01e0642fda5cf400f543e7c40\';\nconst IMAGE_URL = \'https://samples.clarifai.com/metro-north.jpg\';\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\nstub.PostModelOutputs(\n    {\n        user_app_id: {\n            "user_id": USER_ID,\n            "app_id": APP_ID\n        },\n        model_id: MODEL_ID, \n        version_id: MODEL_VERSION_ID,\n        inputs: [\n            { data: { image: { url: IMAGE_URL, allow_duplicate_url: true } } }\n        ]\n    },\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            throw new Error("Post model outputs failed, status: " + response.status.description);\n        }\n\n        // Since we have one input, one output will exist here.\n        const output = response.outputs[0];\n\n        console.log("Predicted concepts:");\n        for (const concept of output.data.concepts) {\n            console.log(concept.name + " " + concept.value);\n        }\n    }\n\n);',g='//index.js file\n\n///////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, URL of the image\n// we want as an input, and max concepts. Change these strings to run your own example.\n//////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the portal under Authentification\nconst PAT = \'YOUR_PAT_HERE\';\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\nconst USER_ID = \'clarifai\';\nconst APP_ID = \'main\';\n// Change these to whatever you want to process\nconst MODEL_ID = \'general-image-recognition\';\nconst MODEL_VERSION_ID = \'aa7f35c01e0642fda5cf400f543e7c40\'; \nconst IMAGE_URL = \'https://samples.clarifai.com/metro-north.jpg\';\nconst MAX_CONCEPTS = 3;\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\nstub.PostModelOutputs(\n    {\n        user_app_id: {\n            "user_id": USER_ID,\n            "app_id": APP_ID\n        },\n        model_id: MODEL_ID, \n        version_id: MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n        inputs: [\n            { data: { image: { url: IMAGE_URL, allow_duplicate_url: true } } }\n        ],\n        model: { output_info: { output_config: { max_concepts: MAX_CONCEPTS } } }\n    },\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            throw new Error("Post model outputs failed, status: " + response.status.description);\n        }\n\n        // Since we have one input, one output will exist here.\n        const output = response.outputs[0];\n\n        console.log("Predicted concepts:");\n        for (const concept of output.data.concepts) {\n            console.log(concept.name + " " + concept.value);\n        }\n    }\n\n);',I='//index.js file\n\n/////////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, URL of the image\n// we want as an input, and minimum value. Change these strings to run your own example.\n////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the portal under Authentification\nconst PAT = \'YOUR_PAT_HERE\';\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\nconst USER_ID = \'clarifai\';\nconst APP_ID = \'main\';\n// Change these to whatever you want to process\nconst MODEL_ID = \'general-image-recognition\';\nconst MODEL_VERSION_ID = \'aa7f35c01e0642fda5cf400f543e7c40\'; \nconst IMAGE_URL = \'https://samples.clarifai.com/metro-north.jpg\';\nconst MINIMUM_VALUE = 0.95;\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\nstub.PostModelOutputs(\n    {\n        user_app_id: {\n            "user_id": USER_ID,\n            "app_id": APP_ID\n        },\n        model_id: MODEL_ID, \n        version_id: MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n        inputs: [\n            { data: { image: { url: IMAGE_URL, allow_duplicate_url: true } } }\n        ],\n        model: { output_info: { output_config: { min_value: MINIMUM_VALUE } } }\n    },\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            throw new Error("Post model outputs failed, status: " + response.status.description);\n        }\n\n        // Since we have one input, one output will exist here.\n        const output = response.outputs[0];\n\n        console.log("Predicted concepts:");\n        for (const concept of output.data.concepts) {\n            console.log(concept.name + " " + concept.value);\n        }\n    }\n\n);',E='//index.js file\n\n////////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, URL of the image\n// we want as an input, and concept name and ID. Change these strings to run your own example.\n///////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the portal under Authentification\nconst PAT = \'YOUR_PAT_HERE\';\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\nconst USER_ID = \'clarifai\';\nconst APP_ID = \'main\';\n// Change these to whatever you want to process\nconst MODEL_ID = \'general-image-recognition\';\nconst MODEL_VERSION_ID = \'aa7f35c01e0642fda5cf400f543e7c40\'; \nconst IMAGE_URL = \'https://samples.clarifai.com/metro-north.jpg\';\nconst CONCEPT_NAME = "train";\nconst CONCEPT_ID = "ai_6kTjGfF6";\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\nstub.PostModelOutputs(\n    {\n        user_app_id: {\n            "user_id": USER_ID,\n            "app_id": APP_ID\n        },\n        model_id: MODEL_ID, \n        version_id: MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n        inputs: [\n            { data: { image: { url: IMAGE_URL, allow_duplicate_url: true } } }\n        ],\n        // When selecting concepts, value is ignored, so no need to specify it.\n        model: { output_info: { output_config: { select_concepts: [{ name: CONCEPT_NAME }, { id: CONCEPT_ID }] } } }\n    },\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            throw new Error("Post model outputs failed, status: " + response.status.description);\n        }\n\n        // Since we have one input, one output will exist here.\n        const output = response.outputs[0];\n\n        console.log("Predicted concepts:");\n        for (const concept of output.data.concepts) {\n            console.log(concept.name + " " + concept.value);\n        }\n    }\n\n);',O='package com.clarifai.example;\n\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.status.StatusCode;\n\npublic class ClarifaiExample {\n\n    /////////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model ID, model version ID, and\n    // URL of the image we want as an input. Change these strings to run your own example.\n    /////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n    // Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    static final String USER_ID = "clarifai";   \n    static final String APP_ID = "main";\n    // Change these to whatever you want to process\n    static final String MODEL_ID = "general-image-recognition";\n    static final String MODEL_VERSION_ID = "aa7f35c01e0642fda5cf400f543e7c40";\n    static final String IMAGE_URL = "https://samples.clarifai.com/metro-north.jpg";\n    \n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\t\n\n    public static void main(String[] args) {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n            .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        MultiOutputResponse postModelOutputsResponse = stub.postModelOutputs(\n            PostModelOutputsRequest.newBuilder()\n            .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n            .setModelId(MODEL_ID) \n            .setVersionId(MODEL_VERSION_ID) // This is optional. Defaults to the latest model version\n            .addInputs(\n                Input.newBuilder().setData(\n                    Data.newBuilder().setImage(\n                        Image.newBuilder().setUrl(IMAGE_URL)\n                    )\n                )\n            )\n            .build()\n        );\n\n        if (postModelOutputsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("Post model outputs failed, status: " + postModelOutputsResponse.getStatus());\n        }\n\n        // Since we have one input, one output will exist here.\n        Output output = postModelOutputsResponse.getOutputs(0);\n\n        System.out.println("Predicted concepts:");\n        for (Concept concept: output.getData().getConceptsList()) {\n            System.out.printf("%s %.2f%n", concept.getName(), concept.getValue());\n        }\n\n    }\n\n}',w='package com.clarifai.example;\n\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.status.StatusCode;\n\npublic class ClarifaiExample {\n\n    /////////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, URL of the image\n    // we want as an input, and max concepts. Change these strings to run your own example.\n    ////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n    // Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    static final String USER_ID = "clarifai";    \n    static final String APP_ID = "main";\n    // Change these to whatever you want to process\n    static final String MODEL_ID = "general-image-recognition";\n    static final String MODEL_VERSION_ID = "aa7f35c01e0642fda5cf400f543e7c40";\n    static final String IMAGE_URL = "https://samples.clarifai.com/metro-north.jpg";\n    static final int MAX_CONCEPTS = 3;\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\t\n\n    public static void main(String[] args) {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n            .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        MultiOutputResponse postModelOutputsResponse = stub.postModelOutputs(\n            PostModelOutputsRequest.newBuilder()\n            .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n            .setModelId(MODEL_ID)\n            .setVersionId(MODEL_VERSION_ID)  // This is optional. Defaults to the latest model version\n            .addInputs(\n                Input.newBuilder().setData(\n                    Data.newBuilder().setImage(\n                        Image.newBuilder().setUrl(IMAGE_URL)\n                    )\n                )\n            )\n            .setModel(\n                Model.newBuilder().setOutputInfo(\n                    OutputInfo.newBuilder().setOutputConfig(\n                        OutputConfig.newBuilder().setMaxConcepts(MAX_CONCEPTS)\n                    )\n                )\n            )\n            .build()\n        );\n\n        if (postModelOutputsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("Post model outputs failed, status: " + postModelOutputsResponse.getStatus());\n        }\n\n        // Since we have one input, one output will exist here\n        Output output = postModelOutputsResponse.getOutputs(0);\n\n        System.out.println("Predicted concepts:");\n        for (Concept concept: output.getData().getConceptsList()) {\n            System.out.printf("%s %.2f%n", concept.getName(), concept.getValue());\n        }\n\n    }\n\n}',T='package com.clarifai.example;\n\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.status.StatusCode;\n\npublic class ClarifaiExample {\n\n    /////////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, URL of the image\n    // we want as an input, and minimum value. Change these strings to run your own example.\n    ////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n    // Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    static final String USER_ID = "clarifai";    \n    static final String APP_ID = "main";\n    // Change these to whatever you want to process\n    static final String MODEL_ID = "general-image-recognition";\n    static final String MODEL_VERSION_ID = "aa7f35c01e0642fda5cf400f543e7c40";\n    static final String IMAGE_URL = "https://samples.clarifai.com/metro-north.jpg";\n    static final float MINIMUM_VALUE = 0.95f;\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\t\n\n    public static void main(String[] args) {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n            .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        MultiOutputResponse postModelOutputsResponse = stub.postModelOutputs(\n            PostModelOutputsRequest.newBuilder()\n            .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n            .setModelId(MODEL_ID) \n            .setVersionId(MODEL_VERSION_ID)  // This is optional. Defaults to the latest model version\n            .addInputs(\n                Input.newBuilder().setData(\n                    Data.newBuilder().setImage(\n                        Image.newBuilder().setUrl(IMAGE_URL)\n                    )\n                )\n            )\n            .setModel(\n                Model.newBuilder().setOutputInfo(\n                    OutputInfo.newBuilder().setOutputConfig(\n                        OutputConfig.newBuilder().setMinValue(MINIMUM_VALUE)\n                    )\n                )\n            )\n            .build()\n        );\n\n        if (postModelOutputsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("Post model outputs failed, status: " + postModelOutputsResponse.getStatus());\n        }\n\n        // Since we have one input, one output will exist here\n        Output output = postModelOutputsResponse.getOutputs(0);\n\n        System.out.println("Predicted concepts:");\n        for (Concept concept: output.getData().getConceptsList()) {\n            System.out.printf("%s %.2f%n", concept.getName(), concept.getValue());\n        }\n\n    }\n\n}',D='package com.clarifai.example;\n\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.status.StatusCode;\n\npublic class ClarifaiExample {\n\n    ////////////////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, user and app ID, model details, URL of the image\n    // we want as an input, and concept name and ID. Change these strings to run your own example.\n    ///////////////////////////////////////////////////////////////////////////////////////////////////////\n\n    //Your PAT (Personal Access Token) can be found in the portal under Authentication\n    static final String PAT = "YOUR_PAT_HERE";\n    // Specify the correct user_id/app_id pairings\n    // Since you\'re making inferences outside your app\'s scope\n    static final String USER_ID = "clarifai";    \n    static final String APP_ID = "main";\n    // Change these to whatever you want to process\n    static final String MODEL_ID = "general-image-recognition";\n    static final String MODEL_VERSION_ID = "aa7f35c01e0642fda5cf400f543e7c40";\n    static final String IMAGE_URL = "https://samples.clarifai.com/metro-north.jpg";\n    static final String CONCEPT_NAME = "train";\n    static final String CONCEPT_ID = "ai_6kTjGfF6";\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\t\n\n    public static void main(String[] args) {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n            .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        MultiOutputResponse postModelOutputsResponse = stub.postModelOutputs(\n            PostModelOutputsRequest.newBuilder()\n            .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n            .setModelId(MODEL_ID) \n            .setVersionId(MODEL_VERSION_ID)  // This is optional. Defaults to the latest model version\n            .addInputs(\n                Input.newBuilder().setData(\n                    Data.newBuilder().setImage(\n                        Image.newBuilder().setUrl(IMAGE_URL)\n                    )\n                )\n            )\n            .setModel(\n                Model.newBuilder().setOutputInfo(\n                    OutputInfo.newBuilder().setOutputConfig(\n                        OutputConfig.newBuilder()\n                        // When selecting concepts, value is ignored, so no need to specify it\n                        .addSelectConcepts(Concept.newBuilder().setName(CONCEPT_NAME))\n                        .addSelectConcepts(Concept.newBuilder().setId(CONCEPT_ID))\n                    )\n                )\n            )\n            .build()\n        );\n\n        if (postModelOutputsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("Post model outputs failed, status: " + postModelOutputsResponse.getStatus());\n        }\n\n        // Since we have one input, one output will exist here\n        Output output = postModelOutputsResponse.getOutputs(0);\n\n        System.out.println("Predicted concepts:");\n        for (Concept concept: output.getData().getConceptsList()) {\n            System.out.printf("%s %.2f%n", concept.getName(), concept.getValue());\n        }\n\n    }\n\n}',b="<?php\n\nrequire __DIR__ . '/vendor/autoload.php';\n\n////////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model ID, model version ID, and \n// URL of the image we want as an input. Change these strings to run your own example.\n///////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the portal under Authentification\n$PAT = 'YOUR_PAT_HERE';\n// Specify the correct user_id/app_id pairings\n// Since you're making inferences outside your app's scope\n$USER_ID = 'clarifai';\n$APP_ID = 'main';\n// Change these to whatever you want to process\n$MODEL_ID = 'general-image-recognition';\n$MODEL_VERSION_ID = 'aa7f35c01e0642fda5cf400f543e7c40';\n$IMAGE_URL = 'https://samples.clarifai.com/metro-north.jpg';\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Image;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\PostModelOutputsRequest;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ['Authorization' => ['Key ' . $PAT ]];\n\n$userDataObject = new UserAppIDSet([\n    'user_id' => $USER_ID, \n    'app_id' => $APP_ID \n]);\n\n// Let's make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client->PostModelOutputs(\n    // The request object carries the request along with the request status and other metadata related to the request itself\n    new PostModelOutputsRequest([\n        'user_app_id' => $userDataObject,\n        'model_id' => $MODEL_ID, \n        'version_id' => $MODEL_VERSION_ID,\n        'inputs' => [\n            new Input([ // The Input object wraps the Data object in order to meet the API specification                \n                'data' => new Data([ // The Data object is constructed around the Image object. It offers a container that has additional image independent\n                                    // metadata. In this particular use case, no other metadata is needed to be specified\n                    'image' => new Image([ // In the Clarifai platform, an image is defined by a special Image object\n                        'url' => $IMAGE_URL\n                    ])\n                ])\n            ])\n        ]       \n    ]),\n    $metadata\n)->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception(\"Error: {$status->details}\");\n}\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure \n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    throw new Exception(\"Failure response: \" . $response->getStatus()->getDescription() . \" \" .\n        $response->getStatus()->getDetails());\n}\n\n// The output of a successful call can be used in many ways. In this example, we loop through all of the predicted concepts \n// and print them out along with their numerical prediction value (confidence)\necho \"Predicted concepts: </br>\";\nforeach ($response->getOutputs()[0]->getData()->getConcepts() as $concept) {\n    echo $concept->getName() . \": \" . number_format($concept->getValue(), 2) . \"</br>\";\n}\n\n?>",C="<?php\n\nrequire __DIR__ . '/vendor/autoload.php';\n\n///////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, URL of the image\n// we want as an input, and max concepts. Change these strings to run your own example.\n//////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the portal under Authentification\n$PAT = \"YOUR_PAT_HERE\";\n// Specify the correct user_id/app_id pairings\n// Since you're making inferences outside your app's scope\n$USER_ID = \"clarifai\";\n$APP_ID = \"main\";\n// Change these to whatever you want to process\n$MODEL_ID = 'general-image-recognition';\n$MODEL_VERSION_ID = 'aa7f35c01e0642fda5cf400f543e7c40';  \n$IMAGE_URL = 'https://samples.clarifai.com/metro-north.jpg';\n$MAX_CONCEPTS = 3;\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Image;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\Model;\nuse Clarifai\\Api\\OutputInfo;\nuse Clarifai\\Api\\OutputConfig;\nuse Clarifai\\Api\\PostModelOutputsRequest;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ['Authorization' => ['Key ' . $PAT ]];\n\n$userDataObject = new UserAppIDSet([\n    'user_id' => $USER_ID, \n    'app_id' => $APP_ID \n]);\n\n// Let's make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client->PostModelOutputs(\n    // The request object carries the request along with the request status and other metadata related to the request itself\n    new PostModelOutputsRequest([\n        'user_app_id' => $userDataObject,\n        'model_id' => $MODEL_ID, \n        'version_id' => $MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n        'inputs' => [\n            new Input([ // The Input object wraps the Data object in order to meet the API specification                \n                'data' => new Data([ // The Data object is constructed around the Image object. It offers a container that has additional image independent\n                                    // metadata. In this particular use case, no other metadata is needed to be specified\n                    'image' => new Image([ // In the Clarifai platform, an image is defined by a special Image object\n                        'url' => $IMAGE_URL\n                    ])\n                ])\n            ])\n        ],\n        // The model object is a wrapper around the OutputInfo object\n        \"model\" => new Model([\n            //The OutputInfo object is a wrapper around the OutputConfig object\n            \"output_info\" => new OutputInfo([\n                // Output configuration can be specified by the OutputConfig object. Here we specify the max number of concepts \n                // to return at 3.\n                \"output_config\" => new OutputConfig([\n                   \"max_concepts\" => $MAX_CONCEPTS\n                ])\n            ])\n        ])\n\n    ]),\n    $metadata\n)->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception(\"Error: {$status->details}\");\n}\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure \n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    throw new Exception(\"Failure response: \" . $response->getStatus()->getDescription() . \" \" .\n        $response->getStatus()->getDetails());\n}\n\n// The output of a successful call can be used in many ways. In this example, we loop through all of the predicted concepts \n// and print them out along with their numerical prediction value (confidence)\necho \"Predicted concepts: </br>\";\nforeach ($response->getOutputs()[0]->getData()->getConcepts() as $concept) {\n    echo $concept->getName() . \": \" . number_format($concept->getValue(), 2) . \"</br>\";\n}\n\n?>",y="<?php\n\nrequire __DIR__ . '/vendor/autoload.php';\n\n////////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, URL of the image\n// we want as an input, and minimum value. Change these strings to run your own example.\n////////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the portal under Authentification\n$PAT = \"YOUR_PAT_HERE\";\n// Specify the correct user_id/app_id pairings\n// Since you're making inferences outside your app's scope\n$USER_ID = \"clarifai\";\n$APP_ID = \"main\";\n// Change these to whatever you want to process\n$MODEL_ID = 'general-image-recognition';\n$MODEL_VERSION_ID = 'aa7f35c01e0642fda5cf400f543e7c40';\n$IMAGE_URL = 'https://samples.clarifai.com/metro-north.jpg';\n$MINIMUM_VALUE = 0.95;\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Image;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\Model;\nuse Clarifai\\Api\\OutputInfo;\nuse Clarifai\\Api\\OutputConfig;\nuse Clarifai\\Api\\PostModelOutputsRequest;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ['Authorization' => ['Key ' . $PAT ]];\n\n$userDataObject = new UserAppIDSet([\n    'user_id' => $USER_ID, \n    'app_id' => $APP_ID \n]);\n\n// Let's make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client->PostModelOutputs(\n    // The request object carries the request along with the request status and other metadata related to the request itself\n    new PostModelOutputsRequest([\n        'user_app_id' => $userDataObject,\n        'model_id' => $MODEL_ID, \n        'version_id' => $MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n        'inputs' => [\n            new Input([ // The Input object wraps the Data object in order to meet the API specification                \n                'data' => new Data([ // The Data object is constructed around the Image object. It offers a container that has additional image independent\n                                    // metadata. In this particular use case, no other metadata is needed to be specified\n                    'image' => new Image([ // In the Clarifai platform, an image is defined by a special Image object\n                        'url' => $IMAGE_URL\n                    ])\n                ])\n            ])\n        ],\n        // The model object is a wrapper around the OutputInfo object\n        \"model\" => new Model([\n            //The OutputInfo object is a wrapper around the OutputConfig object\n            \"output_info\" => new OutputInfo([\n                // Output configuration can be specified by the OutputConfig object. Here we specify the minimum \n                // threshold value to 0.95.\n                \"output_config\" => new OutputConfig([\n                   \"min_value\" => $MINIMUM_VALUE\n                ])\n            ])\n        ])\n\n    ]),\n    $metadata\n)->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception(\"Error: {$status->details}\");\n}\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure \n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    throw new Exception(\"Failure response: \" . $response->getStatus()->getDescription() . \" \" .\n        $response->getStatus()->getDetails());\n}\n\n// The output of a successful call can be used in many ways. In this example, we loop through all of the predicted concepts \n// and print them out along with their numerical prediction value (confidence)\necho \"Predicted concepts: </br>\";\nforeach ($response->getOutputs()[0]->getData()->getConcepts() as $concept) {\n    echo $concept->getName() . \": \" . number_format($concept->getValue(), 2) . \"</br>\";\n}\n\n?>",S='<?php\n\nrequire __DIR__ . \'/vendor/autoload.php\';\n\n///////////////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, user and app ID, model details, URL of the image\n// we want as an input, and concept name and ID. Change these strings to run your own example.\n/////////////////////////////////////////////////////////////////////////////////////////////////////\n\n// Your PAT (Personal Access Token) can be found in the portal under Authentification\n$PAT = "YOUR_PAT_HERE";\n// Specify the correct user_id/app_id pairings\n// Since you\'re making inferences outside your app\'s scope\n$USER_ID = "clarifai";\n$APP_ID = "main";\n// Change these to whatever you want to process\n$MODEL_ID = \'general-image-recognition\'; \n$MODEL_VERSION_ID = \'aa7f35c01e0642fda5cf400f543e7c40\';   \n$IMAGE_URL = \'https://samples.clarifai.com/metro-north.jpg\';\n$CONCEPT_NAME = "train";\n$CONCEPT_ID = "ai_6kTjGfF6";\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\Api\\Concept;\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Image;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\Model;\nuse Clarifai\\Api\\OutputInfo;\nuse Clarifai\\Api\\OutputConfig;\nuse Clarifai\\Api\\PostModelOutputsRequest;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = [\'Authorization\' => [\'Key \' . $PAT ]];\n\n$userDataObject = new UserAppIDSet([\n    \'user_id\' => $USER_ID, \n    \'app_id\' => $APP_ID \n]);\n\n// Let\'s make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client->PostModelOutputs(\n    // The request object carries the request along with the request status and other metadata related to the request itself\n    new PostModelOutputsRequest([\n        \'user_app_id\' => $userDataObject,\n        \'model_id\' => $MODEL_ID, \n        \'version_id\' => $MODEL_VERSION_ID, // This is optional. Defaults to the latest model version\n        \'inputs\' => [\n            new Input([ // The Input object wraps the Data object in order to meet the API specification                \n                \'data\' => new Data([ // The Data object is constructed around the Image object. It offers a container that has additional image independent\n                                    // metadata. In this particular use case, no other metadata is needed to be specified\n                    \'image\' => new Image([ // In the Clarifai platform, an image is defined by a special Image object\n                        \'url\' => $IMAGE_URL\n                    ])\n                ])\n            ])\n        ],\n        // The model object is a wrapper around the OutputInfo object\n        "model" => new Model([\n            //The OutputInfo object is a wrapper around the OutputConfig object\n            "output_info" => new OutputInfo([\n                // Output configuration can be specified by the OutputConfig object. Here we specify a concept by both the name and the id \n                // for what we want to narrow down to in the results.\n                "output_config" => new OutputConfig([\n                    "select_concepts" => [\n                        // When selecting concepts, value is ignored, so no need to specify it\n                        new Concept([\n                            "name" => $CONCEPT_NAME\n                        ]),\n                        new Concept([\n                            "id" => $CONCEPT_ID\n                        ])\n                    ]\n                ])\n            ])\n        ])\n\n    ]),\n    $metadata\n)->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception("Error: {$status->details}");\n}\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure \n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    throw new Exception("Failure response: " . $response->getStatus()->getDescription() . " " .\n        $response->getStatus()->getDetails());\n}\n\n// The output of a successful call can be used in many ways. In this example, we loop through all of the predicted concepts \n// and print them out along with their numerical prediction value (confidence)\necho "Predicted concepts: </br>";\nforeach ($response->getOutputs()[0]->getData()->getConcepts() as $concept) {\n    echo $concept->getName() . ": " . number_format($concept->getValue(), 2) . "</br>";\n}\n\n?>',A='# Version ID is optional. It defaults to the latest model version, if omitted.\n\ncurl -X POST "https://api.clarifai.com/v2/users/clarifai/apps/main/models/general-image-recognition/versions/aa7f35c01e0642fda5cf400f543e7c40/outputs" \\\n  -H "Authorization: Key YOUR_PAT_HERE" \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "inputs": [\n      {\n        "data": {\n          "image": {\n            "url": "https://samples.clarifai.com/metro-north.jpg"\n          }\n        }\n      }\n    ]\n  }\'\n  \n',v='curl -X POST "https://api.clarifai.com/v2/users/clarifai/apps/main/models/general-image-recognition/outputs" \\\n  -H "Authorization: Key YOUR_PAT_HERE" \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "inputs": [\n      {\n        "data": {\n          "image": {\n            "url": "https://samples.clarifai.com/metro-north.jpg"\n          }\n        }\n      }\n    ],\n    "model":{\n      "output_info":{\n        "output_config":{\n          "max_concepts": 3\n        }\n      }\n    }\n  }\'',P='curl -X POST "https://api.clarifai.com/v2/users/clarifai/apps/main/models/general-image-recognition/outputs" \\\n  -H "Authorization: Key YOUR_PAT_HERE" \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "inputs": [\n      {\n        "data": {\n          "image": {\n            "url": "https://samples.clarifai.com/metro-north.jpg"\n          }\n        }\n      }\n    ],\n    "model":{\n      "output_info":{\n        "output_config":{\n          "min_value": 0.95\n        }\n      }\n    }\n  }\'',N='curl -X POST "https://api.clarifai.com/v2/users/clarifai/apps/main/models/general-image-recognition/outputs" \\\n  -H "authorization: Key YOUR_PAT_HERE" \\\n  -H "content-Type: application/json" \\\n  -d \'{\n  "inputs": [\n    {\n      "data": {\n        "image": {\n          "url": "https://samples.clarifai.com/metro-north.jpg"\n        }\n      }\n    }\n  ],\n  "model": {\n    "output_info": {\n      "output_config": {\n        "select_concepts": [\n          {"name": "train"},\n          {"id": "ai_6kTjGfF6"}\n        ]\n      }\n    }\n  }\n}\'\n\n',M="Predicted concepts:\ntrain 1.00\nstation 1.00",R='id: "9c43849f2ead4ff0bfba37ab70c85a37"\nstatus {\n  code: SUCCESS\n  description: "Ok"\n}\ncreated_at {\n  seconds: 1643792042\n  nanos: 215003702\n}\nmodel {\n  id: "general-image-recognition"\n  name: "general"\n  created_at {\n    seconds: 1457543499\n    nanos: 608845000\n  }\n  app_id: "main"\n  output_info {\n    output_config {\n    }\n    message: "Show output_info with: GET /models/{model_id}/output_info"\n    fields_map {\n      fields {\n        key: "concepts"\n        value {\n          string_value: "softmax"\n        }\n      }\n    }\n  }\n  model_version {\n    id: "aa7f35c01e0642fda5cf400f543e7c40"\n    created_at {\n      seconds: 1520370624\n      nanos: 454834000\n    }\n    status {\n      code: MODEL_TRAINED\n      description: "Model is trained and ready"\n    }\n    visibility {\n      gettable: PUBLIC\n    }\n    app_id: "main"\n    user_id: "clarifai"\n    metadata {\n    }\n  }\n  display_name: "general-visual-classifier"\n  user_id: "clarifai"\n  input_info {\n    fields_map {\n      fields {\n        key: "image"\n        value {\n          string_value: "images"\n        }\n      }\n    }\n  }\n  train_info {\n  }\n  model_type_id: "visual-classifier"\n  visibility {\n    gettable: PUBLIC\n  }\n  description: "Image recognition model for identifying different concepts in images and video including objects, themes, moods, and more."\n  metadata {\n  }\n  notes: "**General Information**\\n\\n- Purpose: Classifier for a variety of concepts, common objects, etc. This model is a great all-purpose solution for most visual recognition needs with industry-leading performance.\\n\\n- Architecture: Customized InceptionV2\\n\\n- Intended Use: image indexing by tags, filtering, cascade routing\\n\\n- Limitations: works well when content is prevalent in the image\\n\\n\\n\\n **\\nTraining/Test Data**\\n\\nThe model was trained and tested on an internal dataset with approximately 10,000 concepts and 20M images, with multiple concepts per image. The class distributions on train and validation sets are long-tailed. The validation set was annotated using a combination of originally curated labels with incomplete annotations, where were further completed by adding additional labels proposed a newer version of this model (aa7f35c01e0642fda5cf400f543e7c40) at a low threshold and verified by human annotators. "\n  modified_at {\n    seconds: 1634831222\n    nanos: 80260000\n  }\n  import_info {\n  }\n}\ninput {\n  id: "979f534672624dd5ac44db862555b154"\n  data {\n    image {\n      url: "https://samples.clarifai.com/metro-north.jpg"\n    }\n  }\n}\ndata {\n  concepts {\n    id: "ai_HLmqFqBf"\n    name: "train"\n    value: 0.9996053576469421\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_6kTjGfF6"\n    name: "station"\n    value: 0.9980105757713318\n    app_id: "main"\n  }\n}\n',k="Predicted concepts:\ntrain 1.00\nrailway 1.00\nsubway system 1.00",L='id: "0248198a8fd44077afcd2bc56be413ba"\nstatus {\n  code: SUCCESS\n  description: "Ok"\n}\ncreated_at {\n  seconds: 1643794231\n  nanos: 147923521\n}\nmodel {\n  id: "general-image-recognition"\n  name: "general"\n  created_at {\n    seconds: 1457543499\n    nanos: 608845000\n  }\n  app_id: "main"\n  output_info {\n    output_config {\n    }\n    message: "Show output_info with: GET /models/{model_id}/output_info"\n    fields_map {\n      fields {\n        key: "concepts"\n        value {\n          string_value: "softmax"\n        }\n      }\n    }\n  }\n  model_version {\n    id: "aa7f35c01e0642fda5cf400f543e7c40"\n    created_at {\n      seconds: 1520370624\n      nanos: 454834000\n    }\n    status {\n      code: MODEL_TRAINED\n      description: "Model is trained and ready"\n    }\n    visibility {\n      gettable: PUBLIC\n    }\n    app_id: "main"\n    user_id: "clarifai"\n    metadata {\n    }\n  }\n  display_name: "general-visual-classifier"\n  user_id: "clarifai"\n  input_info {\n    fields_map {\n      fields {\n        key: "image"\n        value {\n          string_value: "images"\n        }\n      }\n    }\n  }\n  train_info {\n  }\n  model_type_id: "visual-classifier"\n  visibility {\n    gettable: PUBLIC\n  }\n  description: "Image recognition model for identifying different concepts in images and video including objects, themes, moods, and more."\n  metadata {\n  }\n  notes: "**General Information**\\n\\n- Purpose: Classifier for a variety of concepts, common objects, etc. This model is a great all-purpose solution for most visual recognition needs with industry-leading performance.\\n\\n- Architecture: Customized InceptionV2\\n\\n- Intended Use: image indexing by tags, filtering, cascade routing\\n\\n- Limitations: works well when content is prevalent in the image\\n\\n\\n\\n **\\nTraining/Test Data**\\n\\nThe model was trained and tested on an internal dataset with approximately 10,000 concepts and 20M images, with multiple concepts per image. The class distributions on train and validation sets are long-tailed. The validation set was annotated using a combination of originally curated labels with incomplete annotations, where were further completed by adding additional labels proposed a newer version of this model (aa7f35c01e0642fda5cf400f543e7c40) at a low threshold and verified by human annotators. "\n  modified_at {\n    seconds: 1634831222\n    nanos: 80260000\n  }\n  import_info {\n  }\n}\ninput {\n  id: "98eed12013334f3a80bad386d6fa391b"\n  data {\n    image {\n      url: "https://samples.clarifai.com/metro-north.jpg"\n    }\n  }\n}\ndata {\n  concepts {\n    id: "ai_HLmqFqBf"\n    name: "train"\n    value: 0.9996053576469421\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_fvlBqXZR"\n    name: "railway"\n    value: 0.9992986917495728\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_SHNDcmJ3"\n    name: "subway system"\n    value: 0.9982514977455139\n    app_id: "main"\n  }\n}\n',U="Predicted concepts:\ntrain 1.00\nrailway 1.00\nsubway system 1.00\nstation 1.00\nlocomotive 1.00\ntransportation system 1.00\ntravel 0.99\ncommuter 0.98\nplatform 0.98\nlight 0.97\ntrain station 0.97\nblur 0.97\ncity 0.96\nroad 0.96\nurban 0.96\ntraffic 0.96",x='id: "6a23c0c0893d42b5a7f5973dcc4a2757"\nstatus {\n  code: SUCCESS\n  description: "Ok"\n}\ncreated_at {\n  seconds: 1643800207\n  nanos: 69912867\n}\nmodel {\n  id: "general-image-recognition"\n  name: "general"\n  created_at {\n    seconds: 1457543499\n    nanos: 608845000\n  }\n  app_id: "main"\n  output_info {\n    output_config {\n    }\n    message: "Show output_info with: GET /models/{model_id}/output_info"\n    fields_map {\n      fields {\n        key: "concepts"\n        value {\n          string_value: "softmax"\n        }\n      }\n    }\n  }\n  model_version {\n    id: "aa7f35c01e0642fda5cf400f543e7c40"\n    created_at {\n      seconds: 1520370624\n      nanos: 454834000\n    }\n    status {\n      code: MODEL_TRAINED\n      description: "Model is trained and ready"\n    }\n    visibility {\n      gettable: PUBLIC\n    }\n    app_id: "main"\n    user_id: "clarifai"\n    metadata {\n    }\n  }\n  display_name: "general-visual-classifier"\n  user_id: "clarifai"\n  input_info {\n    fields_map {\n      fields {\n        key: "image"\n        value {\n          string_value: "images"\n        }\n      }\n    }\n  }\n  train_info {\n  }\n  model_type_id: "visual-classifier"\n  visibility {\n    gettable: PUBLIC\n  }\n  description: "Image recognition model for identifying different concepts in images and video including objects, themes, moods, and more."\n  metadata {\n  }\n  notes: "**General Information**\\n\\n- Purpose: Classifier for a variety of concepts, common objects, etc. This model is a great all-purpose solution for most visual recognition needs with industry-leading performance.\\n\\n- Architecture: Customized InceptionV2\\n\\n- Intended Use: image indexing by tags, filtering, cascade routing\\n\\n- Limitations: works well when content is prevalent in the image\\n\\n\\n\\n **\\nTraining/Test Data**\\n\\nThe model was trained and tested on an internal dataset with approximately 10,000 concepts and 20M images, with multiple concepts per image. The class distributions on train and validation sets are long-tailed. The validation set was annotated using a combination of originally curated labels with incomplete annotations, where were further completed by adding additional labels proposed a newer version of this model (aa7f35c01e0642fda5cf400f543e7c40) at a low threshold and verified by human annotators. "\n  modified_at {\n    seconds: 1634831222\n    nanos: 80260000\n  }\n  import_info {\n  }\n}\ninput {\n  id: "76a59b937b9943adbcba4a54b9a079fe"\n  data {\n    image {\n      url: "https://samples.clarifai.com/metro-north.jpg"\n    }\n  }\n}\ndata {\n  concepts {\n    id: "ai_HLmqFqBf"\n    name: "train"\n    value: 0.9996053576469421\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_fvlBqXZR"\n    name: "railway"\n    value: 0.9992986917495728\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_SHNDcmJ3"\n    name: "subway system"\n    value: 0.9982514977455139\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_6kTjGfF6"\n    name: "station"\n    value: 0.9980105757713318\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_RRXLczch"\n    name: "locomotive"\n    value: 0.9972571730613708\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_Xxjc3MhT"\n    name: "transportation system"\n    value: 0.9969801306724548\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_VRmbGVWh"\n    name: "travel"\n    value: 0.988979697227478\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_jlb9q33b"\n    name: "commuter"\n    value: 0.9808752536773682\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_2gkfMDsM"\n    name: "platform"\n    value: 0.9806439876556396\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_n9vjC1jB"\n    name: "light"\n    value: 0.9742040634155273\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_sQQj52KZ"\n    name: "train station"\n    value: 0.9687404036521912\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_l4WckcJN"\n    name: "blur"\n    value: 0.9672204256057739\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_WBQfVV0p"\n    name: "city"\n    value: 0.9614798426628113\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_TZ3C79C6"\n    name: "road"\n    value: 0.9613829851150513\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_CpFBRWzD"\n    name: "urban"\n    value: 0.9603424072265625\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_tr0MBp64"\n    name: "traffic"\n    value: 0.9599347710609436\n    app_id: "main"\n  }\n}',j="Predicted concepts:\ntrain 1.00\nrailway 1.00\nsubway system 1.00\nstation 1.00\nlocomotive 1.00\ntransportation system 1.00\ntravel 0.99\ncommuter 0.98\nplatform 0.98\nlight 0.97\ntrain station 0.97\nblur 0.97\ncity 0.96\nroad 0.96\nurban 0.96\ntraffic 0.96\nstreet 0.95\npublic 0.93\ntramway 0.93\nbusiness 0.93",$='id: "f2885a9eda0e42c5b264ee344cf80152"\nstatus {\n  code: SUCCESS\n  description: "Ok"\n}\ncreated_at {\n  seconds: 1643803135\n  nanos: 222744005\n}\nmodel {\n  id: "general-image-recognition"\n  name: "general"\n  created_at {\n    seconds: 1457543499\n    nanos: 608845000\n  }\n  app_id: "main"\n  output_info {\n    output_config {\n    }\n    message: "Show output_info with: GET /models/{model_id}/output_info"\n    fields_map {\n      fields {\n        key: "concepts"\n        value {\n          string_value: "softmax"\n        }\n      }\n    }\n  }\n  model_version {\n    id: "aa7f35c01e0642fda5cf400f543e7c40"\n    created_at {\n      seconds: 1520370624\n      nanos: 454834000\n    }\n    status {\n      code: MODEL_TRAINED\n      description: "Model is trained and ready"\n    }\n    visibility {\n      gettable: PUBLIC\n    }\n    app_id: "main"\n    user_id: "clarifai"\n    metadata {\n    }\n  }\n  display_name: "general-visual-classifier"\n  user_id: "clarifai"\n  input_info {\n    fields_map {\n      fields {\n        key: "image"\n        value {\n          string_value: "images"\n        }\n      }\n    }\n  }\n  train_info {\n  }\n  model_type_id: "visual-classifier"\n  visibility {\n    gettable: PUBLIC\n  }\n  description: "Image recognition model for identifying different concepts in images and video including objects, themes, moods, and more."\n  metadata {\n  }\n  notes: "**General Information**\\n\\n- Purpose: Classifier for a variety of concepts, common objects, etc. This model is a great all-purpose solution for most visual recognition needs with industry-leading performance.\\n\\n- Architecture: Customized InceptionV2\\n\\n- Intended Use: image indexing by tags, filtering, cascade routing\\n\\n- Limitations: works well when content is prevalent in the image\\n\\n\\n\\n **\\nTraining/Test Data**\\n\\nThe model was trained and tested on an internal dataset with approximately 10,000 concepts and 20M images, with multiple concepts per image. The class distributions on train and validation sets are long-tailed. The validation set was annotated using a combination of originally curated labels with incomplete annotations, where were further completed by adding additional labels proposed a newer version of this model (aa7f35c01e0642fda5cf400f543e7c40) at a low threshold and verified by human annotators. "\n  modified_at {\n    seconds: 1634831222\n    nanos: 80260000\n  }\n  import_info {\n  }\n}\ninput {\n  id: "be28eb8cdfd34d199e2f69981cc827aa"\n  data {\n    image {\n      url: "https://samples.clarifai.com/metro-north.jpg"\n    }\n  }\n}\ndata {\n  concepts {\n    id: "ai_HLmqFqBf"\n    name: "train"\n    value: 0.9996053576469421\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_fvlBqXZR"\n    name: "railway"\n    value: 0.9992986917495728\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_SHNDcmJ3"\n    name: "subway system"\n    value: 0.9982514977455139\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_6kTjGfF6"\n    name: "station"\n    value: 0.9980105757713318\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_RRXLczch"\n    name: "locomotive"\n    value: 0.9972571730613708\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_Xxjc3MhT"\n    name: "transportation system"\n    value: 0.9969801306724548\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_VRmbGVWh"\n    name: "travel"\n    value: 0.988979697227478\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_jlb9q33b"\n    name: "commuter"\n    value: 0.9808752536773682\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_2gkfMDsM"\n    name: "platform"\n    value: 0.9806439876556396\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_n9vjC1jB"\n    name: "light"\n    value: 0.9742040634155273\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_sQQj52KZ"\n    name: "train station"\n    value: 0.9687404036521912\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_l4WckcJN"\n    name: "blur"\n    value: 0.9672204256057739\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_WBQfVV0p"\n    name: "city"\n    value: 0.9614798426628113\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_TZ3C79C6"\n    name: "road"\n    value: 0.9613829255104065\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_CpFBRWzD"\n    name: "urban"\n    value: 0.9603424072265625\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_tr0MBp64"\n    name: "traffic"\n    value: 0.959934651851654\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_GjVpxXrs"\n    name: "street"\n    value: 0.9474142789840698\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_mcSHVRfS"\n    name: "public"\n    value: 0.9343124032020569\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_J6d1kV8t"\n    name: "tramway"\n    value: 0.9318979382514954\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_6lhccv44"\n    name: "business"\n    value: 0.9294139742851257\n    app_id: "main"\n  }\n}\n',H={description:"Learn about model prediction parameters.",sidebar_position:6},B="Prediction Parameters",V={unversionedId:"api-guide/predict/prediction-parameters",id:"api-guide/predict/prediction-parameters",title:"Prediction Parameters",description:"Learn about model prediction parameters.",source:"@site/docs/api-guide/predict/prediction-parameters.md",sourceDirName:"api-guide/predict",slug:"/api-guide/predict/prediction-parameters",permalink:"/api-guide/predict/prediction-parameters",draft:!1,tags:[],version:"current",sidebarPosition:6,frontMatter:{description:"Learn about model prediction parameters.",sidebar_position:6},sidebar:"tutorialSidebar",previous:{title:"Audio",permalink:"/api-guide/predict/audio"},next:{title:"Multilingual Classification",permalink:"/api-guide/predict/multilingual-classification"}},G={},Y=[{value:"Select Concepts",id:"select-concepts",level:2},{value:"Maximum Concepts",id:"maximum-concepts",level:2},{value:"Minimum Prediction Value",id:"minimum-prediction-value",level:2},{value:"By Model Version ID",id:"by-model-version-id",level:2}],q={toc:Y},Z="wrapper";function X(n){let{components:e,...t}=n;return(0,i.kt)(Z,(0,a.Z)({},q,t,{components:e,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"prediction-parameters"},"Prediction Parameters"),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Learn about model prediction parameters")),(0,i.kt)("hr",null),(0,i.kt)("p",null,"You can set additional parameters to gain flexibility in the predict operation."),(0,i.kt)("h2",{id:"select-concepts"},"Select Concepts"),(0,i.kt)("p",null,"By putting this additional parameter on your predict calls, you can receive predict value","(","s",")"," for ",(0,i.kt)("strong",{parentName:"p"},"only")," the concepts that you want to. You can specify particular concepts by either their id and/or their name. "),(0,i.kt)("p",null,"The concept names and ids are case sensitive; and so, these must be exact matches."),(0,i.kt)("admonition",{type:"tip"},(0,i.kt)("p",{parentName:"admonition"},"To retrieve an entire list of concepts from a given model, and get their ids and names, use the ",(0,i.kt)("a",{parentName:"p",href:"https://docs.clarifai.com/api-guide/model/create-get-update-and-delete#get-model-output-info-by-id"},(0,i.kt)("inlineCode",{parentName:"a"},"GetModelOutputInfo"))," endpoint.")),(0,i.kt)("admonition",{type:"caution"},(0,i.kt)("p",{parentName:"admonition"},"If you submit a request with not an exact match of the concept id or name, you will receive an invalid model argument error. However, if one or more matches while one or more do not, the API will respond with a Mixed Success.")),(0,i.kt)("p",null,"Below is an example of how you would select concepts and receive predictions from Clarifai's ",(0,i.kt)("a",{parentName:"p",href:"https://clarifai.com/clarifai/main/models/general-image-recognition"},(0,i.kt)("inlineCode",{parentName:"a"},"general-image-recognition"))," model. "),(0,i.kt)("admonition",{type:"info"},(0,i.kt)("p",{parentName:"admonition"},"The initialization code used in the following examples is outlined in detail on the ",(0,i.kt)("a",{parentName:"p",href:"https://docs.clarifai.com/api-guide/api-overview/api-clients/#client-installation-instructions"},"client installation page."))),(0,i.kt)(o.Z,{mdxType:"Tabs"},(0,i.kt)(s.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,i.kt)(r.Z,{className:"language-python",mdxType:"CodeBlock"},l)),(0,i.kt)(s.Z,{value:"js_rest",label:"JavaScript (REST)",mdxType:"TabItem"},(0,i.kt)(r.Z,{className:"language-javascript",mdxType:"CodeBlock"},h)),(0,i.kt)(s.Z,{value:"nodejs",label:"NodeJS",mdxType:"TabItem"},(0,i.kt)(r.Z,{className:"language-javascript",mdxType:"CodeBlock"},E)),(0,i.kt)(s.Z,{value:"java",label:"Java",mdxType:"TabItem"},(0,i.kt)(r.Z,{className:"language-java",mdxType:"CodeBlock"},D)),(0,i.kt)(s.Z,{value:"php",label:"PHP",mdxType:"TabItem"},(0,i.kt)(r.Z,{className:"language-php",mdxType:"CodeBlock"},S)),(0,i.kt)(s.Z,{value:"curl",label:"cURL",mdxType:"TabItem"},(0,i.kt)(r.Z,{className:"language-bash",mdxType:"CodeBlock"},N))),(0,i.kt)("details",null,(0,i.kt)("summary",null,"Text Output Example"),(0,i.kt)(r.Z,{className:"language-text",mdxType:"CodeBlock"},M)),(0,i.kt)("details",null,(0,i.kt)("summary",null,"JSON Output Example"),(0,i.kt)(r.Z,{className:"language-javascript",mdxType:"CodeBlock"},R)),(0,i.kt)("h2",{id:"maximum-concepts"},"Maximum Concepts"),(0,i.kt)("p",null,"Setting the ",(0,i.kt)("inlineCode",{parentName:"p"},"max_concepts")," parameter will customize how many concepts and their corresponding probability scores the predict endpoint will return. If not specified, the predict endpoint will return the top 20 concepts. "),(0,i.kt)("p",null,"You can currently set the max concepts parameter to any number in the range of ","[","1-200","]",". "),(0,i.kt)("p",null,"If your use case requires more concepts, please contact ",(0,i.kt)("a",{parentName:"p",href:"mailto:support@clarifai.com"},"Support"),"."),(0,i.kt)("p",null,"Below is an example of how you would set maximum concepts and receive predictions from Clarifai's ",(0,i.kt)("a",{parentName:"p",href:"https://clarifai.com/clarifai/main/models/general-image-recognition"},(0,i.kt)("inlineCode",{parentName:"a"},"general-image-recognition"))," model. "),(0,i.kt)(o.Z,{mdxType:"Tabs"},(0,i.kt)(s.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,i.kt)(r.Z,{className:"language-python",mdxType:"CodeBlock"},p)),(0,i.kt)(s.Z,{value:"js_rest",label:"JavaScript (REST)",mdxType:"TabItem"},(0,i.kt)(r.Z,{className:"language-javascript",mdxType:"CodeBlock"},m)),(0,i.kt)(s.Z,{value:"nodejs",label:"NodeJS",mdxType:"TabItem"},(0,i.kt)(r.Z,{className:"language-javascript",mdxType:"CodeBlock"},g)),(0,i.kt)(s.Z,{value:"java",label:"Java",mdxType:"TabItem"},(0,i.kt)(r.Z,{className:"language-java",mdxType:"CodeBlock"},w)),(0,i.kt)(s.Z,{value:"php",label:"PHP",mdxType:"TabItem"},(0,i.kt)(r.Z,{className:"language-php",mdxType:"CodeBlock"},C)),(0,i.kt)(s.Z,{value:"curl",label:"cURL",mdxType:"TabItem"},(0,i.kt)(r.Z,{className:"language-bash",mdxType:"CodeBlock"},v))),(0,i.kt)("details",null,(0,i.kt)("summary",null,"Text Output Example"),(0,i.kt)(r.Z,{className:"language-text",mdxType:"CodeBlock"},k)),(0,i.kt)("details",null,(0,i.kt)("summary",null,"JSON Output Example"),(0,i.kt)(r.Z,{className:"language-javascript",mdxType:"CodeBlock"},L)),(0,i.kt)("h2",{id:"minimum-prediction-value"},"Minimum Prediction Value"),(0,i.kt)("p",null,"This parameter lets you set a minimum probability threshold for the outputs you want to view for the Predict operation. "),(0,i.kt)("p",null,"For example if you want to see all concepts with a probability score of .95 or higher, this parameter will allow you to accomplish that. "),(0,i.kt)("p",null,"Also note that if you don't specify the number of ",(0,i.kt)("inlineCode",{parentName:"p"},"max_concepts"),", you will only see the top 20. If your result can contain more values you will have to increase the number of maximum concepts as well."),(0,i.kt)("p",null,"Below is an example of how you would set a minimum probability threshold and receive predictions from Clarifai's ",(0,i.kt)("a",{parentName:"p",href:"https://clarifai.com/clarifai/main/models/general-image-recognition"},(0,i.kt)("inlineCode",{parentName:"a"},"general-image-recognition"))," model. "),(0,i.kt)(o.Z,{mdxType:"Tabs"},(0,i.kt)(s.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,i.kt)(r.Z,{className:"language-python",mdxType:"CodeBlock"},u)),(0,i.kt)(s.Z,{value:"js_rest",label:"JavaScript (REST)",mdxType:"TabItem"},(0,i.kt)(r.Z,{className:"language-javascript",mdxType:"CodeBlock"},f)),(0,i.kt)(s.Z,{value:"nodejs",label:"NodeJS",mdxType:"TabItem"},(0,i.kt)(r.Z,{className:"language-javascript",mdxType:"CodeBlock"},I)),(0,i.kt)(s.Z,{value:"java",label:"Java",mdxType:"TabItem"},(0,i.kt)(r.Z,{className:"language-java",mdxType:"CodeBlock"},T)),(0,i.kt)(s.Z,{value:"php",label:"PHP",mdxType:"TabItem"},(0,i.kt)(r.Z,{className:"language-php",mdxType:"CodeBlock"},y)),(0,i.kt)(s.Z,{value:"curl",label:"cURL",mdxType:"TabItem"},(0,i.kt)(r.Z,{className:"language-bash",mdxType:"CodeBlock"},P))),(0,i.kt)("details",null,(0,i.kt)("summary",null,"Text Output Example"),(0,i.kt)(r.Z,{className:"language-text",mdxType:"CodeBlock"},U)),(0,i.kt)("details",null,(0,i.kt)("summary",null,"JSON Output Example"),(0,i.kt)(r.Z,{className:"language-javascript",mdxType:"CodeBlock"},x)),(0,i.kt)("h2",{id:"by-model-version-id"},"By Model Version ID"),(0,i.kt)("p",null,"Every time you train a custom model, it creates a new model version. By specifying ",(0,i.kt)("inlineCode",{parentName:"p"},"version_id")," in your predict call, you can continue to predict on a previous version, for consistent prediction results. Clarifai also updates its pre-built models on a regular basis."),(0,i.kt)("p",null,"If you are looking for consistent results from your predict calls, use ",(0,i.kt)("inlineCode",{parentName:"p"},"version_id"),". If the model ",(0,i.kt)("inlineCode",{parentName:"p"},"version_id")," is not specified, predict will default to the most current model."),(0,i.kt)("p",null,"Below is an example of how you would set a model version ID and receive predictions from Clarifai's ",(0,i.kt)("a",{parentName:"p",href:"https://clarifai.com/clarifai/main/models/general-image-recognition"},(0,i.kt)("inlineCode",{parentName:"a"},"general-image-recognition"))," model. "),(0,i.kt)(o.Z,{mdxType:"Tabs"},(0,i.kt)(s.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,i.kt)(r.Z,{className:"language-python",mdxType:"CodeBlock"},c)),(0,i.kt)(s.Z,{value:"js_rest",label:"JavaScript (REST)",mdxType:"TabItem"},(0,i.kt)(r.Z,{className:"language-javascript",mdxType:"CodeBlock"},d)),(0,i.kt)(s.Z,{value:"nodejs",label:"NodeJS",mdxType:"TabItem"},(0,i.kt)(r.Z,{className:"language-javascript",mdxType:"CodeBlock"},_)),(0,i.kt)(s.Z,{value:"java",label:"Java",mdxType:"TabItem"},(0,i.kt)(r.Z,{className:"language-java",mdxType:"CodeBlock"},O)),(0,i.kt)(s.Z,{value:"php",label:"PHP",mdxType:"TabItem"},(0,i.kt)(r.Z,{className:"language-php",mdxType:"CodeBlock"},b)),(0,i.kt)(s.Z,{value:"curl",label:"cURL",mdxType:"TabItem"},(0,i.kt)(r.Z,{className:"language-bash",mdxType:"CodeBlock"},A))),(0,i.kt)("details",null,(0,i.kt)("summary",null,"Text Output Example"),(0,i.kt)(r.Z,{className:"language-text",mdxType:"CodeBlock"},j)),(0,i.kt)("details",null,(0,i.kt)("summary",null,"JSON Output Example"),(0,i.kt)(r.Z,{className:"language-javascript",mdxType:"CodeBlock"},$)))}X.isMDXComponent=!0}}]);
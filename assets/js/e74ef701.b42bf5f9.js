"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[8466],{28453:(e,n,s)=>{s.d(n,{R:()=>c,x:()=>l});var i=s(96540);const r={},d=i.createContext(r);function c(e){const n=i.useContext(d);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:c(e.components),i.createElement(d.Provider,{value:n},e.children)}},72931:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>t,contentTitle:()=>l,default:()=>h,frontMatter:()=>c,metadata:()=>i,toc:()=>a});const i=JSON.parse('{"id":"compute/deployments/cloud-instances","title":"Supported Cloud Instances","description":"Learn about the instance types we support","source":"@site/docs/compute/deployments/cloud-instances.md","sourceDirName":"compute/deployments","slug":"/compute/deployments/cloud-instances","permalink":"/compute/deployments/cloud-instances","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"description":"Learn about the instance types we support","sidebar_position":5,"pagination_next":null},"sidebar":"tutorialSidebar","previous":{"title":"Manage Your Compute","permalink":"/compute/deployments/manage-compute"}}');var r=s(74848),d=s(28453);const c={description:"Learn about the instance types we support",sidebar_position:5,pagination_next:null},l="Supported Cloud Instances",t={},a=[{value:"Amazon Web Services (AWS) Instances",id:"amazon-web-services-aws-instances",level:2},{value:"T3A Instances",id:"t3a-instances",level:3},{value:"G4DN Instances",id:"g4dn-instances",level:3},{value:"G5 Instances",id:"g5-instances",level:3},{value:"G6 Instances",id:"g6-instances",level:3},{value:"Google Cloud Platform (GCP) Instances",id:"google-cloud-platform-gcp-instances",level:2},{value:"N2-Standard Instances",id:"n2-standard-instances",level:3},{value:"G2-Standard Instances",id:"g2-standard-instances",level:3},{value:"A2 &amp; A3 High-Performance Instances",id:"a2--a3-high-performance-instances",level:3},{value:"TPU v5e &amp; v5p High-Performance Instances",id:"tpu-v5e--v5p-high-performance-instances",level:3},{value:"Vultr Cloud Servers Instances",id:"vultr-cloud-servers-instances",level:2},{value:"VC2 Instances",id:"vc2-instances",level:3},{value:"VCG Instances",id:"vcg-instances",level:3},{value:"GH200 &amp; MI300X High-Performance GPU Instances",id:"gh200--mi300x-high-performance-gpu-instances",level:3},{value:"Oracle Distributed Cloud Instances",id:"oracle-distributed-cloud-instances",level:2},{value:"NVIDIA-A10G GPU Instances",id:"nvidia-a10g-gpu-instances",level:3},{value:"AMD-MI300X GPU Instances",id:"amd-mi300x-gpu-instances",level:3},{value:"Standard CPU-Only Instances",id:"standard-cpu-only-instances",level:3}];function o(e){const n={a:"a",admonition:"admonition",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,d.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"supported-cloud-instances",children:"Supported Cloud Instances"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Learn about the instance types we support"})}),"\n",(0,r.jsx)("hr",{}),"\n",(0,r.jsx)(n.p,{children:"We offer a range of instance types designed to handle a variety of machine learning workloads. These cloud instances vary in their CPU, RAM (Random Access Memory), and GPU configurations, which allow you to orchestrate the right balance of performance and cost for your use case."}),"\n",(0,r.jsx)(n.admonition,{title:"pricing",type:"info",children:(0,r.jsxs)(n.p,{children:["To learn more about pricing for each instance type, see the ",(0,r.jsx)(n.a,{href:"https://www.clarifai.com/pricing",children:"pricing page"}),"."]})}),"\n",(0,r.jsx)(n.h2,{id:"amazon-web-services-aws-instances",children:"Amazon Web Services (AWS) Instances"}),"\n",(0,r.jsx)(n.h3,{id:"t3a-instances",children:"T3A Instances"}),"\n",(0,r.jsx)(n.p,{children:"The AWS T3A series is intended for cost\u2011effective, general\u2011purpose workloads that do not require GPU acceleration. It provides a balanced mix of CPU and memory, making it suitable for lightweight use cases."}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Instance Type"}),(0,r.jsx)(n.th,{children:"GPU"}),(0,r.jsx)(n.th,{children:"GPU MEMORY"}),(0,r.jsx)(n.th,{children:"CPU"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"t3a.medium"})}),(0,r.jsx)(n.td,{children:"-"}),(0,r.jsx)(n.td,{children:"-"}),(0,r.jsx)(n.td,{children:"1.5 cores (2.89Gi)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"t3a.large"})}),(0,r.jsx)(n.td,{children:"-"}),(0,r.jsx)(n.td,{children:"-"}),(0,r.jsx)(n.td,{children:"1.5 cores (6.4Gi)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"t3a.xlarge"})}),(0,r.jsx)(n.td,{children:"-"}),(0,r.jsx)(n.td,{children:"-"}),(0,r.jsx)(n.td,{children:"3.5 cores (13.55Gi)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"t3a.2xlarge"})}),(0,r.jsx)(n.td,{children:"-"}),(0,r.jsx)(n.td,{children:"-"}),(0,r.jsx)(n.td,{children:"7.5 cores (28.35Gi)"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Key Features"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["vCPU (virtual CPUs) performance \u2014 Burstable performance that adapts to workload spikes. For example, ",(0,r.jsx)(n.code,{children:"t3a.medium"})," provides ~1.5 vCPUs, while ",(0,r.jsx)(n.code,{children:"t3a.2xlarge"})," scales up to ~7.5 vCPUs."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Memory \u2014 Ranges from 2.89 GiB to 28.35 GiB, enabling efficient in-memory data handling for lightweight to moderately intensive workloads."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Efficiency \u2014 Optimized for cost savings compared to other instance families, making them budget-friendly for everyday use."}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Example Use Case"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Running simple models such as for classification or regression tasks."}),"\n"]}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Note"}),": The CPU values (e.g.,\u202f1.5\u202fcores) are baseline vCPU allocations expressed as fractional units. The instance can burst up to its full vCPU count (e.g., 2\u202fvCPUs for ",(0,r.jsx)(n.code,{children:"t3a.medium"}),") by consuming CPU credits."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"g4dn-instances",children:"G4DN Instances"}),"\n",(0,r.jsx)(n.p,{children:"The AWS G4dn series is built for GPU-accelerated workloads at a moderate scale. These instances combine NVIDIA T4 GPUs with balanced CPU and memory resources, making them well-suited for small-to-medium machine learning and inference tasks."}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Instance Type"}),(0,r.jsx)(n.th,{children:"GPU"}),(0,r.jsx)(n.th,{children:"GPU MEMORY"}),(0,r.jsx)(n.th,{children:"CPU"})]})}),(0,r.jsx)(n.tbody,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"g4dn.xlarge"})}),(0,r.jsx)(n.td,{children:"1x NVIDIA-T4 (15Gi)"}),(0,r.jsx)(n.td,{children:"15Gi"}),(0,r.jsx)(n.td,{children:"3.5 cores (13.86Gi)"})]})})]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Key Features"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"NVIDIA T4 GPU \u2014 Designed for inference and light training, offering strong efficiency for workloads at a lower cost compared to heavier GPU families."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"vCPUs and RAM \u2014 Provides ~3.5 vCPUs (baseline) and ~13.86 GiB memory, giving enough capacity to manage GPU-accelerated tasks, preprocessing, and orchestration."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Balanced performance \u2014 Ideal when you need GPU acceleration without the overhead of large, expensive GPU instances."}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Example Use Cases"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Inference workloads, such as running NLP models such as BERT-base for summarization, classification, or question answering."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Light training smaller models or experimenting with prototypes before scaling to larger GPU families."}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"g5-instances",children:"G5 Instances"}),"\n",(0,r.jsx)(n.p,{children:"The AWS G5 series provides high-performance GPU capabilities for workloads that demand more memory and compute power. These instances are optimized for deep learning training, large-scale inference, and advanced computer vision tasks."}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Instance Type"}),(0,r.jsx)(n.th,{children:"GPU"}),(0,r.jsx)(n.th,{children:"GPU MEMORY"}),(0,r.jsx)(n.th,{children:"CPU"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"g5.xlarge"})}),(0,r.jsx)(n.td,{children:"1x NVIDIA-A10G (22.49Gi)"}),(0,r.jsx)(n.td,{children:"22.49Gi"}),(0,r.jsx)(n.td,{children:"3.5 cores (13.55Gi)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"g5.2xlarge"})}),(0,r.jsx)(n.td,{children:"1x NVIDIA-A10G (22.49Gi)"}),(0,r.jsx)(n.td,{children:"22.49Gi"}),(0,r.jsx)(n.td,{children:"7.5 cores (28.35Gi)"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Key Features"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"NVIDIA A10G GPU \u2014 High compute throughput and memory bandwidth, enabling faster training for deep learning and support for more complex models compared to T4 GPUs."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Scalable CPU & memory \u2014 From ~3.5 to ~7.5 vCPUs and 13.55 to 28.35 GiB of RAM, supporting data-heavy preprocessing, augmentation, and orchestration alongside GPU tasks."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Balanced design \u2014 Efficient for both training and inference, bridging the gap between lightweight GPU instances (like G4dn) and specialized multi-GPU clusters."}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Example Use Cases"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Training mid-sized NLP models like GPT-2 or T5 for text generation, or training image segmentation models like UNet or Mask R-CNN for medical imaging."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Running object tracking, pose estimation, or other GPU-accelerated pipelines for video analytics."}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.admonition,{title:"How to Choose the Best GPU",type:"tip",children:(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://www.clarifai.com/blog/nvidia-a10-vs-l40s-gpus-for-ai-workloads",children:"NVIDIA A10 or NVIDIA L40S?"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://www.clarifai.com/blog/nvidia-a10-vs-a100-choosing-the-right-gpu-for-ai-workloads",children:"NVIDIA A10 or NVIDIA A100?"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://www.clarifai.com/blog/nvidia-b200-vs-h100",children:"NVIDIA B200 or NVIDIA H100?"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://www.clarifai.com/blog/nvidia-a100-vs.-h100-choosing-the-right-gpu-for-your-ai-workloads",children:"NVIDIA A100 or NVIDIA H100?"})}),"\n"]})}),"\n",(0,r.jsx)(n.h3,{id:"g6-instances",children:"G6 Instances"}),"\n",(0,r.jsx)(n.p,{children:"The AWS G6 series introduces next-generation NVIDIA GPUs for the most demanding machine learning and simulation workloads. These instances scale from single-GPU mid-tier setups to multi-GPU, high-memory configurations capable of handling large-scale model training."}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Instance Type"}),(0,r.jsx)(n.th,{children:"GPU"}),(0,r.jsx)(n.th,{children:"GPU MEMORY"}),(0,r.jsx)(n.th,{children:"CPU"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"g6.xlarge"})}),(0,r.jsx)(n.td,{children:"1x NVIDIA-L4 (22.49Gi)"}),(0,r.jsx)(n.td,{children:"22.49Gi"}),(0,r.jsx)(n.td,{children:"3.5 cores (13.55Gi)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"g6.2xlarge"})}),(0,r.jsx)(n.td,{children:"1x NVIDIA-L4 (22.49Gi)"}),(0,r.jsx)(n.td,{children:"22.49Gi"}),(0,r.jsx)(n.td,{children:"7.5 cores (28.35Gi)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"g6e.xlarge"})}),(0,r.jsx)(n.td,{children:"1x NVIDIA-L40S (44.99Gi)"}),(0,r.jsx)(n.td,{children:"44.99Gi"}),(0,r.jsx)(n.td,{children:"3.5 cores (28.35Gi)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"g6e.2xlarge"})}),(0,r.jsx)(n.td,{children:"1x NVIDIA-L40S (44.99Gi)"}),(0,r.jsx)(n.td,{children:"44.99Gi"}),(0,r.jsx)(n.td,{children:"7.5 cores (57.95Gi)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"g6e.12xlarge"})}),(0,r.jsx)(n.td,{children:"4x NVIDIA-L40S (44.99Gi)"}),(0,r.jsx)(n.td,{children:"179.95Gi"}),(0,r.jsx)(n.td,{children:"47.4 cores (351.44Gi)"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Key Features"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Next-Gen GPUs \u2014 NVIDIA L4 GPUs target efficient inference and fine-tuning, while L40S GPUs deliver high throughput for large-scale training."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Scalable GPU memory \u2014 From 22.49 GiB (L4) to nearly 180 GiB (multi-L40S), supporting workloads from mid-sized tasks to multi-modal foundation models."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["High vCPU & RAM options \u2014 Up to 47.4 cores and 351 GiB RAM in ",(0,r.jsx)(n.code,{children:"g6e.12xlarge"}),", enabling massive parallelism and data-heavy preprocessing."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Flexible tiers \u2014 Ranges from cost-efficient single-GPU instances to powerful multi-GPU setups."}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Example Use Cases"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"G6 (L4 instances) support mid-tier workloads such as fine-tuning BERT-large, or computer vision tasks like text-to-image generation and object recognition."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"G6e (L40S instances) support advanced training workloads, including large-scale language models (e.g., GPT-4, T5-XL) or multi-modal tasks requiring both vision and language."}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"google-cloud-platform-gcp-instances",children:"Google Cloud Platform (GCP) Instances"}),"\n",(0,r.jsx)(n.h3,{id:"n2-standard-instances",children:"N2-Standard Instances"}),"\n",(0,r.jsx)(n.p,{children:"The GCP N2-Standard series offers cost-effective, general-purpose compute for workloads that don\u2019t require GPU acceleration. These instances balance CPU and memory, making them well-suited for lightweight applications, preprocessing, and small-scale model deployment."}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Instance Type"}),(0,r.jsx)(n.th,{children:"GPU"}),(0,r.jsx)(n.th,{children:"GPU MEMORY"}),(0,r.jsx)(n.th,{children:"CPU"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"n2-standard-2"})}),(0,r.jsx)(n.td,{children:"-"}),(0,r.jsx)(n.td,{children:"-"}),(0,r.jsx)(n.td,{children:"1.4 cores (5.42Gi)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"n2-standard-4"})}),(0,r.jsx)(n.td,{children:"-"}),(0,r.jsx)(n.td,{children:"-"}),(0,r.jsx)(n.td,{children:"3.4 cores (12.63Gi)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"n2-standard-8"})}),(0,r.jsx)(n.td,{children:"-"}),(0,r.jsx)(n.td,{children:"-"}),(0,r.jsx)(n.td,{children:"7.3 cores (27.67Gi)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"n2-standard-16"})}),(0,r.jsx)(n.td,{children:"-"}),(0,r.jsx)(n.td,{children:"-"}),(0,r.jsx)(n.td,{children:"15.3 cores (57.75Gi)"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Key Features"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"vCPUs \u2014 Baseline performance scales from ~1.4 to ~15.3 cores, with the ability to burst to the full allocation (2 to 16 vCPUs). Optimized for CPU-intensive tasks, such as running traditional models."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Memory (RAM) \u2014 From 5.42 GiB to 57.75 GiB, supporting in-memory data handling for lightweight to moderately intensive workloads."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Cost efficiency \u2014 Designed to deliver consistent performance at a lower cost, ideal for everyday compute tasks without GPU requirements.."}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Example Use Case"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Running small-scale machine learning models or serving simple inference workloads."}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"g2-standard-instances",children:"G2-Standard Instances"}),"\n",(0,r.jsx)(n.p,{children:"The GCP G2-Standard series provides GPU acceleration with NVIDIA L4 GPUs, designed for moderate machine learning and inference workloads. These instances scale from small setups to larger configurations, balancing cost with performance for small-to-medium tasks."}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Instance Type"}),(0,r.jsx)(n.th,{children:"GPU"}),(0,r.jsx)(n.th,{children:"GPU MEMORY"}),(0,r.jsx)(n.th,{children:"CPU"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"g2-standard-4"})}),(0,r.jsx)(n.td,{children:"1x NVIDIA-L4 (22.49Gi)"}),(0,r.jsx)(n.td,{children:"22.49Gi"}),(0,r.jsx)(n.td,{children:"3.4 cores (12.63Gi)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"g2-standard-8"})}),(0,r.jsx)(n.td,{children:"1x NVIDIA-L4 (22.49Gi)"}),(0,r.jsx)(n.td,{children:"22.49Gi"}),(0,r.jsx)(n.td,{children:"7.3 cores (27.67Gi)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"g2-standard-12"})}),(0,r.jsx)(n.td,{children:"1x NVIDIA-L4 (22.49Gi)"}),(0,r.jsx)(n.td,{children:"22.49Gi"}),(0,r.jsx)(n.td,{children:"11.3 cores (42.71Gi)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"g2-standard-16"})}),(0,r.jsx)(n.td,{children:"1x NVIDIA-L4 (22.49Gi)"}),(0,r.jsx)(n.td,{children:"22.49Gi"}),(0,r.jsx)(n.td,{children:"15.3 cores (57.75Gi)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"g2-standard-32"})}),(0,r.jsx)(n.td,{children:"1x NVIDIA-L4 (22.49Gi)"}),(0,r.jsx)(n.td,{children:"22.49Gi"}),(0,r.jsx)(n.td,{children:"31.3 cores (118.07Gi)"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Key Features"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"NVIDIA L4 GPUs \u2014 Optimized for inference and light training, delivering strong efficiency for vision and NLP tasks at lower cost compared to heavier GPU families."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"CPU & memory scaling \u2014 From ~3.4 to ~31.3 cores and 12.63 GiB to 118.07 GiB RAM, allowing smooth orchestration of preprocessing, data loading, and GPU-bound tasks."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Cost-performance balance \u2014 A versatile option for teams that need GPU acceleration without the expense of A100/H100-based instances."}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Example Use Cases"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Running transformer-based models like BERT-base for summarization, classification, or Q&A."}),"\n",(0,r.jsx)(n.li,{children:"Fine-tuning smaller computer vision models for object detection or image classification."}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"a2--a3-high-performance-instances",children:"A2 & A3 High-Performance Instances"}),"\n",(0,r.jsx)(n.p,{children:"The A2 and A3 series are GCP\u2019s flagship high-performance GPU instances, designed for large-scale deep learning, high-performance inference, and real-time AI workloads. With NVIDIA A100 and H100 GPUs, they scale from single-GPU setups to multi-GPU powerhouse configurations capable of training foundation models."}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Instance Type"}),(0,r.jsx)(n.th,{children:"GPU"}),(0,r.jsx)(n.th,{children:"GPU MEMORY"}),(0,r.jsx)(n.th,{children:"CPU"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"a2-ultragpu-1g"})}),(0,r.jsx)(n.td,{children:"1x NVIDIA-A100 (80Gi)"}),(0,r.jsx)(n.td,{children:"80Gi"}),(0,r.jsx)(n.td,{children:"11.3 cores (159.23Gi)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"a3-highgpu-1g"})}),(0,r.jsx)(n.td,{children:"1x NVIDIA-H100 (79.65Gi)"}),(0,r.jsx)(n.td,{children:"79.65Gi"}),(0,r.jsx)(n.td,{children:"25.3 cores (221.95Gi)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"a3-highgpu-8g"})}),(0,r.jsx)(n.td,{children:"8x NVIDIA-H100 (79.65Gi)"}),(0,r.jsx)(n.td,{children:"637.18Gi"}),(0,r.jsx)(n.td,{children:"206.8 cores (1,827.19Gi)"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Key Features"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["Next-generation GPUs \u2014 A100 (80 GiB) excels at large-scale training with strong throughput and memory bandwidth. H100 (80 GiB) delivers significant improvements for transformer-based models, enabling faster training and inference. Multi-GPU (",(0,r.jsx)(n.code,{children:"a3-highgpu-8g"}),") configurations scale this power dramatically."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["vMassive CPU & RAM scaling \u2014 From 11.3 cores / 159 GiB RAM in ",(0,r.jsx)(n.code,{children:"a2-ultragpu-1g"})," to 206.8 cores / 1.8 TiB RAM in ",(0,r.jsx)(n.code,{children:"a3-highgpu-8g"}),", ensuring parallel data pipelines can keep pace with GPU compute."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Flexible tiers \u2014 Options for single-GPU tasks or multi-GPU clusters, matching workloads of different scales and budgets."}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Example Use Cases"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Single-GPU (A2 / A3-1g) can be used for training or fine-tuning mid-to-large language models (e.g., GPT-3, T5-XL) or advanced vision models."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Multi-GPU (A3-8g) can be used for training large-scale, next-generation foundation models (e.g., GPT-4, PaLM, multi-modal transformers) where scale and GPU memory aggregation are critical."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Deploying video analytics, autonomous systems, or robotics pipelines that demand real-time, ultra-low latency."}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"tpu-v5e--v5p-high-performance-instances",children:"TPU v5e & v5p High-Performance Instances"}),"\n",(0,r.jsx)(n.p,{children:"Google\u2019s cloud TPU v5e and v5p instances are purpose-built accelerators optimized for deep learning training and inference. Unlike GPUs, TPUs (Tensor Processing Units) are specialized for matrix-heavy tensor operations, making them ideal for transformer-based models and large-scale distributed training."}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Instance Type"}),(0,r.jsx)(n.th,{children:"GPU"}),(0,r.jsx)(n.th,{children:"GPU MEMORY"}),(0,r.jsx)(n.th,{children:"CPU"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"ct5lp-hightpu-4t"})}),(0,r.jsx)(n.td,{children:"4x GOOGLE-TPU-v5e (-Gi)"}),(0,r.jsx)(n.td,{children:"-"}),(0,r.jsx)(n.td,{children:"111.1 cores (180.79Gi)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"ct5lp-hightpu-1t"})}),(0,r.jsx)(n.td,{children:"1x GOOGLE-TPU-v5e (-Gi)"}),(0,r.jsx)(n.td,{children:"-"}),(0,r.jsx)(n.td,{children:"23.3 cores (42.71Gi)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"ct5p-hightpu-4t"})}),(0,r.jsx)(n.td,{children:"4x GOOGLE-TPU-v5p (-Gi)"}),(0,r.jsx)(n.td,{children:"-"}),(0,r.jsx)(n.td,{children:"206.8 cores (431.67Gi)"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Key Features"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Specialized Tensor Processing Units (TPUs) \u2014 TPU v5e provides a balanced design optimized for cost-efficiency in large-scale training and inference, which is great for productionizing ML workloads where throughput matters. TPU v5p provides higher-performance generation with faster interconnects and larger scaling potential, designed for frontier model training."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Scalable CPU & memory \u2014 From 23.3 cores / 42.71 GiB RAM in the 1-core v5e instance to 206.8 cores / 431.67 GiB RAM in the 4-core v5p, ensuring sufficient orchestration power for massive training workloads."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"No exposed GPU memory \u2014 TPU memory is not presented like GPU VRAM but is instead managed by the TPU runtime for high-efficiency tensor operations."}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Example Use Cases"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"TPU v5e (1t, 4t) can be used for cost-efficient training of language models (e.g., BERT, T5-small/XL) and vision transformers (ViT)."}),"\n",(0,r.jsx)(n.li,{children:"TPU v5p (4t) can be used for training large foundation models such as PaLM, Gemini-like multi-modal architectures, or massive LLMs where performance and throughput at scale are critical."}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"vultr-cloud-servers-instances",children:"Vultr Cloud Servers Instances"}),"\n",(0,r.jsx)(n.h3,{id:"vc2-instances",children:"VC2 Instances"}),"\n",(0,r.jsx)(n.p,{children:"The Vultr VC2 series provides general-purpose compute instances optimized for workloads that do not require GPU acceleration. With a balance of CPU and memory, these instances are best suited for lightweight use cases."}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Instance Type"}),(0,r.jsx)(n.th,{children:"GPU"}),(0,r.jsx)(n.th,{children:"GPU MEMORY"}),(0,r.jsx)(n.th,{children:"CPU"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"vc2-2c-4gb"})}),(0,r.jsx)(n.td,{children:"-"}),(0,r.jsx)(n.td,{children:"-"}),(0,r.jsx)(n.td,{children:"1.4 cores (3.54Gi)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"vc2-4c-8gb "})}),(0,r.jsx)(n.td,{children:"-"}),(0,r.jsx)(n.td,{children:"-"}),(0,r.jsx)(n.td,{children:"3.4 cores (7.34Gi)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"vc2-6c-16gb"})}),(0,r.jsx)(n.td,{children:"-"}),(0,r.jsx)(n.td,{children:"-"}),(0,r.jsx)(n.td,{children:"5.4 cores (14.94Gi)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"vc2-8c-32gb"})}),(0,r.jsx)(n.td,{children:"-"}),(0,r.jsx)(n.td,{children:"-"}),(0,r.jsx)(n.td,{children:"7.4 cores (30.14Gi)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"vc2-16c-64gb"})}),(0,r.jsx)(n.td,{children:"-"}),(0,r.jsx)(n.td,{children:"-"}),(0,r.jsx)(n.td,{children:"15.4 cores (60.54Gi)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"vc2-24c-96gb"})}),(0,r.jsx)(n.td,{children:"-"}),(0,r.jsx)(n.td,{children:"-"}),(0,r.jsx)(n.td,{children:"23.4 cores (90.94Gi)"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Key Features"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["Scalable CPU and RAM \u2014 Configurations range from 1.4 cores / 3.54 GiB RAM (",(0,r.jsx)(n.code,{children:"vc2-2c-4gb"}),") up to 23.4 cores / 90.94 GiB RAM (",(0,r.jsx)(n.code,{children:"vc2-24c-96gb"}),")."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Cost-effective \u2014 Optimized for environments where GPU acceleration is unnecessary, making them a good fit for traditional compute workloads."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Flexibility \u2014 Suitable for a broad range of general-purpose tasks, with instance sizes that scale from small testing environments to larger services."}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Example Use Cases"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Suitable for lightweight applications as well as development and testing environments."}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"vcg-instances",children:"VCG Instances"}),"\n",(0,r.jsx)(n.p,{children:"The Vultr VCG series provides instances with dedicated NVIDIA GPUs, enabling acceleration for deep learning, inference, and GPU-intensive applications. These instances scale from entry-level GPU setups to multi-GPU clusters, making them versatile for workloads ranging from experimentation to frontier AI model training."}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Instance Type"}),(0,r.jsx)(n.th,{children:"GPU"}),(0,r.jsx)(n.th,{children:"GPU MEMORY"}),(0,r.jsx)(n.th,{children:"CPU"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"vcg-a16-6c-64g-16vram"})}),(0,r.jsx)(n.td,{children:"1x NVIDIA-A16 (16Gi)"}),(0,r.jsx)(n.td,{children:"16Gi"}),(0,r.jsx)(n.td,{children:"5.4 cores (60.54Gi)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"vcg-a100-12c-120g-80vram"})}),(0,r.jsx)(n.td,{children:"1x NVIDIA-A100 (80Gi)"}),(0,r.jsx)(n.td,{children:"80Gi"}),(0,r.jsx)(n.td,{children:"11.4 cores (113.74Gi)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"vcg-l40s-16c-180g-48vram"})}),(0,r.jsx)(n.td,{children:"1x NVIDIA-L40S (44.99Gi)"}),(0,r.jsx)(n.td,{children:"44.99Gi"}),(0,r.jsx)(n.td,{children:"15.4 cores (170.74Gi)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"vcg-b200-248c-2826g-1536vram"})}),(0,r.jsx)(n.td,{children:"8x NVIDIA-B200 (179.06Gi)"}),(0,r.jsx)(n.td,{children:"1,432.49Gi"}),(0,r.jsx)(n.td,{children:"255.4 cores (1,945.34Gi)"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Key Features"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Range of NVIDIA GPUs \u2014 From the A16 (lightweight inference) to the A100 (high-performance training), L40S (next-gen accelerated workloads), and B200 clusters (frontier-scale AI with 8 GPUs)."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"High vCPU and RAM configurations \u2014 Scales from 5.4 cores / 60 GiB RAM in entry-level instances to 255 cores / 1.9 TiB RAM in multi-GPU setups, ensuring GPU workloads are matched with sufficient CPU and memory."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Scalable GPU memory \u2014 Ranges from 16 GiB (A16) for smaller tasks up to 1.4 TiB (8 \xd7 B200) for extreme AI training."}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Example Use Cases"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"High-performance training and inference for large-scale deep learning models."}),"\n",(0,r.jsx)(n.li,{children:"Running AI inference workloads with optimized GPU acceleration."}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"gh200--mi300x-high-performance-gpu-instances",children:"GH200 & MI300X High-Performance GPU Instances"}),"\n",(0,r.jsx)(n.p,{children:"Vultr offers high-performance GPU instances powered by NVIDIA and AMD accelerators. These instances are built for AI training, inference, and HPC (high-performance computing) workloads that demand extreme compute and memory bandwidth."}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Instance Type"}),(0,r.jsx)(n.th,{children:"GPU"}),(0,r.jsx)(n.th,{children:"GPU MEMORY"}),(0,r.jsx)(n.th,{children:"CPU"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"vbm-72c-480gb-gh200-gpu"})}),(0,r.jsx)(n.td,{children:"1x NVIDIA-GH200 (95.58Gi)"}),(0,r.jsx)(n.td,{children:"95.58Gi"}),(0,r.jsx)(n.td,{children:"71.4 cores (455.74Gi)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"vbm-256c-2048gb-8-mi3000x-gpu"})}),(0,r.jsx)(n.td,{children:"8x AMD-MI300X (127.82Gi)"}),(0,r.jsx)(n.td,{children:"1,022.53Gi"}),(0,r.jsx)(n.td,{children:"255.4 cores (1,945.34Gi)"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Key Features"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"NVIDIA GH200 superchip \u2014 Combines Hopper GPU architecture with Grace CPU integration, delivering ultra-fast memory bandwidth and low-latency compute, ideal for training massive AI models and real-time inference."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"AMD MI300X GPUs \u2014 Designed for frontier AI workloads, offering huge HBM3 memory (128 GiB per GPU) and scaling efficiency with 8 GPUs per instance. Excellent for distributed training of very large models."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"High CPU & RAM configurations \u2014 From 71 cores / 455 GiB RAM (GH200) up to 255 cores / 1.9 TiB RAM (MI300X cluster), ensuring orchestration and preprocessing don\u2019t bottleneck GPU performance."}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Example Use Cases"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"NVIDIA GH200 (single GPU instance) can be used for training and inference for large language models (e.g., LLaMA 2\u201370B, GPT-3 scale). It can also be used for real-time multi-modal use cases requiring tight GPU-CPU integration, such as speech-to-speech AI assistants or interactive robotics."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"AMD MI300X (8-GPU cluster instance) can be used for training frontier LLMs and multi-modal models (GPT-4, Gemini-class, or open LLMs at >100B parameters)."}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"oracle-distributed-cloud-instances",children:"Oracle Distributed Cloud Instances"}),"\n",(0,r.jsx)(n.h3,{id:"nvidia-a10g-gpu-instances",children:"NVIDIA-A10G GPU Instances"}),"\n",(0,r.jsx)(n.p,{children:"The Oracle A10G series provides NVIDIA GPUs optimized for inference, visualization, and moderate ML training tasks. These instances combine consistent vCPU allocations with scalable GPU configurations, from single-GPU VMs to multi-GPU bare metal nodes."}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Instance Type"}),(0,r.jsx)(n.th,{children:"GPU"}),(0,r.jsx)(n.th,{children:"GPU Memory"}),(0,r.jsx)(n.th,{children:"CPU"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"VM.GPU.A10.1"})}),(0,r.jsx)(n.td,{children:"1 \xd7 NVIDIA-A10G (22.49Gi)"}),(0,r.jsx)(n.td,{children:"22.49Gi"}),(0,r.jsx)(n.td,{children:"14.8 cores (227.41Gi)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"BM.GPU.A10.4"})}),(0,r.jsx)(n.td,{children:"4 \xd7 NVIDIA-A10G (22.49Gi)"}),(0,r.jsx)(n.td,{children:"89.95Gi"}),(0,r.jsx)(n.td,{children:"14.8 cores (227.41Gi)"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Key Features"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"NVIDIA A10G GPUs with 22.49 GiB memory each."}),"\n",(0,r.jsx)(n.li,{children:"Same CPU allocation (14.8 cores / 227 GiB RAM) across single- and four-GPU configurations."}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Example Use Cases"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"VM.A10.1 (single GPU) can be used for small-scale ML inference, 3D rendering, and graphics-heavy applications."}),"\n",(0,r.jsx)(n.li,{children:"BM.A10.4 (four GPUs) can be used for larger inference workloads, distributed graphics rendering, or multi-GPU training of medium-sized models."}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"amd-mi300x-gpu-instances",children:"AMD-MI300X GPU Instances"}),"\n",(0,r.jsx)(n.p,{children:"The Oracle MI300X family provides cutting-edge AMD Instinct MI300X GPUs, purpose-built for large-scale AI training and HPC (high-performance computing) workloads. These instances feature massive GPU memory and CPU scaling, supporting frontier use cases."}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Instance Type"}),(0,r.jsx)(n.th,{children:"GPU"}),(0,r.jsx)(n.th,{children:"GPU Memory"}),(0,r.jsx)(n.th,{children:"CPU"})]})}),(0,r.jsx)(n.tbody,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"BM.GPU.MI300X.8"})}),(0,r.jsx)(n.td,{children:"8 \xd7 AMD-MI300X (127.82Gi)"}),(0,r.jsx)(n.td,{children:"1,022.53Gi"}),(0,r.jsx)(n.td,{children:"111.5 cores (1,945.01Gi)"})]})})]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Key Features"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"8 \xd7 AMD MI300X GPUs, each with 128 GiB HBM3 memory (over 1 TiB GPU memory total)."}),"\n",(0,r.jsx)(n.li,{children:"Extremely high CPU scaling, 111.5 cores, and nearly 2 TiB of system RAM."}),"\n",(0,r.jsx)(n.li,{children:"Suited for frontier-scale AI training and supercomputing-class workloads."}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Example Use Cases"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Training LLMs and multi-modal models (>100B parameters)."}),"\n",(0,r.jsx)(n.li,{children:"High-performance computing simulations, such as weather forecasting and scientific modeling."}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"standard-cpu-only-instances",children:"Standard CPU-Only Instances"}),"\n",(0,r.jsx)(n.p,{children:"The Oracle E6 Flex series provides CPU-only instances for workloads that do not require GPU acceleration."}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Instance Type"}),(0,r.jsx)(n.th,{children:"GPU"}),(0,r.jsx)(n.th,{children:"GPU Memory"}),(0,r.jsx)(n.th,{children:"CPU"})]})}),(0,r.jsx)(n.tbody,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"VM.Standard.E6.Flex"})}),(0,r.jsx)(n.td,{children:"\u2013"}),(0,r.jsx)(n.td,{children:"\u2013"}),(0,r.jsx)(n.td,{children:"0.8 cores (14.61Gi)"})]})})]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Key Features"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"No GPU, pure vCPU, and memory resources."}),"\n",(0,r.jsx)(n.li,{children:"0.8 cores and 14.61 GiB RAM, suitable for supporting tasks."}),"\n",(0,r.jsx)(n.li,{children:"Cost-efficient for non-GPU workloads."}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Example Use Cases"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Running control-plane services or lightweight applications alongside GPU clusters."}),"\n",(0,r.jsx)(n.li,{children:"Workloads requiring basic compute and memory without acceleration."}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,d.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(o,{...e})}):o(e)}}}]);
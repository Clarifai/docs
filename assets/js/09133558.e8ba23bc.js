"use strict";(self.webpackChunkdocs_new=self.webpackChunkdocs_new||[]).push([[9319],{85162:function(n,e,t){t.d(e,{Z:function(){return s}});var a=t(67294),i=t(34334),o="tabItem_Ymn6";function s(n){let{children:e,hidden:t,className:s}=n;return a.createElement("div",{role:"tabpanel",className:(0,i.Z)(o,s),hidden:t},e)}},65488:function(n,e,t){t.d(e,{Z:function(){return m}});var a=t(83117),i=t(67294),o=t(34334),s=t(72389),r=t(67392),l=t(7094),p=t(12466),c="tabList__CuJ",u="tabItem_LNqP";function d(n){var e,t;const{lazy:s,block:d,defaultValue:m,values:h,groupId:_,className:g}=n,f=i.Children.map(n.children,(n=>{if((0,i.isValidElement)(n)&&"value"in n.props)return n;throw new Error("Docusaurus error: Bad <Tabs> child <"+("string"==typeof n.type?n.type:n.type.name)+'>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.')})),A=null!=h?h:f.map((n=>{let{props:{value:e,label:t,attributes:a}}=n;return{value:e,label:t,attributes:a}})),E=(0,r.l)(A,((n,e)=>n.value===e.value));if(E.length>0)throw new Error('Docusaurus error: Duplicate values "'+E.map((n=>n.value)).join(", ")+'" found in <Tabs>. Every value needs to be unique.');const I=null===m?m:null!=(e=null!=m?m:null==(t=f.find((n=>n.props.default)))?void 0:t.props.value)?e:f[0].props.value;if(null!==I&&!A.some((n=>n.value===I)))throw new Error('Docusaurus error: The <Tabs> has a defaultValue "'+I+'" but none of its children has the corresponding value. Available values are: '+A.map((n=>n.value)).join(", ")+". If you intend to show no default tab, use defaultValue={null} instead.");const{tabGroupChoices:D,setTabGroupChoices:w}=(0,l.U)(),[T,R]=(0,i.useState)(I),b=[],{blockElementScrollPositionUntilNextRender:O}=(0,p.o5)();if(null!=_){const n=D[_];null!=n&&n!==T&&A.some((e=>e.value===n))&&R(n)}const v=n=>{const e=n.currentTarget,t=b.indexOf(e),a=A[t].value;a!==T&&(O(e),R(a),null!=_&&w(_,String(a)))},y=n=>{var e;let t=null;switch(n.key){case"ArrowRight":{var a;const e=b.indexOf(n.currentTarget)+1;t=null!=(a=b[e])?a:b[0];break}case"ArrowLeft":{var i;const e=b.indexOf(n.currentTarget)-1;t=null!=(i=b[e])?i:b[b.length-1];break}}null==(e=t)||e.focus()};return i.createElement("div",{className:(0,o.Z)("tabs-container",c)},i.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,o.Z)("tabs",{"tabs--block":d},g)},A.map((n=>{let{value:e,label:t,attributes:s}=n;return i.createElement("li",(0,a.Z)({role:"tab",tabIndex:T===e?0:-1,"aria-selected":T===e,key:e,ref:n=>b.push(n),onKeyDown:y,onFocus:v,onClick:v},s,{className:(0,o.Z)("tabs__item",u,null==s?void 0:s.className,{"tabs__item--active":T===e})}),null!=t?t:e)}))),s?(0,i.cloneElement)(f.filter((n=>n.props.value===T))[0],{className:"margin-top--md"}):i.createElement("div",{className:"margin-top--md"},f.map(((n,e)=>(0,i.cloneElement)(n,{key:e,hidden:n.props.value!==T})))))}function m(n){const e=(0,s.Z)();return i.createElement(d,(0,a.Z)({key:String(e)},n))}},6190:function(n,e,t){t.r(e),t.d(e,{assets:function(){return u},contentTitle:function(){return p},default:function(){return h},frontMatter:function(){return l},metadata:function(){return c},toc:function(){return d}});var a=t(83117),i=(t(67294),t(3905)),o=t(65488),s=t(85162),r=t(66066);const l={description:"Make predictions on image inputs",sidebar_position:1},p="Images",c={unversionedId:"api-guide/predict/images",id:"api-guide/predict/images",title:"Images",description:"Make predictions on image inputs",source:"@site/docs/api-guide/predict/images.md",sourceDirName:"api-guide/predict",slug:"/api-guide/predict/images",permalink:"/api-guide/predict/images",draft:!1,tags:[],version:"current",sidebarPosition:1,frontMatter:{description:"Make predictions on image inputs",sidebar_position:1},sidebar:"tutorialSidebar",previous:{title:"Making Predictions",permalink:"/api-guide/predict/"},next:{title:"Video",permalink:"/api-guide/predict/video"}},u={},d=[{value:"Predict via URL",id:"predict-via-url",level:2},{value:"Predict via Bytes",id:"predict-via-bytes",level:2}],m={toc:d};function h(n){let{components:e,...t}=n;return(0,i.kt)("wrapper",(0,a.Z)({},m,t,{components:e,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"images"},"Images"),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Make predictions on image inputs")),(0,i.kt)("hr",null),(0,i.kt)("p",null,"To get predictions for an input, you need to supply an image and the model you'd like to get predictions from. You can supply an image either with a publicly accessible URL or by directly sending bytes. "),(0,i.kt)("p",null,"You can send up to 128 images in one API call. The file size of each image input should be less than 20MB. "),(0,i.kt)("p",null,"You specify the ",(0,i.kt)("a",{parentName:"p",href:"https://docs.clarifai.com/api-guide/model/"},"model")," you'd like to use with the ",(0,i.kt)("inlineCode",{parentName:"p"},"MODEL_ID")," parameter."),(0,i.kt)("admonition",{type:"tip"},(0,i.kt)("p",{parentName:"admonition"},"When you take an image with a digital device (such as a smartphone camera) the image's meta-information (such as the orientation value for how the camera is held) is stored in the image's ",(0,i.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Exif"},"Exif's data"),". And when you use a photo viewer to check the image on your computer, the photo viewer will respect that orientation value and automatically rotate the image to present it the way it was viewed. This allows you to see a correctly-oriented image no matter how the camera was held."),(0,i.kt)("p",{parentName:"admonition"},"So, when you want to make predictions from an image taken with a digital device, you need to strip the Exif data from the image. Since the Clarifai platform does not account for the Exif data, removing it allows you to make accurate predictions using images in their desired rotation.")),(0,i.kt)("h2",{id:"predict-via-url"},"Predict via URL"),(0,i.kt)("p",null,"Below is an example of how you would send image URLs and receive predictions from the Clarifai's ",(0,i.kt)("inlineCode",{parentName:"p"},"general-image-recognition")," model. "),(0,i.kt)("admonition",{type:"info"},(0,i.kt)("p",{parentName:"admonition"},"The initialization code used in the following examples is outlined in detail on the ",(0,i.kt)("a",{parentName:"p",href:"https://docs.clarifai.com/api-guide/api-overview/api-clients/#client-installation-instructions"},"client installation page."))),(0,i.kt)(o.Z,{mdxType:"Tabs"},(0,i.kt)(s.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,i.kt)(r.Z,{className:"language-python",mdxType:"CodeBlock"},"########################################################################################\n# In this section, we set the user authentication, app ID, model details, and the URL\n# of the image we want as an input. Change these strings to run your own example.\n#######################################################################################\n\nUSER_ID = 'YOUR_USER_ID_HERE'\n# Your PAT (Personal Access Token) can be found in the portal under Authentification\nPAT = 'YOUR_PAT_HERE'\nAPP_ID = 'YOUR_APP_ID_HERE'\n# Change these to whatever model and image URL you want to use\nMODEL_ID = 'general-image-recognition'\nIMAGE_URL = 'https://samples.clarifai.com/metro-north.jpg'\n# This is optional. You can specify a model version or the empty string for the default\nMODEL_VERSION_ID = ''\n\n############################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n############################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (('authorization', 'Key ' + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\npost_model_outputs_response = stub.PostModelOutputs(\n    service_pb2.PostModelOutputsRequest(\n        user_app_id=userDataObject,  # The userDataObject is created in the overview and is required when using a PAT\n        model_id=MODEL_ID,\n        version_id=MODEL_VERSION_ID,  # This is optional. Defaults to the latest model version\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    image=resources_pb2.Image(\n                        url=IMAGE_URL\n                    )\n                )\n            )\n        ]\n    ),\n    metadata=metadata\n)\nif post_model_outputs_response.status.code != status_code_pb2.SUCCESS:\n    print(post_model_outputs_response.status)\n    raise Exception(\"Post model outputs failed, status: \" + post_model_outputs_response.status.description)\n\n# Since we have one input, one output will exist here\noutput = post_model_outputs_response.outputs[0]\n\nprint(\"Predicted concepts:\")\nfor concept in output.data.concepts:\n    print(\"%s %.2f\" % (concept.name, concept.value))\n\n# Uncomment this line to print the full Response JSON\n#print(output)")),(0,i.kt)(s.Z,{value:"js_rest",label:"JavaScript (REST)",mdxType:"TabItem"},(0,i.kt)(r.Z,{className:"language-javascript",mdxType:"CodeBlock"},"\x3c!--index.html file--\x3e\n\n<script>\n    //////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, app ID, model details, and the URL\n    // of the image we want as an input. Change these strings to run your own example.\n    /////////////////////////////////////////////////////////////////////////////////////////\n\n    const USER_ID = 'YOUR_USER_ID_HERE';\n    // Your PAT (Personal Access Token) can be found in the portal under Authentification\n    const PAT = 'YOUR_PAT_HERE';\n    const APP_ID = 'YOUR_APP_ID_HERE';\n    // Change these to whatever model and image URL you want to use\n    const MODEL_ID = 'general-image-recognition';\n    const MODEL_VERSION_ID = 'aa7f35c01e0642fda5cf400f543e7c40';    \n    const IMAGE_URL = 'https://samples.clarifai.com/metro-north.jpg';\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    const raw = JSON.stringify({\n        \"user_app_id\": {\n            \"user_id\": USER_ID,\n            \"app_id\": APP_ID\n        },\n        \"inputs\": [\n            {\n                \"data\": {\n                    \"image\": {\n                        \"url\": IMAGE_URL\n                    }\n                }\n            }\n        ]\n    });\n\n    const requestOptions = {\n        method: 'POST',\n        headers: {\n            'Accept': 'application/json',\n            'Authorization': 'Key ' + PAT\n        },\n        body: raw\n    };\n\n    // NOTE: MODEL_VERSION_ID is optional, you can also call prediction with the MODEL_ID only\n    // https://api.clarifai.com/v2/models/{YOUR_MODEL_ID}/outputs\n    // this will default to the latest version_id\n\n    fetch(\"https://api.clarifai.com/v2/models/\" + MODEL_ID + \"/versions/\" + MODEL_VERSION_ID + \"/outputs\", requestOptions)\n        .then(response => response.text())\n        .then(result => console.log(result))\n        .catch(error => console.log('error', error));\n<\/script>")),(0,i.kt)(s.Z,{value:"nodejs",label:"NodeJS",mdxType:"TabItem"},(0,i.kt)(r.Z,{className:"language-javascript",mdxType:"CodeBlock"},'//index.js file\n\n/////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, app ID, model details, and the URL\n// of the image we want as an input. Change these strings to run your own example.\n////////////////////////////////////////////////////////////////////////////////////////\n\nconst USER_ID = \'YOUR_USER_ID_HERE\';\n// Your PAT (Personal Access Token) can be found in the portal under Authentification\nconst PAT = \'YOUR_PAT_HERE\';\nconst APP_ID = \'YOUR_APP_ID_HERE\';\n// Change these to whatever model and image URL you want to use\nconst MODEL_ID = \'general-image-recognition\';\nconst IMAGE_URL = \'https://samples.clarifai.com/metro-north.jpg\';\n// This is optional.You can specify a model version or the empty string for the default\nconst MODEL_VERSION_ID = \'\';\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\nstub.PostModelOutputs(\n    {\n        user_app_id: {\n            "user_id": USER_ID,\n            "app_id": APP_ID\n        },\n        model_id: MODEL_ID,\n        version_id: MODEL_VERSION_ID, // This is optional. Defaults to the latest model version.\n        inputs: [\n            { data: { image: { url: IMAGE_URL, allow_duplicate_url: true } } }\n        ]\n    },\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            throw new Error("Post model outputs failed, status: " + response.status.description);\n        }\n\n        // Since we have one input, one output will exist here.\n        const output = response.outputs[0];\n\n        console.log("Predicted concepts:");\n        for (const concept of output.data.concepts) {\n            console.log(concept.name + " " + concept.value);\n        }\n    }\n\n);')),(0,i.kt)(s.Z,{value:"java",label:"Java",mdxType:"TabItem"},(0,i.kt)(r.Z,{className:"language-java",mdxType:"CodeBlock"},'package com.clarifai.example;\n\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.status.StatusCode;\n\npublic class ClarifaiExample {\n\t\n\t//////////////////////////////////////////////////////////////////////////////////////////\n\t// In this section, we set the user authentication, app ID, model details, and the URL\n\t// of the image we want as an input. Change these strings to run your own example.\n\t//////////////////////////////////////////////////////////////////////////////////////////\n\t\n\tstatic final String USER_ID = "YOUR_USER_ID_HERE";\n\t// Your PAT (Personal Access Token) can be found in the portal under Authentication\n\tstatic final String PAT = "YOUR_PAT_HERE";\n\tstatic final String APP_ID = "YOUR_APP_ID_HERE";\n\t// Change these to whatever model and image URL you want to use\n\tstatic final String MODEL_ID = "general-image-recognition";\t\n\tstatic final String IMAGE_URL = "https://samples.clarifai.com/metro-north.jpg";\n\t// This is optional. You can specify a model version or an empty string for the default\n\tstatic final String MODEL_VERSION_ID = "";\t\t\n\t\n\t///////////////////////////////////////////////////////////////////////////////////\n\t// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n\t///////////////////////////////////////////////////////////////////////////////////\t\n\n\tpublic static void main(String[] args) {\n\t\t\n\t\tV2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n\t\t\t    .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\t\t\n\t\tMultiOutputResponse postModelOutputsResponse = stub.postModelOutputs(\n\t\t    PostModelOutputsRequest.newBuilder()\n\t\t    \t.setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\t\t \t\t     \n\t\t        .setModelId(MODEL_ID)\n\t\t        .setVersionId(MODEL_VERSION_ID)  // This is optional. Defaults to the latest model version.\n\t\t        .addInputs(\n\t\t            Input.newBuilder().setData(\n\t\t                Data.newBuilder().setImage(\n\t\t                    Image.newBuilder().setUrl(IMAGE_URL)\n\t\t                )\n\t\t            )\n\t\t        )\n\t\t        .build()\n\t\t);\n\n\t\tif (postModelOutputsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n\t\t  throw new RuntimeException("Post model outputs failed, status: " + postModelOutputsResponse.getStatus());\n\t\t}\n\n\t\t// Since we have one input, one output will exist here.\n\t\tOutput output = postModelOutputsResponse.getOutputs(0);\n\n\t\tSystem.out.println("Predicted concepts:");\n\t\tfor (Concept concept : output.getData().getConceptsList()) {\n\t\t    System.out.printf("%s %.2f%n", concept.getName(), concept.getValue());\n\t\t}\n\n\t}\n\n}\n')),(0,i.kt)(s.Z,{value:"php",label:"PHP",mdxType:"TabItem"},(0,i.kt)(r.Z,{className:"language-php",mdxType:"CodeBlock"},"<?php\n\nrequire __DIR__ . '/vendor/autoload.php';\n\n//////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, app ID, model details, and the URL\n// of the image we want as an input. Change these strings to run your own example.\n//////////////////////////////////////////////////////////////////////////////////////////\n\n$USER_ID = 'YOUR_USER_ID_HERE';\n// Your PAT (Personal Access Token) can be found in the portal under Authentification\n$PAT = 'YOUR_PAT_HERE';\n$APP_ID = 'YOUR_APP_ID_HERE';\n// Change these to whatever model and image URL you want to use\n$MODEL_ID = 'general-image-recognition';\n$IMAGE_URL = 'https://samples.clarifai.com/metro-north.jpg';\n// This is optional. You can specify a model version or the empty string for the default\n$MODEL_VERSION_ID = '';\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Image;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\PostModelOutputsRequest;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ['Authorization' => ['Key ' . $PAT ]];\n\n$userDataObject = new UserAppIDSet([\n    'user_id' => $USER_ID, \n    'app_id' => $APP_ID \n]);\n\n// Let's make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client->PostModelOutputs(\n    // The request object carries the request along with the request status and other metadata related to the request itself\n    new PostModelOutputsRequest([\n        'user_app_id' => $userDataObject,\n        'model_id' => $MODEL_ID,  \n        'version_id' => $MODEL_VERSION_ID,\n        'inputs' => [\n            new Input([ // The Input object wraps the Data object in order to meet the API specification                \n                'data' => new Data([ // The Data object is constructed around the Image object. It offers a container that has additional image independent\n                                    // metadata. In this particular use case, no other metadata is needed to be specified\n                    'image' => new Image([ // In the Clarifai platform, an image is defined by a special Image object\n                        'url' => $IMAGE_URL\n                    ])\n                ])\n            ])\n        ]\n    ]),\n    $metadata\n)->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception(\"Error: {$status->details}\");\n}\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure \n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    throw new Exception(\"Failure response: \" . $response->getStatus()->getDescription() . \" \" .\n        $response->getStatus()->getDetails());\n}\n\n// The output of a successful call can be used in many ways. In this example, we loop through all of the predicted concepts \n// and print them out along with their numerical prediction value (confidence)\necho \"Predicted concepts: </br>\";\nforeach ($response->getOutputs()[0]->getData()->getConcepts() as $concept) {\n    echo $concept->getName() . \": \" . number_format($concept->getValue(), 2) . \"</br>\";\n}\n\n?>")),(0,i.kt)(s.Z,{value:"curl",label:"cURL",mdxType:"TabItem"},(0,i.kt)(r.Z,{className:"language-bash",mdxType:"CodeBlock"},'curl -X POST "https://api.clarifai.com/v2/users/YOUR_USER_ID_HERE/apps/YOUR_APP_ID_HERE/models/general-image-recognition/versions/aa7f35c01e0642fda5cf400f543e7c40/outputs" \\\n    -H "Authorization: Key YOUR_PAT_HERE" \\\n    -H "Content-Type: application/json" \\\n    -d \'{\n      "inputs": [\n        {\n          "data": {\n            "image": {\n              "url": "https://samples.clarifai.com/metro-north.jpg"\n            }\n          }\n        }\n      ]\n    }\'\n   '))),(0,i.kt)("details",null,(0,i.kt)("summary",null,"Code Output Example"),(0,i.kt)(r.Z,{className:"language-text",mdxType:"CodeBlock"},"Predicted concepts:\ntrain 1.00\nrailway 1.00\nsubway system 1.00\nstation 1.00\nlocomotive 1.00\ntransportation system 1.00\ntravel 0.99\ncommuter 0.98\nplatform 0.98\nlight 0.97\ntrain station 0.97\nblur 0.97\ncity 0.96\nroad 0.96\nurban 0.96\ntraffic 0.96\nstreet 0.95\npublic 0.93\ntramway 0.93\nbusiness 0.93")),(0,i.kt)("details",null,(0,i.kt)("summary",null,"JSON Output Example"),(0,i.kt)(r.Z,{className:"language-javascript",mdxType:"CodeBlock"},'id: "c1064364b2c64740874d714c70db6351"\nstatus {\n  code: SUCCESS\n  description: "Ok"\n}\ncreated_at {\n  seconds: 1643219050\n  nanos: 357487464\n}\nmodel {\n  id: "general-image-recognition"\n  name: "general"\n  created_at {\n    seconds: 1457543499\n    nanos: 608845000\n  }\n  app_id: "main"\n  output_info {\n    output_config {\n    }\n    message: "Show output_info with: GET /models/{model_id}/output_info"\n    fields_map {\n      fields {\n        key: "concepts"\n        value {\n          string_value: "softmax"\n        }\n      }\n    }\n  }\n  model_version {\n    id: "aa7f35c01e0642fda5cf400f543e7c40"\n    created_at {\n      seconds: 1520370624\n      nanos: 454834000\n    }\n    status {\n      code: MODEL_TRAINED\n      description: "Model is trained and ready"\n    }\n    visibility {\n      gettable: PUBLIC\n    }\n    app_id: "main"\n    user_id: "clarifai"\n    metadata {\n    }\n  }\n  display_name: "general-visual-classifier"\n  user_id: "clarifai"\n  input_info {\n    fields_map {\n      fields {\n        key: "image"\n        value {\n          string_value: "images"\n        }\n      }\n    }\n  }\n  train_info {\n  }\n  model_type_id: "visual-classifier"\n  visibility {\n    gettable: PUBLIC\n  }\n  description: "Image recognition model for identifying different concepts in images and video including objects, themes, moods, and more."\n  metadata {\n  }\n  notes: "**General Information**\\n\\n- Purpose: Classifier for a variety of concepts, common objects, etc. This model is a great all-purpose solution for most visual recognition needs with industry-leading performance.\\n\\n- Architecture: Customized InceptionV2\\n\\n- Intended Use: image indexing by tags, filtering, cascade routing\\n\\n- Limitations: works well when content is prevalent in the image\\n\\n\\n\\n **\\nTraining/Test Data**\\n\\nThe model was trained and tested on an internal dataset with approximately 10,000 concepts and 20M images, with multiple concepts per image. The class distributions on train and validation sets are long-tailed. The validation set was annotated using a combination of originally curated labels with incomplete annotations, where were further completed by adding additional labels proposed a newer version of this model (aa7f35c01e0642fda5cf400f543e7c40) at a low threshold and verified by human annotators. "\n  modified_at {\n    seconds: 1634831222\n    nanos: 80260000\n  }\n  import_info {\n  }\n}\ninput {\n  id: "03722c867ba74e25870d81d90975a490"\n  data {\n    image {\n      url: "https://samples.clarifai.com/metro-north.jpg"\n    }\n  }\n}\ndata {\n  concepts {\n    id: "ai_HLmqFqBf"\n    name: "train"\n    value: 0.9996053576469421\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_fvlBqXZR"\n    name: "railway"\n    value: 0.9992986917495728\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_SHNDcmJ3"\n    name: "subway system"\n    value: 0.9982514977455139\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_6kTjGfF6"\n    name: "station"\n    value: 0.9980105757713318\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_RRXLczch"\n    name: "locomotive"\n    value: 0.9972571730613708\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_Xxjc3MhT"\n    name: "transportation system"\n    value: 0.9969801306724548\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_VRmbGVWh"\n    name: "travel"\n    value: 0.9889795780181885\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_jlb9q33b"\n    name: "commuter"\n    value: 0.9808752536773682\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_2gkfMDsM"\n    name: "platform"\n    value: 0.9806439876556396\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_n9vjC1jB"\n    name: "light"\n    value: 0.9742040634155273\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_sQQj52KZ"\n    name: "train station"\n    value: 0.9687404036521912\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_l4WckcJN"\n    name: "blur"\n    value: 0.9672204256057739\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_WBQfVV0p"\n    name: "city"\n    value: 0.9614798426628113\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_TZ3C79C6"\n    name: "road"\n    value: 0.9613829851150513\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_CpFBRWzD"\n    name: "urban"\n    value: 0.9603424668312073\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_tr0MBp64"\n    name: "traffic"\n    value: 0.959934651851654\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_GjVpxXrs"\n    name: "street"\n    value: 0.9474142789840698\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_mcSHVRfS"\n    name: "public"\n    value: 0.9343122839927673\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_J6d1kV8t"\n    name: "tramway"\n    value: 0.9318979382514954\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_6lhccv44"\n    name: "business"\n    value: 0.9294138550758362\n    app_id: "main"\n  }\n}\n')),(0,i.kt)("h2",{id:"predict-via-bytes"},"Predict via Bytes"),(0,i.kt)("p",null,"Below is an example of how you would send the bytes of an image and receive predictions from the Clarifai's ",(0,i.kt)("inlineCode",{parentName:"p"},"general-image-recognition")," model."),(0,i.kt)(o.Z,{mdxType:"Tabs"},(0,i.kt)(s.Z,{value:"python",label:"Python",mdxType:"TabItem"},(0,i.kt)(r.Z,{className:"language-python",mdxType:"CodeBlock"},"##############################################################################################\n# In this section, we set the user authentication, app ID, model details, and the location\n# of the image we want as an input. Change these strings to run your own example.\n#############################################################################################\n\nUSER_ID = 'YOUR_USER_ID_HERE'\n# Your PAT (Personal Access Token) can be found in the portal under Authentification\nPAT = 'YOUR_PAT_HERE'\nAPP_ID = 'YOUR_APP_ID_HERE'\n# Change these to whatever model and image input you want to use\nMODEL_ID = 'general-image-recognition'\nIMAGE_FILE_LOCATION = 'YOUR_IMAGE_FILE_LOCATION_HERE'\n# This is optional. You can specify a model version or the empty string for the default\nMODEL_VERSION_ID = ''\n\n############################################################################\n# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n############################################################################\n\nfrom clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\nfrom clarifai_grpc.grpc.api.status import status_code_pb2\n\nchannel = ClarifaiChannel.get_grpc_channel()\nstub = service_pb2_grpc.V2Stub(channel)\n\nmetadata = (('authorization', 'Key ' + PAT),)\n\nuserDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n\nwith open(IMAGE_FILE_LOCATION, \"rb\") as f:\n    file_bytes = f.read()\n\npost_model_outputs_response = stub.PostModelOutputs(\n    service_pb2.PostModelOutputsRequest(\n        user_app_id=userDataObject,  # The userDataObject is created in the overview and is required when using a PAT\n        model_id=MODEL_ID,\n        version_id=MODEL_VERSION_ID,  # This is optional. Defaults to the latest model version\n        inputs=[\n            resources_pb2.Input(\n                data=resources_pb2.Data(\n                    image=resources_pb2.Image(\n                        base64=file_bytes\n                    )\n                )\n            )\n        ]\n    ),\n    metadata=metadata\n)\nif post_model_outputs_response.status.code != status_code_pb2.SUCCESS:\n    print(post_model_outputs_response.status)\n    raise Exception(\"Post model outputs failed, status: \" + post_model_outputs_response.status.description)\n\n# Since we have one input, one output will exist here\noutput = post_model_outputs_response.outputs[0]\n\nprint(\"Predicted concepts:\")\nfor concept in output.data.concepts:\n    print(\"%s %.2f\" % (concept.name, concept.value))\n\n# Uncomment this line to print the full Response JSON\n#print(output)")),(0,i.kt)(s.Z,{value:"js_rest",label:"JavaScript (REST)",mdxType:"TabItem"},(0,i.kt)(r.Z,{className:"language-javascript",mdxType:"CodeBlock"},"\x3c!--index.html file--\x3e\n\n<script>\n    ///////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, app ID, model details, and the location\n    // of the image we want as an input. Change these strings to run your own example.\n    //////////////////////////////////////////////////////////////////////////////////////////////\n\n    const USER_ID = 'YOUR_USER_ID_HERE';\n    // Your PAT (Personal Access Token) can be found in the portal under Authentification\n    const PAT = 'YOUR_PAT_HERE';\n    const APP_ID = 'YOUR_APP_ID_HERE';\n    // Change these to whatever model and image input you want to use\n    const MODEL_ID = 'general-image-recognition';\n    const MODEL_VERSION_ID = 'aa7f35c01e0642fda5cf400f543e7c40';    \n    const IMAGE_BYTES_STRING = '/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAUDBAQEAwUEBAQFBQUGBwwIBwcHBw8LCwkMEQ8SEhEPERETFhwXExQaFRERGCEYGh0dHx8fExciJCIeJBweHx7/2wBDAQUFBQcGBw4ICA4eFBEUHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh7/wAARCAAoACgDASIAAhEBAxEB/8QAGQAAAgMBAAAAAAAAAAAAAAAAAAYDBQcE/8QAMBAAAQMDAwMDAgQHAAAAAAAAAQIDBAAFEQYSIQcTMTJBURRhCBYikSNScXKhsdH/xAAZAQACAwEAAAAAAAAAAAAAAAAFBgIDBAf/xAAtEQABAwMBBgQHAQAAAAAAAAABAgMRAAQhMQUSE0FRYQaBocEUFiJCcrHR8P/aAAwDAQACEQMRAD8A3+RYY1unSYzCS0ttZUkAgktn0q5yT7jPyDUC4wdGwycH5U2Kt9ZQ7VI1qw5PkvQy3CSVPpf7aQjuKyFH25xzn3pHn3TVNy01Hl2hyy6YdkSpKsS9sl/6RlI3rRu3dxWd6spwnAGPIJTfl925fcLaoSDHXvyo6i9SlCQrU9wKln3OyWiaDN1RAbW3kKbSd7gPtwMkH/tTWy9afuy1iPfnXMAblITwkE4yf08cn3pSbYt1uts24XH6fUbiLAuY1MWyGkLEmUW0rcCRvUpQ5CtwKQCPgi4S1ZbDe4sd9NntDEe79m3uOBLTr0IR9jzodSMqUpTu9JJ8owD7UTT4ZCfv9PbP7860m+s+HBSrejWRuz2kAxoesGYxTW/Zlpkwo1vkuSly3UgKWQUhHJUvIHsAaKTemF8XE6sWmxyZkiaZrMh1jv8ArQNpUVqB8FW0njHqx4zRVVhsph1KlKk5xQ+7uHmikaSJrQerMByet2IwvtuTLa4xv2k7Rk84H9x/esHv92d01boenLXGcuiWrFIhLlpbcaQ2/JdK3VJCkAq2pAR7Zz7YxWudY9fxNIdQbNGkR5TyX4aisNNpUMFZAzkj4NK0jq9ZpbLr0PSlzkhrlZDaQlP3P8Q4/ap3F87bPucJEkx/hHv60b2TYXLrKN5sramYECSQRk9M6c6zmJ+eb5Hi22M7cnWGIQgFLbX0zSo4PDa1YBcTgDyMjJ/qbGPabH08SJt1Uzc9QqRliGg5QySPKvgc+TyfYDmmTUWpNYz7ctxoQdPQshCktupckDJUPUcJT6DwMq8YyaQ9VL0pCS8zapcq4SVOBZmPDO8/cnknlWcDBwn4NYnPjLkQ+qE9OtOVlYpeVHDCEkkkJyT+SuQzy5Y0ru6Ez511/Efa5s1fdkOtyVurIxgdlQAA9gOKKPwolU7remU5hCGYEgo38KUv9I/0TRTDYJCWQBSF4rIN/CRgAR0iTpVD1j1g/qDqJcJqlKcjB9bcda142MpOEJAzgeMnjyTSyze5KEuNRpDoDvC0oe4X9iAeaKKFK+oya6fbOqYbDTeEiAPKpHdS3gBLYc7RQkp3ApQog+cq8nwPJrljzxnPZbUfnugn/NFFRgEVch9xKsH0H8pg6e3x3T3UC1ajaZITGkJLoS4MKbOUrzz/ACKVRRRVzVwtoQmhG1NkWu0HuI+JI8u/Kv/Z';\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\n\n    const raw = JSON.stringify({\n        \"user_app_id\": {\n            \"user_id\": USER_ID,\n            \"app_id\": APP_ID\n        },\n        \"inputs\": [\n            {\n                \"data\": {\n                    \"image\": {\n                        \"base64\": IMAGE_BYTES_STRING\n                    }\n                }\n            }\n        ]\n    });\n\n    const requestOptions = {\n        method: 'POST',\n        headers: {\n            'Accept': 'application/json',\n            'Authorization': 'Key ' + PAT\n        },\n        body: raw\n    };\n\n    // NOTE: MODEL_VERSION_ID is optional, you can also call prediction with the MODEL_ID only\n    // https://api.clarifai.com/v2/models/{YOUR_MODEL_ID}/outputs\n    // this will default to the latest version_id\n\n    fetch(\"https://api.clarifai.com/v2/models/\" + MODEL_ID + \"/versions/\" + MODEL_VERSION_ID + \"/outputs\", requestOptions)\n        .then(response => response.text())\n        .then(result => console.log(result))\n        .catch(error => console.log('error', error));\n<\/script>")),(0,i.kt)(s.Z,{value:"nodejs",label:"NodeJS",mdxType:"TabItem"},(0,i.kt)(r.Z,{className:"language-javascript",mdxType:"CodeBlock"},'//index.js file\n\n///////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, app ID, model details, and the location\n// of the image we want as an input. Change these strings to run your own example.\n//////////////////////////////////////////////////////////////////////////////////////////////\n\nconst USER_ID = \'YOUR_USER_ID_HERE\';\n// Your PAT (Personal Access Token) can be found in the portal under Authentification\nconst PAT = \'YOUR_PAT_HERE\';\nconst APP_ID = \'YOUR_APP_ID_HERE\';\n// Change these to whatever model and image input you want to use\nconst MODEL_ID = \'general-image-recognition\';\nconst IMAGE_FILE_LOCATION = \'YOUR_IMAGE_FILE_LOCATION_HERE\';\n// This is optional.You can specify a model version or the empty string for the default\nconst MODEL_VERSION_ID = \'\';\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nconst { ClarifaiStub, grpc } = require("clarifai-nodejs-grpc");\n\nconst stub = ClarifaiStub.grpc();\n\n// This will be used by every Clarifai endpoint call\nconst metadata = new grpc.Metadata();\nmetadata.set("authorization", "Key " + PAT);\n\nconst fs = require("fs");\nconst imageBytes = fs.readFileSync(IMAGE_FILE_LOCATION);\n\nstub.PostModelOutputs(\n    {\n        user_app_id: {\n            "user_id": USER_ID,\n            "app_id": APP_ID\n        },\n        model_id: MODEL_ID,\n        version_id: MODEL_VERSION_ID, // This is optional. Defaults to the latest model version.\n        inputs: [\n            { data: { image: { base64: imageBytes } } }\n        ]\n    },\n    metadata,\n    (err, response) => {\n        if (err) {\n            throw new Error(err);\n        }\n\n        if (response.status.code !== 10000) {\n            throw new Error("Post model outputs failed, status: " + response.status.description);\n        }\n\n        // Since we have one input, one output will exist here.\n        const output = response.outputs[0];\n\n        console.log("Predicted concepts:");\n        for (const concept of output.data.concepts) {\n            console.log(concept.name + " " + concept.value);\n        }\n    }\n);')),(0,i.kt)(s.Z,{value:"java",label:"Java",mdxType:"TabItem"},(0,i.kt)(r.Z,{className:"language-java",mdxType:"CodeBlock"},'package com.clarifai.example;\n\nimport com.clarifai.grpc.api.*;\nimport com.clarifai.channel.ClarifaiChannel;\nimport com.clarifai.credentials.ClarifaiCallCredentials;\nimport com.clarifai.grpc.api.status.StatusCode;\nimport com.google.protobuf.ByteString;\nimport java.io.File;\nimport java.io.IOException;\nimport java.nio.file.Files;\n\npublic class ClarifaiExample {\n\n    ///////////////////////////////////////////////////////////////////////////////////////////////\n    // In this section, we set the user authentication, app ID, model details, and the location\n    // of the image we want as an input. Change these strings to run your own example.\n    //////////////////////////////////////////////////////////////////////////////////////////////\n\n    static final String USER_ID = "YOUR_USER_ID_HERE";\n    //Your PAT (Personal Access Token) can be found in the portal under Authentication\t\n    static final String PAT = "YOUR_PAT_HERE";\n    static final String APP_ID = "YOUR_APP_ID_HERE";\n    // Change these to whatever model and image input you want to use\n    static final String MODEL_ID = "general-image-recognition";    \n    static final String IMAGE_FILE_LOCATION = "YOUR_IMAGE_FILE_LOCATION_HERE";\n    // This is optional.You can specify a model version or an empty string for the default\n    static final String MODEL_VERSION_ID = "";\n\n    ///////////////////////////////////////////////////////////////////////////////////\n    // YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n    ///////////////////////////////////////////////////////////////////////////////////\t\n\n    public static void main(String[] args) throws IOException {\n\n        V2Grpc.V2BlockingStub stub = V2Grpc.newBlockingStub(ClarifaiChannel.INSTANCE.getGrpcChannel())\n            .withCallCredentials(new ClarifaiCallCredentials(PAT));\n\n        MultiOutputResponse postModelOutputsResponse = stub.postModelOutputs(\n            PostModelOutputsRequest.newBuilder()\n            .setUserAppId(UserAppIDSet.newBuilder().setUserId(USER_ID).setAppId(APP_ID))\n            .setModelId(MODEL_ID)\n            .setVersionId(MODEL_VERSION_ID) // This is optional. Defaults to the latest model version\n            .addInputs(\n                Input.newBuilder().setData(\n                    Data.newBuilder().setImage(\n                        Image.newBuilder()\n                        .setBase64(ByteString.copyFrom(Files.readAllBytes(\n                            new File(IMAGE_FILE_LOCATION).toPath()\n                        )))\n                    )\n                )\n            )\n            .build()\n        );\n\n        if (postModelOutputsResponse.getStatus().getCode() != StatusCode.SUCCESS) {\n            throw new RuntimeException("Post model outputs failed, status: " + postModelOutputsResponse.getStatus());\n        }\n\n        // Since we have one input, one output will exist here.\n        Output output = postModelOutputsResponse.getOutputs(0);\n\n        System.out.println("Predicted concepts:");\n        for (Concept concept: output.getData().getConceptsList()) {\n            System.out.printf("%s %.2f%n", concept.getName(), concept.getValue());\n        }\n\n    }\n\n}')),(0,i.kt)(s.Z,{value:"php",label:"PHP",mdxType:"TabItem"},(0,i.kt)(r.Z,{className:"language-php",mdxType:"CodeBlock"},"<?php\n\nrequire __DIR__ . '/vendor/autoload.php';\n\n///////////////////////////////////////////////////////////////////////////////////////////////\n// In this section, we set the user authentication, app ID, model details, and the location\n// of the image we want as an input. Change these strings to run your own example.\n//////////////////////////////////////////////////////////////////////////////////////////////\n\n$USER_ID = 'YOUR_USER_ID_HERE';\n// Your PAT (Personal Access Token) can be found in the portal under Authentification\n$PAT = 'YOUR_PAT_HERE';\n$APP_ID = 'YOUR_APP_ID_HERE';\n// Change these to whatever model and image input you want to use\n$MODEL_ID = 'general-image-recognition';\n$IMAGE_FILE_LOCATION = 'YOUR_IMAGE_FILE_LOCATION_HERE';\n// This is optional. You can specify a model version or the empty string for the default\n$MODEL_VERSION_ID = '';\n\n///////////////////////////////////////////////////////////////////////////////////\n// YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n///////////////////////////////////////////////////////////////////////////////////\n\nuse Clarifai\\ClarifaiClient;\nuse Clarifai\\Api\\Data;\nuse Clarifai\\Api\\Image;\nuse Clarifai\\Api\\Input;\nuse Clarifai\\Api\\PostModelOutputsRequest;\nuse Clarifai\\Api\\Status\\StatusCode;\nuse Clarifai\\Api\\UserAppIDSet;\n\n$client = ClarifaiClient::grpc();\n\n$metadata = ['Authorization' => ['Key ' . $PAT ]];\n\n$userDataObject = new UserAppIDSet([\n    'user_id' => $USER_ID, \n    'app_id' => $APP_ID \n]);\n\n$imageData = file_get_contents($IMAGE_FILE_LOCATION); // Get the image bytes data from the location\n\n// Let's make a RPC call to the Clarifai platform. It uses the opened gRPC client channel to communicate a\n// request and then wait for the response\n[$response, $status] = $client->PostModelOutputs(\n    // The request object carries the request along with the request status and other metadata related to the request itself\n    new PostModelOutputsRequest([\n        'user_app_id' => $userDataObject,\n        'model_id' => $MODEL_ID,  \n        'version_id' => $MODEL_VERSION_ID,\n        'inputs' => [\n            new Input([ // The Input object wraps the Data object in order to meet the API specification                \n                'data' => new Data([ // The Data object is constructed around the Image object. It offers a container that has additional image independent\n                                    // metadata. In this particular use case, no other metadata is needed to be specified\n                    'image' => new Image([ // In the Clarifai platform, an image is defined by a special Image object\n                        'base64' => $imageData\n                    ])\n                ])\n            ])\n        ]\n    ]),\n    $metadata\n)->wait();\n\n// A response is returned and the first thing we do is check the status of it\n// A successful response will have a status code of 0; otherwise, there is some error\nif ($status->code !== 0) {\n    throw new Exception(\"Error: {$status->details}\");\n}\n// In addition to the RPC response status, there is a Clarifai API status that reports if the operation was a success or failure \n// (not just that the communication was successful)\nif ($response->getStatus()->getCode() != StatusCode::SUCCESS) {\n    throw new Exception(\"Failure response: \" . $response->getStatus()->getDescription() . \" \" .\n        $response->getStatus()->getDetails());\n}\n\n// The output of a successful call can be used in many ways. In this example, we loop through all of the predicted concepts \n// and print them out along with their numerical prediction value (confidence)\necho \"Predicted concepts: </br>\";\nforeach ($response->getOutputs()[0]->getData()->getConcepts() as $concept) {\n    echo $concept->getName() . \": \" . number_format($concept->getValue(), 2) . \"</br>\";\n}\n\n?>")),(0,i.kt)(s.Z,{value:"curl",label:"cURL",mdxType:"TabItem"},(0,i.kt)(r.Z,{className:"language-bash",mdxType:"CodeBlock"},'# Smaller files (195 KB or less)\n\ncurl -X POST "https://api.clarifai.com/v2/users/YOUR_USER_ID_HERE/apps/YOUR_APP_ID_HERE/models/general-image-recognition/versions/aa7f35c01e0642fda5cf400f543e7c40/outputs" \\\n  -H "Authorization: Key YOUR_PAT_HERE" \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "inputs": [\n      {\n        "data": {\n          "image": {\n            "base64": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAUDBAQEAwUEBAQFBQUGBwwIBwcHBw8LCwkMEQ8SEhEPERETFhwXExQaFRERGCEYGh0dHx8fExciJCIeJBweHx7/2wBDAQUFBQcGBw4ICA4eFBEUHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh4eHh7/wAARCAAoACgDASIAAhEBAxEB/8QAGQAAAgMBAAAAAAAAAAAAAAAAAAYDBQcE/8QAMBAAAQMDAwMDAgQHAAAAAAAAAQIDBAAFEQYSIQcTMTJBURRhCBYikSNScXKhsdH/xAAZAQACAwEAAAAAAAAAAAAAAAAFBgIDBAf/xAAtEQABAwMBBgQHAQAAAAAAAAABAgMRAAQhMQUSE0FRYQaBocEUFiJCcrHR8P/aAAwDAQACEQMRAD8A3+RYY1unSYzCS0ttZUkAgktn0q5yT7jPyDUC4wdGwycH5U2Kt9ZQ7VI1qw5PkvQy3CSVPpf7aQjuKyFH25xzn3pHn3TVNy01Hl2hyy6YdkSpKsS9sl/6RlI3rRu3dxWd6spwnAGPIJTfl925fcLaoSDHXvyo6i9SlCQrU9wKln3OyWiaDN1RAbW3kKbSd7gPtwMkH/tTWy9afuy1iPfnXMAblITwkE4yf08cn3pSbYt1uts24XH6fUbiLAuY1MWyGkLEmUW0rcCRvUpQ5CtwKQCPgi4S1ZbDe4sd9NntDEe79m3uOBLTr0IR9jzodSMqUpTu9JJ8owD7UTT4ZCfv9PbP7860m+s+HBSrejWRuz2kAxoesGYxTW/Zlpkwo1vkuSly3UgKWQUhHJUvIHsAaKTemF8XE6sWmxyZkiaZrMh1jv8ArQNpUVqB8FW0njHqx4zRVVhsph1KlKk5xQ+7uHmikaSJrQerMByet2IwvtuTLa4xv2k7Rk84H9x/esHv92d01boenLXGcuiWrFIhLlpbcaQ2/JdK3VJCkAq2pAR7Zz7YxWudY9fxNIdQbNGkR5TyX4aisNNpUMFZAzkj4NK0jq9ZpbLr0PSlzkhrlZDaQlP3P8Q4/ap3F87bPucJEkx/hHv60b2TYXLrKN5sramYECSQRk9M6c6zmJ+eb5Hi22M7cnWGIQgFLbX0zSo4PDa1YBcTgDyMjJ/qbGPabH08SJt1Uzc9QqRliGg5QySPKvgc+TyfYDmmTUWpNYz7ctxoQdPQshCktupckDJUPUcJT6DwMq8YyaQ9VL0pCS8zapcq4SVOBZmPDO8/cnknlWcDBwn4NYnPjLkQ+qE9OtOVlYpeVHDCEkkkJyT+SuQzy5Y0ru6Ez511/Efa5s1fdkOtyVurIxgdlQAA9gOKKPwolU7remU5hCGYEgo38KUv9I/0TRTDYJCWQBSF4rIN/CRgAR0iTpVD1j1g/qDqJcJqlKcjB9bcda142MpOEJAzgeMnjyTSyze5KEuNRpDoDvC0oe4X9iAeaKKFK+oya6fbOqYbDTeEiAPKpHdS3gBLYc7RQkp3ApQog+cq8nwPJrljzxnPZbUfnugn/NFFRgEVch9xKsH0H8pg6e3x3T3UC1ajaZITGkJLoS4MKbOUrzz/ACKVRRRVzVwtoQmhG1NkWu0HuI+JI8u/Kv/Z"\n          }\n        }\n      }\n    ]\n  }\'\n\n# Larger Files (Greater than 195 KB)\n\ncurl -X POST "https://api.clarifai.com/v2/users/YOUR_USER_ID_HERE/apps/YOUR_APP_ID_HERE/models/general-image-recognition/versions/aa7f35c01e0642fda5cf400f543e7c40/outputs" \\\n  -H "Authorization: Key YOUR_PAT_HERE" \\\n  -H "Content-Type: application/json" \\\n  -d @-  << FILEIN\n  {\n    "inputs": [\n      {\n        "data": {\n          "image": {\n            "base64": "$(base64 /home/user/image.png)"\n          }\n        }\n      }\n    ]\n  }\nFILEIN'))),(0,i.kt)("details",null,(0,i.kt)("summary",null,"Code Output Example"),(0,i.kt)(r.Z,{className:"language-text",mdxType:"CodeBlock"},"Predicted concepts:\ntrain 1.00\nrailway 1.00\nsubway system 1.00\nstation 1.00\nlocomotive 1.00\ntransportation system 1.00\ntravel 0.99\ncommuter 0.98\nplatform 0.98\nlight 0.97\ntrain station 0.97\nblur 0.97\ncity 0.96\nroad 0.96\nurban 0.96\ntraffic 0.96\nstreet 0.95\npublic 0.93\ntramway 0.93\nbusiness 0.93")),(0,i.kt)("details",null,(0,i.kt)("summary",null,"JSON Output Example"),(0,i.kt)(r.Z,{className:"language-javascript",mdxType:"CodeBlock"},'id: "72d8af665de44822a5e26fe75ab7f84c"\nstatus {\n  code: SUCCESS\n  description: "Ok"\n}\ncreated_at {\n  seconds: 1643218577\n  nanos: 52063722\n}\nmodel {\n  id: "general-image-recognition"\n  name: "general"\n  created_at {\n    seconds: 1457543499\n    nanos: 608845000\n  }\n  app_id: "main"\n  output_info {\n    output_config {\n    }\n    message: "Show output_info with: GET /models/{model_id}/output_info"\n    fields_map {\n      fields {\n        key: "concepts"\n        value {\n          string_value: "softmax"\n        }\n      }\n    }\n  }\n  model_version {\n    id: "aa7f35c01e0642fda5cf400f543e7c40"\n    created_at {\n      seconds: 1520370624\n      nanos: 454834000\n    }\n    status {\n      code: MODEL_TRAINED\n      description: "Model is trained and ready"\n    }\n    visibility {\n      gettable: PUBLIC\n    }\n    app_id: "main"\n    user_id: "clarifai"\n    metadata {\n    }\n  }\n  display_name: "general-visual-classifier"\n  user_id: "clarifai"\n  input_info {\n    fields_map {\n      fields {\n        key: "image"\n        value {\n          string_value: "images"\n        }\n      }\n    }\n  }\n  train_info {\n  }\n  model_type_id: "visual-classifier"\n  visibility {\n    gettable: PUBLIC\n  }\n  description: "Image recognition model for identifying different concepts in images and video including objects, themes, moods, and more."\n  metadata {\n  }\n  notes: "**General Information**\\n\\n- Purpose: Classifier for a variety of concepts, common objects, etc. This model is a great all-purpose solution for most visual recognition needs with industry-leading performance.\\n\\n- Architecture: Customized InceptionV2\\n\\n- Intended Use: image indexing by tags, filtering, cascade routing\\n\\n- Limitations: works well when content is prevalent in the image\\n\\n\\n\\n **\\nTraining/Test Data**\\n\\nThe model was trained and tested on an internal dataset with approximately 10,000 concepts and 20M images, with multiple concepts per image. The class distributions on train and validation sets are long-tailed. The validation set was annotated using a combination of originally curated labels with incomplete annotations, where were further completed by adding additional labels proposed a newer version of this model (aa7f35c01e0642fda5cf400f543e7c40) at a low threshold and verified by human annotators. "\n  modified_at {\n    seconds: 1634831222\n    nanos: 80260000\n  }\n  import_info {\n  }\n}\ninput {\n  id: "8b57a8364ed9494aa200af2d422b3fee"\n  data {\n    image {\n      url: "https://samples.clarifai.com/placeholder.gif"\n      base64: "true"\n    }\n  }\n}\ndata {\n  concepts {\n    id: "ai_HLmqFqBf"\n    name: "train"\n    value: 0.9996053576469421\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_fvlBqXZR"\n    name: "railway"\n    value: 0.9992986917495728\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_SHNDcmJ3"\n    name: "subway system"\n    value: 0.9982514977455139\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_6kTjGfF6"\n    name: "station"\n    value: 0.9980105757713318\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_RRXLczch"\n    name: "locomotive"\n    value: 0.9972571730613708\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_Xxjc3MhT"\n    name: "transportation system"\n    value: 0.9969801306724548\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_VRmbGVWh"\n    name: "travel"\n    value: 0.9889795780181885\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_jlb9q33b"\n    name: "commuter"\n    value: 0.9808752536773682\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_2gkfMDsM"\n    name: "platform"\n    value: 0.9806439876556396\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_n9vjC1jB"\n    name: "light"\n    value: 0.9742040634155273\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_sQQj52KZ"\n    name: "train station"\n    value: 0.9687404036521912\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_l4WckcJN"\n    name: "blur"\n    value: 0.9672204256057739\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_WBQfVV0p"\n    name: "city"\n    value: 0.9614798426628113\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_TZ3C79C6"\n    name: "road"\n    value: 0.9613829851150513\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_CpFBRWzD"\n    name: "urban"\n    value: 0.9603424072265625\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_tr0MBp64"\n    name: "traffic"\n    value: 0.959934651851654\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_GjVpxXrs"\n    name: "street"\n    value: 0.9474143981933594\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_mcSHVRfS"\n    name: "public"\n    value: 0.9343124032020569\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_J6d1kV8t"\n    name: "tramway"\n    value: 0.9318978190422058\n    app_id: "main"\n  }\n  concepts {\n    id: "ai_6lhccv44"\n    name: "business"\n    value: 0.9294139742851257\n    app_id: "main"\n  }\n}\n')))}h.isMDXComponent=!0}}]);
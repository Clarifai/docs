<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-portal-guide/model/deep-training/visual-detection-templates">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.0">
<title data-rh="true">Visual Detection Templates | Clarifai Guide</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://docs.clarifai.com/portal-guide/model/deep-training/visual-detection-templates"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Visual Detection Templates | Clarifai Guide"><meta data-rh="true" name="description" content="Learn about our visual detection templates"><meta data-rh="true" property="og:description" content="Learn about our visual detection templates"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://docs.clarifai.com/portal-guide/model/deep-training/visual-detection-templates"><link data-rh="true" rel="alternate" href="https://docs.clarifai.com/portal-guide/model/deep-training/visual-detection-templates" hreflang="en"><link data-rh="true" rel="alternate" href="https://docs.clarifai.com/portal-guide/model/deep-training/visual-detection-templates" hreflang="x-default"><link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-EN8LWMPFVR"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-EN8LWMPFVR",{anonymize_ip:!0})</script>


<!-- Google Tag Manager -->
    <script>!function(e,t,a,n,g){e[n]=e[n]||[],e[n].push({"gtm.start":(new Date).getTime(),event:"gtm.js"});var m=t.getElementsByTagName(a)[0],r=t.createElement(a);r.async=!0,r.src="https://www.googletagmanager.com/gtm.js?id=GTM-5W9P7GR",m.parentNode.insertBefore(r,m)}(window,document,"script","dataLayer")</script>
    <!-- End Google Tag Manager --><link rel="stylesheet" href="/assets/css/styles.c79847c7.css">
<link rel="preload" href="/assets/js/runtime~main.9ebdf72f.js" as="script">
<link rel="preload" href="/assets/js/main.19defeb1.js" as="script">
</head>
<body class="navigation-with-keyboard" data-theme="light">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}return t}()||function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="Clarifai" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/logo.svg" alt="Clarifai" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">Clarifai Guide</b></a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/Clarifai/docs" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><a href="https://join.slack.com/t/clarifaicommunity/shared_invite/zt-1jehqesme-l60djcd3c_4a1eCV~uPUjQ" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Community Slack<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><a href="https://api.clarifai.com/api-doc/?url=https://api.clarifai.com/v2/swagger.json" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Swagger API Guide<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="dsla-search-wrapper"><div class="dsla-search-field" data-tags="default,docs-default-current"></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/">Welcome</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link">Clarifai Basics</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/clarifai-basics/start-here-5-mins-or-less">Start Here (5 mins or less!)</a><button aria-label="Toggle the collapsible sidebar category &#x27;Start Here (5 mins or less!)&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/clarifai-basics/glossary">Key Terminology to Know</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/clarifai-basics/applications/">Applications</a><button aria-label="Toggle the collapsible sidebar category &#x27;Applications&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/clarifai-basics/authentication/">Authentication</a><button aria-label="Toggle the collapsible sidebar category &#x27;Authentication&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/clarifai-basics/community">Get Started With Community Portal</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link">Tutorials</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/tutorials/node-js-tutorial">Add AI to a Node.js Web App</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/tutorials/how-to-evaluate-an-image-classification-model">How to Evaluate An Image Classification Model</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/tutorials/image-classification-detection-segmentation">Image classification vs detection vs segmentation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/tutorials/organizations-and-teams">Organization and Teams</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link">API Guide</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/api-guide/api-overview/">Clarifai API Basics</a><button aria-label="Toggle the collapsible sidebar category &#x27;Clarifai API Basics&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/api-guide/data/">Your Data</a><button aria-label="Toggle the collapsible sidebar category &#x27;Your Data&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/api-guide/predict/">Making Predictions</a><button aria-label="Toggle the collapsible sidebar category &#x27;Making Predictions&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/api-guide/concepts/">Creating and Managing Concepts</a><button aria-label="Toggle the collapsible sidebar category &#x27;Creating and Managing Concepts&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/api-guide/annotate/">Labeling Your Data</a><button aria-label="Toggle the collapsible sidebar category &#x27;Labeling Your Data&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/api-guide/model/">Creating and Training Models</a><button aria-label="Toggle the collapsible sidebar category &#x27;Creating and Training Models&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/api-guide/evaluate/">Evaluating Models</a><button aria-label="Toggle the collapsible sidebar category &#x27;Evaluating Models&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/api-guide/workflows/">Creating Workflows</a><button aria-label="Toggle the collapsible sidebar category &#x27;Creating Workflows&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/api-guide/search/">Search, Sort, Filter, and Save</a><button aria-label="Toggle the collapsible sidebar category &#x27;Search, Sort, Filter, and Save&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/api-guide/advanced-topics/">Advanced Topics</a><button aria-label="Toggle the collapsible sidebar category &#x27;Advanced Topics&#x27;" type="button" class="clean-btn menu__caret"></button></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--active">Portal Guide</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/portal-guide/portal-overview">Clarifai Portal Basics</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/portal-guide/data/">Your Data</a><button aria-label="Toggle the collapsible sidebar category &#x27;Your Data&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/portal-guide/ppredict">Making Predictions</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/portal-guide/concepts/">Creating and Managing Concepts</a><button aria-label="Toggle the collapsible sidebar category &#x27;Creating and Managing Concepts&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/portal-guide/annotate/">Labeling Your Data</a><button aria-label="Toggle the collapsible sidebar category &#x27;Labeling Your Data&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/portal-guide/datasets/">Datasets</a><button aria-label="Toggle the collapsible sidebar category &#x27;Datasets&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" aria-expanded="true" tabindex="0" href="/portal-guide/model/">Creating and Training Models</a><button aria-label="Toggle the collapsible sidebar category &#x27;Creating and Training Models&#x27;" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/portal-guide/model/clarifai-models">Clarifai Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/portal-guide/model/pcustom-model-walkthrough">Custom Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/portal-guide/model/training-basics">Training Basics</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/portal-guide/model/training-faqs">Model Training FAQs</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/portal-guide/model/model-types/">Model Types</a><button aria-label="Toggle the collapsible sidebar category &#x27;Model Types&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" aria-expanded="true" tabindex="0" href="/portal-guide/model/deep-training/">Deep Fine-Tuning Templates</a><button aria-label="Toggle the collapsible sidebar category &#x27;Deep Fine-Tuning Templates&#x27;" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/portal-guide/model/deep-training/visual-classification-templates">Visual Classification Templates</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/portal-guide/model/deep-training/visual-detection-templates">Visual Detection Templates</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/portal-guide/model/deep-training/visual-embedding-templates">Visual Embedding Templates</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/portal-guide/model/deep-training/visual-segmenter-templates">Visual Segmenter Templates</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/portal-guide/model/agent-system-operators/">Agent System Operators</a><button aria-label="Toggle the collapsible sidebar category &#x27;Agent System Operators&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/portal-guide/model/hf-model-importer">Import Models from Hugging Face</a></li></ul></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/portal-guide/model-versions/">Creating and Managing Model Versions</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/portal-guide/evaluate/">Evaluating Models</a><button aria-label="Toggle the collapsible sidebar category &#x27;Evaluating Models&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/portal-guide/workflows/">Creating Workflows</a><button aria-label="Toggle the collapsible sidebar category &#x27;Creating Workflows&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/portal-guide/modules/">Modules</a><button aria-label="Toggle the collapsible sidebar category &#x27;Modules&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/portal-guide/psearch/">Smart Search: Search, Sort, Filter, &amp; Save</a><button aria-label="Toggle the collapsible sidebar category &#x27;Smart Search: Search, Sort, Filter, &amp; Save&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/portal-guide/clarifai-organizations/">Clarifai Organizations</a><button aria-label="Toggle the collapsible sidebar category &#x27;Clarifai Organizations&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/portal-guide/usage-dashboard">Usage Dashboard</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/portal-guide/advanced-topics">Advanced Topics</a><button aria-label="Toggle the collapsible sidebar category &#x27;Advanced Topics&#x27;" type="button" class="clean-btn menu__caret"></button></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link">Data Labeling Services</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/data-labeling-services/labeling-services">Scribe LabelForce</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link">Product Updates</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/product-updates/upcoming-api-changes/">Upcoming Platform Changes</a><button aria-label="Toggle the collapsible sidebar category &#x27;Upcoming Platform Changes&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/product-updates/changelog">Changelog</a><button aria-label="Toggle the collapsible sidebar category &#x27;Changelog&#x27;" type="button" class="clean-btn menu__caret"></button></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link">Additional Resources</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a href="https://status.clarifai.com/" target="_blank" rel="noopener noreferrer" class="menu__link menuExternalLink_NmtK" tabindex="0">API Status<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a href="https://www.clarifai.com/blog/" target="_blank" rel="noopener noreferrer" class="menu__link menuExternalLink_NmtK" tabindex="0">Clarifai Blog<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a href="https://help.clarifai.com/" target="_blank" rel="noopener noreferrer" class="menu__link menuExternalLink_NmtK" tabindex="0">Clarifai Help<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a href="https://clarifai.com/explore" target="_blank" rel="noopener noreferrer" class="menu__link menuExternalLink_NmtK" tabindex="0">Clarifai Community<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></li></ul></nav></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Portal Guide</span><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/portal-guide/model/"><span itemprop="name">Creating and Training Models</span></a><meta itemprop="position" content="2"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/portal-guide/model/deep-training/"><span itemprop="name">Deep Fine-Tuning Templates</span></a><meta itemprop="position" content="3"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Visual Detection Templates</span><meta itemprop="position" content="4"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>Visual Detection Templates</h1><p><strong>Learn about our visual detection templates</strong></p><hr><p>Detection templates make it easy to build models that can identify objects within a region of your images or videos. Detection models return concepts and bounding boxes.</p><p>Each template comes with its own hyperparameters, which you can tune to influence “how” your model learns. With hyperparameters, you can customize and fine-tune a template to suit your specific tasks and achieve better performance.</p><div class="theme-admonition theme-admonition-tip alert alert--success admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_S0QG"><p>You can customize most hyperparameters by specifying the following values:</p><ul><li><code>minimum</code>—the minimum value a given parameter can take.</li><li><code>maximum</code>—the maximum value a given parameter can take.</li><li><code>step</code>—determines how much you can increment or decrement the minimum or maximum value in a single click/change.</li></ul></div></div><h2 class="anchor anchorWithStickyNavbar_LWe7" id="mmdetection_yolof">MMDetection_YoloF<a href="#mmdetection_yolof" class="hash-link" aria-label="Direct link to MMDetection_YoloF" title="Direct link to MMDetection_YoloF">​</a></h2><p>This is a deep learning template model from MMDetection that focuses on object detection using the YOLO (You Only Look Once) framework.</p><p>MMDetection, short for &quot;OpenMMLab Detection Toolbox and Benchmark,&quot; is an open-source software framework developed by OpenMMLab. It is designed to facilitate research and development in the field of object detection and instance segmentation. MMDetection provides a comprehensive collection of state-of-the-art models, datasets, and evaluation metrics, making it a valuable resource for both academic and industrial applications.</p><p>The <strong>MMDetection_YoloF</strong> template leverages the power of convolutional neural networks (CNNs) and advanced techniques like anchor-based prediction and feature pyramid networks to accurately detect and localize objects in images or videos.</p><p>With its robust architecture and pretrained weights, <strong>MMDetection_YoloF</strong> provides a strong foundation for developers and researchers to build custom object detection solutions for various use cases.</p><p>The <strong>MMDetection_YoloF</strong> template supports the following hyperparameters:</p><ul><li><strong>Image size</strong>—This is the image size for training and inference. When a single value is specified, it typically means that the images will be resized so that the minimum side (either width or height) of each image matches that value. On the other hand, when more than one value is provided, and it is combined with &quot;keep_aspect_ratio=False&quot;, it means that the images will be resized to the exact width and height specified.</li><li><strong>Max aspect_ratio</strong>—When &quot;keep_aspect_ratio&quot; is set to True, it is used to control the maximum length of the longer side of an image relative to the shorter side during image resizing. The minimum value it supports for customization is <code>1.0</code>, while the maximum is <code>5.0</code>.</li><li><strong>Keep aspect_ratio</strong>—This is a boolean that determines whether to keep the original aspect ratio of the image during resizing. If set to True (default, recommended), the aspect ratio of the image will be preserved when resizing. The image will be resized, maintaining the same aspect ratio, to fit within the desired dimensions. If set to False, non-aspect-preserving resizes will be used. In this case, the image may be distorted as it is resized to exactly match the specified dimensions, ignoring the original aspect ratio.</li><li><strong>Batch size</strong>—The number of images used during training. Increased batch size allows for a better approximation of gradient over those samples. Batches allow for stochastic gradient descent, by choosing a random set of X images for each training update. You may want to increase the batch size if the model is large and takes a long time to train. You may also want to increase the batch size if your total number of model concepts is larger than the batch size (you may want to increase it to around 2x the category count). The minimum value it supports for customization is <code>1</code>, while the maximum is <code>32</code>—with an incremental or decremental step of <code>1</code>. </li><li><strong>Num epochs</strong>—This is the total number of epochs to train for. An epoch is defined as one-pass over the entire dataset. If you increase it, the model will take longer to train, but it could make the model more robust. The minimum value it supports for customization is <code>1</code>, while the maximum is <code>200</code>—with an incremental or decremental step of <code>1</code>.  </li><li><strong>Min samples_per_epoch</strong>—For very small datasets, this specifies the minimum number of samples processed in one epoch during training. When dealing with very small datasets, it&#x27;s essential to be cautious about the number of samples per epoch to avoid overfitting and unstable training. For small datasets, a common approach is to repeat the dataset multiple times to increase the number of effective epochs.</li><li><strong>Per item_lrate</strong>—This is the initial learning rate per item; it&#x27;s the rate that the model weights are changed per item. The <strong>lrate</strong> (learning rate) is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function. The overall learning rate (per step) is calculated by <code>lrate = batch_size * per_item_lrate</code>. The minimum value it supports for customization is <code>0.0</code>.</li><li><strong>Pretrained weights</strong>—This specifies whether to init the model with pre-trained weights. You can choose either <code>None</code> or <code>coco</code> (default) for this parameter. </li><li><strong>Frozen stages</strong>—This specifies the backbone network stages to keep frozen during training. The minimum value it supports for customization is <code>1</code>, while the maximum is <code>4</code>—with an incremental or decremental step of <code>1</code>. </li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="mmdetection_ssd">MMDetection_SSD<a href="#mmdetection_ssd" class="hash-link" aria-label="Direct link to MMDetection_SSD" title="Direct link to MMDetection_SSD">​</a></h2><p>This template is an implementation of the SSD object detection algorithm within the MMDetection framework, offering a convenient and powerful tool for object detection tasks. The Single Shot MultiBox Detector (SSD) architecture is a popular object detection algorithm that offers a good trade-off between accuracy and speed.</p><p>The SSD model in MMDetection is designed to detect and localize objects in images using a single deep neural network. It achieves this by dividing the input image into a grid of cells and predicting object bounding boxes and class probabilities within each cell. SSD incorporates multiple convolutional layers of different scales to capture objects of various sizes and aspect ratios, allowing it to detect objects at different scales in a single pass.</p><p><strong>MMDetection_SSD</strong> provides a pre-configured implementation of the SSD architecture along with trained weights on standard benchmark datasets such as COCO and VOC. This allows users to utilize the model out of the box for various object detection tasks or as a starting point for further customization and fine-tuning.</p><p>By leveraging the MMDetection framework, users can take advantage of its data pre-processing, model training, and evaluation capabilities to train and evaluate the <strong>MMDetection_SSD</strong> model on their own datasets. The framework also provides tools for visualizing and analyzing the detection results.</p><p>The <strong>MMDetection_SSD</strong> template supports the following hyperparameters:</p><ul><li><strong>Image size</strong>—This is the image size for training and inference. Images are scaled for efficient processing, and a lower number will take up less memory and run faster. A higher number will have more pixel information to train on and will increase accuracy. </li><li><strong>Batch size</strong>—The number of images used during training. Increased batch size allows for a better approximation of gradient over those samples. Batches allow for stochastic gradient descent, by choosing a random set of X images for each training update. You may want to increase the batch size if the model is large and takes a long time to train. You may also want to increase the batch size if your total number of model concepts is larger than the batch size (you may want to increase it to around 2x the category count). The minimum value it supports for customization is <code>1</code>, while the maximum is <code>32</code>—with an incremental or decremental step of <code>1</code>. </li><li><strong>Num epochs</strong>—This is the total number of epochs to train for. An epoch is defined as one-pass over the entire dataset. If you increase it, the model will take longer to train, but it could make the model more robust. The minimum value it supports for customization is <code>1</code>, while the maximum is <code>200</code>—with an incremental or decremental step of <code>1</code>.  </li><li><strong>Per item_lrate</strong>—This is the initial learning rate per item; it&#x27;s the rate that the model weights are changed per item. The <strong>lrate</strong> (learning rate) is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function. The overall learning rate (per step) is calculated by <code>lrate = batch_size * per_item_lrate</code>. The minimum value it supports for customization is <code>0.0</code>.</li><li><strong>Pretrained weights</strong>—This specifies whether to init the model with pre-trained weights. You can choose either <code>None</code> or <code>coco</code> (default) for this parameter.  </li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="mmdetection_fasterrcnn">MMDetection_FasterRCNN<a href="#mmdetection_fasterrcnn" class="hash-link" aria-label="Direct link to MMDetection_FasterRCNN" title="Direct link to MMDetection_FasterRCNN">​</a></h2><p><strong>MMDetection_FasterRCNN</strong> refers to a specific model implemented in the MMDetection framework that is based on the Faster R-CNN (Region-based Convolutional Neural Networks) architecture. Faster R-CNN is a widely used and highly effective object detection algorithm.</p><p>The Faster R-CNN algorithm consists of two main components: a region proposal network (RPN) and a region-based CNN for detection. The RPN generates potential object bounding box proposals, and the region-based CNN classifies and refines these proposals to produce the final detection results.</p><p>In MMDetection, the <strong>MMDetection_FasterRCNN</strong> model provides a pre-configured implementation of the Faster R-CNN architecture along with pre-trained weights on standard benchmark datasets like COCO and VOC. It allows users to utilize the model out of the box for object detection tasks or as a starting point for further customization and fine-tuning.</p><p><strong>MMDetection_FasterRCNN</strong> leverages the MMDetection framework&#x27;s capabilities for data preprocessing, model training, and evaluation. Users can train the model on their own datasets, adjust hyperparameters, and analyze the detection results using the provided tools.</p><p>The Faster R-CNN algorithm has been proven to achieve excellent performance in terms of accuracy, making <strong>MMDetection_FasterRCNN</strong> a valuable tool for a wide range of object detection applications.</p><p>The <strong>MMDetection_FasterRCNN</strong> template supports the following hyperparameters:</p><ul><li><strong>Image size</strong>—This is the image size for training and inference. When a single value is specified, it typically means that the images will be resized so that the minimum side (either width or height) of each image matches that value. On the other hand, when more than one value is provided, it means that the images will be resized to the exact width and height specified.</li><li><strong>Random resize_lower</strong>—This is the lower limit for the random resizing. It means that during training, the input images will be randomly resized to a size equal to or larger than this lower limit. It uses the same one or two element format as <code>image_size</code>. And if it&#x27;s empty, it uses <code>image_size</code>.  If the original image size is smaller than the lower limit, it will not be resized, and the original size will be used.</li><li><strong>Random resize_upper</strong>—This is the upper limit for the random resizing. It means that during training, the input images will be randomly resized to a size equal to or smaller than this upper limit. It uses the same one or two element format as <code>image_size</code>. And if it&#x27;s empty, it uses <code>image_size</code>. If the original image size is already smaller than the upper limit, it will not be resized, and the original size will be used.</li><li><strong>Batch size</strong>—The number of images used during training. Increased batch size allows for a better approximation of gradient over those samples. Batches allow for stochastic gradient descent, by choosing a random set of X images for each training update. You may want to increase the batch size if the model is large and takes a long time to train. You may also want to increase the batch size if your total number of model concepts is larger than the batch size (you may want to increase it to around 2x the category count). The minimum value it supports for customization is <code>1</code>, while the maximum is <code>32</code>—with an incremental or decremental step of <code>1</code>.  </li><li><strong>Num epochs</strong>—This is the total number of epochs to train for. An epoch is defined as one-pass over the entire dataset. If you increase it, the model will take longer to train, but it could make the model more robust. The minimum value it supports for customization is <code>1</code>, while the maximum is <code>200</code>—with an incremental or decremental step of <code>1</code>.   </li><li><strong>Per item_lrate</strong>—This is the initial learning rate per item; it&#x27;s the rate that the model weights are changed per item. The <strong>lrate</strong> (learning rate) is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function. The overall learning rate (per step) is calculated by <code>lrate = batch_size * per_item_lrate</code>. The minimum value it supports for customization is <code>0.0</code>.</li><li><strong>Pretrained weights</strong>—This specifies whether to init the model with pre-trained weights. You can choose either <code>None</code> or <code>coco</code> (default) for this parameter. </li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="clarifai_inceptionv4">Clarifai_InceptionV4<a href="#clarifai_inceptionv4" class="hash-link" aria-label="Direct link to Clarifai_InceptionV4" title="Direct link to Clarifai_InceptionV4">​</a></h2><p>This is a visual detector template based on RetinaNet, a popular object detection framework, that utilizes the InceptionV4 architecture as its backbone.</p><p><a href="https://arxiv.org/abs/1602.07261" target="_blank" rel="noopener noreferrer">InceptionV4</a> is a variant of the Inception architecture, which was originally introduced by Google for image classification tasks. The InceptionV4 model is a convolutional neural network (CNN) that is designed to extract high-level features from images for tasks such as object recognition, classification, and detection. It incorporates various innovative techniques, including inception modules with multiple parallel branches, factorized convolutions, and residual connections, to enhance its performance and efficiency.</p><p><strong>Clarifai_InceptionV4</strong> template leverages the strengths of InceptionV4 by applying it at multiple image scales, allowing for robust detection across a range of object sizes.</p><p>Compared to InceptionV2, <strong>Clarifai_InceptionV4</strong> sacrifices speed for increased accuracy. While InceptionV2 is faster, <strong>Clarifai_InceptionV4</strong> is slower but offers improved precision in object detection tasks. This makes it well-suited for applications that prioritize accuracy over real-time inference.</p><p><strong>Clarifai_InceptionV4</strong> is pretrained on either the COCO (Common Objects in Context) dataset or the OpenImages dataset. COCO is a widely used benchmark dataset for object detection, while OpenImages is a large-scale dataset with a diverse range of object categories. Pretraining on these datasets enables the model to learn general representations of objects, improving its ability to detect and classify objects accurately.</p><p>By combining the strengths of the RetinaNet framework, the powerful InceptionV4 backbone, and pretrained weights on COCO or OpenImages, <strong>Clarifai_InceptionV4</strong> provides a robust and accurate solution for object detection tasks, making it a valuable tool for various computer vision applications.</p><p>The <strong>Clarifai_InceptionV4</strong> template supports the following hyperparameters:</p><ul><li><strong>Image size</strong>—This is the input image size, specifically the minimum side dimension. Images are scaled for efficient processing, and a lower number will take up less memory and run faster. A higher number will have more pixel information to train on and will increase accuracy. The valid choices you can provide are: <code>320, 512, or 800</code>.</li><li><strong>Batch size</strong>—The number of images used during training. Increased batch size allows for a better approximation of gradient over those samples. Batches allow for stochastic gradient descent, by choosing a random set of X images for each training update. You may want to increase the batch size if the model is large and takes a long time to train. You may also want to increase the batch size if your total number of model concepts is larger than the batch size (you may want to increase it to around 2x the category count). The minimum value it supports for customization is <code>1</code>, while the maximum is <code>16</code>—with an incremental or decremental step of <code>1</code>. </li><li><strong>Num epochs</strong>—This is the total number of epochs to train for. An epoch is defined as one-pass over the entire dataset. If you increase it, the model will take longer to train, but it could make the model more robust. The minimum value it supports for customization is <code>1</code>, while the maximum is <code>200</code>—with an incremental or decremental step of <code>1</code>.  </li><li><strong>Use perclass_regression</strong>—This is a boolean that specifies whether to use separate coordinate regressors for each class, or one set for all classes. Per-class regression refers to the process of using separate box coordinate regressors for each class in the dataset. This means that for each object class, there is a dedicated set of regression parameters that are learned during the training process to predict the bounding box coordinates (e.g., x, y, width, and height) specific to that class.</li><li><strong>Anchor ratios</strong>—Anchor boxes are predefined bounding boxes of different shapes and sizes that act as reference templates for detecting objects of various scales and aspect ratios in an image. The anchor ratios refer to the width (w) to height (h) ratios of these anchor boxes. They determine the shape of the anchor boxes, allowing the object detector to handle objects with different aspect ratios effectively.</li><li><strong>Use focal_loss</strong>—This is a boolean that specifies whether to use focal loss during training or Online Hard Example Mining (OHEM). Focal loss is a modification of the standard cross-entropy loss that addresses the issue of class imbalance during training. It introduces a modulating factor to downweight the contribution of easy examples while focusing more on hard examples. OHEM is a technique used to alleviate the problem of class imbalance by focusing on challenging samples during training. Instead of using all samples in a batch, OHEM selects the hardest examples (e.g., the ones with the highest loss) and only uses those for backpropagation. By doing so, it gives more importance to the difficult examples, which can lead to more effective learning, especially when dealing with a large number of easy background samples.</li><li><strong>Per item_lrate</strong>—This is the initial learning rate per item; it&#x27;s the rate that the model weights are changed per item. The <strong>lrate</strong> (learning rate) is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function. The overall learning rate (per step) is calculated by <code>lrate = batch_size * per_item_lrate</code>. The minimum value it supports for customization is <code>0.0</code>.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="clarifai_inceptionv2">Clarifai_InceptionV2<a href="#clarifai_inceptionv2" class="hash-link" aria-label="Direct link to Clarifai_InceptionV2" title="Direct link to Clarifai_InceptionV2">​</a></h2><p>This is a visual detector template based on RetinaNet using the Inception V2 backbone architecture, which is applied at multiple image scales. It offers a balance between speed and accuracy. Compared to InceptionV4, InceptionV2 is faster but provides slightly lower accuracy.</p><p>The model can be pre-trained on either the COCO (Common Objects in Context) dataset or the OpenImages dataset. These datasets contain a wide range of labeled images, enabling the model to learn to detect various objects and entities in images. The choice of dataset for pretraining depends on the specific application and the types of objects or entities you want the model to detect.</p><p><strong>Clarifai_InceptionV2</strong> serves as an efficient deep learning template that leverages the Inception V2 backbone architecture, providing a good trade-off between speed and accuracy for object detection tasks.</p><p>The <strong>Clarifai_InceptionV2</strong> template supports the following hyperparameters:</p><ul><li><strong>Image size</strong>—This is the input image size, specifically the minimum side dimension. Images are scaled for efficient processing, and a lower number will take up less memory and run faster. A higher number will have more pixel information to train on and will increase accuracy. The valid choices you can provide are: <code>320, 512, or 800</code>.</li><li><strong>Batch size</strong>—The number of images used during training. Increased batch size allows for a better approximation of gradient over those samples. Batches allow for stochastic gradient descent, by choosing a random set of X images for each training update. You may want to increase the batch size if the model is large and takes a long time to train. You may also want to increase the batch size if your total number of model concepts is larger than the batch size (you may want to increase it to around 2x the category count). The minimum value it supports for customization is <code>1</code>, while the maximum is <code>16</code>—with an incremental or decremental step of <code>1</code>. </li><li><strong>Num epochs</strong>—This is the total number of epochs to train for. An epoch is defined as one-pass over the entire dataset. If you increase it, the model will take longer to train, but it could make the model more robust. The minimum value it supports for customization is <code>1</code>, while the maximum is <code>200</code>—with an incremental or decremental step of <code>1</code>. </li><li><strong>Use perclass_regression</strong>—This is a boolean that specifies whether to use separate coordinate regressors for each class, or one set for all classes. Per-class regression refers to the process of using separate box coordinate regressors for each class in the dataset. This means that for each object class, there is a dedicated set of regression parameters that are learned during the training process to predict the bounding box coordinates (e.g., x, y, width, and height) specific to that class.</li><li><strong>Anchor ratios</strong>—Anchor boxes are predefined bounding boxes of different shapes and sizes that act as reference templates for detecting objects of various scales and aspect ratios in an image. The anchor ratios refer to the width (w) to height (h) ratios of these anchor boxes. They determine the shape of the anchor boxes, allowing the object detector to handle objects with different aspect ratios effectively.</li><li><strong>Use focal_loss</strong>—This is a boolean that specifies whether to use focal loss during training or Online Hard Example Mining (OHEM). Focal loss is a modification of the standard cross-entropy loss that addresses the issue of class imbalance during training. It introduces a modulating factor to downweight the contribution of easy examples while focusing more on hard examples. OHEM is a technique used to alleviate the problem of class imbalance by focusing on challenging samples during training. Instead of using all samples in a batch, OHEM selects the hardest examples (e.g., the ones with the highest loss) and only uses those for backpropagation. By doing so, it gives more importance to the difficult examples, which can lead to more effective learning, especially when dealing with a large number of easy background samples.</li><li><strong>Per item_lrate</strong>—This is the initial learning rate per item; it&#x27;s the rate that the model weights are changed per item. The <strong>lrate</strong> (learning rate) is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function. The overall learning rate (per step) is calculated by <code>lrate = batch_size * per_item_lrate</code>. The minimum value it supports for customization is <code>0.0</code>.</li></ul></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/portal-guide/model/deep-training/visual-classification-templates"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Visual Classification Templates</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/portal-guide/model/deep-training/visual-embedding-templates"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Visual Embedding Templates</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#mmdetection_yolof" class="table-of-contents__link toc-highlight">MMDetection_YoloF</a></li><li><a href="#mmdetection_ssd" class="table-of-contents__link toc-highlight">MMDetection_SSD</a></li><li><a href="#mmdetection_fasterrcnn" class="table-of-contents__link toc-highlight">MMDetection_FasterRCNN</a></li><li><a href="#clarifai_inceptionv4" class="table-of-contents__link toc-highlight">Clarifai_InceptionV4</a></li><li><a href="#clarifai_inceptionv2" class="table-of-contents__link toc-highlight">Clarifai_InceptionV2</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Company</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://clarifai.com" target="_blank" rel="noopener noreferrer" class="footer__link-item">Clarifai Website<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/clarifai" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://join.slack.com/t/clarifaicommunity/shared_invite/zt-1jehqesme-l60djcd3c_4a1eCV~uPUjQ" target="_blank" rel="noopener noreferrer" class="footer__link-item">Slack<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/clarifai" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://www.linkedin.com/company/clarifai" target="_blank" rel="noopener noreferrer" class="footer__link-item">LinkedIn<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://github.com/facebook/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.facebook.com/Clarifai/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Facebook<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2023 Clarifai, Inc.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.9ebdf72f.js"></script>
<script src="/assets/js/main.19defeb1.js"></script>
<!-- Google Tag Manager (noscript) -->
    <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-5W9P7GR" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
    <!-- End Google Tag Manager (noscript) --></body>
</html>
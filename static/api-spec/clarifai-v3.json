{
  "openapi": "3.0.0",
  "info": {
    "title": "Clarifai Public API",
    "description": "\n\nWelcome to the Clarifai API documentation. Clarifai is a leading provider of artificial intelligence and machine learning solutions, specialising in advanced computer vision, natural language, and generative AI technologies. Our powerful API allows developers to integrate cutting-edge image, video recognition, and text-based analytics capabilities into their applications, unlocking a world of visual and sequence understanding possibilities.\n\nThe Clarifai API is a connection point enabling communication between clients and servers. It facilitates interaction between software, making it valuable for integrating Clarifai's AI technology into your products and tools through code. Clarifai API is avaiable in Python,Node.js,PHP and Java.\n\nYou can access Clarifai's API over HTTPS via `https://api.clarifai.com`.\n\n[API Documentation](https://docs.clarifai.com/api-guide/api-overview)\n\n[Clarifai Community](https://clarifai.com/explore)\n\n[API Status](https://status.clarifai.com/)\n\n[Discord](https://discord.gg/XAPE3Vtg)\n\n> _ℹ️ Reload the page in case of missing sections_",
    "version": "1.0.0"
  },
  "servers": [
    {
      "url": "https://api.clarifai.com"
    }
  ],
  "components": {
    "securitySchemes": {
      "apikeyAuth": {
        "type": "http",
        "scheme": "apikey"
      }
    }
  },
  "security": [
    {
      "apikeyAuth": []
    }
  ],
  "tags": [
    {
      "name": "Annotations",
      "description": "Welcome to the Clarifai Annotations API Postman Collection! 🌟\n\n**Description:**  \nThe Clarifai Annotations API Collection empowers you to seamlessly integrate advanced image and video annotation capabilities into your applications. With this collection, you can annotate multimedia content with descriptive information, enabling powerful image recognition and classification. Whether you're building an image moderation system, enhancing search functionalities, or training machine learning models, these endpoints provide you with the tools to annotate and extract valuable insights from your visual data.\n\n## 🔥 Getting started:\n\n1. **Create Annotations:** Annotate images and videos with concepts, bounding boxes, and regions of interest.\n    \n2. **Retrieve Annotations:** Fetch detailed information about annotations associated with specific images or videos.\n    \n3. **Update Annotations:** Modify existing annotations to improve accuracy or adapt to changing requirements.\n    \n4. **Delete Annotations:** Remove annotations when they are no longer needed or to refresh training data.\n    \n\n## 🚀 Pre-requisites:\n\nTo access Clarifai API endpoints, you need to authenticate by including your _**PAT**_ key in the request headers. Obtain your API key by signing up on Clarifai's platform and manage your API usage securely. Also, add User ID and App ID as environment variables.\n\nThe `{{base_url}}` is set as `https://api.clarifai.com`\n\n## 🕹️ **How to Use:**\n\n1. **Import the Collection:** Import this collection into your Postman workspace to begin your Clarifai API integration journey.\n    \n2. **Authentication:** Add your Clarifai API key to the request headers to authenticate your requests.\n    \n3. **Explore Endpoints:** Familiarize yourself with the various endpoints to manage datasets, train models, and generate predictions."
    },
    {
      "name": "Annotations > Find Duplicate Annotations Jobs",
      "description": "Within this folder, you'll discover a suite of functionalities dedicated to identifying and managing duplicate annotations within your image data."
    },
    {
      "name": "Annotations > Annotation Filters",
      "description": "With Annotation Filters, you can easily filter and display relevant information based on tags or annotations."
    },
    {
      "name": "Applications",
      "description": "Welcome to the Clarifai Applications API Collection! 🌟\n\nThe Clarifai Applications API Collection is designed to empower developers and businesses to integrate advanced AI capabilities seamlessly into their applications. With a focus on image and video recognition, this collection provides a set of powerful endpoints for creating, managing, and extracting insights from custom applications.\n\n## 🔥 Getting started:\n\n1. **Create Applications:** Set up custom applications to organize and manage your specific use cases.\n2. **Manage Application**: Modify and edit your apps with ease.\n    \n\n## 🚀 Pre-requisites:\n\nTo access Clarifai API endpoints, you need to authenticate by including your _**PAT**_ key in the request headers. Obtain your API key by signing up on Clarifai's platform and manage your API usage securely. Also, add User ID and App ID as environment variables.\n\nThe `{{base_url}}` is set as `https://api.clarifai.com`\n\n## 🕹️ **How to Use:**\n\n1. **Import the Collection:** Import this collection into your Postman workspace to begin your Clarifai API integration journey.\n2. **Authentication:** Add your Clarifai API key to the request headers to authenticate your requests.\n3. **Explore Endpoints:** Familiarize yourself with the various endpoints to manage apps, train models, and generate predictions."
    },
    {
      "name": "Collectors",
      "description": "Welcome to the Clarifai Collector API Collection! 🌟\n\nThis collection empowers you to seamlessly integrate Clarifai's advanced machine-learning capabilities into your applications. Effectively manage data collection efforts for training and improving machine learning models with the Collector Collection. Whether you're building a smart application, enhancing user experiences, or conducting research, this collection provides easy-to-use endpoints for efficient data collection.\n\n## 🔥 Getting Started:\n\n- **List Collectors:** Retrieve a list of all collectors associated with your Clarifai app.\n    \n- **Create Collector:** Easily create a new collector for your specific data collection needs.\n    \n- **Get Collector Details:** Retrieve detailed information about a specific collector, including its ID, name, description, status, and associated tasks.\n    \n- **Update Collector:** Modify and edit existing collectors as needed.\n    \n\n## 🚀 Pre-requisites:\n\nTo access Clarifai API endpoints in this collection, you need to authenticate by including your PAT key in the request headers. Obtain your API key by signing up on Clarifai's platform and manage your API usage securely. Additionally, make sure to add User ID and App ID as environment variables.\n\nThe `{{base_url}}` is set as `https://api.clarifai.com`\n\n## 🕹️ How to Use:\n\n1. **Import the Collection:** Import this collection into your Postman workspace to kickstart your Clarifai API integration for data collection.\n    \n2. **Authentication:** Add your Clarifai API key to the request headers to authenticate your requests and ensure secure communication with the API.\n    \n3. **Explore Endpoints:** Familiarize yourself with the various endpoints provided in the collection to efficiently manage data collection efforts, enhancing the capabilities of your machine learning models."
    },
    {
      "name": "Concepts",
      "description": "Welcome to the Clarifai Concepts API Collection! 🌟\n\nWith this collection, you can seamlessly integrate Clarifai's advanced machine-learning capabilities into your applications, allowing you to work with concepts effortlessly. Whether you're building a smart application, enhancing user experiences, or conducting research, this collection provides you with easy-to-use endpoints for managing concepts for training AI models.\n\n## 🔥 Getting started:\n\n1. **Create Concept:** Easy creation of concepts using given endpoints.\n    \n2. **Manage Concept**: Modify and edit your concept with ease.\n    \n3. **Languages**: Add concepts of different languages to your application.\n    \n\n## 🚀 Pre-requisites:\n\nTo access Clarifai API endpoints, you need to authenticate by including your _**PAT**_ key in the request headers. Obtain your API key by signing up on Clarifai's platform and manage your API usage securely. Also, add User ID and App ID as environment variables.\n\nThe `{{base_url}}` is set as `https://api.clarifai.com`\n\n## 🕹️ **How to Use:**\n\n1. **Import the Collection:** Import this collection into your Postman workspace to begin your Clarifai API integration journey.\n    \n2. **Authentication:** Add your Clarifai API key to the request headers to authenticate your requests.\n    \n3. **Explore Endpoints:** Familiarize yourself with the various endpoints to manage datasets, train models, and generate predictions."
    },
    {
      "name": "Concepts > Concept Essentials",
      "description": "This carefully curated collection is designed to streamline your experience with managing and manipulating concepts, ensuring a seamless workflow for your API testing and development needs."
    },
    {
      "name": "Concepts > Concept Vocabs",
      "description": "This carefully curated collection is designed to streamline your experience with managing and manipulating vocabs in concepts, ensuring a seamless workflow for your API testing and development needs."
    },
    {
      "name": "Concepts > Concept Languages",
      "description": "This carefully curated collection is designed to streamline your experience with managing and manipulating concepts & languages, ensuring a seamless workflow for your API testing and development needs."
    },
    {
      "name": "Concepts > Concept Graph",
      "description": "This carefully curated collection is designed to streamline your experience managing and manipulating concept graphs, ensuring a seamless workflow for your API testing and development needs."
    },
    {
      "name": "Datasets",
      "description": "Welcome to the Clarifai Dataset API Collection! 🌟\n\nWith this collection, you can seamlessly integrate Clarifai's advanced machine-learning capabilities into your applications, allowing you to work with datasets effortlessly. Whether you're building a smart application, enhancing user experiences, or conducting research, this collection provides you with easy-to-use endpoints for managing datasets for training AI models.\n\n## 🔥 Getting started:\n\n1. **Create Datasets:** Upload and organize your data for efficient processing.\n    \n2. **Manage Datasets**: Modify and edit your dataset with ease.\n    \n3. **Versioning**: Versioning helps track changes made to datasets over time, providing a clear history of modifications and ensuring reproducibility and traceability.\n    \n\n## 🚀 Pre-requisites:\n\nTo access Clarifai API endpoints, you need to authenticate by including your _**PAT**_ key in the request headers. Obtain your API key by signing up on Clarifai's platform and manage your API usage securely. Also, add User ID and App ID as environment variables.\n\nThe `{{base_url}}` is set as `https://api.clarifai.com`\n\n## 🕹️ **How to Use:**\n\n1. **Import the Collection:** Import this collection into your Postman workspace to begin your Clarifai API integration journey.\n    \n2. **Authentication:** Add your Clarifai API key to the request headers to authenticate your requests.\n    \n3. **Explore Endpoints:** Familiarize yourself with the various endpoints to manage datasets, train models, and generate predictions."
    },
    {
      "name": "Datasets > Dataset Essentials",
      "description": "This carefully curated collection is designed to streamline your experience with managing and manipulating datasets, ensuring a seamless workflow for your API testing and development needs."
    },
    {
      "name": "Datasets > Dataset Inputs",
      "description": "This carefully organized folder is designed to streamline the process of collecting diverse and comprehensive input data for your datasets."
    },
    {
      "name": "Datasets > Dataset Versions",
      "description": "This organized and comprehensive folder is designed to streamline your experience in managing dataset versions for your applications."
    },
    {
      "name": "Inputs",
      "description": "Welcome to the Clarifai Inputs API Postman Collection! 🌟\n\nThe Clarifai Inputs API Collection is designed to streamline your interactions with multimedia inputs, such as images and videos. This collection provides easy-to-use endpoints for managing individual inputs, facilitating seamless integration into your workflows. Whether you're working on image classification, object detection, or custom model training, this collection is your gateway to efficient input management.\n\n## 🔥 Getting started:\n\n1. **Add Individual Inputs:** Upload and add individual images or videos to your Clarifai datasets for analysis.\n    \n2. **Get Input Details:** Retrieve detailed information about a specific input using its unique identifier.\n    \n3. **Delete Input:** Remove an input from your datasets, ensuring data accuracy and relevance.\n    \n4. **Search Inputs:** Perform searches based on various criteria to locate specific inputs efficiently.\n    \n\n## 🚀 Pre-requisites:\n\nTo access Clarifai API endpoints, you need to authenticate by including your _**PAT**_ key in the request headers. Obtain your API key by signing up on Clarifai's platform and manage your API usage securely. Also, add User ID and App ID as environment variables.\n\nThe `{{base_url}}` is set as `https://api.clarifai.com`\n\n## 🕹️ **How to Use:**\n\n1. **Import the Collection:** Import this collection into your Postman workspace to begin your Clarifai API integration journey.\n    \n2. **Authentication:** Add your Clarifai API key to the request headers to authenticate your requests.\n    \n3. **Explore Endpoints:** Familiarize yourself with the various endpoints to manage datasets, train models, and generate predictions."
    },
    {
      "name": "Models",
      "description": "Welcome to the Clarifai Models API Postman Collection! 🌟\n\nThe Clarifai Models API Collection is your gateway to unleashing the power of cutting-edge artificial intelligence for image and video analysis. This collection empowers you to seamlessly integrate Clarifai's pre-trained models into your applications, enabling tasks such as object recognition, scene detection, and content moderation.\n\n## 🔥 Getting started:\n\n1. **Pre-trained Models:** Access a variety of pre-trained models for tasks like image and video recognition out of the box.\n    \n2. **Custom Models:** Train custom models tailored to your specific use cases and datasets.\n    \n3. **Model Training:** Train and fine-tune models to recognize specific patterns and objects based on your unique requirements.\n    \n4. **Model Predictions:** Generate predictions for images and videos using Clarifai's advanced machine-learning models.\n    \n\n## 🚀 Pre-requisites:\n\nTo access Clarifai API endpoints, you need to authenticate by including your _**PAT**_ key in the request headers. Obtain your API key by signing up on Clarifai's platform and manage your API usage securely. Also, add User ID and App ID as environment variables.\n\nThe `{{base_url}}` is set as `https://api.clarifai.com`\n\n## 🕹️ **How to Use:**\n\n1. **Import the Collection:** Import this collection into your Postman workspace to begin your Clarifai API integration journey.\n    \n2. **Authentication:** Add your Clarifai API key to the request headers to authenticate your requests.\n    \n3. **Explore Endpoints:** Familiarize yourself with the various endpoints to manage apps, train models, and generate predictions."
    },
    {
      "name": "Models > Custom Config",
      "description": "The Custom Config folder houses a set of API endpoints that allow you to manage custom configurations within your models. Whether you need to fine-tune settings, adjust parameters, or personalize your application's behavior, these endpoints provide the flexibility to tailor the configuration according to your specific requirements."
    },
    {
      "name": "Models > Create Transfer Learned Model"
    },
    {
      "name": "Models > Create Visual Classifier Model Copy"
    },
    {
      "name": "Models > Create Text to Text LLM Model"
    },
    {
      "name": "Models > Create Keyword Filter Model"
    },
    {
      "name": "Models > Create Image Crop Model"
    },
    {
      "name": "Search",
      "description": "Welcome to the Clarifai Search API Collection! 🌟\n\nThe Clarifai Search API Collection is designed to empower developers and businesses to integrate advanced AI capabilities seamlessly into their applications. With a focus on image and video recognition, this collection provides a set of powerful endpoints for searching various attributes.\n\n## 🔥 Getting started:\n\n1. **Ranking:** Display search results based on a ranked order.\n    \n2. **Filter**: Display search results that fulfill a condition.\n    \n\n## 🚀 Pre-requisites:\n\nTo access Clarifai API endpoints, you need to authenticate by including your _**PAT**_ key in the request headers. Obtain your API key by signing up on Clarifai's platform and manage your API usage securely. Also, add User ID and App ID as environment variables.\n\nThe `{{base_url}}` is set as `https://api.clarifai.com`\n\n## 🕹️ **How to Use:**\n\n1. **Import the Collection:** Import this collection into your Postman workspace to begin your Clarifai API integration journey.\n    \n2. **Authentication:** Add your Clarifai API key to the request headers to authenticate your requests.\n    \n3. **Explore Endpoints:** Familiarize yourself with the various endpoints to manage apps, train models, and generate predictions."
    },
    {
      "name": "Search > Basic Search Walkthrough",
      "description": "This folder contains a demo walkthrough of annotation search."
    },
    {
      "name": "Search > Rank Annotations",
      "description": "This folder contains curated list of requests used to perform ranked searches."
    },
    {
      "name": "Search > Filter Inputs"
    },
    {
      "name": "Search > Filter Annotations",
      "description": "This folder contains requests that can perform filter search operations."
    },
    {
      "name": "Search > Metrics"
    },
    {
      "name": "Workflows",
      "description": "Welcome to the Clarifai Workflows API Postman Collection! 🌟\n\nThe Clarifai Workflows API Collection empowers you to streamline your image and video processing workflows effortlessly. Whether you are developing intelligent applications, automating content moderation, or conducting advanced image recognition tasks, this collection provides a set of powerful endpoints to create, manage, and execute workflows.\n\n## 🔥 Getting started:\n\n1. **Create Workflows:** Design and define custom workflows tailored to your specific use cases and business requirements.\n    \n2. **Manage Workflows**: Monitor and manage your workflows by reviewing workflow status.\n    \n3. **Versioning**: Versioning helps track changes made to workflows over time, providing a clear history of modifications and ensuring reproducibility and traceability.\n    \n4. **Workflow Prediction:** Generate predictions for data using workflows.\n    \n\n## 🚀 Pre-requisites:\n\nTo access Clarifai API endpoints, you need to authenticate by including your _**PAT**_ key in the request headers. Obtain your API key by signing up on Clarifai's platform and manage your API usage securely. Also, add User ID and App ID as environment variables.\n\nThe `{{base_url}}` is set as `https://api.clarifai.com`\n\n## 🕹️ **How to Use:**\n\n1. **Import the Collection:** Import this collection into your Postman workspace to begin your Clarifai API integration journey.\n    \n2. **Authentication:** Add your Clarifai API key to the request headers to authenticate your requests.\n    \n3. **Explore Endpoints:** Familiarize yourself with the various endpoints to manage datasets, train models, and generate predictions."
    },
    {
      "name": "Workflows > Workflow Essentials",
      "description": "This curated set of API collections is designed to streamline your workflow, enhance collaboration, and boost productivity."
    },
    {
      "name": "Workflows > Workflow Metrics",
      "description": "The Workflow Metrics Collection serves as a comprehensive toolkit for efficiently monitoring and optimizing the performance of your workflows."
    },
    {
      "name": "Workflows > Workflow Versions",
      "description": "This curated set of API requests is designed to streamline and enhance your version control processes within your workflows."
    },
    {
      "name": "Walkthroughs",
      "description": "This collection provides a comprehensive set of step-by-step walkthroughs designed to guide you through various functionalities and features of the Clarifai Platform using a practical, hands-on approach."
    },
    {
      "name": "Walkthroughs > RAG",
      "description": "In the realm of text generation, Retrieval Augmented Generation (RAG) steps up the game for Large Language Models (LLMs) by fusing information retrieval capabilities with text generation skills, tackling key drawbacks of LLMs. When presented with a query, RAG fetches relevant information from an external knowledge base, which increases precision and contextual appropriateness through the integration of this retrieved data into the input. The Clarifai Python SDK allows you to create RAG-based applications with ease by reducing the number of steps in the process.\n\nClick [here](https://docs.clarifai.com/python-sdk/rag/) to learn more about RAG."
    }
  ],
  "paths": {
    "/v2/users/{user_id}/apps/{app_id}/annotations/find_duplicates/jobs": {
      "get": {
        "tags": [
          "Annotations > Find Duplicate Annotations Jobs"
        ],
        "summary": "List Find Duplicate Annotations Jobs",
        "description": "This endpoint provides a way to retrieve a list of jobs specifically designed to identify duplicate annotations within a given dataset.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |",
        "parameters": [
          {
            "name": "page",
            "in": "query",
            "schema": {
              "type": "integer"
            },
            "example": "1"
          },
          {
            "name": "per_page",
            "in": "query",
            "schema": {
              "type": "integer"
            },
            "example": "1000"
          },
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"35a7840daf5f5775348120339e4feebd\"\n    },\n    \"find_duplicate_annotations_jobs\": []\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/annotations/find_duplicates/jobs?page=1&per_page=1000' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      },
      "post": {
        "tags": [
          "Annotations > Find Duplicate Annotations Jobs"
        ],
        "summary": "Post Find Duplicate Annotations Job",
        "description": "This API endpoint allows you to initiate a job to find duplicate annotations within your dataset.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `find_duplicate_annotations_jobs` | **string** | **Stores the job** |",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "example": {
                  "find_duplicate_annotations_jobs": [
                    {
                      "pca_projection_comparator": {
                        "distance_threshold": 0.000001
                      }
                    }
                  ]
                }
              }
            }
          }
        },
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"bb8f93c73198da32bb63f72c687c9bf5\"\n    },\n    \"find_duplicate_annotations_jobs\": [\n        {\n            \"id\": \"f30849f4eb7b4c21b934ba21f5119ba7\",\n            \"pca_projection_comparator\": {\n                \"distance_threshold\": 0.000001\n            },\n            \"status\": {\n                \"code\": 64000,\n                \"description\": \"Job is queued to be ran.\"\n            }\n        }\n    ]\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/annotations/find_duplicates/jobs' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Content-Type: application/json' \\\n--data '{\n  \"find_duplicate_annotations_jobs\": [\n    {\n      \"pca_projection_comparator\": {\n        \"distance_threshold\": 0.000001\n      }\n    }\n  ]\n}'"
          }
        ]
      },
      "delete": {
        "tags": [
          "Annotations > Find Duplicate Annotations Jobs"
        ],
        "summary": "Delete Find Duplicate Annotations Jobs",
        "description": "This endpoint allows you to delete a specific Find Duplicate Annotations job based on the provided job ID.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `ids` | **string** | **Stores the job id's** |",
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"528bddffe0bd3ab60aee6b3511d065bb\"\n    }\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff --request DELETE 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/annotations/find_duplicates/jobs' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/annotations/find_duplicates/jobs/{find_duplicate_annotations_job_id}": {
      "get": {
        "tags": [
          "Annotations > Find Duplicate Annotations Jobs"
        ],
        "summary": "Get Find Duplicate Annotations Job",
        "description": "This endpoint provides a way to retrieve a list of jobs specifically designed to identify duplicate annotations within a given dataset.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `find_duplicate_annotations_job_id` | **string** | **Stores the job ID** |",
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "find_duplicate_annotations_job_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"a7990e4e72b4b68225dea4d21a520011\"\n    },\n    \"find_duplicate_annotations_job\": {\n        \"id\": \"f30849f4eb7b4c21b934ba21f5119ba7\",\n        \"pca_projection_comparator\": {\n            \"distance_threshold\": 0.000001\n        },\n        \"status\": {\n            \"code\": 64003,\n            \"description\": \"Unable to choose cluster model because cluster model version ID was not specified and there are 0 cluster models in the base workflow\"\n        }\n    }\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/annotations/find_duplicates/jobs/{find_duplicate_annotations_job_id}' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/annotation_filters": {
      "post": {
        "tags": [
          "Annotations > Annotation Filters"
        ],
        "summary": "Post Annotation Filters",
        "description": "This endpoint allows users to add annotation filters to the app.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `annotation_filters` | **list** | **Stores the filter ID** |",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "example": {
                  "annotation_filters": [
                    {
                      "id": "ann-filter-{{$timestamp}}"
                    }
                  ]
                }
              }
            }
          }
        },
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"0c227d67f1ecc011b6aaea50b6d35d41\"\n    },\n    \"annotation_filters\": [\n        {\n            \"id\": \"ann-filter-1701181768\",\n            \"created_at\": \"2023-11-28T14:29:28.313369455Z\",\n            \"modified_at\": \"2023-11-28T14:29:28.313369455Z\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"app_id\": \"test-app-1700638575-empty\"\n        }\n    ]\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/annotation_filters' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Content-Type: application/json' \\\n--data '{\n  \"annotation_filters\": [\n    {\n      \"id\": \"ann-filter-{{$timestamp}}\"\n    }\n  ]\n}'"
          }
        ]
      },
      "get": {
        "tags": [
          "Annotations > Annotation Filters"
        ],
        "summary": "List Annotation Filters",
        "description": "This endpoint allows you to retrieve a list of annotation filters.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `page` | **int** | **Page number for pagination** |\n| `per_page` | **int** | **Number of results per page** |",
        "parameters": [
          {
            "name": "page",
            "in": "query",
            "schema": {
              "type": "integer"
            },
            "example": "1"
          },
          {
            "name": "per_page",
            "in": "query",
            "schema": {
              "type": "integer"
            },
            "example": "100"
          },
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"37ca5f25be8eb845218c0e00759cb081\"\n    },\n    \"annotation_filters\": [\n        {\n            \"id\": \"ann-filter-1701181768\",\n            \"created_at\": \"2023-11-28T14:29:28.313369Z\",\n            \"modified_at\": \"2023-11-28T14:29:28.313369Z\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"app_id\": \"test-app-1700638575-empty\"\n        },\n        {\n            \"id\": \"dataset-1700752059-filter\",\n            \"created_at\": \"2023-11-23T15:07:38.946267Z\",\n            \"modified_at\": \"2023-11-23T15:07:38.946267Z\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"app_id\": \"test-app-1700638575-empty\"\n        },\n        {\n            \"id\": \"image-data-filter\",\n            \"created_at\": \"2023-11-23T07:14:17.188926Z\",\n            \"modified_at\": \"2023-11-23T07:14:17.188926Z\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"app_id\": \"test-app-1700638575-empty\"\n        }\n    ]\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/annotation_filters?page=1&per_page=100' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      },
      "delete": {
        "tags": [
          "Annotations > Annotation Filters"
        ],
        "summary": "Delete Annotation Filters",
        "description": "This endpoint allows you to remove specific annotation filters with ID.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `annotation_filters_ids` | **string** | **Stores the filter ID** |",
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"details\": \"Annotation filter 'ann-filter-1701181768' deleted\",\n        \"req_id\": \"c2d1e5ad2b0a39c68c1fda0e891e061d\"\n    }\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff --request DELETE 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/annotation_filters' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/annotation_filters/{annotation_filter_id}": {
      "get": {
        "tags": [
          "Annotations > Annotation Filters"
        ],
        "summary": "Get Annotation Filter",
        "description": "This endpoint allows you to retrieve a specific annotation filter with ID.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `anotation_filter_id` | **string** | **Stores the filter ID** |",
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "annotation_filter_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"421fb058a7d9c41a6aa9a1b7b8c50698\"\n    },\n    \"annotation_filter\": {\n        \"id\": \"ann-filter-1701181768\",\n        \"created_at\": \"2023-11-28T14:29:28.313369Z\",\n        \"modified_at\": \"2023-11-28T14:29:28.313369Z\",\n        \"user_id\": \"a0btrubbaefn\",\n        \"app_id\": \"test-app-1700638575-empty\"\n    }\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/annotation_filters/{annotation_filter_id}' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/inputs/{input_id}/annotations/{annotation_id}": {
      "get": {
        "tags": [
          "Annotations"
        ],
        "summary": "Get Annotation",
        "description": "This endpoint is used to retrieve a specific annotation with ID.\n\n## **Parameters**\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `input` | **string** | **Stores the input name** |\n| `annotation_id` | **string** | **Stores the annotation ID** |",
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "input_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "annotation_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"741959d0cd1935cbf892ab0d96c3ce76\"\n    },\n    \"annotation\": {\n        \"id\": \"89e09f2006734d528010178904b8b636\",\n        \"input_id\": \"ddf15aba61684e24902fb6127aa97e34\",\n        \"data\": {\n            \"concepts\": [\n                {\n                    \"id\": \"foo1\",\n                    \"name\": \"foo1\",\n                    \"value\": 1,\n                    \"app_id\": \"test-app-1700638575-empty\"\n                }\n            ]\n        },\n        \"user_id\": \"a0btrubbaefn\",\n        \"status\": {\n            \"code\": 24150,\n            \"description\": \"Annotation success\"\n        },\n        \"created_at\": \"2023-11-28T15:17:25.903311Z\",\n        \"modified_at\": \"2023-11-28T15:17:25.903311Z\",\n        \"trusted\": true\n    }\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/inputs/{input_id}/annotations/{annotation_id}' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/annotations": {
      "get": {
        "tags": [
          "Search > Basic Search Walkthrough"
        ],
        "summary": "List Annotations",
        "description": "This endpoint is used to display all available annotations in the app.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |",
        "parameters": [
          {
            "name": "list_all_annotations",
            "in": "query",
            "schema": {
              "type": "boolean"
            },
            "example": "true"
          },
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"15d82a3737a7f9423ad5c503322a68bb\"\n    },\n    \"annotations\": [\n        {\n            \"id\": \"2fd53625dc3549888916b98ad306f2ab\",\n            \"input_id\": \"7\",\n            \"data\": {\n                \"regions\": [\n                    {\n                        \"id\": \"20183edba158bfa273763aafc10e2d97\",\n                        \"region_info\": {\n                            \"bounding_box\": {\n                                \"top_row\": 0,\n                                \"left_col\": 0,\n                                \"bottom_row\": 0.5,\n                                \"right_col\": 0.6\n                            }\n                        },\n                        \"data\": {}\n                    }\n                ]\n            },\n            \"model_version_id\": \"cc2074cff6dc4c02b6f4e1b8606dcb54\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:37:13.708550Z\",\n            \"modified_at\": \"2023-12-06T12:37:13.708550Z\"\n        },\n        {\n            \"id\": \"409e5e773d3a448a895c49a47644039e\",\n            \"input_id\": \"7\",\n            \"data\": {\n                \"regions\": [\n                    {\n                        \"id\": \"361d6a9253be9152968012660258a4bf\",\n                        \"region_info\": {\n                            \"bounding_box\": {\n                                \"top_row\": 0,\n                                \"left_col\": 0,\n                                \"bottom_row\": 0.5,\n                                \"right_col\": 0.5\n                            }\n                        },\n                        \"data\": {}\n                    }\n                ]\n            },\n            \"model_version_id\": \"cc2074cff6dc4c02b6f4e1b8606dcb54\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:37:13.708549Z\",\n            \"modified_at\": \"2023-12-06T12:37:13.708549Z\"\n        },\n        {\n            \"id\": \"d6978a570001461c99805eb2b006571e\",\n            \"input_id\": \"6\",\n            \"data\": {\n                \"regions\": [\n                    {\n                        \"id\": \"361d6a9253be9152968012660258a4bf\",\n                        \"region_info\": {\n                            \"bounding_box\": {\n                                \"top_row\": 0,\n                                \"left_col\": 0,\n                                \"bottom_row\": 0.5,\n                                \"right_col\": 0.5\n                            }\n                        },\n                        \"data\": {}\n                    }\n                ]\n            },\n            \"model_version_id\": \"cc2074cff6dc4c02b6f4e1b8606dcb54\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:37:13.708548Z\",\n            \"modified_at\": \"2023-12-06T12:37:13.708548Z\"\n        },\n        {\n            \"id\": \"728d4a9d469d40e788c1eb3d5dd6ef69\",\n            \"input_id\": \"1\",\n            \"data\": {\n                \"regions\": [\n                    {\n                        \"id\": \"361d6a9253be9152968012660258a4bf\",\n                        \"region_info\": {\n                            \"bounding_box\": {\n                                \"top_row\": 0,\n                                \"left_col\": 0,\n                                \"bottom_row\": 0.5,\n                                \"right_col\": 0.5\n                            }\n                        },\n                        \"data\": {}\n                    }\n                ]\n            },\n            \"model_version_id\": \"cc2074cff6dc4c02b6f4e1b8606dcb54\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:37:13.708547Z\",\n            \"modified_at\": \"2023-12-06T12:37:13.708547Z\"\n        },\n        {\n            \"id\": \"0f5b3c56b9d1445f8229ccdce17e83f3\",\n            \"input_id\": \"2\",\n            \"data\": {\n                \"regions\": [\n                    {\n                        \"id\": \"361d6a9253be9152968012660258a4bf\",\n                        \"region_info\": {\n                            \"bounding_box\": {\n                                \"top_row\": 0,\n                                \"left_col\": 0,\n                                \"bottom_row\": 0.5,\n                                \"right_col\": 0.5\n                            }\n                        },\n                        \"data\": {}\n                    }\n                ]\n            },\n            \"model_version_id\": \"cc2074cff6dc4c02b6f4e1b8606dcb54\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:37:13.708546Z\",\n            \"modified_at\": \"2023-12-06T12:37:13.708546Z\"\n        },\n        {\n            \"id\": \"3a57f387c09745279777780b3fdaa925\",\n            \"input_id\": \"4\",\n            \"data\": {\n                \"regions\": [\n                    {\n                        \"id\": \"86e7d81525e21b12fcf709ba76f44df3\",\n                        \"region_info\": {\n                            \"bounding_box\": {\n                                \"top_row\": 0,\n                                \"left_col\": 0.1,\n                                \"bottom_row\": 0.5,\n                                \"right_col\": 0.5\n                            }\n                        },\n                        \"data\": {}\n                    }\n                ]\n            },\n            \"model_version_id\": \"cc2074cff6dc4c02b6f4e1b8606dcb54\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:37:13.708545Z\",\n            \"modified_at\": \"2023-12-06T12:37:13.708545Z\"\n        },\n        {\n            \"id\": \"cc0df53888e5497bb915ea4ae323829b\",\n            \"input_id\": \"2\",\n            \"data\": {\n                \"regions\": [\n                    {\n                        \"id\": \"4eae01d6e5c036bcc3a66f97ae594ea7\",\n                        \"region_info\": {\n                            \"bounding_box\": {\n                                \"top_row\": 0,\n                                \"left_col\": 0,\n                                \"bottom_row\": 0.4,\n                                \"right_col\": 0.5\n                            }\n                        },\n                        \"data\": {}\n                    }\n                ]\n            },\n            \"model_version_id\": \"cc2074cff6dc4c02b6f4e1b8606dcb54\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:37:13.708544Z\",\n            \"modified_at\": \"2023-12-06T12:37:13.708544Z\"\n        },\n        {\n            \"id\": \"b70410ef1d01488699de9a87d3c1e42f\",\n            \"input_id\": \"5\",\n            \"data\": {\n                \"regions\": [\n                    {\n                        \"id\": \"361d6a9253be9152968012660258a4bf\",\n                        \"region_info\": {\n                            \"bounding_box\": {\n                                \"top_row\": 0,\n                                \"left_col\": 0,\n                                \"bottom_row\": 0.5,\n                                \"right_col\": 0.5\n                            }\n                        },\n                        \"data\": {}\n                    }\n                ]\n            },\n            \"model_version_id\": \"cc2074cff6dc4c02b6f4e1b8606dcb54\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:37:13.708543Z\",\n            \"modified_at\": \"2023-12-06T12:37:13.708543Z\"\n        },\n        {\n            \"id\": \"955341f2bb9142f4a221cec35210d90c\",\n            \"input_id\": \"3\",\n            \"data\": {\n                \"regions\": [\n                    {\n                        \"id\": \"361d6a9253be9152968012660258a4bf\",\n                        \"region_info\": {\n                            \"bounding_box\": {\n                                \"top_row\": 0,\n                                \"left_col\": 0,\n                                \"bottom_row\": 0.5,\n                                \"right_col\": 0.5\n                            }\n                        },\n                        \"data\": {}\n                    }\n                ]\n            },\n            \"model_version_id\": \"cc2074cff6dc4c02b6f4e1b8606dcb54\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:37:13.708542Z\",\n            \"modified_at\": \"2023-12-06T12:37:13.708542Z\"\n        },\n        {\n            \"id\": \"99a7a661a53a437080d7ba27e4597ea4\",\n            \"input_id\": \"4\",\n            \"data\": {\n                \"regions\": [\n                    {\n                        \"id\": \"361d6a9253be9152968012660258a4bf\",\n                        \"region_info\": {\n                            \"bounding_box\": {\n                                \"top_row\": 0,\n                                \"left_col\": 0,\n                                \"bottom_row\": 0.5,\n                                \"right_col\": 0.5\n                            }\n                        },\n                        \"data\": {}\n                    }\n                ]\n            },\n            \"model_version_id\": \"cc2074cff6dc4c02b6f4e1b8606dcb54\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:37:13.708541Z\",\n            \"modified_at\": \"2023-12-06T12:37:13.708541Z\"\n        },\n        {\n            \"id\": \"af0d165e85e24ac8a80f5580a8c9c7d9\",\n            \"input_id\": \"7\",\n            \"data\": {\n                \"regions\": [\n                    {\n                        \"id\": \"20183edba158bfa273763aafc10e2d97\",\n                        \"region_info\": {\n                            \"bounding_box\": {\n                                \"top_row\": 0,\n                                \"left_col\": 0,\n                                \"bottom_row\": 0.5,\n                                \"right_col\": 0.6\n                            }\n                        },\n                        \"data\": {}\n                    }\n                ]\n            },\n            \"model_version_id\": \"aa7f35c01e0642fda5cf400f543e7c40\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:37:13.484421Z\",\n            \"modified_at\": \"2023-12-06T12:37:13.484421Z\"\n        },\n        {\n            \"id\": \"80c663b9e6d74a98b345a3a4a07cae69\",\n            \"input_id\": \"7\",\n            \"data\": {\n                \"regions\": [\n                    {\n                        \"id\": \"361d6a9253be9152968012660258a4bf\",\n                        \"region_info\": {\n                            \"bounding_box\": {\n                                \"top_row\": 0,\n                                \"left_col\": 0,\n                                \"bottom_row\": 0.5,\n                                \"right_col\": 0.5\n                            }\n                        },\n                        \"data\": {}\n                    }\n                ]\n            },\n            \"model_version_id\": \"aa7f35c01e0642fda5cf400f543e7c40\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:37:13.484420Z\",\n            \"modified_at\": \"2023-12-06T12:37:13.484420Z\"\n        },\n        {\n            \"id\": \"fddd3d2729d24aa8b25d91f5ba5e120d\",\n            \"input_id\": \"6\",\n            \"data\": {\n                \"regions\": [\n                    {\n                        \"id\": \"361d6a9253be9152968012660258a4bf\",\n                        \"region_info\": {\n                            \"bounding_box\": {\n                                \"top_row\": 0,\n                                \"left_col\": 0,\n                                \"bottom_row\": 0.5,\n                                \"right_col\": 0.5\n                            }\n                        },\n                        \"data\": {}\n                    }\n                ]\n            },\n            \"model_version_id\": \"aa7f35c01e0642fda5cf400f543e7c40\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:37:13.484419Z\",\n            \"modified_at\": \"2023-12-06T12:37:13.484419Z\"\n        },\n        {\n            \"id\": \"ff3adb416cab4e608e1b83530103eff7\",\n            \"input_id\": \"1\",\n            \"data\": {\n                \"regions\": [\n                    {\n                        \"id\": \"361d6a9253be9152968012660258a4bf\",\n                        \"region_info\": {\n                            \"bounding_box\": {\n                                \"top_row\": 0,\n                                \"left_col\": 0,\n                                \"bottom_row\": 0.5,\n                                \"right_col\": 0.5\n                            }\n                        },\n                        \"data\": {}\n                    }\n                ]\n            },\n            \"model_version_id\": \"aa7f35c01e0642fda5cf400f543e7c40\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:37:13.484418Z\",\n            \"modified_at\": \"2023-12-06T12:37:13.484418Z\"\n        },\n        {\n            \"id\": \"5280eb9844224ab38a1d6b75f8a28632\",\n            \"input_id\": \"2\",\n            \"data\": {\n                \"regions\": [\n                    {\n                        \"id\": \"361d6a9253be9152968012660258a4bf\",\n                        \"region_info\": {\n                            \"bounding_box\": {\n                                \"top_row\": 0,\n                                \"left_col\": 0,\n                                \"bottom_row\": 0.5,\n                                \"right_col\": 0.5\n                            }\n                        },\n                        \"data\": {}\n                    }\n                ]\n            },\n            \"model_version_id\": \"aa7f35c01e0642fda5cf400f543e7c40\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:37:13.484417Z\",\n            \"modified_at\": \"2023-12-06T12:37:13.484417Z\"\n        },\n        {\n            \"id\": \"4e16164d58474ca6ad2af99f88702a79\",\n            \"input_id\": \"4\",\n            \"data\": {\n                \"regions\": [\n                    {\n                        \"id\": \"86e7d81525e21b12fcf709ba76f44df3\",\n                        \"region_info\": {\n                            \"bounding_box\": {\n                                \"top_row\": 0,\n                                \"left_col\": 0.1,\n                                \"bottom_row\": 0.5,\n                                \"right_col\": 0.5\n                            }\n                        },\n                        \"data\": {}\n                    }\n                ]\n            },\n            \"model_version_id\": \"aa7f35c01e0642fda5cf400f543e7c40\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:37:13.484416Z\",\n            \"modified_at\": \"2023-12-06T12:37:13.484416Z\"\n        },\n        {\n            \"id\": \"793fda5d355d47f2bb6e93b8582e45ab\",\n            \"input_id\": \"2\",\n            \"data\": {\n                \"regions\": [\n                    {\n                        \"id\": \"4eae01d6e5c036bcc3a66f97ae594ea7\",\n                        \"region_info\": {\n                            \"bounding_box\": {\n                                \"top_row\": 0,\n                                \"left_col\": 0,\n                                \"bottom_row\": 0.4,\n                                \"right_col\": 0.5\n                            }\n                        },\n                        \"data\": {}\n                    }\n                ]\n            },\n            \"model_version_id\": \"aa7f35c01e0642fda5cf400f543e7c40\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:37:13.484415Z\",\n            \"modified_at\": \"2023-12-06T12:37:13.484415Z\"\n        },\n        {\n            \"id\": \"61ecaae984e24ea49ed5e1d70e0e714d\",\n            \"input_id\": \"5\",\n            \"data\": {\n                \"regions\": [\n                    {\n                        \"id\": \"361d6a9253be9152968012660258a4bf\",\n                        \"region_info\": {\n                            \"bounding_box\": {\n                                \"top_row\": 0,\n                                \"left_col\": 0,\n                                \"bottom_row\": 0.5,\n                                \"right_col\": 0.5\n                            }\n                        },\n                        \"data\": {}\n                    }\n                ]\n            },\n            \"model_version_id\": \"aa7f35c01e0642fda5cf400f543e7c40\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:37:13.484414Z\",\n            \"modified_at\": \"2023-12-06T12:37:13.484414Z\"\n        },\n        {\n            \"id\": \"6d1d088e54f54da7908d7743945ecee6\",\n            \"input_id\": \"3\",\n            \"data\": {\n                \"regions\": [\n                    {\n                        \"id\": \"361d6a9253be9152968012660258a4bf\",\n                        \"region_info\": {\n                            \"bounding_box\": {\n                                \"top_row\": 0,\n                                \"left_col\": 0,\n                                \"bottom_row\": 0.5,\n                                \"right_col\": 0.5\n                            }\n                        },\n                        \"data\": {}\n                    }\n                ]\n            },\n            \"model_version_id\": \"aa7f35c01e0642fda5cf400f543e7c40\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:37:13.484413Z\",\n            \"modified_at\": \"2023-12-06T12:37:13.484413Z\"\n        },\n        {\n            \"id\": \"0a60b856cf6e43bbba6430d247f2e593\",\n            \"input_id\": \"4\",\n            \"data\": {\n                \"regions\": [\n                    {\n                        \"id\": \"361d6a9253be9152968012660258a4bf\",\n                        \"region_info\": {\n                            \"bounding_box\": {\n                                \"top_row\": 0,\n                                \"left_col\": 0,\n                                \"bottom_row\": 0.5,\n                                \"right_col\": 0.5\n                            }\n                        },\n                        \"data\": {}\n                    }\n                ]\n            },\n            \"model_version_id\": \"aa7f35c01e0642fda5cf400f543e7c40\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:37:13.484412Z\",\n            \"modified_at\": \"2023-12-06T12:37:13.484412Z\"\n        },\n        {\n            \"id\": \"a0df351d28514d18a30364fd64a7dfc6\",\n            \"input_id\": \"7\",\n            \"data\": {\n                \"regions\": [\n                    {\n                        \"id\": \"20183edba158bfa273763aafc10e2d97\",\n                        \"region_info\": {\n                            \"bounding_box\": {\n                                \"top_row\": 0,\n                                \"left_col\": 0,\n                                \"bottom_row\": 0.5,\n                                \"right_col\": 0.6\n                            }\n                        },\n                        \"data\": {}\n                    }\n                ]\n            },\n            \"model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:37:13.380259Z\",\n            \"modified_at\": \"2023-12-06T12:37:13.380259Z\"\n        },\n        {\n            \"id\": \"e2ccd9ba6cfa4bf083e29c233f62ad56\",\n            \"input_id\": \"7\",\n            \"data\": {\n                \"regions\": [\n                    {\n                        \"id\": \"361d6a9253be9152968012660258a4bf\",\n                        \"region_info\": {\n                            \"bounding_box\": {\n                                \"top_row\": 0,\n                                \"left_col\": 0,\n                                \"bottom_row\": 0.5,\n                                \"right_col\": 0.5\n                            }\n                        },\n                        \"data\": {}\n                    }\n                ]\n            },\n            \"model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:37:13.380258Z\",\n            \"modified_at\": \"2023-12-06T12:37:13.380258Z\"\n        },\n        {\n            \"id\": \"a565e1d7d82f4b28b249fe24d4f2f5bb\",\n            \"input_id\": \"6\",\n            \"data\": {\n                \"regions\": [\n                    {\n                        \"id\": \"361d6a9253be9152968012660258a4bf\",\n                        \"region_info\": {\n                            \"bounding_box\": {\n                                \"top_row\": 0,\n                                \"left_col\": 0,\n                                \"bottom_row\": 0.5,\n                                \"right_col\": 0.5\n                            }\n                        },\n                        \"data\": {}\n                    }\n                ]\n            },\n            \"model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:37:13.380257Z\",\n            \"modified_at\": \"2023-12-06T12:37:13.380257Z\"\n        },\n        {\n            \"id\": \"8989af69d5c8477c93c9b47a5d603532\",\n            \"input_id\": \"1\",\n            \"data\": {\n                \"regions\": [\n                    {\n                        \"id\": \"361d6a9253be9152968012660258a4bf\",\n                        \"region_info\": {\n                            \"bounding_box\": {\n                                \"top_row\": 0,\n                                \"left_col\": 0,\n                                \"bottom_row\": 0.5,\n                                \"right_col\": 0.5\n                            }\n                        },\n                        \"data\": {}\n                    }\n                ]\n            },\n            \"model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:37:13.380256Z\",\n            \"modified_at\": \"2023-12-06T12:37:13.380256Z\"\n        },\n        {\n            \"id\": \"4fd9ad250be04ec39e5650085d19fdd1\",\n            \"input_id\": \"2\",\n            \"data\": {\n                \"regions\": [\n                    {\n                        \"id\": \"361d6a9253be9152968012660258a4bf\",\n                        \"region_info\": {\n                            \"bounding_box\": {\n                                \"top_row\": 0,\n                                \"left_col\": 0,\n                                \"bottom_row\": 0.5,\n                                \"right_col\": 0.5\n                            }\n                        },\n                        \"data\": {}\n                    }\n                ]\n            },\n            \"model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:37:13.380255Z\",\n            \"modified_at\": \"2023-12-06T12:37:13.380255Z\"\n        },\n        {\n            \"id\": \"c7bf0021a31c46f8826e5ad58d59cf01\",\n            \"input_id\": \"4\",\n            \"data\": {\n                \"regions\": [\n                    {\n                        \"id\": \"86e7d81525e21b12fcf709ba76f44df3\",\n                        \"region_info\": {\n                            \"bounding_box\": {\n                                \"top_row\": 0,\n                                \"left_col\": 0.1,\n                                \"bottom_row\": 0.5,\n                                \"right_col\": 0.5\n                            }\n                        },\n                        \"data\": {}\n                    }\n                ]\n            },\n            \"model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:37:13.380254Z\",\n            \"modified_at\": \"2023-12-06T12:37:13.380254Z\"\n        },\n        {\n            \"id\": \"4d25a0a16ac44bcc8f36cff2aef968f4\",\n            \"input_id\": \"2\",\n            \"data\": {\n                \"regions\": [\n                    {\n                        \"id\": \"4eae01d6e5c036bcc3a66f97ae594ea7\",\n                        \"region_info\": {\n                            \"bounding_box\": {\n                                \"top_row\": 0,\n                                \"left_col\": 0,\n                                \"bottom_row\": 0.4,\n                                \"right_col\": 0.5\n                            }\n                        },\n                        \"data\": {}\n                    }\n                ]\n            },\n            \"model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:37:13.380253Z\",\n            \"modified_at\": \"2023-12-06T12:37:13.380253Z\"\n        },\n        {\n            \"id\": \"769b649117ce4ef0a559902779f18a83\",\n            \"input_id\": \"5\",\n            \"data\": {\n                \"regions\": [\n                    {\n                        \"id\": \"361d6a9253be9152968012660258a4bf\",\n                        \"region_info\": {\n                            \"bounding_box\": {\n                                \"top_row\": 0,\n                                \"left_col\": 0,\n                                \"bottom_row\": 0.5,\n                                \"right_col\": 0.5\n                            }\n                        },\n                        \"data\": {}\n                    }\n                ]\n            },\n            \"model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:37:13.380252Z\",\n            \"modified_at\": \"2023-12-06T12:37:13.380252Z\"\n        },\n        {\n            \"id\": \"faffbe2c24434521aed4563392b8dce6\",\n            \"input_id\": \"3\",\n            \"data\": {\n                \"regions\": [\n                    {\n                        \"id\": \"361d6a9253be9152968012660258a4bf\",\n                        \"region_info\": {\n                            \"bounding_box\": {\n                                \"top_row\": 0,\n                                \"left_col\": 0,\n                                \"bottom_row\": 0.5,\n                                \"right_col\": 0.5\n                            }\n                        },\n                        \"data\": {}\n                    }\n                ]\n            },\n            \"model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:37:13.380251Z\",\n            \"modified_at\": \"2023-12-06T12:37:13.380251Z\"\n        },\n        {\n            \"id\": \"50bf035fd76c4b5eae934cf4498232a7\",\n            \"input_id\": \"4\",\n            \"data\": {\n                \"regions\": [\n                    {\n                        \"id\": \"361d6a9253be9152968012660258a4bf\",\n                        \"region_info\": {\n                            \"bounding_box\": {\n                                \"top_row\": 0,\n                                \"left_col\": 0,\n                                \"bottom_row\": 0.5,\n                                \"right_col\": 0.5\n                            }\n                        },\n                        \"data\": {}\n                    }\n                ]\n            },\n            \"model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:37:13.380250Z\",\n            \"modified_at\": \"2023-12-06T12:37:13.380250Z\"\n        },\n        {\n            \"id\": \"29cb6ebe2072490dac92d69bea9767dc\",\n            \"input_id\": \"1\",\n            \"data\": {\n                \"regions\": [\n                    {\n                        \"id\": \"361d6a9253be9152968012660258a4bf\",\n                        \"region_info\": {\n                            \"bounding_box\": {\n                                \"top_row\": 0,\n                                \"left_col\": 0,\n                                \"bottom_row\": 0.5,\n                                \"right_col\": 0.5\n                            }\n                        },\n                        \"data\": {\n                            \"concepts\": [\n                                {\n                                    \"id\": \"test-concept\",\n                                    \"name\": \"test-concept\",\n                                    \"value\": 1,\n                                    \"app_id\": \"test-app-1701866015\"\n                                }\n                            ]\n                        }\n                    }\n                ]\n            },\n            \"annotation_info\": {\n                \"set\": \"set_to_eval\"\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:37:11.124811Z\",\n            \"modified_at\": \"2023-12-06T12:37:13.804347Z\",\n            \"trusted\": true\n        },\n        {\n            \"id\": \"5c8cd9d38cae44fa97d3d6eae5a58dac\",\n            \"input_id\": \"2\",\n            \"data\": {\n                \"regions\": [\n                    {\n                        \"id\": \"4eae01d6e5c036bcc3a66f97ae594ea7\",\n                        \"region_info\": {\n                            \"bounding_box\": {\n                                \"top_row\": 0,\n                                \"left_col\": 0,\n                                \"bottom_row\": 0.4,\n                                \"right_col\": 0.5\n                            }\n                        },\n                        \"data\": {\n                            \"concepts\": [\n                                {\n                                    \"id\": \"test-concept\",\n                                    \"name\": \"test-concept\",\n                                    \"value\": 1,\n                                    \"app_id\": \"test-app-1701866015\"\n                                }\n                            ]\n                        }\n                    }\n                ]\n            },\n            \"annotation_info\": {\n                \"set\": \"ground_truth\"\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:37:11.124811Z\",\n            \"modified_at\": \"2023-12-06T12:37:13.804347Z\",\n            \"trusted\": true\n        },\n        {\n            \"id\": \"61c66cbba6e348689bee00c170a18c87\",\n            \"input_id\": \"2\",\n            \"data\": {\n                \"regions\": [\n                    {\n                        \"id\": \"361d6a9253be9152968012660258a4bf\",\n                        \"region_info\": {\n                            \"bounding_box\": {\n                                \"top_row\": 0,\n                                \"left_col\": 0,\n                                \"bottom_row\": 0.5,\n                                \"right_col\": 0.5\n                            }\n                        },\n                        \"data\": {\n                            \"concepts\": [\n                                {\n                                    \"id\": \"test-concept\",\n                                    \"name\": \"test-concept\",\n                                    \"value\": 1,\n                                    \"app_id\": \"test-app-1701866015\"\n                                }\n                            ]\n                        }\n                    }\n                ]\n            },\n            \"annotation_info\": {\n                \"set\": \"set_to_eval\"\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:37:11.124811Z\",\n            \"modified_at\": \"2023-12-06T12:37:13.804347Z\",\n            \"trusted\": true\n        },\n        {\n            \"id\": \"679b11005e204ba6adb721d8632df153\",\n            \"input_id\": \"1\",\n            \"data\": {\n                \"regions\": [\n                    {\n                        \"id\": \"361d6a9253be9152968012660258a4bf\",\n                        \"region_info\": {\n                            \"bounding_box\": {\n                                \"top_row\": 0,\n                                \"left_col\": 0,\n                                \"bottom_row\": 0.5,\n                                \"right_col\": 0.5\n                            }\n                        },\n                        \"data\": {\n                            \"concepts\": [\n                                {\n                                    \"id\": \"test-concept\",\n                                    \"name\": \"test-concept\",\n                                    \"value\": 1,\n                                    \"app_id\": \"test-app-1701866015\"\n                                }\n                            ]\n                        }\n                    }\n                ]\n            },\n            \"annotation_info\": {\n                \"set\": \"ground_truth\"\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:37:11.124811Z\",\n            \"modified_at\": \"2023-12-06T12:37:13.804347Z\",\n            \"trusted\": true\n        },\n        {\n            \"id\": \"7c94dfc5a91d498db2b084d19e6b0bcb\",\n            \"input_id\": \"4\",\n            \"data\": {\n                \"regions\": [\n                    {\n                        \"id\": \"361d6a9253be9152968012660258a4bf\",\n                        \"region_info\": {\n                            \"bounding_box\": {\n                                \"top_row\": 0,\n                                \"left_col\": 0,\n                                \"bottom_row\": 0.5,\n                                \"right_col\": 0.5\n                            }\n                        },\n                        \"data\": {\n                            \"concepts\": [\n                                {\n                                    \"id\": \"test-concept-1\",\n                                    \"name\": \"test-concept-1\",\n                                    \"value\": 1,\n                                    \"app_id\": \"test-app-1701866015\"\n                                }\n                            ]\n                        }\n                    }\n                ]\n            },\n            \"annotation_info\": {\n                \"set\": \"ground_truth\"\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:37:11.124811Z\",\n            \"modified_at\": \"2023-12-06T12:37:13.804347Z\",\n            \"trusted\": true\n        },\n        {\n            \"id\": \"9d18e2b02e2642739f4539b71a6bf0f1\",\n            \"input_id\": \"4\",\n            \"data\": {\n                \"regions\": [\n                    {\n                        \"id\": \"86e7d81525e21b12fcf709ba76f44df3\",\n                        \"region_info\": {\n                            \"bounding_box\": {\n                                \"top_row\": 0,\n                                \"left_col\": 0.1,\n                                \"bottom_row\": 0.5,\n                                \"right_col\": 0.5\n                            }\n                        },\n                        \"data\": {\n                            \"concepts\": [\n                                {\n                                    \"id\": \"test-concept-1\",\n                                    \"name\": \"test-concept-1\",\n                                    \"value\": 1,\n                                    \"app_id\": \"test-app-1701866015\"\n                                }\n                            ]\n                        }\n                    }\n                ]\n            },\n            \"annotation_info\": {\n                \"set\": \"set_to_eval\"\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:37:11.124811Z\",\n            \"modified_at\": \"2023-12-06T12:37:13.804347Z\",\n            \"trusted\": true\n        },\n        {\n            \"id\": \"b90359079d704125b1c3da1f53ee733d\",\n            \"input_id\": \"3\",\n            \"data\": {\n                \"regions\": [\n                    {\n                        \"id\": \"361d6a9253be9152968012660258a4bf\",\n                        \"region_info\": {\n                            \"bounding_box\": {\n                                \"top_row\": 0,\n                                \"left_col\": 0,\n                                \"bottom_row\": 0.5,\n                                \"right_col\": 0.5\n                            }\n                        },\n                        \"data\": {\n                            \"concepts\": [\n                                {\n                                    \"id\": \"test-concept-1\",\n                                    \"name\": \"test-concept-1\",\n                                    \"value\": 1,\n                                    \"app_id\": \"test-app-1701866015\"\n                                }\n                            ]\n                        }\n                    }\n                ]\n            },\n            \"annotation_info\": {\n                \"set\": \"set_to_eval\"\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:37:11.124811Z\",\n            \"modified_at\": \"2023-12-06T12:37:13.804347Z\",\n            \"trusted\": true\n        },\n        {\n            \"id\": \"c2ca85db56124b959418b3858da4364a\",\n            \"input_id\": \"6\",\n            \"data\": {\n                \"regions\": [\n                    {\n                        \"id\": \"361d6a9253be9152968012660258a4bf\",\n                        \"region_info\": {\n                            \"bounding_box\": {\n                                \"top_row\": 0,\n                                \"left_col\": 0,\n                                \"bottom_row\": 0.5,\n                                \"right_col\": 0.5\n                            }\n                        },\n                        \"data\": {\n                            \"concepts\": [\n                                {\n                                    \"id\": \"test-concept\",\n                                    \"name\": \"test-concept\",\n                                    \"value\": 1,\n                                    \"app_id\": \"test-app-1701866015\"\n                                }\n                            ]\n                        }\n                    }\n                ]\n            },\n            \"annotation_info\": {\n                \"set\": \"set_to_eval\"\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:37:11.124811Z\",\n            \"modified_at\": \"2023-12-06T12:37:13.804347Z\",\n            \"trusted\": true\n        },\n        {\n            \"id\": \"d7ce3eac2a944a918b9b3e76d2fabd97\",\n            \"input_id\": \"3\",\n            \"data\": {\n                \"regions\": [\n                    {\n                        \"id\": \"361d6a9253be9152968012660258a4bf\",\n                        \"region_info\": {\n                            \"bounding_box\": {\n                                \"top_row\": 0,\n                                \"left_col\": 0,\n                                \"bottom_row\": 0.5,\n                                \"right_col\": 0.5\n                            }\n                        },\n                        \"data\": {\n                            \"concepts\": [\n                                {\n                                    \"id\": \"test-concept\",\n                                    \"name\": \"test-concept\",\n                                    \"value\": 1,\n                                    \"app_id\": \"test-app-1701866015\"\n                                }\n                            ]\n                        }\n                    }\n                ]\n            },\n            \"annotation_info\": {\n                \"set\": \"ground_truth\"\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:37:11.124811Z\",\n            \"modified_at\": \"2023-12-06T12:37:13.804347Z\",\n            \"trusted\": true\n        },\n        {\n            \"id\": \"e0c81a88c5ee4fd7afa8f0f56a000786\",\n            \"input_id\": \"7\",\n            \"data\": {\n                \"regions\": [\n                    {\n                        \"id\": \"20183edba158bfa273763aafc10e2d97\",\n                        \"region_info\": {\n                            \"bounding_box\": {\n                                \"top_row\": 0,\n                                \"left_col\": 0,\n                                \"bottom_row\": 0.5,\n                                \"right_col\": 0.6\n                            }\n                        },\n                        \"data\": {\n                            \"concepts\": [\n                                {\n                                    \"id\": \"test-concept\",\n                                    \"name\": \"test-concept\",\n                                    \"value\": 1,\n                                    \"app_id\": \"test-app-1701866015\"\n                                }\n                            ]\n                        }\n                    }\n                ]\n            },\n            \"annotation_info\": {\n                \"set\": \"set_to_eval\"\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:37:11.124811Z\",\n            \"modified_at\": \"2023-12-06T12:37:13.804347Z\",\n            \"trusted\": true\n        },\n        {\n            \"id\": \"eb96f561d50044f0a8bb13220b966332\",\n            \"input_id\": \"6\",\n            \"data\": {\n                \"regions\": [\n                    {\n                        \"id\": \"361d6a9253be9152968012660258a4bf\",\n                        \"region_info\": {\n                            \"bounding_box\": {\n                                \"top_row\": 0,\n                                \"left_col\": 0,\n                                \"bottom_row\": 0.5,\n                                \"right_col\": 0.5\n                            }\n                        },\n                        \"data\": {\n                            \"concepts\": [\n                                {\n                                    \"id\": \"test-concept\",\n                                    \"name\": \"test-concept\",\n                                    \"value\": 1,\n                                    \"app_id\": \"test-app-1701866015\"\n                                }\n                            ]\n                        }\n                    }\n                ]\n            },\n            \"annotation_info\": {\n                \"set\": \"ground_truth\"\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:37:11.124811Z\",\n            \"modified_at\": \"2023-12-06T12:37:13.804347Z\",\n            \"trusted\": true\n        },\n        {\n            \"id\": \"eeceae3d06704582a7f3ecb5e83af997\",\n            \"input_id\": \"5\",\n            \"data\": {\n                \"regions\": [\n                    {\n                        \"id\": \"361d6a9253be9152968012660258a4bf\",\n                        \"region_info\": {\n                            \"bounding_box\": {\n                                \"top_row\": 0,\n                                \"left_col\": 0,\n                                \"bottom_row\": 0.5,\n                                \"right_col\": 0.5\n                            }\n                        },\n                        \"data\": {\n                            \"concepts\": [\n                                {\n                                    \"id\": \"test-concept\",\n                                    \"name\": \"test-concept\",\n                                    \"value\": 1,\n                                    \"app_id\": \"test-app-1701866015\"\n                                }\n                            ]\n                        }\n                    }\n                ]\n            },\n            \"annotation_info\": {\n                \"set\": \"set_to_eval\"\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:37:11.124811Z\",\n            \"modified_at\": \"2023-12-06T12:37:13.804347Z\",\n            \"trusted\": true\n        },\n        {\n            \"id\": \"fb1377dd16fc4ee2ba1078d9ed0ab4a2\",\n            \"input_id\": \"7\",\n            \"data\": {\n                \"regions\": [\n                    {\n                        \"id\": \"361d6a9253be9152968012660258a4bf\",\n                        \"region_info\": {\n                            \"bounding_box\": {\n                                \"top_row\": 0,\n                                \"left_col\": 0,\n                                \"bottom_row\": 0.5,\n                                \"right_col\": 0.5\n                            }\n                        },\n                        \"data\": {\n                            \"concepts\": [\n                                {\n                                    \"id\": \"test-concept-1\",\n                                    \"name\": \"test-concept-1\",\n                                    \"value\": 1,\n                                    \"app_id\": \"test-app-1701866015\"\n                                }\n                            ]\n                        }\n                    }\n                ]\n            },\n            \"annotation_info\": {\n                \"set\": \"ground_truth\"\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:37:11.124811Z\",\n            \"modified_at\": \"2023-12-06T12:37:13.804347Z\",\n            \"trusted\": true\n        },\n        {\n            \"id\": \"8546d232888f4e36b68bdb07b9ca7b0f\",\n            \"input_id\": \"5\",\n            \"data\": {},\n            \"model_version_id\": \"cc2074cff6dc4c02b6f4e1b8606dcb54\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:34:50.423426Z\",\n            \"modified_at\": \"2023-12-06T12:34:50.423426Z\"\n        },\n        {\n            \"id\": \"d97dfb5d6bf34df192b099633ae4fb1a\",\n            \"input_id\": \"2\",\n            \"data\": {},\n            \"model_version_id\": \"cc2074cff6dc4c02b6f4e1b8606dcb54\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:34:50.423425Z\",\n            \"modified_at\": \"2023-12-06T12:34:50.423425Z\"\n        },\n        {\n            \"id\": \"2a3b5d7b92134df89217f2dff270e3c9\",\n            \"input_id\": \"4\",\n            \"data\": {},\n            \"model_version_id\": \"cc2074cff6dc4c02b6f4e1b8606dcb54\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:34:50.423424Z\",\n            \"modified_at\": \"2023-12-06T12:34:50.423424Z\"\n        },\n        {\n            \"id\": \"ce2220041e184d0293077bcf8e43cfda\",\n            \"input_id\": \"6\",\n            \"data\": {},\n            \"model_version_id\": \"cc2074cff6dc4c02b6f4e1b8606dcb54\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:34:50.423423Z\",\n            \"modified_at\": \"2023-12-06T12:34:50.423423Z\"\n        },\n        {\n            \"id\": \"721e0061966f48898f82d72d7985a01d\",\n            \"input_id\": \"3\",\n            \"data\": {},\n            \"model_version_id\": \"cc2074cff6dc4c02b6f4e1b8606dcb54\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:34:50.423422Z\",\n            \"modified_at\": \"2023-12-06T12:34:50.423422Z\"\n        },\n        {\n            \"id\": \"841afe05288f45f79e6d4a1902b9d509\",\n            \"input_id\": \"1\",\n            \"data\": {},\n            \"model_version_id\": \"cc2074cff6dc4c02b6f4e1b8606dcb54\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:34:50.423421Z\",\n            \"modified_at\": \"2023-12-06T12:34:50.423421Z\"\n        },\n        {\n            \"id\": \"bdc46deacafb4a1a9a8a286eba0b51e9\",\n            \"input_id\": \"7\",\n            \"data\": {},\n            \"model_version_id\": \"cc2074cff6dc4c02b6f4e1b8606dcb54\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:34:50.423420Z\",\n            \"modified_at\": \"2023-12-06T12:34:50.423420Z\"\n        },\n        {\n            \"id\": \"124c425504b648d486b47b15f197f87b\",\n            \"input_id\": \"5\",\n            \"data\": {},\n            \"model_version_id\": \"aa7f35c01e0642fda5cf400f543e7c40\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:34:50.236175Z\",\n            \"modified_at\": \"2023-12-06T12:34:50.236175Z\"\n        },\n        {\n            \"id\": \"fc9dce0ec2684053837bcd561fe61da9\",\n            \"input_id\": \"2\",\n            \"data\": {},\n            \"model_version_id\": \"aa7f35c01e0642fda5cf400f543e7c40\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:34:50.236174Z\",\n            \"modified_at\": \"2023-12-06T12:34:50.236174Z\"\n        },\n        {\n            \"id\": \"efcaead9d76141c3861ab2f24f041ea8\",\n            \"input_id\": \"4\",\n            \"data\": {},\n            \"model_version_id\": \"aa7f35c01e0642fda5cf400f543e7c40\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:34:50.236173Z\",\n            \"modified_at\": \"2023-12-06T12:34:50.236173Z\"\n        },\n        {\n            \"id\": \"351aff48aa4b4617bb0fb8abfc643f6c\",\n            \"input_id\": \"6\",\n            \"data\": {},\n            \"model_version_id\": \"aa7f35c01e0642fda5cf400f543e7c40\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:34:50.236172Z\",\n            \"modified_at\": \"2023-12-06T12:34:50.236172Z\"\n        },\n        {\n            \"id\": \"c11faef4455d47d5a3e01d1d9aad9d0f\",\n            \"input_id\": \"3\",\n            \"data\": {},\n            \"model_version_id\": \"aa7f35c01e0642fda5cf400f543e7c40\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:34:50.236171Z\",\n            \"modified_at\": \"2023-12-06T12:34:50.236171Z\"\n        },\n        {\n            \"id\": \"d9f58af60adb403f9258556cd4c93daa\",\n            \"input_id\": \"1\",\n            \"data\": {},\n            \"model_version_id\": \"aa7f35c01e0642fda5cf400f543e7c40\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:34:50.236170Z\",\n            \"modified_at\": \"2023-12-06T12:34:50.236170Z\"\n        },\n        {\n            \"id\": \"eb16c22db1854895ae01f7ba9845bb7f\",\n            \"input_id\": \"7\",\n            \"data\": {},\n            \"model_version_id\": \"aa7f35c01e0642fda5cf400f543e7c40\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:34:50.236169Z\",\n            \"modified_at\": \"2023-12-06T12:34:50.236169Z\"\n        },\n        {\n            \"id\": \"83bddbfd4a0d452cb348301825bc7415\",\n            \"input_id\": \"5\",\n            \"data\": {},\n            \"model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:34:50.040388Z\",\n            \"modified_at\": \"2023-12-06T12:34:50.040388Z\"\n        },\n        {\n            \"id\": \"09d485cb8c5b43739929ed7d053bb285\",\n            \"input_id\": \"2\",\n            \"data\": {},\n            \"model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:34:50.040387Z\",\n            \"modified_at\": \"2023-12-06T12:34:50.040387Z\"\n        },\n        {\n            \"id\": \"81b4d78f3c8d4b8ebad62a7c82ae23f9\",\n            \"input_id\": \"4\",\n            \"data\": {},\n            \"model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:34:50.040386Z\",\n            \"modified_at\": \"2023-12-06T12:34:50.040386Z\"\n        },\n        {\n            \"id\": \"11e99863f593407285c283f3e5df6496\",\n            \"input_id\": \"6\",\n            \"data\": {},\n            \"model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:34:50.040385Z\",\n            \"modified_at\": \"2023-12-06T12:34:50.040385Z\"\n        },\n        {\n            \"id\": \"ca34623954984928b797a9db9e6d1672\",\n            \"input_id\": \"3\",\n            \"data\": {},\n            \"model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:34:50.040384Z\",\n            \"modified_at\": \"2023-12-06T12:34:50.040384Z\"\n        },\n        {\n            \"id\": \"da86ff6ed97540e09428c82e7307ed0a\",\n            \"input_id\": \"1\",\n            \"data\": {},\n            \"model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:34:50.040383Z\",\n            \"modified_at\": \"2023-12-06T12:34:50.040383Z\"\n        },\n        {\n            \"id\": \"074eb04fa24e4e859bbe36c8cade4cf2\",\n            \"input_id\": \"7\",\n            \"data\": {},\n            \"model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:34:50.040382Z\",\n            \"modified_at\": \"2023-12-06T12:34:50.040382Z\"\n        },\n        {\n            \"id\": \"1d71e1041eee4c569ae886234d0e585c\",\n            \"input_id\": \"6\",\n            \"data\": {},\n            \"user_id\": \"a0btrubbaefn\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:34:45.593192Z\",\n            \"modified_at\": \"2023-12-06T12:34:45.593192Z\",\n            \"trusted\": true,\n            \"input_level\": true\n        },\n        {\n            \"id\": \"1f4120b315e34a3eb15e4c590beb6025\",\n            \"input_id\": \"5\",\n            \"data\": {},\n            \"user_id\": \"a0btrubbaefn\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:34:45.593192Z\",\n            \"modified_at\": \"2023-12-06T12:34:45.593192Z\",\n            \"trusted\": true,\n            \"input_level\": true\n        },\n        {\n            \"id\": \"4438076b63bd40f0956a8b345b7c6f08\",\n            \"input_id\": \"7\",\n            \"data\": {},\n            \"user_id\": \"a0btrubbaefn\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:34:45.593192Z\",\n            \"modified_at\": \"2023-12-06T12:34:45.593192Z\",\n            \"trusted\": true,\n            \"input_level\": true\n        },\n        {\n            \"id\": \"63058214581e4bdfbd3b51c6a573fae6\",\n            \"input_id\": \"4\",\n            \"data\": {},\n            \"user_id\": \"a0btrubbaefn\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:34:45.593192Z\",\n            \"modified_at\": \"2023-12-06T12:34:45.593192Z\",\n            \"trusted\": true,\n            \"input_level\": true\n        },\n        {\n            \"id\": \"a70f8fbcfa2a4089946a98237ac2a0c6\",\n            \"input_id\": \"2\",\n            \"data\": {},\n            \"user_id\": \"a0btrubbaefn\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:34:45.593192Z\",\n            \"modified_at\": \"2023-12-06T12:34:45.593192Z\",\n            \"trusted\": true,\n            \"input_level\": true\n        },\n        {\n            \"id\": \"c41665c665d743a9b98fee7c55de044e\",\n            \"input_id\": \"3\",\n            \"data\": {},\n            \"user_id\": \"a0btrubbaefn\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:34:45.593192Z\",\n            \"modified_at\": \"2023-12-06T12:34:45.593192Z\",\n            \"trusted\": true,\n            \"input_level\": true\n        },\n        {\n            \"id\": \"eb66cb0c3d504a8480aa7420ca02a6fd\",\n            \"input_id\": \"1\",\n            \"data\": {},\n            \"user_id\": \"a0btrubbaefn\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:34:45.593192Z\",\n            \"modified_at\": \"2023-12-06T12:34:45.593192Z\",\n            \"trusted\": true,\n            \"input_level\": true\n        }\n    ]\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/annotations?list_all_annotations=true' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      },
      "post": {
        "tags": [
          "Search > Basic Search Walkthrough"
        ],
        "summary": "Post Annotations With Bounding Boxes",
        "description": "This endpoint allows users to add bounding box annotations for images.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `input_id` | **string** | **Stores the input name** |\n| `concept_id` | **string** | **Stores the concept ID** |\n| `bounding_box` | **string** | **Stores the bounding box coordinates** |",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "example": {
                  "annotations": [
                    {
                      "data": {
                        "regions": [
                          {
                            "region_info": {
                              "bounding_box": {
                                "top_row": 0,
                                "left_col": 0,
                                "bottom_row": 0.5,
                                "right_col": 0.5
                              }
                            },
                            "data": {
                              "concepts": [
                                {
                                  "id": "test-concept",
                                  "name": "test-concept"
                                }
                              ]
                            }
                          }
                        ]
                      },
                      "input_id": "1",
                      "annotation_info": {
                        "set": "ground_truth"
                      },
                      "embed_model_version_id": "bb186755eda04f9cbb6fe32e816be104"
                    },
                    {
                      "data": {
                        "regions": [
                          {
                            "region_info": {
                              "bounding_box": {
                                "top_row": 0,
                                "left_col": 0,
                                "bottom_row": 0.5,
                                "right_col": 0.5
                              }
                            },
                            "data": {
                              "concepts": [
                                {
                                  "id": "test-concept",
                                  "name": "test-concept"
                                }
                              ]
                            }
                          }
                        ]
                      },
                      "input_id": "1",
                      "annotation_info": {
                        "set": "set_to_eval"
                      },
                      "embed_model_version_id": "bb186755eda04f9cbb6fe32e816be104"
                    },
                    {
                      "data": {
                        "regions": [
                          {
                            "region_info": {
                              "bounding_box": {
                                "top_row": 0,
                                "left_col": 0,
                                "bottom_row": 0.4,
                                "right_col": 0.5
                              }
                            },
                            "data": {
                              "concepts": [
                                {
                                  "id": "test-concept",
                                  "name": "test-concept"
                                }
                              ]
                            }
                          }
                        ]
                      },
                      "input_id": "2",
                      "annotation_info": {
                        "set": "ground_truth"
                      },
                      "embed_model_version_id": "bb186755eda04f9cbb6fe32e816be104"
                    },
                    {
                      "data": {
                        "regions": [
                          {
                            "region_info": {
                              "bounding_box": {
                                "top_row": 0,
                                "left_col": 0,
                                "bottom_row": 0.5,
                                "right_col": 0.5
                              }
                            },
                            "data": {
                              "concepts": [
                                {
                                  "id": "test-concept",
                                  "name": "test-concept"
                                }
                              ]
                            }
                          }
                        ]
                      },
                      "input_id": "2",
                      "annotation_info": {
                        "set": "set_to_eval"
                      },
                      "embed_model_version_id": "bb186755eda04f9cbb6fe32e816be104"
                    },
                    {
                      "data": {
                        "regions": [
                          {
                            "region_info": {
                              "bounding_box": {
                                "top_row": 0,
                                "left_col": 0,
                                "bottom_row": 0.5,
                                "right_col": 0.5
                              }
                            },
                            "data": {
                              "concepts": [
                                {
                                  "id": "test-concept",
                                  "name": "test-concept"
                                }
                              ]
                            }
                          }
                        ]
                      },
                      "input_id": "3",
                      "annotation_info": {
                        "set": "ground_truth"
                      },
                      "embed_model_version_id": "bb186755eda04f9cbb6fe32e816be104"
                    },
                    {
                      "data": {
                        "regions": [
                          {
                            "region_info": {
                              "bounding_box": {
                                "top_row": 0,
                                "left_col": 0,
                                "bottom_row": 0.5,
                                "right_col": 0.5
                              }
                            },
                            "data": {
                              "concepts": [
                                {
                                  "id": "test-concept-1",
                                  "name": "test-concept-1"
                                }
                              ]
                            }
                          }
                        ]
                      },
                      "input_id": "3",
                      "annotation_info": {
                        "set": "set_to_eval"
                      },
                      "embed_model_version_id": "bb186755eda04f9cbb6fe32e816be104"
                    },
                    {
                      "data": {
                        "regions": [
                          {
                            "region_info": {
                              "bounding_box": {
                                "top_row": 0,
                                "left_col": 0,
                                "bottom_row": 0.5,
                                "right_col": 0.5
                              }
                            },
                            "data": {
                              "concepts": [
                                {
                                  "id": "test-concept-1",
                                  "name": "test-concept-1"
                                }
                              ]
                            }
                          }
                        ]
                      },
                      "input_id": "4",
                      "annotation_info": {
                        "set": "ground_truth"
                      },
                      "embed_model_version_id": "bb186755eda04f9cbb6fe32e816be104"
                    },
                    {
                      "data": {
                        "regions": [
                          {
                            "region_info": {
                              "bounding_box": {
                                "top_row": 0,
                                "left_col": 0.1,
                                "bottom_row": 0.5,
                                "right_col": 0.5
                              }
                            },
                            "data": {
                              "concepts": [
                                {
                                  "id": "test-concept-1",
                                  "name": "test-concept-1"
                                }
                              ]
                            }
                          }
                        ]
                      },
                      "input_id": "4",
                      "annotation_info": {
                        "set": "set_to_eval"
                      },
                      "embed_model_version_id": "bb186755eda04f9cbb6fe32e816be104"
                    },
                    {
                      "data": {
                        "regions": [
                          {
                            "region_info": {
                              "bounding_box": {
                                "top_row": 0,
                                "left_col": 0,
                                "bottom_row": 0.5,
                                "right_col": 0.5
                              }
                            },
                            "data": {
                              "concepts": [
                                {
                                  "id": "test-concept",
                                  "name": "test-concept"
                                }
                              ]
                            }
                          }
                        ]
                      },
                      "input_id": "5",
                      "annotation_info": {
                        "set": "set_to_eval"
                      },
                      "embed_model_version_id": "bb186755eda04f9cbb6fe32e816be104"
                    },
                    {
                      "data": {
                        "regions": [
                          {
                            "region_info": {
                              "bounding_box": {
                                "top_row": 0,
                                "left_col": 0,
                                "bottom_row": 0.5,
                                "right_col": 0.5
                              }
                            },
                            "data": {
                              "concepts": [
                                {
                                  "id": "test-concept",
                                  "name": "test-concept"
                                }
                              ]
                            }
                          }
                        ]
                      },
                      "input_id": "6",
                      "annotation_info": {
                        "set": "ground_truth"
                      },
                      "embed_model_version_id": "bb186755eda04f9cbb6fe32e816be104"
                    },
                    {
                      "data": {
                        "regions": [
                          {
                            "region_info": {
                              "bounding_box": {
                                "top_row": 0,
                                "left_col": 0,
                                "bottom_row": 0.5,
                                "right_col": 0.5
                              }
                            },
                            "data": {
                              "concepts": [
                                {
                                  "id": "test-concept",
                                  "name": "test-concept"
                                }
                              ]
                            }
                          }
                        ]
                      },
                      "input_id": "6",
                      "annotation_info": {
                        "set": "set_to_eval"
                      },
                      "embed_model_version_id": "bb186755eda04f9cbb6fe32e816be104"
                    },
                    {
                      "data": {
                        "regions": [
                          {
                            "region_info": {
                              "bounding_box": {
                                "top_row": 0,
                                "left_col": 0,
                                "bottom_row": 0.5,
                                "right_col": 0.5
                              }
                            },
                            "data": {
                              "concepts": [
                                {
                                  "id": "test-concept-1",
                                  "name": "test-concept-1"
                                }
                              ]
                            }
                          }
                        ]
                      },
                      "input_id": "7",
                      "annotation_info": {
                        "set": "ground_truth"
                      },
                      "embed_model_version_id": "bb186755eda04f9cbb6fe32e816be104"
                    },
                    {
                      "data": {
                        "regions": [
                          {
                            "region_info": {
                              "bounding_box": {
                                "top_row": 0,
                                "left_col": 0,
                                "bottom_row": 0.5,
                                "right_col": 0.6
                              }
                            },
                            "data": {
                              "concepts": [
                                {
                                  "id": "test-concept",
                                  "name": "test-concept"
                                }
                              ]
                            }
                          }
                        ]
                      },
                      "input_id": "7",
                      "annotation_info": {
                        "set": "set_to_eval"
                      },
                      "embed_model_version_id": "bb186755eda04f9cbb6fe32e816be104"
                    }
                  ]
                }
              }
            }
          }
        },
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"ce5c14da11452086d3a2eeb570f17a5a\"\n    },\n    \"annotations\": [\n        {\n            \"id\": \"679b11005e204ba6adb721d8632df153\",\n            \"input_id\": \"1\",\n            \"data\": {\n                \"regions\": [\n                    {\n                        \"id\": \"361d6a9253be9152968012660258a4bf\",\n                        \"region_info\": {\n                            \"bounding_box\": {\n                                \"top_row\": 0,\n                                \"left_col\": 0,\n                                \"bottom_row\": 0.5,\n                                \"right_col\": 0.5\n                            }\n                        },\n                        \"data\": {\n                            \"concepts\": [\n                                {\n                                    \"id\": \"test-concept\",\n                                    \"name\": \"test-concept\",\n                                    \"value\": 1,\n                                    \"app_id\": \"test-app-1701866015\"\n                                }\n                            ]\n                        }\n                    }\n                ]\n            },\n            \"annotation_info\": {\n                \"set\": \"ground_truth\"\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:37:11.124811911Z\",\n            \"modified_at\": \"2023-12-06T12:37:13.804347126Z\",\n            \"trusted\": true\n        },\n        {\n            \"id\": \"29cb6ebe2072490dac92d69bea9767dc\",\n            \"input_id\": \"1\",\n            \"data\": {\n                \"regions\": [\n                    {\n                        \"id\": \"361d6a9253be9152968012660258a4bf\",\n                        \"region_info\": {\n                            \"bounding_box\": {\n                                \"top_row\": 0,\n                                \"left_col\": 0,\n                                \"bottom_row\": 0.5,\n                                \"right_col\": 0.5\n                            }\n                        },\n                        \"data\": {\n                            \"concepts\": [\n                                {\n                                    \"id\": \"test-concept\",\n                                    \"name\": \"test-concept\",\n                                    \"value\": 1,\n                                    \"app_id\": \"test-app-1701866015\"\n                                }\n                            ]\n                        }\n                    }\n                ]\n            },\n            \"annotation_info\": {\n                \"set\": \"set_to_eval\"\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:37:11.124811911Z\",\n            \"modified_at\": \"2023-12-06T12:37:13.804347126Z\",\n            \"trusted\": true\n        },\n        {\n            \"id\": \"5c8cd9d38cae44fa97d3d6eae5a58dac\",\n            \"input_id\": \"2\",\n            \"data\": {\n                \"regions\": [\n                    {\n                        \"id\": \"4eae01d6e5c036bcc3a66f97ae594ea7\",\n                        \"region_info\": {\n                            \"bounding_box\": {\n                                \"top_row\": 0,\n                                \"left_col\": 0,\n                                \"bottom_row\": 0.4,\n                                \"right_col\": 0.5\n                            }\n                        },\n                        \"data\": {\n                            \"concepts\": [\n                                {\n                                    \"id\": \"test-concept\",\n                                    \"name\": \"test-concept\",\n                                    \"value\": 1,\n                                    \"app_id\": \"test-app-1701866015\"\n                                }\n                            ]\n                        }\n                    }\n                ]\n            },\n            \"annotation_info\": {\n                \"set\": \"ground_truth\"\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:37:11.124811911Z\",\n            \"modified_at\": \"2023-12-06T12:37:13.804347126Z\",\n            \"trusted\": true\n        },\n        {\n            \"id\": \"61c66cbba6e348689bee00c170a18c87\",\n            \"input_id\": \"2\",\n            \"data\": {\n                \"regions\": [\n                    {\n                        \"id\": \"361d6a9253be9152968012660258a4bf\",\n                        \"region_info\": {\n                            \"bounding_box\": {\n                                \"top_row\": 0,\n                                \"left_col\": 0,\n                                \"bottom_row\": 0.5,\n                                \"right_col\": 0.5\n                            }\n                        },\n                        \"data\": {\n                            \"concepts\": [\n                                {\n                                    \"id\": \"test-concept\",\n                                    \"name\": \"test-concept\",\n                                    \"value\": 1,\n                                    \"app_id\": \"test-app-1701866015\"\n                                }\n                            ]\n                        }\n                    }\n                ]\n            },\n            \"annotation_info\": {\n                \"set\": \"set_to_eval\"\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:37:11.124811911Z\",\n            \"modified_at\": \"2023-12-06T12:37:13.804347126Z\",\n            \"trusted\": true\n        },\n        {\n            \"id\": \"d7ce3eac2a944a918b9b3e76d2fabd97\",\n            \"input_id\": \"3\",\n            \"data\": {\n                \"regions\": [\n                    {\n                        \"id\": \"361d6a9253be9152968012660258a4bf\",\n                        \"region_info\": {\n                            \"bounding_box\": {\n                                \"top_row\": 0,\n                                \"left_col\": 0,\n                                \"bottom_row\": 0.5,\n                                \"right_col\": 0.5\n                            }\n                        },\n                        \"data\": {\n                            \"concepts\": [\n                                {\n                                    \"id\": \"test-concept\",\n                                    \"name\": \"test-concept\",\n                                    \"value\": 1,\n                                    \"app_id\": \"test-app-1701866015\"\n                                }\n                            ]\n                        }\n                    }\n                ]\n            },\n            \"annotation_info\": {\n                \"set\": \"ground_truth\"\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:37:11.124811911Z\",\n            \"modified_at\": \"2023-12-06T12:37:13.804347126Z\",\n            \"trusted\": true\n        },\n        {\n            \"id\": \"b90359079d704125b1c3da1f53ee733d\",\n            \"input_id\": \"3\",\n            \"data\": {\n                \"regions\": [\n                    {\n                        \"id\": \"361d6a9253be9152968012660258a4bf\",\n                        \"region_info\": {\n                            \"bounding_box\": {\n                                \"top_row\": 0,\n                                \"left_col\": 0,\n                                \"bottom_row\": 0.5,\n                                \"right_col\": 0.5\n                            }\n                        },\n                        \"data\": {\n                            \"concepts\": [\n                                {\n                                    \"id\": \"test-concept-1\",\n                                    \"name\": \"test-concept-1\",\n                                    \"value\": 1,\n                                    \"app_id\": \"test-app-1701866015\"\n                                }\n                            ]\n                        }\n                    }\n                ]\n            },\n            \"annotation_info\": {\n                \"set\": \"set_to_eval\"\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:37:11.124811911Z\",\n            \"modified_at\": \"2023-12-06T12:37:13.804347126Z\",\n            \"trusted\": true\n        },\n        {\n            \"id\": \"7c94dfc5a91d498db2b084d19e6b0bcb\",\n            \"input_id\": \"4\",\n            \"data\": {\n                \"regions\": [\n                    {\n                        \"id\": \"361d6a9253be9152968012660258a4bf\",\n                        \"region_info\": {\n                            \"bounding_box\": {\n                                \"top_row\": 0,\n                                \"left_col\": 0,\n                                \"bottom_row\": 0.5,\n                                \"right_col\": 0.5\n                            }\n                        },\n                        \"data\": {\n                            \"concepts\": [\n                                {\n                                    \"id\": \"test-concept-1\",\n                                    \"name\": \"test-concept-1\",\n                                    \"value\": 1,\n                                    \"app_id\": \"test-app-1701866015\"\n                                }\n                            ]\n                        }\n                    }\n                ]\n            },\n            \"annotation_info\": {\n                \"set\": \"ground_truth\"\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:37:11.124811911Z\",\n            \"modified_at\": \"2023-12-06T12:37:13.804347126Z\",\n            \"trusted\": true\n        },\n        {\n            \"id\": \"9d18e2b02e2642739f4539b71a6bf0f1\",\n            \"input_id\": \"4\",\n            \"data\": {\n                \"regions\": [\n                    {\n                        \"id\": \"86e7d81525e21b12fcf709ba76f44df3\",\n                        \"region_info\": {\n                            \"bounding_box\": {\n                                \"top_row\": 0,\n                                \"left_col\": 0.1,\n                                \"bottom_row\": 0.5,\n                                \"right_col\": 0.5\n                            }\n                        },\n                        \"data\": {\n                            \"concepts\": [\n                                {\n                                    \"id\": \"test-concept-1\",\n                                    \"name\": \"test-concept-1\",\n                                    \"value\": 1,\n                                    \"app_id\": \"test-app-1701866015\"\n                                }\n                            ]\n                        }\n                    }\n                ]\n            },\n            \"annotation_info\": {\n                \"set\": \"set_to_eval\"\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:37:11.124811911Z\",\n            \"modified_at\": \"2023-12-06T12:37:13.804347126Z\",\n            \"trusted\": true\n        },\n        {\n            \"id\": \"eeceae3d06704582a7f3ecb5e83af997\",\n            \"input_id\": \"5\",\n            \"data\": {\n                \"regions\": [\n                    {\n                        \"id\": \"361d6a9253be9152968012660258a4bf\",\n                        \"region_info\": {\n                            \"bounding_box\": {\n                                \"top_row\": 0,\n                                \"left_col\": 0,\n                                \"bottom_row\": 0.5,\n                                \"right_col\": 0.5\n                            }\n                        },\n                        \"data\": {\n                            \"concepts\": [\n                                {\n                                    \"id\": \"test-concept\",\n                                    \"name\": \"test-concept\",\n                                    \"value\": 1,\n                                    \"app_id\": \"test-app-1701866015\"\n                                }\n                            ]\n                        }\n                    }\n                ]\n            },\n            \"annotation_info\": {\n                \"set\": \"set_to_eval\"\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:37:11.124811911Z\",\n            \"modified_at\": \"2023-12-06T12:37:13.804347126Z\",\n            \"trusted\": true\n        },\n        {\n            \"id\": \"eb96f561d50044f0a8bb13220b966332\",\n            \"input_id\": \"6\",\n            \"data\": {\n                \"regions\": [\n                    {\n                        \"id\": \"361d6a9253be9152968012660258a4bf\",\n                        \"region_info\": {\n                            \"bounding_box\": {\n                                \"top_row\": 0,\n                                \"left_col\": 0,\n                                \"bottom_row\": 0.5,\n                                \"right_col\": 0.5\n                            }\n                        },\n                        \"data\": {\n                            \"concepts\": [\n                                {\n                                    \"id\": \"test-concept\",\n                                    \"name\": \"test-concept\",\n                                    \"value\": 1,\n                                    \"app_id\": \"test-app-1701866015\"\n                                }\n                            ]\n                        }\n                    }\n                ]\n            },\n            \"annotation_info\": {\n                \"set\": \"ground_truth\"\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:37:11.124811911Z\",\n            \"modified_at\": \"2023-12-06T12:37:13.804347126Z\",\n            \"trusted\": true\n        },\n        {\n            \"id\": \"c2ca85db56124b959418b3858da4364a\",\n            \"input_id\": \"6\",\n            \"data\": {\n                \"regions\": [\n                    {\n                        \"id\": \"361d6a9253be9152968012660258a4bf\",\n                        \"region_info\": {\n                            \"bounding_box\": {\n                                \"top_row\": 0,\n                                \"left_col\": 0,\n                                \"bottom_row\": 0.5,\n                                \"right_col\": 0.5\n                            }\n                        },\n                        \"data\": {\n                            \"concepts\": [\n                                {\n                                    \"id\": \"test-concept\",\n                                    \"name\": \"test-concept\",\n                                    \"value\": 1,\n                                    \"app_id\": \"test-app-1701866015\"\n                                }\n                            ]\n                        }\n                    }\n                ]\n            },\n            \"annotation_info\": {\n                \"set\": \"set_to_eval\"\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:37:11.124811911Z\",\n            \"modified_at\": \"2023-12-06T12:37:13.804347126Z\",\n            \"trusted\": true\n        },\n        {\n            \"id\": \"fb1377dd16fc4ee2ba1078d9ed0ab4a2\",\n            \"input_id\": \"7\",\n            \"data\": {\n                \"regions\": [\n                    {\n                        \"id\": \"361d6a9253be9152968012660258a4bf\",\n                        \"region_info\": {\n                            \"bounding_box\": {\n                                \"top_row\": 0,\n                                \"left_col\": 0,\n                                \"bottom_row\": 0.5,\n                                \"right_col\": 0.5\n                            }\n                        },\n                        \"data\": {\n                            \"concepts\": [\n                                {\n                                    \"id\": \"test-concept-1\",\n                                    \"name\": \"test-concept-1\",\n                                    \"value\": 1,\n                                    \"app_id\": \"test-app-1701866015\"\n                                }\n                            ]\n                        }\n                    }\n                ]\n            },\n            \"annotation_info\": {\n                \"set\": \"ground_truth\"\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:37:11.124811911Z\",\n            \"modified_at\": \"2023-12-06T12:37:13.804347126Z\",\n            \"trusted\": true\n        },\n        {\n            \"id\": \"e0c81a88c5ee4fd7afa8f0f56a000786\",\n            \"input_id\": \"7\",\n            \"data\": {\n                \"regions\": [\n                    {\n                        \"id\": \"20183edba158bfa273763aafc10e2d97\",\n                        \"region_info\": {\n                            \"bounding_box\": {\n                                \"top_row\": 0,\n                                \"left_col\": 0,\n                                \"bottom_row\": 0.5,\n                                \"right_col\": 0.6\n                            }\n                        },\n                        \"data\": {\n                            \"concepts\": [\n                                {\n                                    \"id\": \"test-concept\",\n                                    \"name\": \"test-concept\",\n                                    \"value\": 1,\n                                    \"app_id\": \"test-app-1701866015\"\n                                }\n                            ]\n                        }\n                    }\n                ]\n            },\n            \"annotation_info\": {\n                \"set\": \"set_to_eval\"\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\",\n            \"status\": {\n                \"code\": 24150,\n                \"description\": \"Annotation success\"\n            },\n            \"created_at\": \"2023-12-06T12:37:11.124811911Z\",\n            \"modified_at\": \"2023-12-06T12:37:13.804347126Z\",\n            \"trusted\": true\n        }\n    ]\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/annotations' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Content-Type: application/json' \\\n--data '{\n  \"annotations\": [\n    {\n      \"data\": {\n        \"regions\": [\n          {\n            \"region_info\": {\n              \"bounding_box\": {\n                \"top_row\": 0,\n                \"left_col\": 0,\n                \"bottom_row\": 0.5,\n                \"right_col\": 0.5\n              }\n            },\n            \"data\": {\n              \"concepts\": [\n                {\n                  \"id\": \"test-concept\",\n                  \"name\": \"test-concept\"\n                }\n              ]\n            }\n          }\n        ]\n      },\n      \"input_id\": \"1\",\n      \"annotation_info\": {\n        \"set\": \"ground_truth\"\n      },\n      \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\"\n    },\n    {\n      \"data\": {\n        \"regions\": [\n          {\n            \"region_info\": {\n              \"bounding_box\": {\n                \"top_row\": 0,\n                \"left_col\": 0,\n                \"bottom_row\": 0.5,\n                \"right_col\": 0.5\n              }\n            },\n            \"data\": {\n              \"concepts\": [\n                {\n                  \"id\": \"test-concept\",\n                  \"name\": \"test-concept\"\n                }\n              ]\n            }\n          }\n        ]\n      },\n      \"input_id\": \"1\",\n      \"annotation_info\": {\n        \"set\": \"set_to_eval\"\n      },\n      \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\"\n    },\n    {\n      \"data\": {\n        \"regions\": [\n          {\n            \"region_info\": {\n              \"bounding_box\": {\n                \"top_row\": 0,\n                \"left_col\": 0,\n                \"bottom_row\": 0.4,\n                \"right_col\": 0.5\n              }\n            },\n            \"data\": {\n              \"concepts\": [\n                {\n                  \"id\": \"test-concept\",\n                  \"name\": \"test-concept\"\n                }\n              ]\n            }\n          }\n        ]\n      },\n      \"input_id\": \"2\",\n      \"annotation_info\": {\n        \"set\": \"ground_truth\"\n      },\n      \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\"\n    },\n    {\n      \"data\": {\n        \"regions\": [\n          {\n            \"region_info\": {\n              \"bounding_box\": {\n                \"top_row\": 0,\n                \"left_col\": 0,\n                \"bottom_row\": 0.5,\n                \"right_col\": 0.5\n              }\n            },\n            \"data\": {\n              \"concepts\": [\n                {\n                  \"id\": \"test-concept\",\n                  \"name\": \"test-concept\"\n                }\n              ]\n            }\n          }\n        ]\n      },\n      \"input_id\": \"2\",\n      \"annotation_info\": {\n        \"set\": \"set_to_eval\"\n      },\n      \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\"\n    },\n    {\n      \"data\": {\n        \"regions\": [\n          {\n            \"region_info\": {\n              \"bounding_box\": {\n                \"top_row\": 0,\n                \"left_col\": 0,\n                \"bottom_row\": 0.5,\n                \"right_col\": 0.5\n              }\n            },\n            \"data\": {\n              \"concepts\": [\n                {\n                  \"id\": \"test-concept\",\n                  \"name\": \"test-concept\"\n                }\n              ]\n            }\n          }\n        ]\n      },\n      \"input_id\": \"3\",\n      \"annotation_info\": {\n        \"set\": \"ground_truth\"\n      },\n      \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\"\n    },\n    {\n      \"data\": {\n        \"regions\": [\n          {\n            \"region_info\": {\n              \"bounding_box\": {\n                \"top_row\": 0,\n                \"left_col\": 0,\n                \"bottom_row\": 0.5,\n                \"right_col\": 0.5\n              }\n            },\n            \"data\": {\n              \"concepts\": [\n                {\n                  \"id\": \"test-concept-1\",\n                  \"name\": \"test-concept-1\"\n                }\n              ]\n            }\n          }\n        ]\n      },\n      \"input_id\": \"3\",\n      \"annotation_info\": {\n        \"set\": \"set_to_eval\"\n      },\n      \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\"\n    },\n    {\n      \"data\": {\n        \"regions\": [\n          {\n            \"region_info\": {\n              \"bounding_box\": {\n                \"top_row\": 0,\n                \"left_col\": 0,\n                \"bottom_row\": 0.5,\n                \"right_col\": 0.5\n              }\n            },\n            \"data\": {\n              \"concepts\": [\n                {\n                  \"id\": \"test-concept-1\",\n                  \"name\": \"test-concept-1\"\n                }\n              ]\n            }\n          }\n        ]\n      },\n      \"input_id\": \"4\",\n      \"annotation_info\": {\n        \"set\": \"ground_truth\"\n      },\n      \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\"\n    },\n    {\n      \"data\": {\n        \"regions\": [\n          {\n            \"region_info\": {\n              \"bounding_box\": {\n                \"top_row\": 0,\n                \"left_col\": 0.1,\n                \"bottom_row\": 0.5,\n                \"right_col\": 0.5\n              }\n            },\n            \"data\": {\n              \"concepts\": [\n                {\n                  \"id\": \"test-concept-1\",\n                  \"name\": \"test-concept-1\"\n                }\n              ]\n            }\n          }\n        ]\n      },\n      \"input_id\": \"4\",\n      \"annotation_info\": {\n        \"set\": \"set_to_eval\"\n      },\n      \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\"\n    },\n    {\n      \"data\": {\n        \"regions\": [\n          {\n            \"region_info\": {\n              \"bounding_box\": {\n                \"top_row\": 0,\n                \"left_col\": 0,\n                \"bottom_row\": 0.5,\n                \"right_col\": 0.5\n              }\n            },\n            \"data\": {\n              \"concepts\": [\n                {\n                  \"id\": \"test-concept\",\n                  \"name\": \"test-concept\"\n                }\n              ]\n            }\n          }\n        ]\n      },\n      \"input_id\": \"5\",\n      \"annotation_info\": {\n        \"set\": \"set_to_eval\"\n      },\n      \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\"\n    },\n    {\n      \"data\": {\n        \"regions\": [\n          {\n            \"region_info\": {\n              \"bounding_box\": {\n                \"top_row\": 0,\n                \"left_col\": 0,\n                \"bottom_row\": 0.5,\n                \"right_col\": 0.5\n              }\n            },\n            \"data\": {\n              \"concepts\": [\n                {\n                  \"id\": \"test-concept\",\n                  \"name\": \"test-concept\"\n                }\n              ]\n            }\n          }\n        ]\n      },\n      \"input_id\": \"6\",\n      \"annotation_info\": {\n        \"set\": \"ground_truth\"\n      },\n      \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\"\n    },\n    {\n      \"data\": {\n        \"regions\": [\n          {\n            \"region_info\": {\n              \"bounding_box\": {\n                \"top_row\": 0,\n                \"left_col\": 0,\n                \"bottom_row\": 0.5,\n                \"right_col\": 0.5\n              }\n            },\n            \"data\": {\n              \"concepts\": [\n                {\n                  \"id\": \"test-concept\",\n                  \"name\": \"test-concept\"\n                }\n              ]\n            }\n          }\n        ]\n      },\n      \"input_id\": \"6\",\n      \"annotation_info\": {\n        \"set\": \"set_to_eval\"\n      },\n      \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\"\n    },\n    {\n      \"data\": {\n        \"regions\": [\n          {\n            \"region_info\": {\n              \"bounding_box\": {\n                \"top_row\": 0,\n                \"left_col\": 0,\n                \"bottom_row\": 0.5,\n                \"right_col\": 0.5\n              }\n            },\n            \"data\": {\n              \"concepts\": [\n                {\n                  \"id\": \"test-concept-1\",\n                  \"name\": \"test-concept-1\"\n                }\n              ]\n            }\n          }\n        ]\n      },\n      \"input_id\": \"7\",\n      \"annotation_info\": {\n        \"set\": \"ground_truth\"\n      },\n      \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\"\n    },\n    {\n      \"data\": {\n        \"regions\": [\n          {\n            \"region_info\": {\n              \"bounding_box\": {\n                \"top_row\": 0,\n                \"left_col\": 0,\n                \"bottom_row\": 0.5,\n                \"right_col\": 0.6\n              }\n            },\n            \"data\": {\n              \"concepts\": [\n                {\n                  \"id\": \"test-concept\",\n                  \"name\": \"test-concept\"\n                }\n              ]\n            }\n          }\n        ]\n      },\n      \"input_id\": \"7\",\n      \"annotation_info\": {\n        \"set\": \"set_to_eval\"\n      },\n      \"embed_model_version_id\": \"bb186755eda04f9cbb6fe32e816be104\"\n    }\n  ]\n}'"
          }
        ]
      },
      "patch": {
        "tags": [
          "Annotations"
        ],
        "summary": "Patch Annotations",
        "description": "Users can send PATCH requests to update specific attributes, labels, or other metadata related to annotations.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `input` | **string** | **Stores the input name** |\n| `annotation_id` | **string** | **Stores the annotation ID** |\n| `concept_id` | **string** | **Stores the concept ID** |",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "example": {
                  "annotations": [
                    {
                      "input_id": "{{input_id}}",
                      "id": "{{annotation_id}}",
                      "data": {
                        "concepts": [
                          {
                            "id": "people4",
                            "name": "people4",
                            "value": 1
                          }
                        ]
                      }
                    }
                  ],
                  "action": "overwrite"
                }
              }
            }
          }
        },
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "examples": {
                  "example-0": {
                    "summary": "Patch Annotations",
                    "value": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"1780a55d3529264163d0ff66db4e06cf\"\n    },\n    \"annotations\": [\n        {\n            \"id\": \"89e09f2006734d528010178904b8b636\",\n            \"input_id\": \"ddf15aba61684e24902fb6127aa97e34\",\n            \"data\": {\n                \"concepts\": [\n                    {\n                        \"id\": \"people4\",\n                        \"name\": \"people4\",\n                        \"value\": 1,\n                        \"app_id\": \"test-app-1700638575-empty\"\n                    }\n                ]\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"status\": {\n                \"code\": 24250,\n                \"description\": \"Annotation modification success\"\n            },\n            \"created_at\": \"2023-11-28T15:17:25.903311Z\",\n            \"modified_at\": \"2023-11-28T15:19:48.390465842Z\",\n            \"trusted\": true\n        }\n    ]\n}"
                  },
                  "example-1": {
                    "summary": "Patch Annotation Status to Awaiting Review",
                    "value": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"01c78185e9be565a84e227a24c95ca67\"\n    },\n    \"annotations\": [\n        {\n            \"id\": \"89e09f2006734d528010178904b8b636\",\n            \"input_id\": \"ddf15aba61684e24902fb6127aa97e34\",\n            \"data\": {},\n            \"annotation_info\": {\n                \"task_id\": \"a23640ca439d4ad4bbb0b55095b91b21\"\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"status\": {\n                \"code\": 24250,\n                \"description\": \"Annotation modification success\"\n            },\n            \"created_at\": \"2023-11-28T15:17:25.903311Z\",\n            \"modified_at\": \"2023-11-28T15:20:12.136734690Z\"\n        }\n    ]\n}"
                  },
                  "example-2": {
                    "summary": "Patch Annotations metadata and geo",
                    "value": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"f83533afdb69ee8010b6b8ea532a7da3\"\n    },\n    \"annotations\": [\n        {\n            \"id\": \"89e09f2006734d528010178904b8b636\",\n            \"input_id\": \"ddf15aba61684e24902fb6127aa97e34\",\n            \"data\": {},\n            \"annotation_info\": {\n                \"asset_sets\": [\n                    \"102\"\n                ],\n                \"is_golden\": true,\n                \"key\": \"value\"\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"status\": {\n                \"code\": 24250,\n                \"description\": \"Annotation modification success\"\n            },\n            \"created_at\": \"2023-11-28T15:17:25.903311Z\",\n            \"modified_at\": \"2023-11-28T15:20:37.440163418Z\"\n        }\n    ]\n}"
                  },
                  "example-3": {
                    "summary": "Update Concepts Bulk (remove by concept id list)",
                    "value": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"details\": \"All inputs successfully added\",\n        \"req_id\": \"c18eb4c707c6803f322913e5df272bb6\"\n    },\n    \"inputs\": [\n        {\n            \"id\": \"9177dc72a85f46b69bcc6cb7045554df\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"https://samples.clarifai.com/metro-north.jpg\",\n                    \"hosted\": {\n                        \"prefix\": \"https://data-dev.clarifai.com\",\n                        \"suffix\": \"users/a0btrubbaefn/apps/test-app-1700638575-empty/inputs/image/140c856dc82565d2c4d6ea720fceff78\",\n                        \"sizes\": [\n                            \"orig\",\n                            \"tiny\",\n                            \"small\",\n                            \"large\"\n                        ],\n                        \"crossorigin\": \"use-credentials\"\n                    },\n                    \"image_info\": {\n                        \"width\": 512,\n                        \"height\": 384,\n                        \"format\": \"JPEG\",\n                        \"color_mode\": \"YUV\"\n                    }\n                },\n                \"concepts\": [\n                    {\n                        \"id\": \"foo1\",\n                        \"name\": \"foo1\",\n                        \"value\": 1,\n                        \"app_id\": \"test-app-1700638575-empty\"\n                    }\n                ]\n            },\n            \"created_at\": \"2023-11-28T15:24:44.862758Z\",\n            \"modified_at\": \"2023-11-28T15:24:45.101932Z\",\n            \"status\": {\n                \"code\": 30200,\n                \"description\": \"Input modification success\"\n            }\n        }\n    ]\n}"
                  },
                  "example-4": {
                    "summary": "Patch Annotations (Bulk inputs)",
                    "value": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"3a20dd4a75d30f2be9c43791e68481ff\"\n    },\n    \"annotations\": [\n        {\n            \"id\": \"83c4fc71a75a483cae663b2cb612b6a5\",\n            \"input_id\": \"bda54f69edd94e7abe505079adfbfe7d\",\n            \"data\": {\n                \"concepts\": [\n                    {\n                        \"id\": \"foo1\",\n                        \"name\": \"foo1\",\n                        \"value\": 0,\n                        \"app_id\": \"test-app-1700638575-empty\"\n                    }\n                ]\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"status\": {\n                \"code\": 24250,\n                \"description\": \"Annotation modification success\"\n            },\n            \"created_at\": \"2023-11-28T14:13:01.415130Z\",\n            \"modified_at\": \"2023-11-28T14:14:21.645486334Z\",\n            \"trusted\": true\n        }\n    ]\n}"
                  },
                  "example-5": {
                    "summary": "Patch Annotations (bbox)",
                    "value": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"eefa532247842ebc3359d0f0450fff7a\"\n    },\n    \"annotations\": [\n        {\n            \"id\": \"f2520ee1dede4955b4bfc9e2edbd3d43\",\n            \"input_id\": \"2f0336587c564033b40b8d08c32c31ce\",\n            \"data\": {\n                \"regions\": [\n                    {\n                        \"id\": \"361d6a9253be9152968012660258a4bf\",\n                        \"region_info\": {\n                            \"bounding_box\": {\n                                \"top_row\": 0,\n                                \"left_col\": 0,\n                                \"bottom_row\": 0.5,\n                                \"right_col\": 0.5\n                            }\n                        },\n                        \"data\": {\n                            \"concepts\": [\n                                {\n                                    \"id\": \"foo1\",\n                                    \"name\": \"foo1\",\n                                    \"value\": 1,\n                                    \"app_id\": \"test-app-1700638575-empty\"\n                                }\n                            ]\n                        }\n                    }\n                ]\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"status\": {\n                \"code\": 24250,\n                \"description\": \"Annotation modification success\"\n            },\n            \"created_at\": \"2023-11-28T15:29:32.199257Z\",\n            \"modified_at\": \"2023-11-28T15:31:00.192895421Z\",\n            \"trusted\": true\n        }\n    ]\n}"
                  },
                  "example-6": {
                    "summary": "Patch Annotation Track ID",
                    "value": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"597f895ba93de1c1e349efafec3d80c2\"\n    },\n    \"annotations\": [\n        {\n            \"id\": \"9ec8cbe80b07435da463287e9a8ced37\",\n            \"input_id\": \"40c723c6b90248aeae0f7a503db0fb37\",\n            \"data\": {\n                \"frames\": [\n                    {\n                        \"frame_info\": {\n                            \"index\": 1,\n                            \"time\": 1500\n                        },\n                        \"data\": {\n                            \"regions\": [\n                                {\n                                    \"id\": \"b90545a9af67c671ab9dba294dd10717\",\n                                    \"region_info\": {\n                                        \"bounding_box\": {\n                                            \"top_row\": 0,\n                                            \"left_col\": 0,\n                                            \"bottom_row\": 0.5,\n                                            \"right_col\": 0.9272\n                                        }\n                                    },\n                                    \"data\": {\n                                        \"concepts\": [\n                                            {\n                                                \"id\": \"foo1\",\n                                                \"name\": \"foo1\",\n                                                \"value\": 1,\n                                                \"app_id\": \"test-app-1700638575-empty\"\n                                            }\n                                        ]\n                                    },\n                                    \"track_id\": \"track1\"\n                                }\n                            ]\n                        },\n                        \"id\": \"8e5f8672bdda2f2682d59ccc019d48c0\"\n                    }\n                ]\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"status\": {\n                \"code\": 24250,\n                \"description\": \"Annotation modification success\"\n            },\n            \"created_at\": \"2023-11-28T14:16:34.637498Z\",\n            \"modified_at\": \"2023-11-28T14:18:50.837394722Z\",\n            \"trusted\": true\n        }\n    ]\n}"
                  }
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff --request PATCH 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/annotations' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Content-Type: application/json' \\\n--data '{\n  \"annotations\": [\n    {\n      \"input_id\": \"{{input_id}}\",\n      \"id\": \"{{annotation_id}}\",\n      \"data\": {\n        \"concepts\": [\n          {\n            \"id\": \"people4\",\n            \"name\": \"people4\",\n            \"value\": 1\n          }\n        ]\n      }\n    }\n  ],\n  \"action\": \"overwrite\"\n}'"
          }
        ]
      },
      "delete": {
        "tags": [
          "Annotations"
        ],
        "summary": "Delete By ID",
        "description": "This endpoint allows you to remove annotations based on their unique identifier.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `annotation_id` | **string** | **Stores the annotation ID** |",
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"c73f80073ce35a924bbef1ac5b476751\"\n    }\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff --request DELETE 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/annotations' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/task/123/annotations/status": {
      "patch": {
        "tags": [
          "Annotations"
        ],
        "summary": "Patch Annotation Status By Task ID",
        "description": "This endpoint allows you to update the annotation status of a specific task by providing the task ID.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `code` | **string** | **Stores the status code** |",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "example": {
                  "user_ids": [
                    "{{user_id}}"
                  ],
                  "status_code": 24150,
                  "action": "overwrite"
                }
              }
            }
          }
        },
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"cdeb4595a09a0dcaab6b822431b0b665\"\n    },\n    \"user_ids\": [\n        \"a0btrubbaefn\"\n    ]\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff --request PATCH 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/task/123/annotations/status' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Content-Type: application/json' \\\n--data '{\n  \"user_ids\": [\n    \"{{user_id}}\"\n  ],\n  \"status_code\": 24150,\n  \"action\": \"overwrite\"\n}'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/annotations/workers": {
      "get": {
        "tags": [
          "Annotations"
        ],
        "summary": "List Annotation Workers",
        "description": "This endpoint allows you to retrieve information about annotation workers.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |",
        "parameters": [
          {
            "name": "trusted_only",
            "in": "query",
            "schema": {
              "type": "boolean"
            },
            "example": "true"
          },
          {
            "name": "additional_fields",
            "in": "query",
            "schema": {
              "type": "string"
            },
            "example": "all"
          },
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"b57bd1da52c6d4d2f08e08e0a4fd9c88\"\n    },\n    \"workers\": [\n        {\n            \"user\": {\n                \"id\": \"a0btrubbaefn\",\n                \"first_name\": \"adithyan \",\n                \"last_name\": \"Sukumar\",\n                \"company_name\": \"Clarifai\",\n                \"job_title\": \"ML Documentation Analyst\",\n                \"job_role\": \"Data Science / Machine Learning\",\n                \"created_at\": \"2023-10-26T12:44:52.576110Z\",\n                \"visibility\": {\n                    \"gettable\": 50\n                }\n            }\n        }\n    ]\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/annotations/workers?trusted_only=true&additional_fields=all' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/": {
      "post": {
        "tags": [
          "Walkthroughs > RAG"
        ],
        "summary": "Create RAG Application",
        "description": "This endpoint allows users to create an application.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`id`** | **string** | **Stores the Application ID** |\n| `default_workflow.id` | **string** | **Stores the Workflow ID** |",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "example": {
                  "apps": [
                    {
                      "id": "rag-app-{{$timestamp}}",
                      "default_workflow": {
                        "user_id": "clarifai",
                        "app_id": "main",
                        "id": "Text"
                      }
                    }
                  ]
                }
              }
            }
          }
        },
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "200": {
            "description": "OK",
            "headers": {
              "Date": {
                "schema": {
                  "type": "string",
                  "example": "Thu, 02 May 2024 08:46:05 GMT"
                }
              },
              "Content-Type": {
                "schema": {
                  "type": "string",
                  "example": "application/json; charset=UTF-8"
                }
              },
              "Content-Length": {
                "schema": {
                  "type": "integer",
                  "example": "701"
                }
              },
              "Connection": {
                "schema": {
                  "type": "string",
                  "example": "keep-alive"
                }
              },
              "Access-Control-Allow-Headers": {
                "schema": {
                  "type": "string",
                  "example": "Content-Type,Accept,X-Requested-With,Content-Type,Referer,Accept-Encoding,X-CSRF-Token,Authorization,X-Clarifai-Application-Id,X-Clarifai-REST-API-Key,X-Clarifai-Session-Token,X-Clarifai-Client,X-Clarifai-Site,X-RapidAPI-User,x-clarifai-request-id-prefix,x-request-id"
                }
              },
              "Access-Control-Allow-Methods": {
                "schema": {
                  "type": "string",
                  "example": "GET,HEAD,POST,PUT,PATCH,OPTIONS,DELETE"
                }
              },
              "Access-Control-Allow-Origin": {
                "schema": {
                  "type": "string",
                  "example": "*"
                }
              },
              "Grpc-Metadata-Content-Type": {
                "schema": {
                  "type": "string",
                  "example": "application/grpc"
                }
              },
              "X-Clarifai-Request-Id": {
                "schema": {
                  "type": "string",
                  "example": "fdb499456d3143ea920cb5ef0c7a6b6b"
                }
              }
            },
            "content": {
              "application/json": {
                "schema": {
                  "type": "object"
                },
                "example": {
                  "status": {
                    "code": 10000,
                    "description": "Ok",
                    "req_id": "fdb499456d3143ea920cb5ef0c7a6b6b"
                  },
                  "apps": [
                    {
                      "id": "rag-app-1714639564",
                      "name": "rag-app-1714639564",
                      "default_language": "en",
                      "default_workflow_id": "Text",
                      "default_workflow": {
                        "id": "Text",
                        "app_id": "rag-app-1714639564",
                        "created_at": "2024-05-02T08:46:05.508253Z",
                        "metadata": {},
                        "visibility": {
                          "gettable": 10
                        },
                        "user_id": "clarifai",
                        "modified_at": "2024-05-02T08:46:05.508253Z",
                        "version": {
                          "id": "0a7eaab890724ce19a15b75ed72327c1"
                        },
                        "use_cases": [],
                        "check_consents": []
                      },
                      "user_id": "clarifai",
                      "created_at": "2024-05-02T08:46:05.507552086Z",
                      "modified_at": "2024-05-02T08:46:05.507552086Z",
                      "metadata": {},
                      "sample_ms": 1000,
                      "visibility": {
                        "gettable": 10
                      },
                      "is_template": false
                    }
                  ]
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Content-Type: application/json' \\\n--data '{\n  \"apps\": [\n    {\n      \"id\": \"rag-app-{{$timestamp}}\",\n      \"default_workflow\": {\n        \"user_id\": \"clarifai\",\n        \"app_id\": \"main\",\n        \"id\": \"Text\"\n      }\n    }\n  ]\n}'"
          }
        ]
      },
      "patch": {
        "tags": [
          "Applications"
        ],
        "summary": "Patch Application Base Workflow",
        "description": "This request allows the users to change the base workflow of an app.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`id`** | **string** | **Stores the current Application ID** |\n| `name` | string | Stores the new Application ID |\n| `default_language` | string | Sets the language for the app |",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "example": {
                  "apps": [
                    {
                      "id": "{{app_id}}",
                      "default_workflow_id": "Text"
                    }
                  ],
                  "action": "overwrite",
                  "reindex": true
                }
              }
            }
          }
        },
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "200": {
            "description": "OK",
            "headers": {
              "Date": {
                "schema": {
                  "type": "string",
                  "example": "Thu, 02 May 2024 09:30:34 GMT"
                }
              },
              "Content-Type": {
                "schema": {
                  "type": "string",
                  "example": "application/json; charset=UTF-8"
                }
              },
              "Content-Length": {
                "schema": {
                  "type": "integer",
                  "example": "735"
                }
              },
              "Connection": {
                "schema": {
                  "type": "string",
                  "example": "keep-alive"
                }
              },
              "Access-Control-Allow-Headers": {
                "schema": {
                  "type": "string",
                  "example": "Content-Type,Accept,X-Requested-With,Content-Type,Referer,Accept-Encoding,X-CSRF-Token,Authorization,X-Clarifai-Application-Id,X-Clarifai-REST-API-Key,X-Clarifai-Session-Token,X-Clarifai-Client,X-Clarifai-Site,X-RapidAPI-User,x-clarifai-request-id-prefix,x-request-id"
                }
              },
              "Access-Control-Allow-Methods": {
                "schema": {
                  "type": "string",
                  "example": "GET,HEAD,POST,PUT,PATCH,OPTIONS,DELETE"
                }
              },
              "Access-Control-Allow-Origin": {
                "schema": {
                  "type": "string",
                  "example": "*"
                }
              },
              "Grpc-Metadata-Content-Type": {
                "schema": {
                  "type": "string",
                  "example": "application/grpc"
                }
              },
              "X-Clarifai-Request-Id": {
                "schema": {
                  "type": "string",
                  "example": "ecc6b1585a7c4130889c16f6f7771bc6"
                }
              }
            },
            "content": {
              "application/json": {
                "schema": {
                  "type": "object"
                },
                "example": {
                  "status": {
                    "code": 10000,
                    "description": "Ok",
                    "req_id": "ecc6b1585a7c4130889c16f6f7771bc6"
                  },
                  "apps": [
                    {
                      "id": "postman-1714639111",
                      "name": "postman-1714639111",
                      "default_language": "en",
                      "default_workflow_id": "Text",
                      "default_workflow": {
                        "id": "Text",
                        "app_id": "postman-1714639111",
                        "created_at": "2024-05-02T09:30:34.251626817Z",
                        "metadata": {},
                        "visibility": {
                          "gettable": 10
                        },
                        "user_id": "clarifai",
                        "modified_at": "2024-05-02T09:30:34.251626817Z",
                        "version": {
                          "id": "45663e11042a4986bb593d6191b93e24"
                        },
                        "use_cases": [],
                        "check_consents": []
                      },
                      "user_id": "clarifai",
                      "created_at": "2024-05-02T08:38:32.245583Z",
                      "modified_at": "2024-05-02T09:30:34.500830798Z",
                      "metadata": {},
                      "sample_ms": 1000,
                      "visibility": {
                        "gettable": 50
                      },
                      "notes": "# postman-1714639111",
                      "is_template": false
                    }
                  ]
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff --request PATCH 'https://api.clarifai.com/v2/users/{user_id}/apps' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Content-Type: application/json' \\\n--data '{\n  \"apps\": [\n    {\n      \"id\": \"{{app_id}}\",\n      \"default_workflow_id\": \"Text\"\n    }\n  ],\n  \"action\": \"overwrite\",\n  \"reindex\": true\n}'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps": {
      "get": {
        "tags": [
          "Applications"
        ],
        "summary": "List Applications",
        "description": "This request can be used by the users to retrieve a list of applications they have created.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |",
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "200": {
            "description": "OK",
            "headers": {
              "Date": {
                "schema": {
                  "type": "string",
                  "example": "Thu, 02 May 2024 08:57:45 GMT"
                }
              },
              "Content-Type": {
                "schema": {
                  "type": "string",
                  "example": "application/json; charset=UTF-8"
                }
              },
              "Transfer-Encoding": {
                "schema": {
                  "type": "string",
                  "example": "chunked"
                }
              },
              "Connection": {
                "schema": {
                  "type": "string",
                  "example": "keep-alive"
                }
              },
              "Access-Control-Allow-Headers": {
                "schema": {
                  "type": "string",
                  "example": "Content-Type,Accept,X-Requested-With,Content-Type,Referer,Accept-Encoding,X-CSRF-Token,Authorization,X-Clarifai-Application-Id,X-Clarifai-REST-API-Key,X-Clarifai-Session-Token,X-Clarifai-Client,X-Clarifai-Site,X-RapidAPI-User,x-clarifai-request-id-prefix,x-request-id"
                }
              },
              "Access-Control-Allow-Methods": {
                "schema": {
                  "type": "string",
                  "example": "GET,HEAD,POST,PUT,PATCH,OPTIONS,DELETE"
                }
              },
              "Access-Control-Allow-Origin": {
                "schema": {
                  "type": "string",
                  "example": "*"
                }
              },
              "Grpc-Metadata-Content-Type": {
                "schema": {
                  "type": "string",
                  "example": "application/grpc"
                }
              },
              "X-Clarifai-Request-Id": {
                "schema": {
                  "type": "string",
                  "example": "611246ed865c45039d3c3499751f3180"
                }
              }
            },
            "content": {
              "application/json": {
                "schema": {
                  "type": "object"
                },
                "example": {
                  "status": {
                    "code": 10000,
                    "description": "Ok",
                    "req_id": "611246ed865c45039d3c3499751f3180"
                  },
                  "apps": [
                    {
                      "id": "postman-1714639251",
                      "name": "postman-1714639251",
                      "default_language": "en",
                      "default_workflow_id": "Text",
                      "default_workflow": {
                        "id": "Text",
                        "app_id": "postman-1714639251",
                        "created_at": "2024-05-02T08:40:51.836107Z",
                        "metadata": {},
                        "visibility": {
                          "gettable": 10
                        },
                        "user_id": "clarifai",
                        "modified_at": "2024-05-02T08:40:51.836107Z",
                        "use_cases": [],
                        "check_consents": []
                      },
                      "user_id": "clarifai",
                      "created_at": "2024-05-02T08:40:51.809892Z",
                      "modified_at": "2024-05-02T08:40:51.809892Z",
                      "metadata": {},
                      "sample_ms": 1000,
                      "visibility": {
                        "gettable": 10
                      },
                      "is_template": false
                    },
                    {
                      "id": "postman-1714639206",
                      "name": "postman-1714639206",
                      "default_language": "en",
                      "default_workflow_id": "Empty",
                      "default_workflow": {
                        "id": "Empty",
                        "app_id": "postman-1714639206",
                        "created_at": "2024-05-02T08:40:07.405199Z",
                        "metadata": {},
                        "visibility": {
                          "gettable": 10
                        },
                        "user_id": "clarifai",
                        "modified_at": "2024-05-02T08:40:07.405199Z",
                        "use_cases": [],
                        "check_consents": []
                      },
                      "user_id": "clarifai",
                      "created_at": "2024-05-02T08:40:07.404485Z",
                      "modified_at": "2024-05-02T08:40:07.404485Z",
                      "metadata": {},
                      "sample_ms": 1000,
                      "visibility": {
                        "gettable": 10
                      },
                      "is_template": false
                    },
                    {
                      "id": "postman-1714639183",
                      "name": "postman-1714639183",
                      "default_language": "en",
                      "default_workflow_id": "Face",
                      "default_workflow": {
                        "id": "Face",
                        "app_id": "postman-1714639183",
                        "created_at": "2024-05-02T08:39:44.295873Z",
                        "metadata": {},
                        "visibility": {
                          "gettable": 10
                        },
                        "user_id": "clarifai",
                        "modified_at": "2024-05-02T08:39:44.295873Z",
                        "use_cases": [],
                        "check_consents": []
                      },
                      "user_id": "clarifai",
                      "created_at": "2024-05-02T08:39:44.295130Z",
                      "modified_at": "2024-05-02T08:39:44.295130Z",
                      "metadata": {},
                      "sample_ms": 1000,
                      "visibility": {
                        "gettable": 10
                      },
                      "is_template": false
                    },
                    {
                      "id": "postman-1714639160",
                      "name": "postman-1714639160",
                      "default_language": "en",
                      "default_workflow_id": "Food",
                      "default_workflow": {
                        "id": "Food",
                        "app_id": "postman-1714639160",
                        "created_at": "2024-05-02T08:39:20.802066Z",
                        "metadata": {},
                        "visibility": {
                          "gettable": 10
                        },
                        "user_id": "clarifai",
                        "modified_at": "2024-05-02T08:39:20.802066Z",
                        "use_cases": [],
                        "check_consents": []
                      },
                      "user_id": "clarifai",
                      "created_at": "2024-05-02T08:39:20.801740Z",
                      "modified_at": "2024-05-02T08:39:20.801740Z",
                      "metadata": {},
                      "sample_ms": 1000,
                      "visibility": {
                        "gettable": 10
                      },
                      "is_template": false
                    },
                    {
                      "id": "postman-1714639111",
                      "name": "postman-1714639111",
                      "default_language": "en",
                      "default_workflow_id": "Moderation",
                      "default_workflow": {
                        "id": "Moderation",
                        "app_id": "postman-1714639111",
                        "created_at": "2024-05-02T08:38:32.246299Z",
                        "metadata": {},
                        "visibility": {
                          "gettable": 10
                        },
                        "user_id": "clarifai",
                        "modified_at": "2024-05-02T08:38:32.246299Z",
                        "use_cases": [],
                        "check_consents": []
                      },
                      "user_id": "clarifai",
                      "created_at": "2024-05-02T08:38:32.245583Z",
                      "modified_at": "2024-05-02T08:38:32.245583Z",
                      "metadata": {},
                      "sample_ms": 1000,
                      "visibility": {
                        "gettable": 10
                      },
                      "is_template": false
                    }
                  ]
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      },
      "post": {
        "tags": [
          "Search > Basic Search Walkthrough"
        ],
        "summary": "Create Application (General1.5)",
        "description": "This request allows you to create a new application in Clarifai portal.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`id`** | **string** | **Stores the Application ID** |\n| `default_workflow_id` | **string** | **Stores the workflow name** |",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "example": {
                  "apps": [
                    {
                      "id": "test-app-{{$timestamp}}",
                      "default_workflow_id": "General"
                    }
                  ]
                }
              }
            }
          }
        },
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"ed81130c8826c66e09737bdfbc5c9d00\"\n    },\n    \"apps\": [\n        {\n            \"id\": \"test-app-1701866015\",\n            \"name\": \"test-app-1701866015\",\n            \"default_language\": \"en\",\n            \"default_workflow_id\": \"General\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"created_at\": \"2023-12-06T12:33:35.608770314Z\",\n            \"modified_at\": \"2023-12-06T12:33:35.608770314Z\",\n            \"metadata\": {},\n            \"sample_ms\": 1000,\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"data_tier_id\": \"rds\",\n            \"is_template\": false\n        }\n    ]\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Content-Type: application/json' \\\n--data '{\n  \"apps\": [\n    {\n      \"id\": \"test-app-{{$timestamp}}\",\n      \"default_workflow_id\": \"General\"\n    }\n  ]\n}'"
          }
        ]
      }
    },
    "/v2/apps": {
      "get": {
        "tags": [
          "Applications"
        ],
        "summary": "Search Applications",
        "description": "This endpoint allows the users to search for an application by providing the app ID.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| `search` | **string** | **Stores the search query** |",
        "parameters": [
          {
            "name": "search",
            "in": "query",
            "schema": {
              "type": "string"
            },
            "example": "postman"
          }
        ],
        "responses": {
          "200": {
            "description": "OK",
            "headers": {
              "Date": {
                "schema": {
                  "type": "string",
                  "example": "Thu, 02 May 2024 09:03:13 GMT"
                }
              },
              "Content-Type": {
                "schema": {
                  "type": "string",
                  "example": "application/json; charset=UTF-8"
                }
              },
              "Content-Length": {
                "schema": {
                  "type": "integer",
                  "example": "1437"
                }
              },
              "Connection": {
                "schema": {
                  "type": "string",
                  "example": "keep-alive"
                }
              },
              "Access-Control-Allow-Headers": {
                "schema": {
                  "type": "string",
                  "example": "Content-Type,Accept,X-Requested-With,Content-Type,Referer,Accept-Encoding,X-CSRF-Token,Authorization,X-Clarifai-Application-Id,X-Clarifai-REST-API-Key,X-Clarifai-Session-Token,X-Clarifai-Client,X-Clarifai-Site,X-RapidAPI-User,x-clarifai-request-id-prefix,x-request-id"
                }
              },
              "Access-Control-Allow-Methods": {
                "schema": {
                  "type": "string",
                  "example": "GET,HEAD,POST,PUT,PATCH,OPTIONS,DELETE"
                }
              },
              "Access-Control-Allow-Origin": {
                "schema": {
                  "type": "string",
                  "example": "*"
                }
              },
              "Grpc-Metadata-Content-Type": {
                "schema": {
                  "type": "string",
                  "example": "application/grpc"
                }
              },
              "X-Clarifai-Request-Id": {
                "schema": {
                  "type": "string",
                  "example": "0d52b91c4eb843418b9f89613c72b51d"
                }
              }
            },
            "content": {
              "application/json": {
                "schema": {
                  "type": "object"
                },
                "example": {
                  "status": {
                    "code": 10000,
                    "description": "Ok",
                    "req_id": "0d52b91c4eb843418b9f89613c72b51d"
                  },
                  "apps": [
                    {
                      "id": "postman-1714639111",
                      "name": "postman-1714639111",
                      "default_language": "en",
                      "user_id": "clarifai",
                      "created_at": "2024-05-02T08:38:32.245583Z",
                      "modified_at": "2024-05-02T09:02:56.125896Z",
                      "metadata": {},
                      "sample_ms": 1000,
                      "visibility": {
                        "gettable": 50
                      },
                      "is_template": false
                    },
                    {
                      "id": "postman-1714639160",
                      "name": "postman-1714639160",
                      "default_language": "en",
                      "user_id": "clarifai",
                      "created_at": "2024-05-02T08:39:20.801740Z",
                      "modified_at": "2024-05-02T09:02:46.200162Z",
                      "metadata": {},
                      "sample_ms": 1000,
                      "visibility": {
                        "gettable": 50
                      },
                      "is_template": false
                    },
                    {
                      "id": "postman-1714639183",
                      "name": "postman-1714639183",
                      "default_language": "en",
                      "user_id": "clarifai",
                      "created_at": "2024-05-02T08:39:44.295130Z",
                      "modified_at": "2024-05-02T09:02:38.431549Z",
                      "metadata": {},
                      "sample_ms": 1000,
                      "visibility": {
                        "gettable": 50
                      },
                      "is_template": false
                    },
                    {
                      "id": "postman-1714639206",
                      "name": "postman-1714639206",
                      "default_language": "en",
                      "user_id": "clarifai",
                      "created_at": "2024-05-02T08:40:07.404485Z",
                      "modified_at": "2024-05-02T09:02:26.788354Z",
                      "metadata": {},
                      "sample_ms": 1000,
                      "visibility": {
                        "gettable": 50
                      },
                      "is_template": false
                    },
                    {
                      "id": "postman-1714639251",
                      "name": "postman-1714639251",
                      "default_language": "en",
                      "user_id": "clarifai",
                      "created_at": "2024-05-02T08:40:51.809892Z",
                      "modified_at": "2024-05-02T09:02:17.195673Z",
                      "metadata": {},
                      "sample_ms": 1000,
                      "visibility": {
                        "gettable": 50
                      },
                      "is_template": false
                    }
                  ]
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location 'https://api.clarifai.com/v2/apps?search=postman' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}": {
      "get": {
        "tags": [
          "Applications"
        ],
        "summary": "Get Application",
        "description": "This endpoint provides a way to retrieve detailed information about a specific application within the platform.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |",
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "200": {
            "description": "OK",
            "headers": {
              "Date": {
                "schema": {
                  "type": "string",
                  "example": "Thu, 02 May 2024 09:10:55 GMT"
                }
              },
              "Content-Type": {
                "schema": {
                  "type": "string",
                  "example": "application/json; charset=UTF-8"
                }
              },
              "Content-Length": {
                "schema": {
                  "type": "integer",
                  "example": "829"
                }
              },
              "Connection": {
                "schema": {
                  "type": "string",
                  "example": "keep-alive"
                }
              },
              "Access-Control-Allow-Headers": {
                "schema": {
                  "type": "string",
                  "example": "Content-Type,Accept,X-Requested-With,Content-Type,Referer,Accept-Encoding,X-CSRF-Token,Authorization,X-Clarifai-Application-Id,X-Clarifai-REST-API-Key,X-Clarifai-Session-Token,X-Clarifai-Client,X-Clarifai-Site,X-RapidAPI-User,x-clarifai-request-id-prefix,x-request-id"
                }
              },
              "Access-Control-Allow-Methods": {
                "schema": {
                  "type": "string",
                  "example": "GET,HEAD,POST,PUT,PATCH,OPTIONS,DELETE"
                }
              },
              "Access-Control-Allow-Origin": {
                "schema": {
                  "type": "string",
                  "example": "*"
                }
              },
              "Grpc-Metadata-Content-Type": {
                "schema": {
                  "type": "string",
                  "example": "application/grpc"
                }
              },
              "X-Clarifai-Request-Id": {
                "schema": {
                  "type": "string",
                  "example": "4f6c6f013b914bb3874a02e6f64e18ca"
                }
              }
            },
            "content": {
              "application/json": {
                "schema": {
                  "type": "object"
                },
                "example": {
                  "status": {
                    "code": 10000,
                    "description": "Ok",
                    "req_id": "4f6c6f013b914bb3874a02e6f64e18ca"
                  },
                  "app": {
                    "id": "postman-1714639111",
                    "name": "postman-1714639111",
                    "default_language": "en",
                    "default_workflow_id": "Moderation",
                    "default_workflow": {
                      "id": "Moderation",
                      "app_id": "postman-1714639111",
                      "created_at": "2024-05-02T08:38:32.246299Z",
                      "metadata": {},
                      "visibility": {
                        "gettable": 10
                      },
                      "user_id": "clarifai",
                      "modified_at": "2024-05-02T08:38:32.246299Z",
                      "version": {
                        "id": "867e606aee5b42158e38c6ff64bce8db"
                      },
                      "use_cases": [],
                      "check_consents": []
                    },
                    "user_id": "clarifai",
                    "created_at": "2024-05-02T08:38:32.245583Z",
                    "modified_at": "2024-05-02T09:02:56.125896Z",
                    "metadata": {},
                    "sample_ms": 1000,
                    "visibility": {
                      "gettable": 50
                    },
                    "star_count": 1,
                    "is_template": false,
                    "extra_info": {
                      "search_revision_marker": "13c7f08433cccf3c7e1fd2509ddb8bca9d297a1a",
                      "counts": {
                        "workflows": "1"
                      }
                    }
                  }
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      },
      "delete": {
        "tags": [
          "Applications"
        ],
        "summary": "Delete Application",
        "description": "Using this endpoint users can delete applications by providing the app ID.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |",
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "200": {
            "description": "OK",
            "headers": {
              "Date": {
                "schema": {
                  "type": "string",
                  "example": "Thu, 02 May 2024 09:34:04 GMT"
                }
              },
              "Content-Type": {
                "schema": {
                  "type": "string",
                  "example": "application/json; charset=UTF-8"
                }
              },
              "Content-Length": {
                "schema": {
                  "type": "integer",
                  "example": "141"
                }
              },
              "Connection": {
                "schema": {
                  "type": "string",
                  "example": "keep-alive"
                }
              },
              "Access-Control-Allow-Headers": {
                "schema": {
                  "type": "string",
                  "example": "Content-Type,Accept,X-Requested-With,Content-Type,Referer,Accept-Encoding,X-CSRF-Token,Authorization,X-Clarifai-Application-Id,X-Clarifai-REST-API-Key,X-Clarifai-Session-Token,X-Clarifai-Client,X-Clarifai-Site,X-RapidAPI-User,x-clarifai-request-id-prefix,x-request-id"
                }
              },
              "Access-Control-Allow-Methods": {
                "schema": {
                  "type": "string",
                  "example": "GET,HEAD,POST,PUT,PATCH,OPTIONS,DELETE"
                }
              },
              "Access-Control-Allow-Origin": {
                "schema": {
                  "type": "string",
                  "example": "*"
                }
              },
              "Grpc-Metadata-Content-Type": {
                "schema": {
                  "type": "string",
                  "example": "application/grpc"
                }
              },
              "X-Clarifai-Request-Id": {
                "schema": {
                  "type": "string",
                  "example": "792dc85006f64b03b3789f92104af2b4"
                }
              }
            },
            "content": {
              "application/json": {
                "schema": {
                  "type": "object"
                },
                "example": {
                  "status": {
                    "code": 10000,
                    "description": "Ok",
                    "details": "application 'updated-1714642384' deleted",
                    "req_id": "792dc85006f64b03b3789f92104af2b4"
                  }
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff --request DELETE 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/ids": {
      "patch": {
        "tags": [
          "Applications"
        ],
        "summary": "Patch Application ID",
        "description": "This endpoint allows you to change the ID of an app.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`id`** | **string** | **Stores the current Application ID** |\n| `new_id` | string | Stores the new Application ID |",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "example": {
                  "ids": [
                    {
                      "id": "{{app_id}}",
                      "new_id": "updated-{{$timestamp}}"
                    }
                  ],
                  "action": "overwrite"
                }
              }
            }
          }
        },
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "200": {
            "description": "OK",
            "headers": {
              "Date": {
                "schema": {
                  "type": "string",
                  "example": "Thu, 02 May 2024 09:33:05 GMT"
                }
              },
              "Content-Type": {
                "schema": {
                  "type": "string",
                  "example": "application/json; charset=UTF-8"
                }
              },
              "Content-Length": {
                "schema": {
                  "type": "integer",
                  "example": "396"
                }
              },
              "Connection": {
                "schema": {
                  "type": "string",
                  "example": "keep-alive"
                }
              },
              "Access-Control-Allow-Headers": {
                "schema": {
                  "type": "string",
                  "example": "Content-Type,Accept,X-Requested-With,Content-Type,Referer,Accept-Encoding,X-CSRF-Token,Authorization,X-Clarifai-Application-Id,X-Clarifai-REST-API-Key,X-Clarifai-Session-Token,X-Clarifai-Client,X-Clarifai-Site,X-RapidAPI-User,x-clarifai-request-id-prefix,x-request-id"
                }
              },
              "Access-Control-Allow-Methods": {
                "schema": {
                  "type": "string",
                  "example": "GET,HEAD,POST,PUT,PATCH,OPTIONS,DELETE"
                }
              },
              "Access-Control-Allow-Origin": {
                "schema": {
                  "type": "string",
                  "example": "*"
                }
              },
              "Grpc-Metadata-Content-Type": {
                "schema": {
                  "type": "string",
                  "example": "application/grpc"
                }
              },
              "X-Clarifai-Request-Id": {
                "schema": {
                  "type": "string",
                  "example": "33bbe427e08f4031b57d39679f8322d2"
                }
              }
            },
            "content": {
              "application/json": {
                "schema": {
                  "type": "object"
                },
                "example": {
                  "status": {
                    "code": 10000,
                    "description": "Ok",
                    "req_id": "33bbe427e08f4031b57d39679f8322d2"
                  },
                  "apps": [
                    {
                      "id": "updated-1714642384",
                      "name": "postman-1714639111",
                      "default_language": "en",
                      "user_id": "clarifai",
                      "created_at": "2024-05-02T08:38:32.245583Z",
                      "modified_at": "2024-05-02T09:33:05.470622Z",
                      "metadata": {},
                      "sample_ms": 1000,
                      "visibility": {
                        "gettable": 50
                      },
                      "notes": "# postman-1714639111",
                      "is_template": false
                    }
                  ]
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff --request PATCH 'https://api.clarifai.com/v2/users/{user_id}/apps/ids' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Content-Type: application/json' \\\n--data '{\n  \"ids\": [\n    {\n      \"id\": \"{{app_id}}\",\n      \"new_id\": \"updated-{{$timestamp}}\"\n    }\n  ],\n  \"action\": \"overwrite\"\n}'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/collectors": {
      "post": {
        "tags": [
          "Collectors"
        ],
        "summary": "Add Collectors",
        "description": "The purpose of this endpoint is to enable users to create new collectors.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `id` | **string** | **Stores the Collector ID** |\n| `pre_queue_workflow_id` | **string** | **Stores the pre-workflow ID** |\n| `post_inputs_key_id` | **string** | **Stores the PAT key** |",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "example": {
                  "collectors": [
                    {
                      "id": "collector",
                      "description": "collector",
                      "pre_queue_workflow_id": "apparel",
                      "collector_source": {
                        "api_post_model_outputs_collector_source": {
                          "caller_user_id": "",
                          "model_user_id": "{{user_id}}",
                          "model_app_id": "{{app_id}}",
                          "model_id": "{{model_id}}",
                          "model_version_id": "{{version_id}}",
                          "post_inputs_key_id": "{{key}}"
                        }
                      }
                    }
                  ]
                }
              }
            }
          }
        },
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"48467e4c997cfdf7ad7f1559d72e271d\"\n    },\n    \"collectors\": [\n        {\n            \"id\": \"collector\",\n            \"description\": \"collector\",\n            \"created_at\": \"2023-11-27T09:13:29.345719123Z\",\n            \"pre_queue_workflow_id\": \"apparel\",\n            \"collector_source\": {\n                \"api_post_model_outputs_collector_source\": {\n                    \"model_user_id\": \"a0btrubbaefn\",\n                    \"model_app_id\": \"test-app-1700638575-empty\",\n                    \"model_id\": \"custom-config\",\n                    \"model_version_id\": \"6166f5c0fd844e2ba62814819fa72ae6\",\n                    \"post_inputs_key_id\": \"60d6c5e1e9f7449b8d7e4ed67c2bb6aa\"\n                }\n            },\n            \"status\": {\n                \"code\": 52001\n            }\n        }\n    ]\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/collectors' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Content-Type: application/json' \\\n--data '{\n  \"collectors\": [\n    {\n      \"id\": \"collector\",\n      \"description\": \"collector\",\n      \"pre_queue_workflow_id\": \"apparel\",\n      \"collector_source\": {\n        \"api_post_model_outputs_collector_source\": {\n          \"caller_user_id\": \"\",\n          \"model_user_id\": \"{{user_id}}\",\n          \"model_app_id\": \"{{app_id}}\",\n          \"model_id\": \"{{model_id}}\",\n          \"model_version_id\": \"{{version_id}}\",\n          \"post_inputs_key_id\": \"{{key}}\"\n        }\n      }\n    }\n  ]\n}'"
          }
        ]
      },
      "patch": {
        "tags": [
          "Collectors"
        ],
        "summary": "Patch Collectors  To Overwrite Status",
        "description": "This request allows you to update the status of a specific Collector, providing a way to modify its configuration.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `id` | **string** | **Stores the Collector ID** |\n| `pre_queue_workflow_id` | **string** | **Stores the pre-workflow ID** |\n| `post_inputs_key_id` | **string** | **Stores the PAT key** |\n| `code` | **string** | **Stores the status code** |",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "example": {
                  "collectors": [
                    {
                      "id": "collector",
                      "description": "collector",
                      "pre_queue_workflow_id": "apparel",
                      "collector_source": {
                        "api_post_model_outputs_collector_source": {
                          "caller_user_id": "",
                          "model_user_id": "{{user_id}}",
                          "model_app_id": "{{app_id}}",
                          "model_id": "{{model_id}}",
                          "model_version_id": "{{version_id}}",
                          "post_inputs_key_id": "{{key}}"
                        }
                      },
                      "status": {
                        "code": 52002
                      }
                    }
                  ],
                  "action": "overwrite"
                }
              }
            }
          }
        },
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"a8420f59a3ace78f66379eec5354f05f\"\n    },\n    \"collectors\": [\n        {\n            \"id\": \"collector\",\n            \"description\": \"collector\",\n            \"created_at\": \"2023-11-27T09:13:29.345719Z\",\n            \"pre_queue_workflow_id\": \"apparel\",\n            \"collector_source\": {\n                \"api_post_model_outputs_collector_source\": {\n                    \"model_user_id\": \"a0btrubbaefn\",\n                    \"model_app_id\": \"test-app-1700638575-empty\",\n                    \"model_id\": \"custom-config\",\n                    \"model_version_id\": \"6166f5c0fd844e2ba62814819fa72ae6\",\n                    \"post_inputs_key_id\": \"60d6c5e1e9f7449b8d7e4ed67c2bb6aa\"\n                }\n            },\n            \"status\": {\n                \"code\": 52002\n            }\n        }\n    ]\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff --request PATCH 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/collectors' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Content-Type: application/json' \\\n--data '{\n  \"collectors\": [\n    {\n      \"id\": \"collector\",\n      \"description\": \"collector\",\n      \"pre_queue_workflow_id\": \"apparel\",\n      \"collector_source\": {\n        \"api_post_model_outputs_collector_source\": {\n          \"caller_user_id\": \"\",\n          \"model_user_id\": \"{{user_id}}\",\n          \"model_app_id\": \"{{app_id}}\",\n          \"model_id\": \"{{model_id}}\",\n          \"model_version_id\": \"{{version_id}}\",\n          \"post_inputs_key_id\": \"{{key}}\"\n        }\n      },\n      \"status\": {\n        \"code\": 52002\n      }\n    }\n  ],\n  \"action\": \"overwrite\"\n}'"
          }
        ]
      },
      "get": {
        "tags": [
          "Collectors"
        ],
        "summary": "List All Collectors (app only)",
        "description": "This endpoint allows you to list all available collectors in an app.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |",
        "parameters": [
          {
            "name": "page",
            "in": "query",
            "schema": {
              "type": "integer"
            },
            "example": "1"
          },
          {
            "name": "per_page",
            "in": "query",
            "schema": {
              "type": "integer"
            },
            "example": "10"
          },
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"ad8a31c846d113aa5bea1edd08a055bd\"\n    },\n    \"collectors\": [\n        {\n            \"id\": \"collector\",\n            \"description\": \"collector\",\n            \"created_at\": \"2023-11-27T09:13:29.345719Z\",\n            \"pre_queue_workflow_id\": \"apparel\",\n            \"collector_source\": {\n                \"api_post_model_outputs_collector_source\": {\n                    \"model_user_id\": \"a0btrubbaefn\",\n                    \"model_app_id\": \"test-app-1700638575-empty\",\n                    \"model_id\": \"custom-config\",\n                    \"model_version_id\": \"6166f5c0fd844e2ba62814819fa72ae6\",\n                    \"post_inputs_key_id\": \"60d6c5e1e9f7449b8d7e4ed67c2bb6aa\"\n                }\n            },\n            \"status\": {\n                \"code\": 52002\n            }\n        }\n    ]\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/collectors?page=1&per_page=10' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      },
      "delete": {
        "tags": [
          "Collectors"
        ],
        "summary": "Delete Collector",
        "description": "This endpoint allows users to remove a collector by providing the collector ID.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `collector_id` | **string** | **Stores the Collector ID** |",
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"details\": \"collectors collector deleted\",\n        \"req_id\": \"22a518ed8f54f3f4e75856de91b416c2\"\n    }\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff --request DELETE 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/collectors' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/collectors/{collector_id}": {
      "get": {
        "tags": [
          "Collectors"
        ],
        "summary": "Get Collector",
        "description": "This endpoint allows you to retrieve information about collectors.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `collector_id` | **string** | **Stores the Collector ID** |",
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "collector_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"a8420f59a3ace78f66379eec5354f05f\"\n    },\n    \"collectors\": [\n        {\n            \"id\": \"collector\",\n            \"description\": \"collector\",\n            \"created_at\": \"2023-11-27T09:13:29.345719Z\",\n            \"pre_queue_workflow_id\": \"apparel\",\n            \"collector_source\": {\n                \"api_post_model_outputs_collector_source\": {\n                    \"model_user_id\": \"a0btrubbaefn\",\n                    \"model_app_id\": \"test-app-1700638575-empty\",\n                    \"model_id\": \"custom-config\",\n                    \"model_version_id\": \"6166f5c0fd844e2ba62814819fa72ae6\",\n                    \"post_inputs_key_id\": \"60d6c5e1e9f7449b8d7e4ed67c2bb6aa\"\n                }\n            },\n            \"status\": {\n                \"code\": 52002\n            }\n        }\n    ]\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/collectors/{collector_id}' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/concepts/{concept_id}": {
      "get": {
        "tags": [
          "Concepts > Concept Essentials"
        ],
        "summary": "By ID",
        "description": "This endpoint allows you to retrieve detailed information about a specific concept using its unique identifier.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `concept_id` | **string** | **Stores the concept ID** |",
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "concept_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"20de9a514196392e8e94855eb894f979\"\n    },\n    \"concept\": {\n        \"id\": \"foo\",\n        \"name\": \"blah\",\n        \"value\": 1,\n        \"created_at\": \"2023-11-22T09:22:33.743356Z\",\n        \"language\": \"en\",\n        \"app_id\": \"test-app-1700638575-empty\",\n        \"visibility\": {\n            \"gettable\": 10\n        },\n        \"user_id\": \"a0btrubbaefn\"\n    }\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/concepts/{concept_id}' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/concepts": {
      "get": {
        "tags": [
          "Concepts > Concept Essentials"
        ],
        "summary": "List All Concepts (does app only)",
        "description": "This request will list all available concepts in your application.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |",
        "parameters": [
          {
            "name": "page",
            "in": "query",
            "schema": {
              "type": "integer"
            },
            "example": "1"
          },
          {
            "name": "per_page",
            "in": "query",
            "schema": {
              "type": "integer"
            },
            "example": "100"
          },
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"9d5967bf638b82088bf28100ba7ae037\"\n    },\n    \"concepts\": [\n        {\n            \"id\": \"apple1\",\n            \"name\": \"apple1\",\n            \"value\": 1,\n            \"created_at\": \"2023-11-27T13:30:08.838951Z\",\n            \"language\": \"en\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\"\n        },\n        {\n            \"id\": \"bar1\",\n            \"name\": \"bar1\",\n            \"value\": 1,\n            \"created_at\": \"2023-11-27T13:30:08.838949Z\",\n            \"language\": \"en\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\"\n        },\n        {\n            \"id\": \"foo1\",\n            \"name\": \"foo1\",\n            \"value\": 1,\n            \"created_at\": \"2023-11-27T13:30:08.838946Z\",\n            \"language\": \"en\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\"\n        },\n        {\n            \"id\": \"id-toxic\",\n            \"name\": \"id-toxic\",\n            \"value\": 1,\n            \"created_at\": \"2023-11-23T09:41:18.499746Z\",\n            \"language\": \"en\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\"\n        },\n        {\n            \"id\": \"id-severe_toxic\",\n            \"name\": \"id-severe_toxic\",\n            \"value\": 1,\n            \"created_at\": \"2023-11-23T09:41:18.499743Z\",\n            \"language\": \"en\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\"\n        },\n        {\n            \"id\": \"id-obscene\",\n            \"name\": \"id-obscene\",\n            \"value\": 1,\n            \"created_at\": \"2023-11-23T09:41:18.499741Z\",\n            \"language\": \"en\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\"\n        },\n        {\n            \"id\": \"id-threat\",\n            \"name\": \"id-threat\",\n            \"value\": 1,\n            \"created_at\": \"2023-11-23T09:41:18.499738Z\",\n            \"language\": \"en\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\"\n        },\n        {\n            \"id\": \"id-insult\",\n            \"name\": \"id-insult\",\n            \"value\": 1,\n            \"created_at\": \"2023-11-23T09:41:18.499734Z\",\n            \"language\": \"en\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\"\n        },\n        {\n            \"id\": \"id-identity_hate\",\n            \"name\": \"id-identity_hate\",\n            \"value\": 1,\n            \"created_at\": \"2023-11-23T09:41:18.499731Z\",\n            \"language\": \"en\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\"\n        },\n        {\n            \"id\": \"product\",\n            \"name\": \"product\",\n            \"value\": 1,\n            \"created_at\": \"2023-11-23T07:33:43.888526Z\",\n            \"language\": \"en\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\"\n        },\n        {\n            \"id\": \"lifestyle\",\n            \"name\": \"lifestyle\",\n            \"value\": 1,\n            \"created_at\": \"2023-11-23T07:33:43.888523Z\",\n            \"language\": \"en\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\"\n        },\n        {\n            \"id\": \"abacus\",\n            \"name\": \"abacus\",\n            \"value\": 1,\n            \"created_at\": \"2023-11-23T07:21:39.949084Z\",\n            \"language\": \"en\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\"\n        },\n        {\n            \"id\": \"apple\",\n            \"name\": \"apple\",\n            \"value\": 1,\n            \"created_at\": \"2023-11-22T09:22:33.743361Z\",\n            \"language\": \"en\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\"\n        },\n        {\n            \"id\": \"bar\",\n            \"name\": \"bar\",\n            \"value\": 1,\n            \"created_at\": \"2023-11-22T09:22:33.743359Z\",\n            \"language\": \"en\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\"\n        },\n        {\n            \"id\": \"foo\",\n            \"name\": \"blah\",\n            \"value\": 1,\n            \"created_at\": \"2023-11-22T09:22:33.743356Z\",\n            \"language\": \"en\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\"\n        },\n        {\n            \"id\": \"lambo\",\n            \"name\": \"lambo\",\n            \"value\": 1,\n            \"created_at\": \"2023-11-22T07:48:10.023930Z\",\n            \"language\": \"en\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\"\n        },\n        {\n            \"id\": \"asdf123\",\n            \"name\": \"asdf123\",\n            \"value\": 1,\n            \"created_at\": \"2023-11-22T07:44:21.573542Z\",\n            \"language\": \"en\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\"\n        },\n        {\n            \"id\": \"ferrari\",\n            \"name\": \"ferrari\",\n            \"value\": 1,\n            \"created_at\": \"2023-11-22T07:43:00.757259Z\",\n            \"language\": \"en\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\"\n        },\n        {\n            \"id\": \"mattid2\",\n            \"name\": \"mattid2\",\n            \"value\": 1,\n            \"created_at\": \"2023-11-22T07:43:00.757257Z\",\n            \"language\": \"en\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\"\n        },\n        {\n            \"id\": \"ferrari23\",\n            \"name\": \"ferrari23\",\n            \"value\": 1,\n            \"created_at\": \"2023-11-22T07:42:25.283829Z\",\n            \"language\": \"en\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\"\n        },\n        {\n            \"id\": \"outdoors23\",\n            \"name\": \"outdoors23\",\n            \"value\": 1,\n            \"created_at\": \"2023-11-22T07:41:17.634334Z\",\n            \"language\": \"en\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\"\n        },\n        {\n            \"id\": \"louis-vuitton\",\n            \"name\": \"louis-vuitton\",\n            \"value\": 1,\n            \"created_at\": \"2023-11-22T07:40:13.306628Z\",\n            \"language\": \"en\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\"\n        },\n        {\n            \"id\": \"train\",\n            \"name\": \"train\",\n            \"value\": 1,\n            \"created_at\": \"2023-11-22T07:39:41.417902Z\",\n            \"language\": \"en\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\"\n        },\n        {\n            \"id\": \"car\",\n            \"name\": \"car\",\n            \"value\": 1,\n            \"created_at\": \"2023-11-22T07:39:41.417900Z\",\n            \"language\": \"en\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\"\n        },\n        {\n            \"id\": \"test_concept1\",\n            \"name\": \"test_concept1\",\n            \"value\": 1,\n            \"created_at\": \"2023-11-22T07:39:18.273331Z\",\n            \"language\": \"en\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\"\n        }\n    ]\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/concepts?page=1&per_page=100' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      },
      "post": {
        "tags": [
          "Concepts > Concept Essentials"
        ],
        "summary": "Add Concept",
        "description": "This endpoint allows you to create and add new concepts to the app.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `concept_id` | **string** | **Stores the concept ID** |",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "example": {
                  "concepts": [
                    {
                      "id": "foo"
                    },
                    {
                      "id": "bar"
                    },
                    {
                      "id": "apple"
                    }
                  ]
                }
              }
            }
          }
        },
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"adcb480e025ae5e6166bc851a9b2f16a\"\n    },\n    \"concepts\": [\n        {\n            \"id\": \"foo1\",\n            \"name\": \"foo1\",\n            \"value\": 1,\n            \"created_at\": \"2023-11-27T13:30:08.838946702Z\",\n            \"language\": \"en\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\"\n        },\n        {\n            \"id\": \"bar1\",\n            \"name\": \"bar1\",\n            \"value\": 1,\n            \"created_at\": \"2023-11-27T13:30:08.838949138Z\",\n            \"language\": \"en\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\"\n        },\n        {\n            \"id\": \"apple1\",\n            \"name\": \"apple1\",\n            \"value\": 1,\n            \"created_at\": \"2023-11-27T13:30:08.838951133Z\",\n            \"language\": \"en\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\"\n        }\n    ]\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/concepts' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Content-Type: application/json' \\\n--data '{\n  \"concepts\": [\n    {\n      \"id\": \"foo\"\n    },\n    {\n      \"id\": \"bar\"\n    },\n    {\n      \"id\": \"apple\"\n    }\n  ]\n}'"
          }
        ]
      },
      "patch": {
        "tags": [
          "Concepts > Concept Essentials"
        ],
        "summary": "Patch Concepts",
        "description": "This request allows users to update concepts in their app.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `concept_id` | **string** | **Stores the concept ID** |",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "example": {
                  "concepts": [
                    {
                      "id": "foo",
                      "name": "blah"
                    }
                  ],
                  "action": "overwrite"
                }
              }
            }
          }
        },
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"c9f38ea0fef8738f212b28446c1c22f2\"\n    },\n    \"concepts\": [\n        {\n            \"id\": \"foo\",\n            \"name\": \"blah\",\n            \"value\": 1,\n            \"created_at\": \"2023-11-27T13:31:02.410658239Z\",\n            \"language\": \"en\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"user_id\": \"a0btrubbaefn\"\n        }\n    ]\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff --request PATCH 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/concepts' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Content-Type: application/json' \\\n--data '{\n  \"concepts\": [\n    {\n      \"id\": \"foo\",\n      \"name\": \"blah\"\n    }\n  ],\n  \"action\": \"overwrite\"\n}'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/concepts/status": {
      "get": {
        "tags": [
          "Concepts > Concept Essentials"
        ],
        "summary": "Get Concepts Count",
        "description": "Using this endpoint, you can count the number of concepts in your app.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |",
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"83abc992f832317e4bcc48677ef38038\"\n    },\n    \"concept_counts\": [\n        {\n            \"id\": \"ferrari23\",\n            \"name\": \"ferrari23\",\n            \"concept_type_count\": {\n                \"positive\": 3,\n                \"negative\": 4\n            },\n            \"detail_concept_count\": {\n                \"processed\": {\n                    \"positive\": 3,\n                    \"negative\": 4\n                },\n                \"to_process\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"errors\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"processing\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                }\n            }\n        },\n        {\n            \"id\": \"foo\",\n            \"name\": \"blah\",\n            \"concept_type_count\": {\n                \"positive\": 1,\n                \"negative\": 0\n            },\n            \"detail_concept_count\": {\n                \"processed\": {\n                    \"positive\": 1,\n                    \"negative\": 0\n                },\n                \"to_process\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"errors\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"processing\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                }\n            }\n        },\n        {\n            \"id\": \"id-toxic\",\n            \"name\": \"id-toxic\",\n            \"concept_type_count\": {\n                \"positive\": 0,\n                \"negative\": 0\n            },\n            \"detail_concept_count\": {\n                \"processed\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"to_process\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"errors\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"processing\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                }\n            }\n        },\n        {\n            \"id\": \"bar1\",\n            \"name\": \"bar1\",\n            \"concept_type_count\": {\n                \"positive\": 0,\n                \"negative\": 0\n            },\n            \"detail_concept_count\": {\n                \"processed\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"to_process\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"errors\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"processing\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                }\n            }\n        },\n        {\n            \"id\": \"louis-vuitton\",\n            \"name\": \"louis-vuitton\",\n            \"concept_type_count\": {\n                \"positive\": 1,\n                \"negative\": 0\n            },\n            \"detail_concept_count\": {\n                \"processed\": {\n                    \"positive\": 1,\n                    \"negative\": 0\n                },\n                \"to_process\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"errors\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"processing\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                }\n            }\n        },\n        {\n            \"id\": \"bar\",\n            \"name\": \"bar\",\n            \"concept_type_count\": {\n                \"positive\": 0,\n                \"negative\": 0\n            },\n            \"detail_concept_count\": {\n                \"processed\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"to_process\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"errors\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"processing\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                }\n            }\n        },\n        {\n            \"id\": \"ferrari\",\n            \"name\": \"ferrari\",\n            \"concept_type_count\": {\n                \"positive\": 1,\n                \"negative\": 0\n            },\n            \"detail_concept_count\": {\n                \"processed\": {\n                    \"positive\": 1,\n                    \"negative\": 0\n                },\n                \"to_process\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"errors\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"processing\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                }\n            }\n        },\n        {\n            \"id\": \"id-insult\",\n            \"name\": \"id-insult\",\n            \"concept_type_count\": {\n                \"positive\": 0,\n                \"negative\": 0\n            },\n            \"detail_concept_count\": {\n                \"processed\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"to_process\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"errors\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"processing\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                }\n            }\n        },\n        {\n            \"id\": \"id-obscene\",\n            \"name\": \"id-obscene\",\n            \"concept_type_count\": {\n                \"positive\": 0,\n                \"negative\": 0\n            },\n            \"detail_concept_count\": {\n                \"processed\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"to_process\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"errors\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"processing\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                }\n            }\n        },\n        {\n            \"id\": \"id-threat\",\n            \"name\": \"id-threat\",\n            \"concept_type_count\": {\n                \"positive\": 0,\n                \"negative\": 0\n            },\n            \"detail_concept_count\": {\n                \"processed\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"to_process\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"errors\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"processing\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                }\n            }\n        },\n        {\n            \"id\": \"train\",\n            \"name\": \"train\",\n            \"concept_type_count\": {\n                \"positive\": 1,\n                \"negative\": 0\n            },\n            \"detail_concept_count\": {\n                \"processed\": {\n                    \"positive\": 1,\n                    \"negative\": 0\n                },\n                \"to_process\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"errors\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"processing\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                }\n            }\n        },\n        {\n            \"id\": \"asdf123\",\n            \"name\": \"asdf123\",\n            \"concept_type_count\": {\n                \"positive\": 1,\n                \"negative\": 0\n            },\n            \"detail_concept_count\": {\n                \"processed\": {\n                    \"positive\": 1,\n                    \"negative\": 0\n                },\n                \"to_process\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"errors\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"processing\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                }\n            }\n        },\n        {\n            \"id\": \"test_concept1\",\n            \"name\": \"test_concept1\",\n            \"concept_type_count\": {\n                \"positive\": 1,\n                \"negative\": 0\n            },\n            \"detail_concept_count\": {\n                \"processed\": {\n                    \"positive\": 1,\n                    \"negative\": 0\n                },\n                \"to_process\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"errors\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"processing\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                }\n            }\n        },\n        {\n            \"id\": \"outdoors23\",\n            \"name\": \"outdoors23\",\n            \"concept_type_count\": {\n                \"positive\": 7,\n                \"negative\": 1\n            },\n            \"detail_concept_count\": {\n                \"processed\": {\n                    \"positive\": 7,\n                    \"negative\": 1\n                },\n                \"to_process\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"errors\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"processing\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                }\n            }\n        },\n        {\n            \"id\": \"car\",\n            \"name\": \"car\",\n            \"concept_type_count\": {\n                \"positive\": 1,\n                \"negative\": 0\n            },\n            \"detail_concept_count\": {\n                \"processed\": {\n                    \"positive\": 1,\n                    \"negative\": 0\n                },\n                \"to_process\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"errors\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"processing\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                }\n            }\n        },\n        {\n            \"id\": \"foo1\",\n            \"name\": \"foo1\",\n            \"concept_type_count\": {\n                \"positive\": 0,\n                \"negative\": 0\n            },\n            \"detail_concept_count\": {\n                \"processed\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"to_process\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"errors\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"processing\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                }\n            }\n        },\n        {\n            \"id\": \"id-identity_hate\",\n            \"name\": \"id-identity_hate\",\n            \"concept_type_count\": {\n                \"positive\": 0,\n                \"negative\": 0\n            },\n            \"detail_concept_count\": {\n                \"processed\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"to_process\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"errors\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"processing\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                }\n            }\n        },\n        {\n            \"id\": \"lambo\",\n            \"name\": \"lambo\",\n            \"concept_type_count\": {\n                \"positive\": 0,\n                \"negative\": 1\n            },\n            \"detail_concept_count\": {\n                \"processed\": {\n                    \"positive\": 0,\n                    \"negative\": 1\n                },\n                \"to_process\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"errors\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"processing\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                }\n            }\n        },\n        {\n            \"id\": \"lifestyle\",\n            \"name\": \"lifestyle\",\n            \"concept_type_count\": {\n                \"positive\": 0,\n                \"negative\": 0\n            },\n            \"detail_concept_count\": {\n                \"processed\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"to_process\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"errors\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"processing\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                }\n            }\n        },\n        {\n            \"id\": \"apple1\",\n            \"name\": \"apple1\",\n            \"concept_type_count\": {\n                \"positive\": 0,\n                \"negative\": 0\n            },\n            \"detail_concept_count\": {\n                \"processed\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"to_process\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"errors\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"processing\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                }\n            }\n        },\n        {\n            \"id\": \"mattid2\",\n            \"name\": \"mattid2\",\n            \"concept_type_count\": {\n                \"positive\": 2,\n                \"negative\": 0\n            },\n            \"detail_concept_count\": {\n                \"processed\": {\n                    \"positive\": 2,\n                    \"negative\": 0\n                },\n                \"to_process\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"errors\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"processing\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                }\n            }\n        },\n        {\n            \"id\": \"id-severe_toxic\",\n            \"name\": \"id-severe_toxic\",\n            \"concept_type_count\": {\n                \"positive\": 0,\n                \"negative\": 0\n            },\n            \"detail_concept_count\": {\n                \"processed\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"to_process\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"errors\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"processing\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                }\n            }\n        },\n        {\n            \"id\": \"apple\",\n            \"name\": \"apple\",\n            \"concept_type_count\": {\n                \"positive\": 0,\n                \"negative\": 0\n            },\n            \"detail_concept_count\": {\n                \"processed\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"to_process\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"errors\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"processing\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                }\n            }\n        },\n        {\n            \"id\": \"product\",\n            \"name\": \"product\",\n            \"concept_type_count\": {\n                \"positive\": 0,\n                \"negative\": 0\n            },\n            \"detail_concept_count\": {\n                \"processed\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"to_process\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"errors\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"processing\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                }\n            }\n        },\n        {\n            \"id\": \"abacus\",\n            \"name\": \"abacus\",\n            \"concept_type_count\": {\n                \"positive\": 0,\n                \"negative\": 0\n            },\n            \"detail_concept_count\": {\n                \"processed\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"to_process\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"errors\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                },\n                \"processing\": {\n                    \"positive\": 0,\n                    \"negative\": 0\n                }\n            }\n        }\n    ]\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/concepts/status' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/concepts/searches": {
      "post": {
        "tags": [
          "Concepts > Concept Essentials"
        ],
        "summary": "Search Concepts in Chinese",
        "description": "This endpoint is particularly useful for finding relevant concepts in a specific language.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `concept_query` | **string** | **Stores the concept name to search** |",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "example": {
                  "concept_query": {
                    "name": "人*",
                    "language": "zh"
                  }
                }
              }
            }
          }
        },
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"ad684f45cb83ec93306907c593e60d56\"\n    },\n    \"concepts\": []\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/concepts/searches' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Content-Type: application/json' \\\n--data '{\n  \"concept_query\": {\n    \"name\": \"人*\",\n    \"language\": \"zh\"\n  }\n}'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/concepts/{concept_id}/references": {
      "get": {
        "tags": [
          "Concepts > Concept Essentials"
        ],
        "summary": "References by concept id",
        "description": "This endpoint allows users to get the references for a particular concept.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `concept_id` | **string** | **Stores the concept ID** |",
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "concept_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"56c3241a85ee9a5165ab5d471200892d\"\n    },\n    \"concept_references\": []\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/concepts/{concept_id}/references' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/vocabs/{vocab_id}": {
      "get": {
        "tags": [
          "Concepts > Concept Vocabs"
        ],
        "summary": "By ID",
        "description": "This endpoint helps users retrieve concept vocabs by a unique identifier.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `vocab_id` | **string** | **Stores the vocab ID** |",
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "vocab_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"09b24cf91835de9d9fb5bb6577ddef4d\"\n    },\n    \"vocab\": {\n        \"id\": \"vocab_something_id\",\n        \"name\": \"new name\",\n        \"description\": \"new description\",\n        \"app_id\": \"test-app-1700638575-empty\",\n        \"created_at\": \"2023-11-27T13:32:25.837695Z\",\n        \"visibility\": {\n            \"gettable\": 10\n        }\n    }\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/vocabs/{vocab_id}' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      },
      "delete": {
        "tags": [
          "Concepts > Concept Vocabs"
        ],
        "summary": "Delete Vocab",
        "description": "This endpoint is used to remove a specific vocab by providing a vocab ID.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `vocab_id` | **string** | **Stores the vocab ID** |",
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "vocab_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"f08b110f023cdba1d0e47262647221bd\"\n    }\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff --request DELETE 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/vocabs/{vocab_id}' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/vocabs": {
      "get": {
        "tags": [
          "Concepts > Concept Vocabs"
        ],
        "summary": "List All Vocabs (does app only)",
        "description": "This endpoint will list all vocabs in an app.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |",
        "parameters": [
          {
            "name": "page",
            "in": "query",
            "schema": {
              "type": "integer"
            },
            "example": "1"
          },
          {
            "name": "per_page",
            "in": "query",
            "schema": {
              "type": "integer"
            },
            "example": "100"
          },
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"b3c0a00c0a3721932caa719caeadf04b\"\n    },\n    \"vocabs\": [\n        {\n            \"id\": \"vocab_something_id\",\n            \"name\": \"new name\",\n            \"description\": \"new description\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"created_at\": \"2023-11-27T13:32:25.837695Z\",\n            \"visibility\": {\n                \"gettable\": 10\n            }\n        }\n    ]\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/vocabs?page=1&per_page=100' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      },
      "post": {
        "tags": [
          "Concepts > Concept Vocabs"
        ],
        "summary": "Add Vocab",
        "description": "This endpoint allows the user to add vocab to the application.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `vocab_id` | **string** | **Stores the vocab ID** |",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "example": {
                  "vocabs": [
                    {
                      "id": "vocab_something_id",
                      "name": "vocab_something_name",
                      "description": "this is a simple vocab for local"
                    }
                  ]
                }
              }
            }
          }
        },
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"629a546ae6a7b58887f105d34aefd003\"\n    },\n    \"vocabs\": [\n        {\n            \"id\": \"vocab_something_id\",\n            \"name\": \"vocab_something_name\",\n            \"description\": \"this is a simple vocab for local\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"created_at\": \"2023-11-27T13:32:25.837695200Z\",\n            \"visibility\": {\n                \"gettable\": 10\n            }\n        }\n    ]\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/vocabs' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Content-Type: application/json' \\\n--data '{\n  \"vocabs\": [\n    {\n      \"id\": \"vocab_something_id\",\n      \"name\": \"vocab_something_name\",\n      \"description\": \"this is a simple vocab for local\"\n    }\n  ]\n}'"
          }
        ]
      },
      "patch": {
        "tags": [
          "Concepts > Concept Vocabs"
        ],
        "summary": "Patch Vocabs",
        "description": "This endpoint allows users to modify and update vocabs in the application.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `vocab_id` | **string** | **Stores the vocab ID** |",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "example": {
                  "vocabs": [
                    {
                      "id": "vocab_something_id",
                      "name": "new name",
                      "description": "new description"
                    }
                  ],
                  "action": "overwrite"
                }
              }
            }
          }
        },
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"3f2c1c82f319f36efad9cb7cd4516c1c\"\n    },\n    \"vocabs\": [\n        {\n            \"id\": \"vocab_something_id\",\n            \"name\": \"new name\",\n            \"description\": \"new description\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"created_at\": \"2023-11-27T13:32:36.739471070Z\",\n            \"visibility\": {\n                \"gettable\": 10\n            }\n        }\n    ]\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff --request PATCH 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/vocabs' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Content-Type: application/json' \\\n--data '{\n  \"vocabs\": [\n    {\n      \"id\": \"vocab_something_id\",\n      \"name\": \"new name\",\n      \"description\": \"new description\"\n    }\n  ],\n  \"action\": \"overwrite\"\n}'"
          }
        ]
      },
      "delete": {
        "tags": [
          "Concepts > Concept Vocabs"
        ],
        "summary": "Delete Batch of Vocabs",
        "description": "This endpoint is useful when removing a batch of vocabs.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `ids` | **string** | **Stores the vocab ID** |",
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"6302b2703e9c90b1ef49126c1c910f0f\"\n    }\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff --request DELETE 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/vocabs' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/vocabs/{vocab_id}/concepts": {
      "get": {
        "tags": [
          "Concepts > Concept Vocabs"
        ],
        "summary": "List All Concepts in Vocabs (does app only)",
        "description": "This request will list all concepts associated with a vocab in an app.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `vocab_id` | **string** | **Stores the vocab ID** |",
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "vocab_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"ef879496138ac610dc2d8d3ee71d1e08\"\n    },\n    \"concepts\": [\n        {\n            \"id\": \"foo1\",\n            \"name\": \"foo1\",\n            \"value\": 1,\n            \"created_at\": \"2023-11-27T13:30:08.838946Z\",\n            \"language\": \"en\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\"\n        }\n    ]\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/vocabs/{vocab_id}/concepts' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      },
      "post": {
        "tags": [
          "Concepts > Concept Vocabs"
        ],
        "summary": "Add Concepts to Vocab",
        "description": "This endpoint allows users to add concepts to a vocab.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `vocab_id` | **string** | **Stores the vocab ID** |\n| `concept_id` | **string** | **Stores the concept ID** |",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "example": {
                  "concepts": [
                    {
                      "id": "foo1"
                    }
                  ]
                }
              }
            }
          }
        },
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "vocab_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"81ceb7e4b998a0331c24cae9614f797b\"\n    },\n    \"concepts\": [\n        {\n            \"id\": \"foo1\",\n            \"name\": \"foo1\",\n            \"value\": 1,\n            \"created_at\": \"2023-11-27T13:30:08.838946Z\",\n            \"language\": \"en\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\"\n        }\n    ]\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/vocabs/{vocab_id}/concepts' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Content-Type: application/json' \\\n--data '{\n  \"concepts\": [\n    {\n      \"id\": \"foo1\"\n    }\n  ]\n}'"
          }
        ]
      },
      "delete": {
        "tags": [
          "Concepts > Concept Vocabs"
        ],
        "summary": "Delete ALL Concepts in Vocab",
        "description": "This endpoint allows users to remove all concepts in a vocab.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `vocab_id` | **string** | **Stores the vocab ID** |",
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "vocab_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"b03bf720d26530b2335830b1a12a9d5f\"\n    }\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff --request DELETE 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/vocabs/{vocab_id}/concepts' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/vocabs/{vocab_id}/concepts/{concept_id}": {
      "delete": {
        "tags": [
          "Concepts > Concept Vocabs"
        ],
        "summary": "Delete Concept from Vocab",
        "description": "This endpoint allows users to remove a specific concept from the vocab.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `vocab_id` | **string** | **Stores the vocab ID** |\n| `concept_id` | **string** | **Stores the concept ID** |",
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "vocab_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "concept_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"76fb63fb07a721c11d8e4ecd846b1a7f\"\n    }\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff --request DELETE 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/vocabs/{vocab_id}/concepts/{concept_id}' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/concepts/{concept_id}/languages": {
      "get": {
        "tags": [
          "Concepts > Concept Languages"
        ],
        "summary": "By ID List Languages",
        "description": "This endpoint retrieves the languages associated with a concept.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `concept_id` | **string** | **Stores the concept ID** |",
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "concept_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"60d9c59477e9fd25d55c1d165f7f53bc\"\n    },\n    \"concept_languages\": [\n        {\n            \"id\": \"ko\",\n            \"name\": \"개\"\n        },\n        {\n            \"id\": \"en\",\n            \"name\": \"foo1\"\n        }\n    ]\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/concepts/{concept_id}/languages' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      },
      "patch": {
        "tags": [
          "Concepts > Concept Languages"
        ],
        "summary": "By ID Patch Translationed names",
        "description": "This endpoint allows you to update translated names for concepts identified by their unique ID.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `concept_id` | **string** | **Stores the concept ID** |",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "example": {
                  "concept_languages": [
                    {
                      "id": "ko",
                      "name": "개"
                    }
                  ],
                  "action": "overwrite"
                }
              }
            }
          }
        },
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "concept_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"592e0a09886025d7547c494af88a7dbe\"\n    },\n    \"concept_languages\": [\n        {\n            \"id\": \"ko\",\n            \"name\": \"개\"\n        }\n    ]\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff --request PATCH 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/concepts/{concept_id}/languages' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Content-Type: application/json' \\\n--data '{\n  \"concept_languages\": [\n    {\n      \"id\": \"ko\",\n      \"name\": \"개\"\n    }\n  ],\n  \"action\": \"overwrite\"\n}'"
          }
        ]
      },
      "post": {
        "tags": [
          "Concepts > Concept Languages"
        ],
        "summary": "By ID Add Translations",
        "description": "This endpoint enables users to enrich their concept metadata by adding translations for specified concept IDs.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `concept_id` | **string** | **Stores the concept ID** |",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "example": {
                  "concept_languages": [
                    {
                      "id": "ko",
                      "name": "개"
                    }
                  ]
                }
              }
            }
          }
        },
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "concept_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"f0f3886b4b9c0b1f3d2b7bd6387e2dd8\"\n    },\n    \"concept_languages\": [\n        {\n            \"id\": \"ko\",\n            \"name\": \"개\"\n        }\n    ]\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/concepts/{concept_id}/languages' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Content-Type: application/json' \\\n--data '{\n  \"concept_languages\": [\n    {\n      \"id\": \"ko\",\n      \"name\": \"개\"\n    }\n  ]\n}'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/concepts/{concept_id}/languages/ko": {
      "get": {
        "tags": [
          "Concepts > Concept Languages"
        ],
        "summary": "By ID By Language Translation",
        "description": "This endpoint will return a list of concepts in a different language.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `concept_id` | **string** | **Stores the concept ID** |",
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "concept_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"780494ac6640ea4ce06bc4984a09a202\"\n    },\n    \"concept_language\": {\n        \"id\": \"ko\",\n        \"name\": \"개\"\n    }\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/concepts/{concept_id}/languages/ko' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/concepts/{concept_id}/relations": {
      "get": {
        "tags": [
          "Concepts > Concept Graph"
        ],
        "summary": "Hyponyms by concept id",
        "description": "This endpoint allows you to retrieve hyponyms associated with a specific concept\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `concept_id` | **string** | **Stores the concept ID** |",
        "parameters": [
          {
            "name": "predicate",
            "in": "query",
            "schema": {
              "type": "string"
            },
            "example": "hyponym"
          },
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "concept_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"5cc609ef33d11c8d1e9fe85cd6633b50\"\n    },\n    \"concept_relations\": [\n        {\n            \"id\": \"158b66413419406a8819dc3327e72ca2\",\n            \"subject_concept\": {\n                \"id\": \"foo1\",\n                \"name\": \"foo1\",\n                \"value\": 1,\n                \"created_at\": \"2023-11-27T13:30:08.838946Z\",\n                \"language\": \"en\",\n                \"app_id\": \"test-app-1700638575-empty\",\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"user_id\": \"a0btrubbaefn\"\n            },\n            \"object_concept\": {\n                \"id\": \"foo\",\n                \"name\": \"blah\",\n                \"value\": 1,\n                \"created_at\": \"2023-11-22T09:22:33.743356Z\",\n                \"language\": \"en\",\n                \"app_id\": \"test-app-1700638575-empty\",\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"user_id\": \"a0btrubbaefn\"\n            },\n            \"predicate\": \"hyponym\",\n            \"visibility\": {\n                \"gettable\": 10\n            }\n        }\n    ]\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/concepts/{concept_id}/relations?predicate=hyponym' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      },
      "post": {
        "tags": [
          "Concepts > Concept Graph"
        ],
        "summary": "Create Relation (hyponum)",
        "description": "This endpoint allows you to establish relationships between concepts, specifically creating a hyponym relationship.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `concept_id` | **string** | **Stores the concept ID** |\n| `concept_relation` | **string** | **Stores the hyponum** |",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "example": {
                  "concept_relations": [
                    {
                      "object_concept": {
                        "id": "foo"
                      },
                      "predicate": "hyponym"
                    }
                  ]
                }
              }
            }
          }
        },
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "concept_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"concept_relations\": [\n        {\n            \"object_concept\": {\n                \"id\": \"foo\"\n            },\n            \"predicate\": \"hyponym\"\n        }\n    ]\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/concepts/{concept_id}/relations' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Content-Type: application/json' \\\n--data '{\n  \"concept_relations\": [\n    {\n      \"object_concept\": {\n        \"id\": \"foo\"\n      },\n      \"predicate\": \"hyponym\"\n    }\n  ]\n}'"
          }
        ]
      },
      "delete": {
        "tags": [
          "Concepts > Concept Graph"
        ],
        "summary": "Delete Relation (hyponum)",
        "description": "This endpoint is used to remove a concept with hyponum.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `concept_id` | **string** | **Stores the concept ID** |\n| `ids` | **string** | **Stores the relation names** |",
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "concept_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"details\": \"concept relations [158b66413419406a8819dc3327e72ca2] deleted\",\n        \"req_id\": \"624f10de51590358159fb2f9809bb6ed\"\n    }\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff --request DELETE 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/concepts/{concept_id}/relations' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/datasets": {
      "post": {
        "tags": [
          "Datasets > Dataset Essentials"
        ],
        "summary": "Post Datasets",
        "description": "This endpoint allows you to create a dataset in an app.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `dataset` | **string** | **Stores the Dataset ID** |",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "example": {
                  "datasets": [
                    {
                      "id": "dataset-{{$timestamp}}",
                      "description": "This is a sample dataset",
                      "metadata": {
                        "time": "10:00 AM"
                      }
                    }
                  ]
                }
              }
            }
          }
        },
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "400": {
            "description": "Bad Request",
            "headers": {
              "Date": {
                "schema": {
                  "type": "string",
                  "example": "Fri, 03 Nov 2023 13:18:35 GMT"
                }
              },
              "Content-Type": {
                "schema": {
                  "type": "string",
                  "example": "application/json; charset=UTF-8"
                }
              },
              "Transfer-Encoding": {
                "schema": {
                  "type": "string",
                  "example": "chunked"
                }
              },
              "Connection": {
                "schema": {
                  "type": "string",
                  "example": "keep-alive"
                }
              },
              "Access-Control-Allow-Headers": {
                "schema": {
                  "type": "string",
                  "example": "Content-Type,Accept,X-Requested-With,Content-Type,Referer,Accept-Encoding,X-CSRF-Token,Authorization,X-Clarifai-Application-Id,X-Clarifai-REST-API-Key,X-Clarifai-Session-Token,X-Clarifai-Client,X-Clarifai-Site,X-RapidAPI-User,x-clarifai-request-id-prefix,x-request-id"
                }
              },
              "Access-Control-Allow-Methods": {
                "schema": {
                  "type": "string",
                  "example": "GET,HEAD,POST,PUT,PATCH,OPTIONS,DELETE"
                }
              },
              "Access-Control-Allow-Origin": {
                "schema": {
                  "type": "string",
                  "example": "*"
                }
              },
              "Grpc-Metadata-Content-Type": {
                "schema": {
                  "type": "string",
                  "example": "application/grpc"
                }
              },
              "X-Clarifai-Request-Id": {
                "schema": {
                  "type": "string",
                  "example": "b36d2221260d13598d9dd99e880eb1ce"
                }
              },
              "Strict-Transport-Security": {
                "schema": {
                  "type": "string",
                  "example": "max-age=15724800; includeSubDomains"
                }
              }
            },
            "content": {
              "application/json": {
                "schema": {
                  "type": "object"
                },
                "example": {
                  "status": {
                    "code": 10000,
                    "description": "Ok",
                    "req_id": "07b960b5b3d4adc93f872f5da6f49f83"
                  },
                  "datasets": [
                    {
                      "id": "image_dataset_5",
                      "created_at": "2023-11-09T09:11:52.378409492Z",
                      "modified_at": "2023-11-09T09:11:52.378409492Z",
                      "app_id": "named-entity-recognition",
                      "user_id": "a0btrubbaefn",
                      "description": "This is a sample image dataset",
                      "metadata": {
                        "src": "https://example_dataset.com"
                      },
                      "visibility": {
                        "gettable": 10
                      },
                      "default_annotation_filter": {
                        "id": "image_dataset_5-filter",
                        "created_at": "2023-11-09T09:11:52.378409492Z",
                        "modified_at": "2023-11-09T09:11:52.378409492Z",
                        "user_id": "a0btrubbaefn",
                        "app_id": "named-entity-recognition"
                      }
                    }
                  ]
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/datasets' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Content-Type: application/json' \\\n--data '{\n  \"datasets\": [\n    {\n      \"id\": \"dataset-{{$timestamp}}\",\n      \"description\": \"This is a sample dataset\",\n      \"metadata\": {\n        \"time\": \"10:00 AM\"\n      }\n    }\n  ]\n}'"
          }
        ]
      },
      "get": {
        "tags": [
          "Datasets > Dataset Essentials"
        ],
        "summary": "List Datasets",
        "description": "This endpoint retrieves a list of datasets available in the platform.",
        "parameters": [
          {
            "name": "page",
            "in": "query",
            "schema": {
              "type": "integer"
            },
            "example": "1"
          },
          {
            "name": "per_page",
            "in": "query",
            "schema": {
              "type": "integer"
            },
            "example": "100"
          },
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"959a47cafe967ae3352c6b179d6addee\"\n    },\n    \"datasets\": [\n        {\n            \"id\": \"public-dataset-kaustav\",\n            \"created_at\": \"2023-08-10T08:49:31.921106Z\",\n            \"modified_at\": \"2023-08-10T08:57:19.116825Z\",\n            \"app_id\": \"another-org-app\",\n            \"user_id\": \"test_org123\",\n            \"description\": \"This is the new foo dataset\",\n            \"metadata\": {},\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"default_annotation_filter\": {\n                \"id\": \"public-dataset-kaustav-filter\",\n                \"created_at\": \"2023-08-10T08:49:31.921106Z\",\n                \"modified_at\": \"2023-08-10T08:49:31.921106Z\",\n                \"user_id\": \"test_org123\",\n                \"app_id\": \"another-org-app\"\n            }\n        }\n    ]\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/datasets?page=1&per_page=100' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      },
      "patch": {
        "tags": [
          "Datasets > Dataset Essentials"
        ],
        "summary": "Patch Datasets With Default Annotation Filter",
        "description": "This endpoint allows you to update datasets with a default annotation filter .\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `dataset_id` | **string** | **Stores the Dataset ID** |",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "example": {
                  "datasets": [
                    {
                      "id": "{{dataset_id}}",
                      "description": "This is a sample dataset",
                      "metadata": {
                        "time": "12:00 PM"
                      },
                      "default_annotation_filter": {
                        "id": "{{annotation_filter_id}}"
                      }
                    }
                  ],
                  "action": "merge"
                }
              }
            }
          }
        },
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"d3aa8bd2543919409c56797c77cd7de6\"\n    },\n    \"datasets\": [\n        {\n            \"id\": \"dataset-1700567542\",\n            \"created_at\": \"2023-11-21T11:52:22.594537Z\",\n            \"modified_at\": \"2023-11-21T12:00:56.353221990Z\",\n            \"app_id\": \"app-food5\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"description\": \"This is a sample text dataset\",\n            \"metadata\": {\n                \"src\": \"https://example_text.com\",\n                \"time\": \"10:00 AM\"\n            },\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"default_annotation_filter\": {\n                \"id\": \"dataset-1700567542-filter\",\n                \"created_at\": \"2023-11-21T11:52:22.594537Z\",\n                \"modified_at\": \"2023-11-21T11:52:22.594537Z\",\n                \"user_id\": \"a0btrubbaefn\",\n                \"app_id\": \"app-food5\"\n            }\n        }\n    ]\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff --request PATCH 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/datasets' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Content-Type: application/json' \\\n--data '{\n  \"datasets\": [\n    {\n      \"id\": \"{{dataset_id}}\",\n      \"description\": \"This is a sample dataset\",\n      \"metadata\": {\n        \"time\": \"12:00 PM\"\n      },\n      \"default_annotation_filter\": {\n        \"id\": \"{{annotation_filter_id}}\"\n      }\n    }\n  ],\n  \"action\": \"merge\"\n}'"
          }
        ]
      },
      "delete": {
        "tags": [
          "Datasets > Dataset Essentials"
        ],
        "summary": "Delete Datasets",
        "description": "The `DeleteDatasets` endpoint allows you to delete one or multiple datasets from your app.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `dataset_id` | **string** | **Stores the Dataset ID** |",
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"details\": \"Dataset 'dataset-1700567542' deleted\",\n        \"req_id\": \"acb965ed60f8e706b83969529b614a9b\"\n    }\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff --request DELETE 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/datasets' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/datasets/{dataset_id}": {
      "get": {
        "tags": [
          "Datasets > Dataset Essentials"
        ],
        "summary": "Get Dataset",
        "description": "This endpoint allows users to retrieve a specific dataset from the app by providing a dataset ID.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `dataset_id` | **string** | **Stores the Dataset ID** |",
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "dataset_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"8e6b7d66329e37437896f27e546cd3ac\"\n    },\n    \"dataset\": {\n        \"id\": \"dataset-1700567542\",\n        \"created_at\": \"2023-11-21T11:52:22.594537Z\",\n        \"modified_at\": \"2023-11-21T11:52:40.597362Z\",\n        \"app_id\": \"app-food5\",\n        \"user_id\": \"a0btrubbaefn\",\n        \"description\": \"This is a sampledataset\",\n        \"metadata\": {\n            \"time\": \"10:00 AM\"\n        },\n        \"visibility\": {\n            \"gettable\": 10\n        },\n        \"default_annotation_filter\": {\n            \"id\": \"dataset-1700567542-filter\",\n            \"created_at\": \"2023-11-21T11:52:22.594537Z\",\n            \"modified_at\": \"2023-11-21T11:52:22.594537Z\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"app_id\": \"app-food5\"\n        },\n        \"version\": {\n            \"id\": \"dataset-version-1700567560\",\n            \"created_at\": \"2023-11-21T11:52:40.588864Z\",\n            \"modified_at\": \"2023-11-21T11:52:46.354908Z\",\n            \"app_id\": \"app-food5\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"dataset_id\": \"dataset-1700567542\",\n            \"annotation_filter_config\": {\n                \"annotation_filter\": {\n                    \"id\": \"dataset-1700567542-filter\",\n                    \"created_at\": \"2023-11-21T11:52:22.594537Z\",\n                    \"modified_at\": \"2023-11-21T11:52:22.594537Z\",\n                    \"user_id\": \"a0btrubbaefn\",\n                    \"app_id\": \"app-food5\"\n                }\n            },\n            \"status\": {\n                \"code\": 64015,\n                \"description\": \"Dataset version is ready to be used\"\n            },\n            \"description\": \"this ia sample description\",\n            \"metrics\": {\n                \"/\": {\n                    \"inputs_count\": \"0\",\n                    \"unlabeled_inputs_count\": \"0\",\n                    \"inputs_with_metadata_count\": \"0\",\n                    \"inputs_with_geo_count\": \"0\",\n                    \"regions_count\": \"0\",\n                    \"bounding_boxes_count\": \"0\",\n                    \"polygons_count\": \"0\",\n                    \"points_count\": \"0\",\n                    \"masks_count\": \"0\",\n                    \"frames_count\": \"0\",\n                    \"embeddings_count\": \"0\",\n                    \"positive_input_tags_count\": \"0\",\n                    \"positive_region_tags_count\": \"0\",\n                    \"positive_frame_tags_count\": \"0\"\n                }\n            },\n            \"export_info\": {\n                \"clarifai_data_protobuf\": {\n                    \"format\": 1,\n                    \"status\": {\n                        \"code\": 64200,\n                        \"description\": \"Dataset version export success\",\n                        \"percent_completed\": 100\n                    },\n                    \"url\": \"https://s3.amazonaws.com/clarifai-data-dumps/dev/app/b808f9f39531405d961b291b677311bd/dumps/3e6fd292993b404bbe86fbee1f30426e/exports/clarifai-data-protobuf.zip\",\n                    \"size\": \"184\",\n                    \"include_embeddings\": true\n                },\n                \"clarifai_data_json\": {\n                    \"format\": 3,\n                    \"status\": {\n                        \"code\": 64200,\n                        \"description\": \"Dataset version export success\",\n                        \"percent_completed\": 100\n                    },\n                    \"url\": \"https://s3.amazonaws.com/clarifai-data-dumps/dev/app/b808f9f39531405d961b291b677311bd/dumps/3e6fd292993b404bbe86fbee1f30426e/exports/clarifai-data-json.zip\",\n                    \"size\": \"180\",\n                    \"include_embeddings\": true\n                }\n            },\n            \"metadata\": {\n                \"time\": \"10:00 AM\"\n            },\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"embed_model_version_ids\": [\n                \"9fe2c8962c104327bc87b8f8104b161a\"\n            ]\n        }\n    }\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/datasets/{dataset_id}' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/datasets/{dataset_id}/inputs": {
      "post": {
        "tags": [
          "Datasets > Dataset Inputs"
        ],
        "summary": "Post Dataset Inputs Using Search",
        "description": "This API endpoint enables you to search and retrieve dataset inputs using specified search criteria.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `dataset_id` | **string** | **Stores the Dataset ID** |",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "example": {
                  "search": {
                    "query": {
                      "filters": [
                        {
                          "input": {
                            "status": {
                              "code": "INPUT_DOWNLOAD_SUCCESS"
                            }
                          }
                        }
                      ]
                    }
                  }
                }
              }
            }
          }
        },
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "dataset_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"b59458424b52cc71ad8f6a6e75e4772f\"\n    },\n    \"dataset_inputs\": [],\n    \"dataset_inputs_search_add_job\": {\n        \"id\": \"de12f9d2fabd4eacb1420415e5f2c04e\",\n        \"created_at\": \"2023-11-21T12:07:06.643648168Z\",\n        \"modified_at\": \"2023-11-21T12:07:06.643648168Z\",\n        \"status\": {\n            \"code\": 64000,\n            \"description\": \"Job is queued to be ran.\"\n        },\n        \"dataset_id\": \"dataset-1700568258\",\n        \"search\": {\n            \"query\": {\n                \"filters\": [\n                    {\n                        \"input\": {\n                            \"status\": {\n                                \"code\": 30000\n                            }\n                        }\n                    }\n                ]\n            }\n        }\n    }\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/datasets/{dataset_id}/inputs' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Content-Type: application/json' \\\n--data '{\n  \"search\": {\n    \"query\": {\n      \"filters\": [\n        {\n          \"input\": {\n            \"status\": {\n              \"code\": \"INPUT_DOWNLOAD_SUCCESS\"\n            }\n          }\n        }\n      ]\n    }\n  }\n}'"
          }
        ]
      },
      "get": {
        "tags": [
          "Datasets > Dataset Inputs"
        ],
        "summary": "List Dataset Inputs",
        "description": "The `ListDatasetInputs` endpoint allows you to retrieve a list of inputs associated with a dataset.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `dataset_id` | **string** | **Stores the Dataset ID** |",
        "parameters": [
          {
            "name": "page",
            "in": "query",
            "schema": {
              "type": "integer"
            },
            "example": "1"
          },
          {
            "name": "per_page",
            "in": "query",
            "schema": {
              "type": "integer"
            },
            "example": "100"
          },
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "dataset_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"33f56b80231f84552df6491d098da156\"\n    },\n    \"dataset_inputs\": [\n        {\n            \"created_at\": \"2023-11-21T12:06:16.220048Z\",\n            \"input\": {\n                \"id\": \"input1\",\n                \"data\": {\n                    \"image\": {\n                        \"url\": \"https://samples.clarifai.com/metro-north.jpg\",\n                        \"hosted\": {\n                            \"prefix\": \"https://data-dev.clarifai.com\",\n                            \"suffix\": \"users/a0btrubbaefn/apps/app-food5/inputs/image/140c856dc82565d2c4d6ea720fceff78\",\n                            \"sizes\": [\n                                \"orig\",\n                                \"tiny\",\n                                \"small\",\n                                \"large\"\n                            ],\n                            \"crossorigin\": \"use-credentials\"\n                        },\n                        \"image_info\": {\n                            \"width\": 512,\n                            \"height\": 384,\n                            \"format\": \"JPEG\",\n                            \"color_mode\": \"YUV\"\n                        }\n                    }\n                },\n                \"created_at\": \"2023-11-21T12:04:45.209215Z\",\n                \"modified_at\": \"2023-11-21T12:04:47.517983Z\",\n                \"status\": {\n                    \"code\": 30000,\n                    \"description\": \"Download complete\"\n                }\n            }\n        }\n    ]\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/datasets/{dataset_id}/inputs?page=1&per_page=100' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      },
      "delete": {
        "tags": [
          "Datasets > Dataset Inputs"
        ],
        "summary": "Delete Dataset Inputs",
        "description": "The `Delete Dataset Inputs` endpoint allows you to remove specific inputs from the dataset.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `dataset_id` | **string** | **Stores the Dataset ID** |\n| `input_id` | **string** | **Stores the input ID** |",
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "dataset_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"details\": \"Dataset input 'input1' deleted\",\n        \"req_id\": \"241874235113576547f0dfb7a740e2d5\"\n    }\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff --request DELETE 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/datasets/{dataset_id}/inputs' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/bulk_operations": {
      "post": {
        "tags": [
          "Datasets > Dataset Inputs"
        ],
        "summary": "Post Dataset Inputs Using Bulk Operations",
        "description": "This API endpoint enables you to add bulk inputs to the dataset.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `dataset_id` | **string** | **Stores the Dataset ID** |",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "example": {
                  "bulk_operations": [
                    {
                      "input_ids": {
                        "input_ids": [
                          "{{input_id}}"
                        ]
                      },
                      "operation": {
                        "add_to_dataset": {
                          "dataset_id": "{{dataset_id}}"
                        }
                      }
                    }
                  ]
                }
              }
            }
          }
        },
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"b1a49b2ebc6f99179feaef0b61881009\"\n    },\n    \"bulk_operation\": [\n        {\n            \"id\": \"d030d5d5fc2c48569b391be46f5730fa\",\n            \"input_ids\": {\n                \"input_ids\": [\n                    \"input1\"\n                ]\n            },\n            \"operation\": {},\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"status\": {\n                \"code\": 25402\n            },\n            \"progress\": {},\n            \"created_by\": \"a0btrubbaefn\",\n            \"created_at\": \"2023-11-23T15:07:46.230215012Z\",\n            \"last_modified_at\": \"2023-11-23T15:07:46.230215012Z\"\n        }\n    ]\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/bulk_operations' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Content-Type: application/json' \\\n--data '{\n  \"bulk_operations\": [\n    {\n      \"input_ids\": {\n        \"input_ids\": [\n          \"{{input_id}}\"\n        ]\n      },\n      \"operation\": {\n        \"add_to_dataset\": {\n          \"dataset_id\": \"{{dataset_id}}\"\n        }\n      }\n    }\n  ]\n}'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/datasets/{dataset_id}/inputs/{input_id}": {
      "get": {
        "tags": [
          "Datasets > Dataset Inputs"
        ],
        "summary": "Get Dataset Input",
        "description": "The `GetDatasetInput` endpoint allows you to retrieve inputs from the dataset.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `dataset_id` | **string** | **Stores the Dataset ID** |\n| `input_id` | **string** | **Stores the input ID** |",
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "dataset_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "input_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"c172d3c910f99e3c60512a4006bde46c\"\n    },\n    \"dataset_input\": {\n        \"created_at\": \"2023-11-21T12:06:16.220048Z\",\n        \"input\": {\n            \"id\": \"input1\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"https://samples.clarifai.com/metro-north.jpg\",\n                    \"hosted\": {\n                        \"prefix\": \"https://data-dev.clarifai.com\",\n                        \"suffix\": \"users/a0btrubbaefn/apps/app-food5/inputs/image/140c856dc82565d2c4d6ea720fceff78\",\n                        \"sizes\": [\n                            \"orig\",\n                            \"tiny\",\n                            \"small\",\n                            \"large\"\n                        ],\n                        \"crossorigin\": \"use-credentials\"\n                    },\n                    \"image_info\": {\n                        \"width\": 512,\n                        \"height\": 384,\n                        \"format\": \"JPEG\",\n                        \"color_mode\": \"YUV\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-21T12:04:45.209215Z\",\n            \"modified_at\": \"2023-11-21T12:04:47.517983Z\",\n            \"status\": {\n                \"code\": 30000,\n                \"description\": \"Download complete\"\n            }\n        }\n    }\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/datasets/{dataset_id}/inputs/{input_id}' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/datasets/{dataset_id}/versions": {
      "post": {
        "tags": [
          "Datasets > Dataset Versions"
        ],
        "summary": "Post Dataset Versions With Default Annotation Filter",
        "description": "This request allows you to create a new dataset version with a default annotation filter.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `dataset_id` | **string** | **Stores the Dataset ID** |",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "example": {
                  "dataset_versions": [
                    {
                      "id": "dataset-version-{{$timestamp}}",
                      "description": "dataset version description"
                    }
                  ]
                }
              }
            }
          }
        },
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "dataset_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"76765abe8b71f585040dc426a29c697a\"\n    },\n    \"dataset_versions\": [\n        {\n            \"id\": \"dataset-version-1700568177\",\n            \"created_at\": \"2023-11-21T12:02:57.664331763Z\",\n            \"modified_at\": \"2023-11-21T12:02:57.664331763Z\",\n            \"app_id\": \"app-food5\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"dataset_id\": \"dataset-1700568138\",\n            \"annotation_filter_config\": {\n                \"annotation_filter\": {\n                    \"id\": \"dataset-1700568138-filter\",\n                    \"created_at\": \"2023-11-21T12:02:18.000612Z\",\n                    \"modified_at\": \"2023-11-21T12:02:18.000612Z\",\n                    \"user_id\": \"a0btrubbaefn\",\n                    \"app_id\": \"app-food5\"\n                }\n            },\n            \"status\": {\n                \"code\": 64005,\n                \"description\": \"Dataset version is pending to be processed\"\n            },\n            \"description\": \"dataset version description\",\n            \"metadata\": {},\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"embed_model_version_ids\": [\n                \"9fe2c8962c104327bc87b8f8104b161a\"\n            ]\n        }\n    ]\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/datasets/{dataset_id}/versions' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Content-Type: application/json' \\\n--data '{\n  \"dataset_versions\": [\n    {\n      \"id\": \"dataset-version-{{$timestamp}}\",\n      \"description\": \"dataset version description\"\n    }\n  ]\n}'"
          }
        ]
      },
      "get": {
        "tags": [
          "Datasets > Dataset Versions"
        ],
        "summary": "List Dataset Versions",
        "description": "This endpoint allows you to retrieve a list of `DatasetVersions` available in the app.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `dataset_id` | **string** | **Stores the Dataset ID** |",
        "parameters": [
          {
            "name": "page",
            "in": "query",
            "schema": {
              "type": "integer"
            },
            "example": "1"
          },
          {
            "name": "per_page",
            "in": "query",
            "schema": {
              "type": "integer"
            },
            "example": "100"
          },
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "dataset_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"e8c2e045b8790a706e574614f826ce18\"\n    },\n    \"dataset_versions\": [\n        {\n            \"id\": \"dataset-version-1700568177\",\n            \"created_at\": \"2023-11-21T12:02:57.664331Z\",\n            \"modified_at\": \"2023-11-21T12:02:58.667045Z\",\n            \"app_id\": \"app-food5\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"dataset_id\": \"dataset-1700568138\",\n            \"annotation_filter_config\": {\n                \"annotation_filter\": {\n                    \"id\": \"dataset-1700568138-filter\",\n                    \"created_at\": \"2023-11-21T12:02:18.000612Z\",\n                    \"modified_at\": \"2023-11-21T12:02:18.000612Z\",\n                    \"user_id\": \"a0btrubbaefn\",\n                    \"app_id\": \"app-food5\"\n                }\n            },\n            \"status\": {\n                \"code\": 64015,\n                \"description\": \"Dataset version is ready to be used\"\n            },\n            \"description\": \"dataset version description\",\n            \"metrics\": {\n                \"/\": {\n                    \"inputs_count\": \"0\",\n                    \"unlabeled_inputs_count\": \"0\",\n                    \"inputs_with_metadata_count\": \"0\",\n                    \"inputs_with_geo_count\": \"0\",\n                    \"regions_count\": \"0\",\n                    \"bounding_boxes_count\": \"0\",\n                    \"polygons_count\": \"0\",\n                    \"points_count\": \"0\",\n                    \"masks_count\": \"0\",\n                    \"frames_count\": \"0\",\n                    \"embeddings_count\": \"0\",\n                    \"positive_input_tags_count\": \"0\",\n                    \"positive_region_tags_count\": \"0\",\n                    \"positive_frame_tags_count\": \"0\"\n                }\n            },\n            \"metadata\": {},\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"embed_model_version_ids\": [\n                \"9fe2c8962c104327bc87b8f8104b161a\"\n            ]\n        },\n        {\n            \"id\": \"dataset-version-1700568163\",\n            \"created_at\": \"2023-11-21T12:02:44.585245Z\",\n            \"modified_at\": \"2023-11-21T12:02:45.720808Z\",\n            \"app_id\": \"app-food5\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"dataset_id\": \"dataset-1700568138\",\n            \"annotation_filter_config\": {\n                \"annotation_filter\": {\n                    \"id\": \"dataset-1700568138-filter\",\n                    \"created_at\": \"2023-11-21T12:02:18.000612Z\",\n                    \"modified_at\": \"2023-11-21T12:02:18.000612Z\",\n                    \"user_id\": \"a0btrubbaefn\",\n                    \"app_id\": \"app-food5\"\n                }\n            },\n            \"status\": {\n                \"code\": 64015,\n                \"description\": \"Dataset version is ready to be used\"\n            },\n            \"description\": \"This is a sample image dataset\",\n            \"metrics\": {\n                \"/\": {\n                    \"inputs_count\": \"0\",\n                    \"unlabeled_inputs_count\": \"0\",\n                    \"inputs_with_metadata_count\": \"0\",\n                    \"inputs_with_geo_count\": \"0\",\n                    \"regions_count\": \"0\",\n                    \"bounding_boxes_count\": \"0\",\n                    \"polygons_count\": \"0\",\n                    \"points_count\": \"0\",\n                    \"masks_count\": \"0\",\n                    \"frames_count\": \"0\",\n                    \"embeddings_count\": \"0\",\n                    \"positive_input_tags_count\": \"0\",\n                    \"positive_region_tags_count\": \"0\",\n                    \"positive_frame_tags_count\": \"0\"\n                }\n            },\n            \"metadata\": {\n                \"src\": \"https://example_dataset.com\"\n            },\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"embed_model_version_ids\": [\n                \"9fe2c8962c104327bc87b8f8104b161a\"\n            ]\n        },\n        {\n            \"id\": \"dataset-version-1700568146\",\n            \"created_at\": \"2023-11-21T12:02:26.709523Z\",\n            \"modified_at\": \"2023-11-21T12:02:29.925102Z\",\n            \"app_id\": \"app-food5\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"dataset_id\": \"dataset-1700568138\",\n            \"annotation_filter_config\": {\n                \"annotation_filter\": {\n                    \"id\": \"dataset-1700568138-filter\",\n                    \"created_at\": \"2023-11-21T12:02:18.000612Z\",\n                    \"modified_at\": \"2023-11-21T12:02:18.000612Z\",\n                    \"user_id\": \"a0btrubbaefn\",\n                    \"app_id\": \"app-food5\"\n                }\n            },\n            \"status\": {\n                \"code\": 64015,\n                \"description\": \"Dataset version is ready to be used\"\n            },\n            \"description\": \"This is a sample text dataset\",\n            \"metrics\": {\n                \"/\": {\n                    \"inputs_count\": \"0\",\n                    \"unlabeled_inputs_count\": \"0\",\n                    \"inputs_with_metadata_count\": \"0\",\n                    \"inputs_with_geo_count\": \"0\",\n                    \"regions_count\": \"0\",\n                    \"bounding_boxes_count\": \"0\",\n                    \"polygons_count\": \"0\",\n                    \"points_count\": \"0\",\n                    \"masks_count\": \"0\",\n                    \"frames_count\": \"0\",\n                    \"embeddings_count\": \"0\",\n                    \"positive_input_tags_count\": \"0\",\n                    \"positive_region_tags_count\": \"0\",\n                    \"positive_frame_tags_count\": \"0\"\n                }\n            },\n            \"metadata\": {\n                \"src\": \"https://example_text.com\"\n            },\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"embed_model_version_ids\": [\n                \"9fe2c8962c104327bc87b8f8104b161a\"\n            ]\n        },\n        {\n            \"id\": \"dataset-version-1700568141\",\n            \"created_at\": \"2023-11-21T12:02:22.587696Z\",\n            \"modified_at\": \"2023-11-21T12:02:25.533335Z\",\n            \"app_id\": \"app-food5\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"dataset_id\": \"dataset-1700568138\",\n            \"annotation_filter_config\": {\n                \"annotation_filter\": {\n                    \"id\": \"dataset-1700568138-filter\",\n                    \"created_at\": \"2023-11-21T12:02:18.000612Z\",\n                    \"modified_at\": \"2023-11-21T12:02:18.000612Z\",\n                    \"user_id\": \"a0btrubbaefn\",\n                    \"app_id\": \"app-food5\"\n                }\n            },\n            \"status\": {\n                \"code\": 64015,\n                \"description\": \"Dataset version is ready to be used\"\n            },\n            \"description\": \"this ia sample description\",\n            \"metrics\": {\n                \"/\": {\n                    \"inputs_count\": \"0\",\n                    \"unlabeled_inputs_count\": \"0\",\n                    \"inputs_with_metadata_count\": \"0\",\n                    \"inputs_with_geo_count\": \"0\",\n                    \"regions_count\": \"0\",\n                    \"bounding_boxes_count\": \"0\",\n                    \"polygons_count\": \"0\",\n                    \"points_count\": \"0\",\n                    \"masks_count\": \"0\",\n                    \"frames_count\": \"0\",\n                    \"embeddings_count\": \"0\",\n                    \"positive_input_tags_count\": \"0\",\n                    \"positive_region_tags_count\": \"0\",\n                    \"positive_frame_tags_count\": \"0\"\n                }\n            },\n            \"metadata\": {\n                \"time\": \"10:00 AM\"\n            },\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"embed_model_version_ids\": [\n                \"9fe2c8962c104327bc87b8f8104b161a\"\n            ]\n        }\n    ]\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/datasets/{dataset_id}/versions?page=1&per_page=100' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      },
      "patch": {
        "tags": [
          "Datasets > Dataset Versions"
        ],
        "summary": "Patch Dataset Versions",
        "description": "This request allows you to update specific versions of datasets in your application.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `dataset_id` | **string** | **Stores the Dataset ID** |\n| `dataset_version_id` | **string** | **Stores a specific version number of the dataset** |",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "example": {
                  "dataset_versions": [
                    {
                      "id": "{{dataset_version_id}}",
                      "description": "dataset version updated description"
                    }
                  ],
                  "action": "overwrite"
                }
              }
            }
          }
        },
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "dataset_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"ac83a6d4265cf2109598162680cf40d8\"\n    },\n    \"dataset_versions\": [\n        {\n            \"id\": \"dataset-version-1700568177\",\n            \"created_at\": \"2023-11-21T12:02:57.664331Z\",\n            \"modified_at\": \"2023-11-21T12:03:40.141993675Z\",\n            \"app_id\": \"app-food5\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"dataset_id\": \"dataset-1700568138\",\n            \"annotation_filter_config\": {\n                \"annotation_filter\": {\n                    \"id\": \"dataset-1700568138-filter\",\n                    \"created_at\": \"2023-11-21T12:02:18.000612Z\",\n                    \"modified_at\": \"2023-11-21T12:02:18.000612Z\",\n                    \"user_id\": \"a0btrubbaefn\",\n                    \"app_id\": \"app-food5\"\n                }\n            },\n            \"status\": {\n                \"code\": 64015,\n                \"description\": \"Dataset version is ready to be used\"\n            },\n            \"description\": \"dataset version updated description\",\n            \"metrics\": {\n                \"/\": {\n                    \"inputs_count\": \"0\",\n                    \"unlabeled_inputs_count\": \"0\",\n                    \"inputs_with_metadata_count\": \"0\",\n                    \"inputs_with_geo_count\": \"0\",\n                    \"regions_count\": \"0\",\n                    \"bounding_boxes_count\": \"0\",\n                    \"polygons_count\": \"0\",\n                    \"points_count\": \"0\",\n                    \"masks_count\": \"0\",\n                    \"frames_count\": \"0\",\n                    \"embeddings_count\": \"0\",\n                    \"positive_input_tags_count\": \"0\",\n                    \"positive_region_tags_count\": \"0\",\n                    \"positive_frame_tags_count\": \"0\"\n                }\n            },\n            \"metadata\": {},\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"embed_model_version_ids\": [\n                \"9fe2c8962c104327bc87b8f8104b161a\"\n            ]\n        }\n    ]\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff --request PATCH 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/datasets/{dataset_id}/versions' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Content-Type: application/json' \\\n--data '{\n  \"dataset_versions\": [\n    {\n      \"id\": \"{{dataset_version_id}}\",\n      \"description\": \"dataset version updated description\"\n    }\n  ],\n  \"action\": \"overwrite\"\n}'"
          }
        ]
      },
      "delete": {
        "tags": [
          "Datasets > Dataset Versions"
        ],
        "summary": "Delete Dataset Versions",
        "description": "This endpoint allows you to remove specific versions of a dataset from your app.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `dataset_id` | **string** | **Stores the Dataset ID** |\n| `dataset_version_id` | **string** | **Stores a specific version number of the dataset** |",
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "dataset_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"details\": \"Dataset version 'dataset-version-1700568177' deleted\",\n        \"req_id\": \"e60d98ec521a6b36c3ec43c08ed424f4\"\n    }\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff --request DELETE 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/datasets/{dataset_id}/versions' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/datasets/{dataset_id}/versions/{dataset_version_id}": {
      "get": {
        "tags": [
          "Datasets > Dataset Versions"
        ],
        "summary": "Get Dataset Version",
        "description": "This endpoint allows you to retrieve details about a specific `DatasetVersion` using its unique identifier.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `dataset_id` | **string** | **Stores the Dataset ID** |\n| `dataset_version_id` | **string** | **Stores a specific version number of the dataset** |",
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "dataset_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "dataset_version_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"490e13eeceb65c918b8c65fb50a0b162\"\n    },\n    \"dataset_version\": {\n        \"id\": \"dataset-version-1700568177\",\n        \"created_at\": \"2023-11-21T12:02:57.664331Z\",\n        \"modified_at\": \"2023-11-21T12:02:58.667045Z\",\n        \"app_id\": \"app-food5\",\n        \"user_id\": \"a0btrubbaefn\",\n        \"dataset_id\": \"dataset-1700568138\",\n        \"annotation_filter_config\": {\n            \"annotation_filter\": {\n                \"id\": \"dataset-1700568138-filter\",\n                \"created_at\": \"2023-11-21T12:02:18.000612Z\",\n                \"modified_at\": \"2023-11-21T12:02:18.000612Z\",\n                \"user_id\": \"a0btrubbaefn\",\n                \"app_id\": \"app-food5\"\n            }\n        },\n        \"status\": {\n            \"code\": 64015,\n            \"description\": \"Dataset version is ready to be used\"\n        },\n        \"description\": \"dataset version description\",\n        \"metrics\": {\n            \"/\": {\n                \"inputs_count\": \"0\",\n                \"unlabeled_inputs_count\": \"0\",\n                \"inputs_with_metadata_count\": \"0\",\n                \"inputs_with_geo_count\": \"0\",\n                \"regions_count\": \"0\",\n                \"bounding_boxes_count\": \"0\",\n                \"polygons_count\": \"0\",\n                \"points_count\": \"0\",\n                \"masks_count\": \"0\",\n                \"frames_count\": \"0\",\n                \"embeddings_count\": \"0\",\n                \"positive_input_tags_count\": \"0\",\n                \"positive_region_tags_count\": \"0\",\n                \"positive_frame_tags_count\": \"0\"\n            }\n        },\n        \"metadata\": {},\n        \"visibility\": {\n            \"gettable\": 10\n        },\n        \"embed_model_version_ids\": [\n            \"9fe2c8962c104327bc87b8f8104b161a\"\n        ]\n    }\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/datasets/{dataset_id}/versions/{dataset_version_id}' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/datasets/{dataset_id}/versions/{dataset_version_id}/exports": {
      "put": {
        "tags": [
          "Datasets > Dataset Versions"
        ],
        "summary": "Put Dataset Version Exports",
        "description": "This endpoint allows you to update the export settings for a specific version of a dataset.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `dataset_id` | **string** | **Stores the Dataset ID** |\n| `dataset_version_id` | **string** | **Stores a specific version number of the dataset** |",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "example": {
                  "exports": [
                    {
                      "format": "CLARIFAI_DATA_PROTOBUF",
                      "include_embeddings": true
                    },
                    {
                      "format": "CLARIFAI_DATA_JSON",
                      "include_embeddings": true
                    }
                  ]
                }
              }
            }
          }
        },
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "dataset_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "dataset_version_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "examples": {
                  "example-0": {
                    "summary": "PutDatasetVersionExports_text",
                    "value": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"fc9d14334433bacdb4ad548bab8e2437\"\n    },\n    \"exports\": [\n        {\n            \"format\": 1,\n            \"status\": {\n                \"code\": 64200,\n                \"description\": \"Dataset version export success\",\n                \"percent_completed\": 100\n            },\n            \"url\": \"https://s3.amazonaws.com/clarifai-data-dumps/dev/app/b808f9f39531405d961b291b677311bd/dumps/3e6fd292993b404bbe86fbee1f30426e/exports/clarifai-data-protobuf.zip\",\n            \"size\": \"184\",\n            \"include_embeddings\": true\n        }\n    ]\n}"
                  },
                  "example-1": {
                    "summary": "PutDatasetVersionExports_text Copy",
                    "value": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"85859e0f6ef9278ac979848d11969c95\"\n    },\n    \"exports\": [\n        {\n            \"format\": 3,\n            \"status\": {\n                \"code\": 64200,\n                \"description\": \"Dataset version export success\",\n                \"percent_completed\": 100\n            },\n            \"url\": \"https://s3.amazonaws.com/clarifai-data-dumps/dev/app/b808f9f39531405d961b291b677311bd/dumps/3e6fd292993b404bbe86fbee1f30426e/exports/clarifai-data-json.zip\",\n            \"size\": \"180\",\n            \"include_embeddings\": true\n        }\n    ]\n}"
                  }
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff --request PUT 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/datasets/{dataset_id}/versions/{dataset_version_id}/exports' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Content-Type: application/json' \\\n--data '{\n  \"exports\": [\n    {\n      \"format\": \"CLARIFAI_DATA_PROTOBUF\",\n      \"include_embeddings\": true\n    },\n    {\n      \"format\": \"CLARIFAI_DATA_JSON\",\n      \"include_embeddings\": true\n    }\n  ]\n}'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/inputs/": {
      "get": {
        "tags": [
          "Search > Basic Search Walkthrough"
        ],
        "summary": "Get All Inputs",
        "description": "This request is used to display all inputs present in the current application.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |",
        "parameters": [
          {
            "name": "per_page",
            "in": "query",
            "schema": {
              "type": "integer"
            },
            "example": "1000"
          },
          {
            "name": "page",
            "in": "query",
            "schema": {
              "type": "integer"
            },
            "example": "1"
          },
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"2893fa9601755ef816d503607cf4fcbb\"\n    },\n    \"inputs\": [\n        {\n            \"id\": \"7\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://s7d1.scene7.com/is/image/BedBathandBeyond/56879143899890p\",\n                    \"hosted\": {\n                        \"prefix\": \"https://data-dev.clarifai.com\",\n                        \"suffix\": \"users/a0btrubbaefn/apps/test-app-1701866015/inputs/image/151ba0b95ded7054437f44a7595e3736\",\n                        \"sizes\": [\n                            \"orig\",\n                            \"tiny\",\n                            \"small\",\n                            \"large\"\n                        ],\n                        \"crossorigin\": \"use-credentials\"\n                    },\n                    \"image_info\": {\n                        \"width\": 400,\n                        \"height\": 400,\n                        \"format\": \"JPEG\",\n                        \"color_mode\": \"YUV\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-12-06T12:34:45.593192Z\",\n            \"modified_at\": \"2023-12-06T12:34:50.627841Z\",\n            \"status\": {\n                \"code\": 30000,\n                \"description\": \"Download complete\"\n            }\n        },\n        {\n            \"id\": \"6\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/EnrVc0B.jpg\",\n                    \"hosted\": {\n                        \"prefix\": \"https://data-dev.clarifai.com\",\n                        \"suffix\": \"users/a0btrubbaefn/apps/test-app-1701866015/inputs/image/5fcff2ae1739cd5964d75bd258cc4388\",\n                        \"sizes\": [\n                            \"orig\",\n                            \"tiny\",\n                            \"small\",\n                            \"large\"\n                        ],\n                        \"crossorigin\": \"use-credentials\"\n                    },\n                    \"image_info\": {\n                        \"width\": 1200,\n                        \"height\": 900,\n                        \"format\": \"JPEG\",\n                        \"color_mode\": \"YUV\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-12-06T12:34:45.593192Z\",\n            \"modified_at\": \"2023-12-06T12:34:50.627841Z\",\n            \"status\": {\n                \"code\": 30000,\n                \"description\": \"Download complete\"\n            }\n        },\n        {\n            \"id\": \"5\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/eXCE9mf.jpg\",\n                    \"hosted\": {\n                        \"prefix\": \"https://data-dev.clarifai.com\",\n                        \"suffix\": \"users/a0btrubbaefn/apps/test-app-1701866015/inputs/image/a6f72e75368bdc556b82d829417c53b1\",\n                        \"sizes\": [\n                            \"orig\",\n                            \"tiny\",\n                            \"small\",\n                            \"large\"\n                        ],\n                        \"crossorigin\": \"use-credentials\"\n                    },\n                    \"image_info\": {\n                        \"width\": 1280,\n                        \"height\": 720,\n                        \"format\": \"JPEG\",\n                        \"color_mode\": \"YUV\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-12-06T12:34:45.593192Z\",\n            \"modified_at\": \"2023-12-06T12:34:50.627841Z\",\n            \"status\": {\n                \"code\": 30000,\n                \"description\": \"Download complete\"\n            }\n        },\n        {\n            \"id\": \"4\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/GeMQsiQ.jpg\",\n                    \"hosted\": {\n                        \"prefix\": \"https://data-dev.clarifai.com\",\n                        \"suffix\": \"users/a0btrubbaefn/apps/test-app-1701866015/inputs/image/ce718f0ca1bdcac7f36bcc871fa68001\",\n                        \"sizes\": [\n                            \"orig\",\n                            \"tiny\",\n                            \"small\",\n                            \"large\"\n                        ],\n                        \"crossorigin\": \"use-credentials\"\n                    },\n                    \"image_info\": {\n                        \"width\": 4096,\n                        \"height\": 3040,\n                        \"format\": \"JPEG\",\n                        \"color_mode\": \"YUV\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-12-06T12:34:45.593192Z\",\n            \"modified_at\": \"2023-12-06T12:34:50.627841Z\",\n            \"status\": {\n                \"code\": 30000,\n                \"description\": \"Download complete\"\n            }\n        },\n        {\n            \"id\": \"3\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/9Knw6RS.jpg\",\n                    \"hosted\": {\n                        \"prefix\": \"https://data-dev.clarifai.com\",\n                        \"suffix\": \"users/a0btrubbaefn/apps/test-app-1701866015/inputs/image/d9b6a576e9f5975db07556c882d32dc7\",\n                        \"sizes\": [\n                            \"orig\",\n                            \"tiny\",\n                            \"small\",\n                            \"large\"\n                        ],\n                        \"crossorigin\": \"use-credentials\"\n                    },\n                    \"image_info\": {\n                        \"width\": 1600,\n                        \"height\": 1032,\n                        \"format\": \"JPEG\",\n                        \"color_mode\": \"YUV\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-12-06T12:34:45.593192Z\",\n            \"modified_at\": \"2023-12-06T12:34:50.627841Z\",\n            \"status\": {\n                \"code\": 30000,\n                \"description\": \"Download complete\"\n            }\n        },\n        {\n            \"id\": \"2\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/It5JRaj.jpg\",\n                    \"hosted\": {\n                        \"prefix\": \"https://data-dev.clarifai.com\",\n                        \"suffix\": \"users/a0btrubbaefn/apps/test-app-1701866015/inputs/image/2ef6a4da17cdd348a5166fd6f30db3e2\",\n                        \"sizes\": [\n                            \"orig\",\n                            \"tiny\",\n                            \"small\",\n                            \"large\"\n                        ],\n                        \"crossorigin\": \"use-credentials\"\n                    },\n                    \"image_info\": {\n                        \"width\": 768,\n                        \"height\": 1024,\n                        \"format\": \"JPEG\",\n                        \"color_mode\": \"YUV\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-12-06T12:34:45.593192Z\",\n            \"modified_at\": \"2023-12-06T12:34:50.627841Z\",\n            \"status\": {\n                \"code\": 30000,\n                \"description\": \"Download complete\"\n            }\n        },\n        {\n            \"id\": \"1\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/HEoT5xR.png\",\n                    \"hosted\": {\n                        \"prefix\": \"https://data-dev.clarifai.com\",\n                        \"suffix\": \"users/a0btrubbaefn/apps/test-app-1701866015/inputs/image/89f4b090a85df4802d6cf5b767a66f37\",\n                        \"sizes\": [\n                            \"orig\",\n                            \"tiny\",\n                            \"small\",\n                            \"large\"\n                        ],\n                        \"crossorigin\": \"use-credentials\"\n                    },\n                    \"image_info\": {\n                        \"width\": 934,\n                        \"height\": 587,\n                        \"format\": \"PNG\",\n                        \"color_mode\": \"RGB\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-12-06T12:34:45.593192Z\",\n            \"modified_at\": \"2023-12-06T12:34:50.627841Z\",\n            \"status\": {\n                \"code\": 30000,\n                \"description\": \"Download complete\"\n            }\n        }\n    ]\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/inputs?per_page=1000&page=1' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      },
      "delete": {
        "tags": [
          "Inputs"
        ],
        "summary": "Delete Batch By IDs",
        "description": "This feature is particularly useful when dealing with large datasets or when batch deletion operations are required. You can remove the inputs in batches by providing the input ID's.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| **`ids`** | **array of strings** | **Stores the Input IDs** |",
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"9f63c351c3488fd89cee3e27fad75e69\"\n    }\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff --request DELETE 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/inputs' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/inputs/status": {
      "get": {
        "tags": [
          "Inputs"
        ],
        "summary": "Inputs Status",
        "description": "This API endpoint is used to retrieve input status from the app.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application name** |",
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"74821832c98d53bf4e03c8bd15c3e09b\"\n    },\n    \"counts\": {\n        \"processed\": 161,\n        \"to_process\": 0,\n        \"errors\": 0,\n        \"processing\": 0,\n        \"reindexed\": 0,\n        \"to_reindex\": 0,\n        \"reindex_errors\": 0,\n        \"reindexing\": 0\n    }\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/inputs/status' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/inputs/stream": {
      "get": {
        "tags": [
          "Inputs"
        ],
        "summary": "Stream All",
        "description": "This endpoint streams all the inputs in an app and is more efficient at pagination than the default /inputs request which is efficient for first few pages only.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| **`last_id`** | **string** | **Last Input id of previous page** |",
        "parameters": [
          {
            "name": "per_page",
            "in": "query",
            "schema": {
              "type": "integer"
            },
            "example": "5"
          },
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "examples": {
                  "example-0": {
                    "summary": "Stream All",
                    "value": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"6236d6ed34bf5bcc205c29e54facd3c8\"\n    },\n    \"inputs\": [\n        {\n            \"id\": \"input1\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"https://samples.clarifai.com/metro-north.jpg\",\n                    \"hosted\": {\n                        \"prefix\": \"https://data-dev.clarifai.com\",\n                        \"suffix\": \"users/a0btrubbaefn/apps/app-food5/inputs/image/140c856dc82565d2c4d6ea720fceff78\",\n                        \"sizes\": [\n                            \"orig\",\n                            \"tiny\",\n                            \"small\",\n                            \"large\"\n                        ],\n                        \"crossorigin\": \"use-credentials\"\n                    },\n                    \"image_info\": {\n                        \"width\": 512,\n                        \"height\": 384,\n                        \"format\": \"JPEG\",\n                        \"color_mode\": \"YUV\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-21T12:04:45.209215Z\",\n            \"modified_at\": \"2023-11-21T12:04:47.517983Z\",\n            \"status\": {\n                \"code\": 30000,\n                \"description\": \"Download complete\"\n            }\n        },\n        {\n            \"id\": \"6aa4f77143824bb2b18a9e51d56f0846\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"https://samples.clarifai.com/metro-north.jpg\",\n                    \"hosted\": {\n                        \"prefix\": \"https://data-dev.clarifai.com\",\n                        \"suffix\": \"users/a0btrubbaefn/apps/app-food5/inputs/image/140c856dc82565d2c4d6ea720fceff78\",\n                        \"sizes\": [\n                            \"orig\",\n                            \"tiny\",\n                            \"small\",\n                            \"large\"\n                        ],\n                        \"crossorigin\": \"use-credentials\"\n                    },\n                    \"image_info\": {\n                        \"width\": 512,\n                        \"height\": 384,\n                        \"format\": \"JPEG\",\n                        \"color_mode\": \"YUV\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:19:33.906800Z\",\n            \"modified_at\": \"2023-11-22T07:19:35.731205Z\",\n            \"status\": {\n                \"code\": 30000,\n                \"description\": \"Download complete\"\n            }\n        }\n    ]\n}"
                  },
                  "example-1": {
                    "summary": "Stream All Next Page",
                    "value": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"27558885a5f24f8347642ab4d2a5f9e9\"\n    },\n    \"inputs\": [\n        {\n            \"id\": \"input1\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"https://samples.clarifai.com/metro-north.jpg\",\n                    \"hosted\": {\n                        \"prefix\": \"https://data-dev.clarifai.com\",\n                        \"suffix\": \"users/a0btrubbaefn/apps/app-food5/inputs/image/140c856dc82565d2c4d6ea720fceff78\",\n                        \"sizes\": [\n                            \"orig\",\n                            \"tiny\",\n                            \"small\",\n                            \"large\"\n                        ],\n                        \"crossorigin\": \"use-credentials\"\n                    },\n                    \"image_info\": {\n                        \"width\": 512,\n                        \"height\": 384,\n                        \"format\": \"JPEG\",\n                        \"color_mode\": \"YUV\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-21T12:04:45.209215Z\",\n            \"modified_at\": \"2023-11-21T12:04:47.517983Z\",\n            \"status\": {\n                \"code\": 30000,\n                \"description\": \"Download complete\"\n            }\n        },\n        {\n            \"id\": \"6aa4f77143824bb2b18a9e51d56f0846\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"https://samples.clarifai.com/metro-north.jpg\",\n                    \"hosted\": {\n                        \"prefix\": \"https://data-dev.clarifai.com\",\n                        \"suffix\": \"users/a0btrubbaefn/apps/app-food5/inputs/image/140c856dc82565d2c4d6ea720fceff78\",\n                        \"sizes\": [\n                            \"orig\",\n                            \"tiny\",\n                            \"small\",\n                            \"large\"\n                        ],\n                        \"crossorigin\": \"use-credentials\"\n                    },\n                    \"image_info\": {\n                        \"width\": 512,\n                        \"height\": 384,\n                        \"format\": \"JPEG\",\n                        \"color_mode\": \"YUV\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:19:33.906800Z\",\n            \"modified_at\": \"2023-11-22T07:19:35.731205Z\",\n            \"status\": {\n                \"code\": 30000,\n                \"description\": \"Download complete\"\n            }\n        }\n    ]\n}"
                  }
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/inputs/stream?per_page=5' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/inputs/{input_id}": {
      "get": {
        "tags": [
          "Inputs"
        ],
        "summary": "Get Input By ID",
        "description": "By using this endpoint users can retrieve input by specifying the input ID.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `input` | **string** | **Stores the Input ID** |",
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "input_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"31e43d802c3c64dd0a1e9b217a140ba8\"\n    },\n    \"input\": {\n        \"id\": \"6aa4f77143824bb2b18a9e51d56f0846\",\n        \"data\": {\n            \"image\": {\n                \"url\": \"https://samples.clarifai.com/metro-north.jpg\",\n                \"hosted\": {\n                    \"prefix\": \"https://data-dev.clarifai.com\",\n                    \"suffix\": \"users/a0btrubbaefn/apps/app-food5/inputs/image/140c856dc82565d2c4d6ea720fceff78\",\n                    \"sizes\": [\n                        \"orig\",\n                        \"tiny\",\n                        \"small\",\n                        \"large\"\n                    ],\n                    \"crossorigin\": \"use-credentials\"\n                },\n                \"image_info\": {\n                    \"width\": 512,\n                    \"height\": 384,\n                    \"format\": \"JPEG\",\n                    \"color_mode\": \"YUV\"\n                }\n            }\n        },\n        \"created_at\": \"2023-11-22T07:19:33.906800Z\",\n        \"modified_at\": \"2023-11-22T07:19:35.731205Z\",\n        \"status\": {\n            \"code\": 30000,\n            \"description\": \"Download complete\"\n        }\n    }\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/inputs/{input_id}' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      },
      "delete": {
        "tags": [
          "Inputs"
        ],
        "summary": "Delete By ID",
        "description": "This endpoint allows you to remove specific inputs fro the app.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `input` | **string** | **Stores the Input ID** |",
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "input_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"45c8006598c920e2e0077a6bbb6c7d0b\"\n    }\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff --request DELETE 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/inputs/{input_id}' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/inputs": {
      "post": {
        "tags": [
          "Search > Filter Annotations"
        ],
        "summary": "Add Images In General App For Annotation Search",
        "description": "This request adds images to the General App in Clarifai for annotation search\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `id` | **string** | **Stores the concept ID** |\n| `value` | **string** | **Stores the concept value** |\n| `url` | **string** | **Stores the image URL** |",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "example": {
                  "inputs": [
                    {
                      "id": "0",
                      "data": {
                        "image": {
                          "url": "https://samples.clarifai.com/search-test-data/img-db8ebbc0-c294-4ec6-879f-b97da05c7ea8.jpg"
                        },
                        "concepts": [
                          {
                            "id": "test-concept",
                            "value": 1
                          }
                        ]
                      }
                    }
                  ]
                }
              }
            }
          }
        },
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"details\": \"All inputs successfully added\",\n        \"req_id\": \"ff8fb7504fb7de76e5326c9de367bcc0\"\n    },\n    \"inputs\": [\n        {\n            \"id\": \"0\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"https://samples.clarifai.com/search-test-data/img-db8ebbc0-c294-4ec6-879f-b97da05c7ea8.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                },\n                \"concepts\": [\n                    {\n                        \"id\": \"test-concept\",\n                        \"name\": \"test-concept\",\n                        \"value\": 1,\n                        \"app_id\": \"test-app-1701866015\"\n                    }\n                ]\n            },\n            \"created_at\": \"2023-12-06T12:54:18.124657897Z\",\n            \"modified_at\": \"2023-12-06T12:54:18.124657897Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        }\n    ]\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/inputs' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Content-Type: application/json' \\\n--data '{\n  \"inputs\": [\n    {\n      \"id\": \"0\",\n      \"data\": {\n        \"image\": {\n          \"url\": \"https://samples.clarifai.com/search-test-data/img-db8ebbc0-c294-4ec6-879f-b97da05c7ea8.jpg\"\n        },\n        \"concepts\": [\n          {\n            \"id\": \"test-concept\",\n            \"value\": 1\n          }\n        ]\n      }\n    }\n  ]\n}'"
          }
        ]
      },
      "patch": {
        "tags": [
          "Inputs"
        ],
        "summary": "Patch Inputs",
        "description": "This endpoint helps in updating and merging multiple concepts concurrently.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application name** |\n| `input` | **string** | **Stores the input** |\n| `concept_id` | **string** | **Stores the concept ID** |\n| `metadata` | **string** | **Stores the metadata** |",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "example": {
                  "inputs": [
                    {
                      "id": "{{input_id}}",
                      "data": {
                        "concepts": [
                          {
                            "id": "mattid2",
                            "value": 1
                          },
                          {
                            "id": "ferrari",
                            "value": 0
                          }
                        ],
                        "metadata": {
                          "my_key": "my_value2"
                        }
                      }
                    }
                  ],
                  "action": "merge"
                }
              }
            }
          }
        },
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "examples": {
                  "example-0": {
                    "summary": "Update Concepts Bulk (merge)",
                    "value": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"details\": \"All inputs successfully added\",\n        \"req_id\": \"7605f592d471de3a995fad75cfb594c9\"\n    },\n    \"inputs\": [\n        {\n            \"id\": \"a6fbf901fe354821acb9a111b9942a7f\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"https://data-dev.clarifai.com/orig/users/a0btrubbaefn/apps/test-app-1700638575-empty/inputs/image/aa78243ca07f89690b225ba7dc69f735\",\n                    \"hosted\": {\n                        \"prefix\": \"https://data-dev.clarifai.com\",\n                        \"suffix\": \"users/a0btrubbaefn/apps/test-app-1700638575-empty/inputs/image/aa78243ca07f89690b225ba7dc69f735\",\n                        \"sizes\": [\n                            \"orig\",\n                            \"tiny\",\n                            \"small\",\n                            \"large\"\n                        ],\n                        \"crossorigin\": \"use-credentials\"\n                    },\n                    \"image_info\": {\n                        \"width\": 40,\n                        \"height\": 40,\n                        \"format\": \"JPEG\",\n                        \"color_mode\": \"YUV\"\n                    }\n                },\n                \"concepts\": [\n                    {\n                        \"id\": \"mattid2\",\n                        \"name\": \"mattid2\",\n                        \"value\": 1,\n                        \"app_id\": \"test-app-1700638575-empty\"\n                    },\n                    {\n                        \"id\": \"ferrari\",\n                        \"name\": \"ferrari\",\n                        \"value\": 0,\n                        \"app_id\": \"test-app-1700638575-empty\"\n                    }\n                ],\n                \"metadata\": {\n                    \"my_key\": \"my_value2\"\n                }\n            },\n            \"created_at\": \"2023-11-22T07:42:05.560535Z\",\n            \"modified_at\": \"2023-11-22T07:42:05.799320Z\",\n            \"status\": {\n                \"code\": 30200,\n                \"description\": \"Input modification success\"\n            }\n        }\n    ]\n}"
                  },
                  "example-1": {
                    "summary": "Update Concepts Bulk (remove by concept id list)",
                    "value": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"details\": \"All inputs successfully added\",\n        \"req_id\": \"bda6a7faf49a0bb9057200c8690e8fbf\"\n    },\n    \"inputs\": [\n        {\n            \"id\": \"a6fbf901fe354821acb9a111b9942a7f\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"https://data-dev.clarifai.com/orig/users/a0btrubbaefn/apps/test-app-1700638575-empty/inputs/image/aa78243ca07f89690b225ba7dc69f735\",\n                    \"hosted\": {\n                        \"prefix\": \"https://data-dev.clarifai.com\",\n                        \"suffix\": \"users/a0btrubbaefn/apps/test-app-1700638575-empty/inputs/image/aa78243ca07f89690b225ba7dc69f735\",\n                        \"sizes\": [\n                            \"orig\",\n                            \"tiny\",\n                            \"small\",\n                            \"large\"\n                        ],\n                        \"crossorigin\": \"use-credentials\"\n                    },\n                    \"image_info\": {\n                        \"width\": 40,\n                        \"height\": 40,\n                        \"format\": \"JPEG\",\n                        \"color_mode\": \"YUV\"\n                    }\n                },\n                \"metadata\": {\n                    \"my_key\": \"my_value2\"\n                }\n            },\n            \"created_at\": \"2023-11-22T07:42:05.560535Z\",\n            \"modified_at\": \"2023-11-22T07:42:05.799320Z\",\n            \"status\": {\n                \"code\": 30200,\n                \"description\": \"Input modification success\"\n            }\n        }\n    ]\n}"
                  },
                  "example-2": {
                    "summary": "Update Concepts Bulk (overwrite)",
                    "value": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"details\": \"All inputs successfully added\",\n        \"req_id\": \"da2200f1490d402d63bcb2dd34d2831b\"\n    },\n    \"inputs\": [\n        {\n            \"id\": \"a6fbf901fe354821acb9a111b9942a7f\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"https://data-dev.clarifai.com/orig/users/a0btrubbaefn/apps/test-app-1700638575-empty/inputs/image/aa78243ca07f89690b225ba7dc69f735\",\n                    \"hosted\": {\n                        \"prefix\": \"https://data-dev.clarifai.com\",\n                        \"suffix\": \"users/a0btrubbaefn/apps/test-app-1700638575-empty/inputs/image/aa78243ca07f89690b225ba7dc69f735\",\n                        \"sizes\": [\n                            \"orig\",\n                            \"tiny\",\n                            \"small\",\n                            \"large\"\n                        ],\n                        \"crossorigin\": \"use-credentials\"\n                    },\n                    \"image_info\": {\n                        \"width\": 40,\n                        \"height\": 40,\n                        \"format\": \"JPEG\",\n                        \"color_mode\": \"YUV\"\n                    }\n                },\n                \"concepts\": [\n                    {\n                        \"id\": \"asdf123\",\n                        \"name\": \"asdf123\",\n                        \"value\": 1,\n                        \"app_id\": \"test-app-1700638575-empty\"\n                    }\n                ]\n            },\n            \"created_at\": \"2023-11-22T07:42:05.560535Z\",\n            \"modified_at\": \"2023-11-22T07:42:05.799320Z\",\n            \"status\": {\n                \"code\": 30200,\n                \"description\": \"Input modification success\"\n            }\n        }\n    ]\n}"
                  },
                  "example-3": {
                    "summary": "Update Concept and Metadata Bulk (remove)",
                    "value": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"details\": \"All inputs successfully added\",\n        \"req_id\": \"114e0b360be41a4f273aba37290005b4\"\n    },\n    \"inputs\": [\n        {\n            \"id\": \"a6fbf901fe354821acb9a111b9942a7f\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"https://data-dev.clarifai.com/orig/users/a0btrubbaefn/apps/test-app-1700638575-empty/inputs/image/aa78243ca07f89690b225ba7dc69f735\",\n                    \"hosted\": {\n                        \"prefix\": \"https://data-dev.clarifai.com\",\n                        \"suffix\": \"users/a0btrubbaefn/apps/test-app-1700638575-empty/inputs/image/aa78243ca07f89690b225ba7dc69f735\",\n                        \"sizes\": [\n                            \"orig\",\n                            \"tiny\",\n                            \"small\",\n                            \"large\"\n                        ],\n                        \"crossorigin\": \"use-credentials\"\n                    },\n                    \"image_info\": {\n                        \"width\": 40,\n                        \"height\": 40,\n                        \"format\": \"JPEG\",\n                        \"color_mode\": \"YUV\"\n                    }\n                },\n                \"concepts\": [\n                    {\n                        \"id\": \"asdf123\",\n                        \"name\": \"asdf123\",\n                        \"value\": 1,\n                        \"app_id\": \"test-app-1700638575-empty\"\n                    }\n                ],\n                \"metadata\": {\n                    \"id\": \"john\",\n                    \"size\": \"large\"\n                }\n            },\n            \"created_at\": \"2023-11-22T07:42:05.560535Z\",\n            \"modified_at\": \"2023-11-22T07:42:05.799320Z\",\n            \"status\": {\n                \"code\": 30200,\n                \"description\": \"Input modification success\"\n            }\n        }\n    ]\n}"
                  },
                  "example-4": {
                    "summary": "Update Metadata on input",
                    "value": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"details\": \"All inputs successfully added\",\n        \"req_id\": \"07393dfc47d019a119538967d607aa72\"\n    },\n    \"inputs\": [\n        {\n            \"id\": \"a6fbf901fe354821acb9a111b9942a7f\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"https://data-dev.clarifai.com/orig/users/a0btrubbaefn/apps/test-app-1700638575-empty/inputs/image/aa78243ca07f89690b225ba7dc69f735\",\n                    \"hosted\": {\n                        \"prefix\": \"https://data-dev.clarifai.com\",\n                        \"suffix\": \"users/a0btrubbaefn/apps/test-app-1700638575-empty/inputs/image/aa78243ca07f89690b225ba7dc69f735\",\n                        \"sizes\": [\n                            \"orig\",\n                            \"tiny\",\n                            \"small\",\n                            \"large\"\n                        ],\n                        \"crossorigin\": \"use-credentials\"\n                    },\n                    \"image_info\": {\n                        \"width\": 40,\n                        \"height\": 40,\n                        \"format\": \"JPEG\",\n                        \"color_mode\": \"YUV\"\n                    }\n                },\n                \"concepts\": [\n                    {\n                        \"id\": \"asdf123\",\n                        \"name\": \"asdf123\",\n                        \"value\": 1,\n                        \"app_id\": \"test-app-1700638575-empty\"\n                    },\n                    {\n                        \"id\": \"mattid2\",\n                        \"name\": \"mattid2\",\n                        \"value\": 1,\n                        \"app_id\": \"test-app-1700638575-empty\"\n                    },\n                    {\n                        \"id\": \"ferrari\",\n                        \"name\": \"ferrari\",\n                        \"value\": 1,\n                        \"app_id\": \"test-app-1700638575-empty\"\n                    }\n                ],\n                \"metadata\": {\n                    \"empty_map\": {},\n                    \"foo_list\": [\n                        \"bar\"\n                    ],\n                    \"foo_map:\": {\n                        \"bar\": true\n                    },\n                    \"id\": \"john\",\n                    \"size\": \"large\"\n                }\n            },\n            \"created_at\": \"2023-11-22T07:42:05.560535Z\",\n            \"modified_at\": \"2023-11-22T07:42:05.799320Z\",\n            \"status\": {\n                \"code\": 30200,\n                \"description\": \"Input modification success\"\n            }\n        }\n    ]\n}"
                  },
                  "example-5": {
                    "summary": "Add bulk large (125 unique images)",
                    "value": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"details\": \"All inputs successfully added\",\n        \"req_id\": \"5928de48a3431219ba7258b90bc196b4\"\n    },\n    \"inputs\": [\n        {\n            \"id\": \"61d9db74dd8842128dff2790913ea9d6\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/hfW6NqG.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"ed1dae9647814633970d4c4801b43539\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/kc1wdXY.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"3bd4f8dabe9e44ef9ffee3b078ae00ad\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/pVLKF5m.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"8c7cb033cb5e4c1da76759fa9b8329ba\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"https://i.imgur.com/Rh24Wr9.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"2d6a756f7b984336a9ff22cb0b8e505f\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/n6xluZH.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"f591b5a17a3f481387fc269677a0c508\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/6V0Jycs.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"fee7bebde6a747c295972c93a3771888\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/Bd54Fz6.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"89c1dcc4514d4d65ae659a2b6809f95f\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/0UvfKxc.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"bf589a48c9324ec1818c0ea91355a050\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/oKNsJZX.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"af150a0185c5483ab88cc594802b2f45\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/SjPB5Gg.jpg?1\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"d07b4992bc6a48568dada3ae2e54d847\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/42fRxr0.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"53080a4dbfd44167ac7f8d870d074617\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/6gyLU.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"ee40a41cf26c4ae7a2845e684c800423\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/jnH8PHu.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"589a4ff3e31e41f5997262553f49f919\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/CLeZeqf.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"b786b91368da41a39e44768a215812fc\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/OnviT4P.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"15652291da6b45f9a9fa2f04cf270fde\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/vIxp2sz.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"695b0246be184127a73f7377ee30d67f\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/MgizREc.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"195ebacb494045ab9d2f3863ba9a782a\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/0nEqe.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"3a7b9bf40ad64971bf878f97b3053413\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/6J0DL.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"c7d1494126b94dea8cdff7a0a4d37dea\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/rMH6ejD.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"fba0b8a4e88943e59472e63db14be91b\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/ivXqY5s.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"7dc7d36a5c4748b487701dc1bd6d559a\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/6A2o0AI.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"6e2252fc336a46be938f042ca2282417\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/of6tf.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"b721eb1abd4e4c2694fa3c78fb0f1fd7\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/j3gFjC5.jpg?1\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"37b91d7245ad44fd8b1b12e4de766c93\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/z7Vuadn.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"4be3f73cb7cd48729cd9703f9d64d2d4\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/cT6DoP1.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"f12c34b84a7241989c0789b4eda69339\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/KTBAp0l.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"f4ce2a90c5fa40788ee46c8ab7d06258\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/78aDnTV.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"c507f799f55f41fe8e658c30db78a010\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/7eRYVon.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"98048e422ac24a68ab29fedccc5467b0\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/43V5zMF.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"50213e3f05c34fdd97d22f1e9e96abeb\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/wVZQoBw.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"075cf044e242419bbb517beab7e39473\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/pWvYOcf.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"b94f123c1d2c40c9bfc0107baa86a609\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/9RbkijC.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"54f6fa6cab604c0084d0f8e27e6e72d3\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/fA2oEnp.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"db6634cd382f456180f83435ba5cd566\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/QU7N0.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"1bdbb76df2824593a3c6d3bc4382476e\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/H74RUsq.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"ddb763c8df9c46a8b2bf31ca3f032311\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/T4VOscB.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"fc386bd4efb0494483112e3e95354c43\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/nDVGDQM.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"37822c0db649441c98abee03f631d416\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/M8ItNje.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"988604141f92463eb8d1567fadb5151b\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/U5E3USJ.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"85fa8b6a88874a4da47d91aaa063238b\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/V5YZCsE.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"a0c8f684a4554eb2aa3b00876c53af04\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/pJ1boUn.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"63457f08d2e94293a7f33e36ecdf9039\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/ExGfa5e.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"92abc0e93f5b4d52b530b05b16609f63\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/O4npXMj.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"61e8ca603fad403aad03bd03c1f4162c\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/OwC4f.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"7a608699126f42d5a3bfbea62741ed3a\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/wfJSTl.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"7d1df0ee9ace4151ad28f577b6748543\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/gXiVUw9.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"1c06e987fbe941a6826400fe5bd1afbe\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/KEyUw.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"4b09b9dcbca54c3fba34bac97565f7e4\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/Gy3YbGg.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"5a19c93a06144221812e06aafdfd6c7c\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/bxsvpYL.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"51cd90783ec04d34a7c4b091323a8a20\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/PJXKVPR.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"d1cf9406dbbd4f8e9c4b1f07e33edf52\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/V9Ny3.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"6e7d99fe5f004c73a643b6805b7ee4c9\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/4SZRE.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"c01311a97efd4cb3a6f96b303d703db0\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/P5ItqXU.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"88d596d73fd64124b365946a2801d295\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/9qnad4D.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"d162adadaa6c472b8ecb515caf534fd9\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/4n5de.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"f8bf4b42ec7d4e56a0040a87a5725c44\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/nuNRI2T.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"ef01e21c816d4d5489c7502624db7473\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/aGn6e1U.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"c994593f6d82445c9a5d4880c71f0e3b\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/4LjuqHS.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"c3b5c55091ee4f04be04cd9d9a2c0b08\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/8XKbGDu.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"95fc75fb048b451d8304d2fe99584c79\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"https://i.imgur.com/MwKMhXN.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"83476cbdc7464d1b9b87b9ec018b609d\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/id1di.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"48ac65781a7e4aaab7cce8dc130cab26\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/5qT2tF9.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"afa32e4fcb404bceaac35dce54ebd655\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/HSQti.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"afc7a6c95cb348628c44c7ccc8af1c29\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/BgOPvmi.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"a3dbca63758845738590d6587c5c5b4f\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/SoprEwD.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"fb96e93d013d473892f8131b9a0eaa68\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/iJKuF.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"cecf0096d8294dda97d3ba8857d3623f\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/BYZpR.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"f47c2eb641cf4565b977105447348cbd\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/YqJdiob.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"5590ba0ffbcc4bac86f777ca7c7ecb5e\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/M8bf1El.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"ae65c39d34f44be2a428f1b27732186a\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/wXGOq.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"40ae14bcd2e0456ca16f1dbdb6d68d80\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"https://i.imgur.com/RvJ2JZd.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"a196439ade54425aa0695f0c86eadfb9\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/o0zc2.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"6461d8d5537e4ce4ad2db55d484c8529\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/SEy1rGu.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"af76f54af6764a468e9e89178e7b9f55\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/bLUPvt0.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"c9abc2fdeceb40e48ed37245352ce7d3\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/Sv7Sn.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"701e534c4f6b4c74b1c2ffc82f27dfdb\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/xXUmNLd.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"fe400937f8e34536bf70eb5e6e1729af\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/NFijzQh.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"ec6a8f35a2f847fab4412d18b326c4a1\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/qnB3N.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"c2293e17e60b470db8359a2df73e2a15\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/QWq2b.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"6c4cc74ebb254fc7b7e1be6e97991283\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/3tIISW3.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"7d6564c88fb642898d26ebf38853217d\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/e3gZ7XU.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"dd4d7da51f2144719ad0e30691239d5f\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/Qj5jLat.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"bb39723295fa49e4a4eb13316f29cf8c\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/W3Kc2gh.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"c3d961b2c8184dc0889f39002bcc0cea\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/COqWtEV.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"d1a44f2aee464c21ad4446b4e07ccc78\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/XKQh3.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"132a291c72d546ac8355ca60a4a7d3df\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/uZ9FQIM.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"55c95d09390d4a228664ab2846d3d55e\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/sBGjh8m.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"cf9c2be8b82c478b9a8bddd1efed12cb\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/tHUBbDA.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"047845046538421b92d4195a64433f5a\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/RMDjQWf.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"6aa5419752ec42a38c7fc811c8faa125\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/qp1nk7r.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"2f5b3630ea514ca1a322f4184ac4b3b6\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/erBfI1X.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"e15d205e5d5d49e3a36c0caabfbe33cd\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/csF0pFX.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"5bcd2a5513c94390bf34de37d6c37442\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/bqgBU.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"60dc2e9799e0488d845bb089cb444a81\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/zYXKwBV.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"5b1dabeb7fc146d1888b9e9ca7ec8955\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/6XzRI.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"f7fb5d28272848ecba0f0ba0bc37eed6\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/mm32f.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"97aab749859b41f2ad2d2fbbbd974cef\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/0kAN3.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"1de1187a5afe41feaaf9a03b9bd14ece\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/1RLDw96.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"208abdfe9c664d03b7d584e70be7d4d8\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/TNhgCBg.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"10cc67e11ddd4333a610690c55039a83\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/HJZDTxF.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"a1cd6303e308413ea2bacc96c3ed715b\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/OdwpBgE.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"4c9106c49205461caf45f3cfcc171503\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/Y5E7fSF.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"7721dce5d8bf48e7b3a0ad9ad28eccfc\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/u2iR1W8.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"18c8e11ffd434b73a05690abcd1997c7\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/TmkKkgl.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"1f4ccf1e32cf41a28be197c38fc1caf1\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/qTQlrgb.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"eaf627f931664183b9b1e63b1a26ed73\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/s09RfeS.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"0d9cbb0f5f42443097776bdb322452b2\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/CzweH2C.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"b26ecff69eb04c7faf8176baaa0589d1\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/TGp3S.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"12774f451e314a779d9ce6fab4c1b8f8\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/ImdNq.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"f4eda0b6dcb047bca844becc773f8bcd\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/cbhQY.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"558700f261eb4e24aa9dbd847827a8d8\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/3DncTyp.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"6941dbce34af4b3e8714a3a432b18c9b\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/O3tu0jG.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"a5340dd51b124256890f37f15f3b7171\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/qPqVjF0.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"0de55db185e04409bb0ffdd8473a728b\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/B5pj5.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"a475f569574547dfaeb2a67635bba702\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/TWsjoWS.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"df54b8145f1e439b89d4c9e800906bff\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/y4sqPR5.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"f340d2c60b77449794bb4f4251c94103\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/y4VMTRe.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"197f6d5810a14dcc9bfc310b7474ea76\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/adliR8e.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"4429be87cb014058afdeb91ecef69e5b\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/799hsjJ.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"92bbb5e0e3d1458fa0605f6e5ec91dab\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/ojBA1CZ.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"a7546400929942b0b0cbb03d77a67053\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/6RVHuNr.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"b94514a7cd8e48068e2b335324d89360\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/LKcxw6e.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"4f190e040bed4fb5a5f55e8fb977b1cb\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/o6dVSnH.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"a7cd7fa80f124ca48799f182c4d7cebc\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"http://i.imgur.com/AXyb1.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"modified_at\": \"2023-11-22T07:47:38.421712858Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        }\n    ]\n}"
                  },
                  "example-6": {
                    "summary": "Update geo point",
                    "value": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"details\": \"All inputs successfully added\",\n        \"req_id\": \"2ada3fc0167a7f3507be7a0bfad148b2\"\n    },\n    \"inputs\": [\n        {\n            \"id\": \"a6fbf901fe354821acb9a111b9942a7f\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"https://data-dev.clarifai.com/orig/users/a0btrubbaefn/apps/test-app-1700638575-empty/inputs/image/aa78243ca07f89690b225ba7dc69f735\",\n                    \"hosted\": {\n                        \"prefix\": \"https://data-dev.clarifai.com\",\n                        \"suffix\": \"users/a0btrubbaefn/apps/test-app-1700638575-empty/inputs/image/aa78243ca07f89690b225ba7dc69f735\",\n                        \"sizes\": [\n                            \"orig\",\n                            \"tiny\",\n                            \"small\",\n                            \"large\"\n                        ],\n                        \"crossorigin\": \"use-credentials\"\n                    },\n                    \"image_info\": {\n                        \"width\": 40,\n                        \"height\": 40,\n                        \"format\": \"JPEG\",\n                        \"color_mode\": \"YUV\"\n                    }\n                },\n                \"concepts\": [\n                    {\n                        \"id\": \"asdf123\",\n                        \"name\": \"asdf123\",\n                        \"value\": 1,\n                        \"app_id\": \"test-app-1700638575-empty\"\n                    },\n                    {\n                        \"id\": \"mattid2\",\n                        \"name\": \"mattid2\",\n                        \"value\": 1,\n                        \"app_id\": \"test-app-1700638575-empty\"\n                    },\n                    {\n                        \"id\": \"ferrari\",\n                        \"name\": \"ferrari\",\n                        \"value\": 1,\n                        \"app_id\": \"test-app-1700638575-empty\"\n                    }\n                ],\n                \"metadata\": {\n                    \"empty_map\": {},\n                    \"foo_list\": [\n                        \"bar\"\n                    ],\n                    \"foo_map:\": {\n                        \"bar\": true\n                    },\n                    \"id\": \"john\",\n                    \"size\": \"small\"\n                },\n                \"geo\": {\n                    \"geo_point\": {\n                        \"longitude\": -75,\n                        \"latitude\": 10\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:42:05.560535Z\",\n            \"modified_at\": \"2023-11-22T07:42:05.799320Z\",\n            \"status\": {\n                \"code\": 30200,\n                \"description\": \"Input modification success\"\n            }\n        }\n    ]\n}"
                  }
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff --request PATCH 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/inputs' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Content-Type: application/json' \\\n--data '{\n  \"inputs\": [\n    {\n      \"id\": \"{{input_id}}\",\n      \"data\": {\n        \"concepts\": [\n          {\n            \"id\": \"mattid2\",\n            \"value\": 1\n          },\n          {\n            \"id\": \"ferrari\",\n            \"value\": 0\n          }\n        ],\n        \"metadata\": {\n          \"my_key\": \"my_value2\"\n        }\n      }\n    }\n  ],\n  \"action\": \"merge\"\n}'"
          }
        ]
      },
      "get": {
        "tags": [
          "Inputs"
        ],
        "summary": "Set Of Inputs By IDs",
        "description": "This endpoint provides a way to retrieve specific inputs from the app using their unique identifier.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application name** |",
        "parameters": [
          {
            "name": "page",
            "in": "query",
            "schema": {
              "type": "integer"
            },
            "example": "1"
          },
          {
            "name": "per_page",
            "in": "query",
            "schema": {
              "type": "integer"
            },
            "example": "20"
          },
          {
            "name": "ids",
            "in": "query",
            "schema": {
              "type": "string"
            },
            "example": "{{input}}"
          },
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"c9c4d3425b4f8bf6894f4a3122011533\"\n    },\n    \"inputs\": [\n        {\n            \"id\": \"a6fbf901fe354821acb9a111b9942a7f\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"https://data-dev.clarifai.com/orig/users/a0btrubbaefn/apps/test-app-1700638575-empty/inputs/image/aa78243ca07f89690b225ba7dc69f735\",\n                    \"hosted\": {\n                        \"prefix\": \"https://data-dev.clarifai.com\",\n                        \"suffix\": \"users/a0btrubbaefn/apps/test-app-1700638575-empty/inputs/image/aa78243ca07f89690b225ba7dc69f735\",\n                        \"sizes\": [\n                            \"orig\",\n                            \"tiny\",\n                            \"small\",\n                            \"large\"\n                        ],\n                        \"crossorigin\": \"use-credentials\"\n                    },\n                    \"image_info\": {\n                        \"width\": 40,\n                        \"height\": 40,\n                        \"format\": \"JPEG\",\n                        \"color_mode\": \"YUV\"\n                    }\n                },\n                \"concepts\": [\n                    {\n                        \"id\": \"asdf123\",\n                        \"name\": \"asdf123\",\n                        \"value\": 1,\n                        \"app_id\": \"test-app-1700638575-empty\"\n                    },\n                    {\n                        \"id\": \"mattid2\",\n                        \"name\": \"mattid2\",\n                        \"value\": 1,\n                        \"app_id\": \"test-app-1700638575-empty\"\n                    },\n                    {\n                        \"id\": \"ferrari\",\n                        \"name\": \"ferrari\",\n                        \"value\": 1,\n                        \"app_id\": \"test-app-1700638575-empty\"\n                    }\n                ],\n                \"metadata\": {\n                    \"empty_map\": {},\n                    \"foo_list\": [\n                        \"bar\"\n                    ],\n                    \"foo_map:\": {\n                        \"bar\": true\n                    },\n                    \"id\": \"john\",\n                    \"size\": \"small\"\n                },\n                \"geo\": {\n                    \"geo_point\": {\n                        \"longitude\": -75,\n                        \"latitude\": 10\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:42:05.560535Z\",\n            \"modified_at\": \"2023-11-22T07:42:05.799320Z\",\n            \"status\": {\n                \"code\": 30000,\n                \"description\": \"Download complete\"\n            }\n        }\n    ]\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/inputs?page=1&per_page=20&ids={{input}}' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/inputs/file": {
      "post": {
        "tags": [
          "Inputs"
        ],
        "summary": "Add CSV",
        "description": "By utilizing this endpoint, you can add inputs as a CSV.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application name** |\n| `url` | **string** | **Stores the URL of the data** |",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "example": {
                  "url": "https://clairfai-temp-img.s3.amazonaws.com/test.csv",
                  "filetype": "csv"
                }
              }
            }
          }
        },
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"details\": \"All inputs successfully added\",\n        \"req_id\": \"19deb51c2c8c4c671743ec0b83c1e56a\"\n    },\n    \"inputs\": [\n        {\n            \"id\": \"b93ecc999d7842f384a00d3258a49879\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"https://samples.clarifai.com/metro-north.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:45:48.224909244Z\",\n            \"modified_at\": \"2023-11-22T07:45:48.224909244Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"2e0af176f66b4015b4abc2690782b21d\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"https://samples.clarifai.com/gun.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:45:48.224909244Z\",\n            \"modified_at\": \"2023-11-22T07:45:48.224909244Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"c57b1f3384d24b9eb6c9662729db03ab\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"https://samples.clarifai.com/logo.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:45:48.224909244Z\",\n            \"modified_at\": \"2023-11-22T07:45:48.224909244Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"0b4f058d84924d0fa6f4743c62b80fab\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"https://samples.clarifai.com/wedding.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:45:48.224909244Z\",\n            \"modified_at\": \"2023-11-22T07:45:48.224909244Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"e2894bad03c3444a928ea06e6647b27d\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"https://samples.clarifai.com/adidas_gun.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:45:48.224909244Z\",\n            \"modified_at\": \"2023-11-22T07:45:48.224909244Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"e7cb293fa29d4d8abd6fc55c662dc82e\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"https://samples.clarifai.com/facebook.png\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:45:48.224909244Z\",\n            \"modified_at\": \"2023-11-22T07:45:48.224909244Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"aa8f7fff4655443fad607670fc36660d\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"https://samples.clarifai.com/dog.tiff\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:45:48.224909244Z\",\n            \"modified_at\": \"2023-11-22T07:45:48.224909244Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"f52b23e3843740f796cafc807bd44c2c\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"https://samples.clarifai.com/penguin.bmp\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:45:48.224909244Z\",\n            \"modified_at\": \"2023-11-22T07:45:48.224909244Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"edd2f3f573574fcd9a5be4b46079afdc\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"https://samples.clarifai.com/logoDarkRGB.png\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:45:48.224909244Z\",\n            \"modified_at\": \"2023-11-22T07:45:48.224909244Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        },\n        {\n            \"id\": \"079321964d2b436c8ca63476b24c9801\",\n            \"data\": {\n                \"image\": {\n                    \"url\": \"https://samples.clarifai.com/family.jpg\",\n                    \"image_info\": {\n                        \"format\": \"UnknownImageFormat\",\n                        \"color_mode\": \"UnknownColorMode\"\n                    }\n                }\n            },\n            \"created_at\": \"2023-11-22T07:45:48.224909244Z\",\n            \"modified_at\": \"2023-11-22T07:45:48.224909244Z\",\n            \"status\": {\n                \"code\": 30001,\n                \"description\": \"Download pending\"\n            }\n        }\n    ]\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/inputs/file' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Content-Type: application/json' \\\n--data '{\n  \"url\": \"https://clairfai-temp-img.s3.amazonaws.com/test.csv\",\n  \"filetype\": \"csv\"\n}'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/inputs/jobs/add/{inputs_add_job_id}": {
      "get": {
        "tags": [
          "Inputs"
        ],
        "summary": "Get Input Adding Job (to see progress)",
        "description": "This endpoint is used to get all inputs with respective job ID's.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application name** |\n| `inputs_add_job_id` | **string** | **Stores the job ID** |",
        "parameters": [
          {
            "name": "per_page",
            "in": "query",
            "schema": {
              "type": "integer"
            },
            "example": "1000"
          },
          {
            "name": "page",
            "in": "query",
            "schema": {
              "type": "integer"
            },
            "example": "1"
          },
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "inputs_add_job_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"77c43c2fe182100b2393153522ece678\"\n    },\n    \"inputs_add_job\": {\n        \"id\": \"inputs-1700639441\",\n        \"progress\": {\n            \"success_count\": \"2\",\n            \"failed_count\": \"1\"\n        },\n        \"created_at\": \"2023-11-22T07:50:41.152226Z\",\n        \"modified_at\": \"2023-11-22T07:50:41.702885Z\",\n        \"status\": {\n            \"code\": 64002,\n            \"description\": \"Job successfully ran.\"\n        }\n    }\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/inputs/jobs/add/{inputs_add_job_id}?per_page=1000&page=1' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/inputs/jobs/add": {
      "get": {
        "tags": [
          "Inputs"
        ],
        "summary": "List Input Adding Jobs",
        "description": "This endpoint is used to get all inputs with respective job ID's.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application name** |\n| `inputs_add_job_id` | **string** | **Stores the job ID** |",
        "parameters": [
          {
            "name": "per_page",
            "in": "query",
            "schema": {
              "type": "integer"
            },
            "example": "1000"
          },
          {
            "name": "page",
            "in": "query",
            "schema": {
              "type": "integer"
            },
            "example": "1"
          },
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"1620a91f6bee6cf694662e1e591fb971\"\n    },\n    \"inputs_add_jobs\": [\n        {\n            \"id\": \"inputs-1700639441\",\n            \"progress\": {\n                \"success_count\": \"2\",\n                \"failed_count\": \"1\"\n            },\n            \"created_at\": \"2023-11-22T07:50:41.152226Z\",\n            \"modified_at\": \"2023-11-22T07:50:41.702885Z\",\n            \"status\": {\n                \"code\": 64002,\n                \"description\": \"Job successfully ran.\"\n            }\n        },\n        {\n            \"id\": \"inputs-add-job-1700638738\",\n            \"progress\": {\n                \"success_count\": \"1\"\n            },\n            \"created_at\": \"2023-11-22T07:38:58.349176Z\",\n            \"modified_at\": \"2023-11-22T07:38:58.564425Z\",\n            \"status\": {\n                \"code\": 64002,\n                \"description\": \"Job successfully ran.\"\n            }\n        }\n    ]\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/inputs/jobs/add?per_page=1000&page=1' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/models": {
      "post": {
        "tags": [
          "Walkthroughs > RAG"
        ],
        "summary": "Create Model",
        "description": "This request allows users to create a model by providing the model ID and model type.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `model_id` | **string** | **Stores the Model ID** |\n| `model_type_id` | **string** | **Stores the model type** |",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "example": {
                  "model": {
                    "id": "rag_prompter",
                    "model_type_id": "rag-prompter"
                  }
                }
              }
            }
          }
        },
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "200": {
            "description": "OK",
            "headers": {
              "Date": {
                "schema": {
                  "type": "string",
                  "example": "Thu, 02 May 2024 08:48:42 GMT"
                }
              },
              "Content-Type": {
                "schema": {
                  "type": "string",
                  "example": "application/json; charset=UTF-8"
                }
              },
              "Content-Length": {
                "schema": {
                  "type": "integer",
                  "example": "484"
                }
              },
              "Connection": {
                "schema": {
                  "type": "string",
                  "example": "keep-alive"
                }
              },
              "Access-Control-Allow-Headers": {
                "schema": {
                  "type": "string",
                  "example": "Content-Type,Accept,X-Requested-With,Content-Type,Referer,Accept-Encoding,X-CSRF-Token,Authorization,X-Clarifai-Application-Id,X-Clarifai-REST-API-Key,X-Clarifai-Session-Token,X-Clarifai-Client,X-Clarifai-Site,X-RapidAPI-User,x-clarifai-request-id-prefix,x-request-id"
                }
              },
              "Access-Control-Allow-Methods": {
                "schema": {
                  "type": "string",
                  "example": "GET,HEAD,POST,PUT,PATCH,OPTIONS,DELETE"
                }
              },
              "Access-Control-Allow-Origin": {
                "schema": {
                  "type": "string",
                  "example": "*"
                }
              },
              "Grpc-Metadata-Content-Type": {
                "schema": {
                  "type": "string",
                  "example": "application/grpc"
                }
              },
              "X-Clarifai-Request-Id": {
                "schema": {
                  "type": "string",
                  "example": "87142962939d46a3b885dccbbc245221"
                }
              }
            },
            "content": {
              "application/json": {
                "schema": {
                  "type": "object"
                },
                "example": {
                  "status": {
                    "code": 10000,
                    "description": "Ok",
                    "req_id": "87142962939d46a3b885dccbbc245221"
                  },
                  "model": {
                    "id": "rag_prompter",
                    "name": "rag_prompter",
                    "created_at": "2024-05-02T08:48:42.363415167Z",
                    "modified_at": "2024-05-02T08:48:42.363415167Z",
                    "app_id": "rag-app-1714639564",
                    "user_id": "clarifai",
                    "model_type_id": "rag-prompter",
                    "visibility": {
                      "gettable": 10
                    },
                    "metadata": {},
                    "presets": {},
                    "toolkits": [],
                    "use_cases": [],
                    "languages": [],
                    "languages_full": [],
                    "check_consents": [],
                    "workflow_recommended": false
                  }
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/models' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Content-Type: application/json' \\\n--data '{\n  \"model\": {\n    \"id\": \"rag_prompter\",\n    \"model_type_id\": \"rag-prompter\"\n  }\n}'"
          }
        ]
      },
      "patch": {
        "tags": [
          "Models"
        ],
        "summary": "Patch Models",
        "description": "This endpoint allows you to add timeout configurations to your model.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application name** |\n| `model.id` | **string** | **Stores the Model ID** |\n| `model.output_info.output_config.training_timeout` | **string** | **Stores the timeout for training** |\n| `action` | **string** | **Specifies the merge action to take** |",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "example": {
                  "models": [
                    {
                      "id": "{{model_id}}",
                      "output_info": {
                        "output_config": {
                          "training_timeout": "14400"
                        }
                      }
                    }
                  ],
                  "action": "merge"
                }
              }
            }
          }
        },
        "parameters": [
          {
            "name": "Authorization",
            "in": "header",
            "schema": {
              "type": "string"
            },
            "example": "Key {{key}}"
          },
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "examples": {
                  "example-0": {
                    "summary": "Patch Models (update timeout)",
                    "value": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"90612ca1d836e3f21ccb19f870f06d80\"\n    },\n    \"models\": [\n        {\n            \"id\": \"openai-create-image-9\",\n            \"name\": \"openai-create-image-9\",\n            \"created_at\": \"2023-11-23T11:16:25.775735Z\",\n            \"modified_at\": \"2023-11-23T11:19:20.672891Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"text-to-text\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"presets\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        }\n    ]\n}"
                  },
                  "example-1": {
                    "summary": "Patch Models (name and output_config)",
                    "value": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"1d2c2436883b352389bf574fb17ab176\"\n    },\n    \"models\": [\n        {\n            \"id\": \"openai-create-image-9\",\n            \"name\": \"newnewnewname\",\n            \"created_at\": \"2023-11-23T11:16:25.775735Z\",\n            \"modified_at\": \"2023-11-23T11:19:38.173686Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"text-to-text\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"presets\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        }\n    ]\n}"
                  },
                  "example-2": {
                    "summary": "Patch Models  (remove concept)",
                    "value": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"9f39f252ba5286b27561cab846e6a673\"\n    },\n    \"models\": [\n        {\n            \"id\": \"openai-create-image-9\",\n            \"name\": \"newname2\",\n            \"created_at\": \"2023-11-23T11:16:25.775735Z\",\n            \"modified_at\": \"2023-11-23T11:28:02.280411Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"text-to-text\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"presets\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        }\n    ]\n}"
                  }
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff --request PATCH 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/models' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Authorization: Key {{key}}' \\\n--header 'Content-Type: application/json' \\\n--data '{\n  \"models\": [\n    {\n      \"id\": \"{{model_id}}\",\n      \"output_info\": {\n        \"output_config\": {\n          \"training_timeout\": \"14400\"\n        }\n      }\n    }\n  ],\n  \"action\": \"merge\"\n}'"
          }
        ]
      },
      "get": {
        "tags": [
          "Models"
        ],
        "summary": "List Models",
        "description": "With this endpoint, users can list all the models in an app.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application name** |",
        "parameters": [
          {
            "name": "Authorization",
            "in": "header",
            "schema": {
              "type": "string"
            },
            "example": "Key {{key}}"
          },
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "examples": {
                  "example-0": {
                    "summary": "List Models in App",
                    "value": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"dd2303b0cb6fe8859693ae9e8f4adfa7\"\n    },\n    \"models\": [\n        {\n            \"id\": \"openai-create-image-9\",\n            \"name\": \"newnewnewname\",\n            \"created_at\": \"2023-11-23T11:16:25.775735Z\",\n            \"modified_at\": \"2023-11-23T11:19:38.173686Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"text-to-text\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"test-model-with-hyperparms-1700738311\",\n            \"name\": \"test-model-with-hyperparms-1700738311\",\n            \"created_at\": \"2023-11-23T11:18:31.353199Z\",\n            \"modified_at\": \"2023-11-23T11:18:31.353199Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"embedding-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"test-model-1700738282\",\n            \"name\": \"test-model-1700738282\",\n            \"created_at\": \"2023-11-23T11:18:01.807941Z\",\n            \"modified_at\": \"2023-11-23T11:18:01.807941Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"embedding-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"test-model-1700738263\",\n            \"name\": \"test-model-1700738263\",\n            \"created_at\": \"2023-11-23T11:17:43.761741Z\",\n            \"modified_at\": \"2023-11-23T11:17:43.761741Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"embedding-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"test-model-1700738240\",\n            \"name\": \"test-model-1700738240\",\n            \"created_at\": \"2023-11-23T11:17:20.719370Z\",\n            \"modified_at\": \"2023-11-23T11:17:20.719370Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"image-crop\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"test-model-1700738220\",\n            \"name\": \"test-model-1700738220\",\n            \"created_at\": \"2023-11-23T11:17:00.314086Z\",\n            \"modified_at\": \"2023-11-23T11:17:00.314086Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"embedding-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"profanity-filter-new2\",\n            \"name\": \"profanity-filter-new2\",\n            \"created_at\": \"2023-11-23T11:16:45.317604Z\",\n            \"modified_at\": \"2023-11-23T11:16:45.317604Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"keyword-filter-operator\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"named-entity-recognition-diseases-english-text\",\n            \"name\": \"burgerz\",\n            \"created_at\": \"2023-11-23T11:15:32.431806Z\",\n            \"modified_at\": \"2023-11-23T11:16:02.900299Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"embedding-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"custom-config\",\n            \"name\": \"custom-config\",\n            \"created_at\": \"2023-11-23T09:41:08.002419Z\",\n            \"modified_at\": \"2023-11-23T09:41:08.002419Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"model_version\": {\n                \"id\": \"1e4c121974f849209abb658cdf682585\",\n                \"created_at\": \"2023-11-23T09:41:18.470087Z\",\n                \"status\": {\n                    \"code\": 21110,\n                    \"description\": \"datasets.dataset.DataBatchEmpty: No databatch found in train set's file directory\\nFailed to create a training dataset, because there are no appropriately annotated inputs. Expected annotations with concepts for model type id text-classifier. \"\n                },\n                \"active_concept_count\": 6,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"test-app-1700638575-empty\",\n                \"user_id\": \"a0btrubbaefn\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"params\": {\n                        \"max_concepts\": 20,\n                        \"min_value\": 0,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {},\n                \"train_info\": {\n                    \"params\": {\n                        \"dataset_id\": \"\",\n                        \"dataset_version_id\": \"\",\n                        \"invalid_data_tolerance_percent\": 5,\n                        \"model_config\": {\n                            \"pretrained_model_name\": \"EleutherAI/gpt-neo-125m\"\n                        },\n                        \"num_gpus\": 1,\n                        \"peft_config\": {\n                            \"peft_type\": \"LORA\"\n                        },\n                        \"template\": \"HF_GPTNeo_125m_lora\",\n                        \"tokenizer_config\": {},\n                        \"trainer_config\": {\n                            \"auto_find_batch_size\": true,\n                            \"num_train_epochs\": 20,\n                            \"output_dir\": \"checkpoint\"\n                        }\n                    }\n                },\n                \"import_info\": {}\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"text-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"imagecl1\",\n            \"name\": \"imagecl1\",\n            \"created_at\": \"2023-11-23T07:58:15.786698Z\",\n            \"modified_at\": \"2023-11-23T07:58:15.786698Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"embedding-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"imagecl\",\n            \"name\": \"imagecl\",\n            \"created_at\": \"2023-11-23T07:57:56.370980Z\",\n            \"modified_at\": \"2023-11-23T07:57:56.370980Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"embedding-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"deep_cls_bg1\",\n            \"name\": \"deep_cls_bg1\",\n            \"created_at\": \"2023-11-23T07:27:32.857722Z\",\n            \"modified_at\": \"2023-11-23T07:27:32.857722Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"model_version\": {\n                \"id\": \"00668896e0a64cf5b37302c000e96f23\",\n                \"created_at\": \"2023-11-23T08:06:12.577745Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 16,\n                \"metrics\": {\n                    \"status\": {\n                        \"code\": 21300,\n                        \"description\": \"Model was successfully evaluated.\"\n                    },\n                    \"summary\": {\n                        \"macro_avg_roc_auc\": 0.52151275,\n                        \"macro_std_roc_auc\": 0.34620512,\n                        \"macro_avg_f1_score\": 0.40380955,\n                        \"macro_std_f1_score\": 0.17236254,\n                        \"macro_avg_precision\": 0.09772728,\n                        \"macro_avg_recall\": 0.52380955\n                    }\n                },\n                \"completed_at\": \"2023-11-23T08:21:08.303040Z\",\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"test-app-1700638575-empty\",\n                \"user_id\": \"a0btrubbaefn\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"concepts\": \"probs\"\n                    },\n                    \"params\": {\n                        \"max_concepts\": 20,\n                        \"min_value\": 0,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"input\"\n                    }\n                },\n                \"train_info\": {\n                    \"params\": {\n                        \"batch_size\": 64,\n                        \"concepts_mutually_exclusive\": false,\n                        \"dataset_id\": \"\",\n                        \"dataset_version_id\": \"\",\n                        \"flip_direction\": \"horizontal\",\n                        \"flip_probability\": 0.5,\n                        \"image_size\": 224,\n                        \"invalid_data_tolerance_percent\": 5,\n                        \"num_epochs\": 60,\n                        \"num_gpus\": 1,\n                        \"per_item_lrate\": 0.00001953125,\n                        \"per_item_min_lrate\": 1.5625e-08,\n                        \"pretrained_weights\": \"ImageNet-1k\",\n                        \"seed\": -1,\n                        \"template\": \"MMClassification_ResNet_50_RSB_A1\",\n                        \"warmup_iters\": 100,\n                        \"warmup_ratio\": 0.0001,\n                        \"weight_decay\": 0.01\n                    }\n                },\n                \"import_info\": {}\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"visual-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"deep_cls_bg\",\n            \"name\": \"deep_cls_bg\",\n            \"created_at\": \"2023-11-23T07:12:59.578284Z\",\n            \"modified_at\": \"2023-11-23T07:12:59.578284Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"visual-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"profanity-filter-new\",\n            \"name\": \"profanity-filter-new\",\n            \"created_at\": \"2023-11-23T07:08:10.016262Z\",\n            \"modified_at\": \"2023-11-23T07:08:10.016262Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"model_version\": {\n                \"id\": \"b56d87c6dc80484291586d10f2feaacd\",\n                \"created_at\": \"2023-11-23T07:08:17.041706Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"test-app-1700638575-empty\",\n                \"user_id\": \"a0btrubbaefn\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"params\": {\n                        \"case_sensitive\": false,\n                        \"keywords\": [\n                            \"\"\n                        ]\n                    }\n                },\n                \"input_info\": {},\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"keyword-filter-operator\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"tiny-random-gpt2\",\n            \"name\": \"tiny-random-gpt2\",\n            \"created_at\": \"2023-11-17T21:31:10.384113Z\",\n            \"modified_at\": \"2023-11-17T21:31:10.384113Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"214531acb40a4af8b56ca79103390466\",\n                \"created_at\": \"2023-11-17T21:31:10.384113Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {\n                    \"params\": {\n                        \"model_config\": {\n                            \"_name_or_path\": \"hf-internal-testing/tiny-random-gpt2\",\n                            \"activation_function\": \"gelu_new\",\n                            \"architectures\": [\n                                \"GPT2LMHeadModel\"\n                            ],\n                            \"attention_probs_dropout_prob\": 0.1,\n                            \"attn_pdrop\": 0.1,\n                            \"bos_token_id\": 98,\n                            \"embd_pdrop\": 0.1,\n                            \"eos_token_id\": 98,\n                            \"gradient_checkpointing\": false,\n                            \"hidden_act\": \"gelu\",\n                            \"hidden_dropout_prob\": 0.1,\n                            \"initializer_range\": 0.02,\n                            \"intermediate_size\": 37,\n                            \"layer_norm_epsilon\": 0.00001,\n                            \"model_type\": \"gpt2\",\n                            \"n_ctx\": 512,\n                            \"n_embd\": 32,\n                            \"n_head\": 4,\n                            \"n_inner\": null,\n                            \"n_layer\": 5,\n                            \"n_positions\": 512,\n                            \"pad_token_id\": 98,\n                            \"reorder_and_upcast_attn\": false,\n                            \"resid_pdrop\": 0.1,\n                            \"scale_attn_by_inverse_layer_idx\": false,\n                            \"scale_attn_weights\": true,\n                            \"summary_activation\": null,\n                            \"summary_first_dropout\": 0.1,\n                            \"summary_proj_to_labels\": true,\n                            \"summary_type\": \"cls_index\",\n                            \"summary_use_proj\": true,\n                            \"torch_dtype\": \"float32\",\n                            \"transformers_version\": \"4.32.1\",\n                            \"type_vocab_size\": 16,\n                            \"use_cache\": true,\n                            \"vocab_size\": 1000\n                        },\n                        \"tokenizer_config\": {\n                            \"add_prefix_space\": false,\n                            \"bos_token\": \"<|endoftext|>\",\n                            \"clean_up_tokenization_spaces\": true,\n                            \"eos_token\": \"<|endoftext|>\",\n                            \"model_max_length\": 512,\n                            \"tokenizer_class\": \"GPT2Tokenizer\",\n                            \"unk_token\": \"<|endoftext|>\"\n                        }\n                    }\n                },\n                \"import_info\": {\n                    \"params\": {\n                        \"model_name\": \"hf-internal-testing/tiny-random-gpt2\",\n                        \"pipeline_name\": \"text-generation\",\n                        \"toolkit\": \"HuggingFace\"\n                    }\n                }\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-to-text\",\n            \"task\": \"text-generation\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"BAAI-bge-base-en-v15\",\n            \"name\": \"BAAI-bge-base-en-v15\",\n            \"created_at\": \"2023-11-01T12:53:30.339664Z\",\n            \"modified_at\": \"2023-11-01T12:53:30.339664Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"23cf79f86459491cb31fd7f0273c9fff\",\n                \"created_at\": \"2023-11-01T12:53:30.339664Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"embeddings\": \"embedding\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    },\n                    \"params\": {\n                        \"text_token_warning_limit\": 512\n                    }\n                },\n                \"train_info\": {\n                    \"params\": {\n                        \"model_config\": {\n                            \"_name_or_path\": \"BAAI/bge-base-en-v1.5\",\n                            \"architectures\": [\n                                \"BertModel\"\n                            ],\n                            \"attention_probs_dropout_prob\": 0.1,\n                            \"classifier_dropout\": null,\n                            \"gradient_checkpointing\": false,\n                            \"hidden_act\": \"gelu\",\n                            \"hidden_dropout_prob\": 0.1,\n                            \"hidden_size\": 768,\n                            \"id2label\": {\n                                \"0\": \"LABEL_0\"\n                            },\n                            \"initializer_range\": 0.02,\n                            \"intermediate_size\": 3072,\n                            \"label2id\": {\n                                \"LABEL_0\": 0\n                            },\n                            \"layer_norm_eps\": 1e-12,\n                            \"max_position_embeddings\": 512,\n                            \"model_type\": \"bert\",\n                            \"num_attention_heads\": 12,\n                            \"num_hidden_layers\": 12,\n                            \"pad_token_id\": 0,\n                            \"position_embedding_type\": \"absolute\",\n                            \"torch_dtype\": \"float32\",\n                            \"transformers_version\": \"4.32.1\",\n                            \"type_vocab_size\": 2,\n                            \"use_cache\": true,\n                            \"vocab_size\": 30522\n                        },\n                        \"tokenizer_config\": {\n                            \"clean_up_tokenization_spaces\": true,\n                            \"cls_token\": \"[CLS]\",\n                            \"do_basic_tokenize\": true,\n                            \"do_lower_case\": true,\n                            \"mask_token\": \"[MASK]\",\n                            \"model_max_length\": 512,\n                            \"never_split\": null,\n                            \"pad_token\": \"[PAD]\",\n                            \"sep_token\": \"[SEP]\",\n                            \"strip_accents\": null,\n                            \"tokenize_chinese_chars\": true,\n                            \"tokenizer_class\": \"BertTokenizer\",\n                            \"unk_token\": \"[UNK]\"\n                        }\n                    }\n                },\n                \"import_info\": {\n                    \"params\": {\n                        \"model_name\": \"BAAI/bge-base-en-v1.5\",\n                        \"pipeline_name\": \"feature-extraction\",\n                        \"toolkit\": \"HuggingFace\"\n                    }\n                }\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-embedder\",\n            \"task\": \"representation-learning\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"BAAI-bge-base-en-cluster\",\n            \"name\": \"BAAI-bge-base-en-cluster\",\n            \"created_at\": \"2023-08-15T14:21:18.083130Z\",\n            \"modified_at\": \"2023-08-15T14:21:18.083130Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"4a47a75c931c4b0784cebc2cd45bc5a2\",\n                \"created_at\": \"2023-09-18T11:00:41.832176Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"metrics\": {\n                    \"status\": {\n                        \"code\": 21300,\n                        \"description\": \"Model was successfully evaluated.\"\n                    }\n                },\n                \"total_input_count\": 293849,\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\"\n                },\n                \"input_info\": {},\n                \"train_info\": {\n                    \"params\": {\n                        \"beta\": 1,\n                        \"coarse_clusters\": 128,\n                        \"dataset_id\": \"quora-dataset-corpus-2\",\n                        \"dataset_version_id\": \"dataset-version-1692900595413\",\n                        \"eval_holdout_fraction\": 0.2,\n                        \"max_num_query_embeddings\": 100,\n                        \"max_visited\": 32,\n                        \"num_results_per_query\": [\n                            1,\n                            5,\n                            10,\n                            20\n                        ],\n                        \"query_holdout_fraction\": 0.1,\n                        \"quota\": 1000,\n                        \"to_be_indexed_queries_fraction\": 0.25,\n                        \"train_iters\": 1,\n                        \"training_timeout\": 72000\n                    },\n                    \"dataset\": {\n                        \"id\": \"quora-dataset-corpus-2\",\n                        \"created_at\": \"2023-08-24T07:40:45.232142Z\",\n                        \"modified_at\": \"2023-08-24T18:09:55.799396Z\",\n                        \"app_id\": \"quora-dataset\",\n                        \"user_id\": \"isaac\",\n                        \"metadata\": {},\n                        \"visibility\": {\n                            \"gettable\": 10\n                        },\n                        \"version\": {\n                            \"id\": \"dataset-version-1692900595413\",\n                            \"created_at\": \"0001-01-01T00:00:00Z\",\n                            \"modified_at\": \"0001-01-01T00:00:00Z\",\n                            \"app_id\": \"quora-dataset\",\n                            \"user_id\": \"isaac\",\n                            \"dataset_id\": \"quora-dataset-corpus-2\",\n                            \"status\": {\n                                \"code\": 99009,\n                                \"description\": \"Internal error\"\n                            },\n                            \"metadata\": {}\n                        }\n                    }\n                },\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"clusterer\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"BAAI-bge-base-en\",\n            \"name\": \"BAAI-bge-base-en\",\n            \"created_at\": \"2023-08-15T11:36:23.145658Z\",\n            \"modified_at\": \"2023-08-15T11:36:23.145658Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"b55d165cc3c64ed4bab3090c7b402188\",\n                \"created_at\": \"2023-08-15T11:36:23.145658Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"embeddings\": \"embedding\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {\n                    \"params\": {\n                        \"model_config\": {\n                            \"_name_or_path\": \"BAAI/bge-base-en\",\n                            \"architectures\": [\n                                \"BertModel\"\n                            ],\n                            \"attention_probs_dropout_prob\": 0.1,\n                            \"classifier_dropout\": null,\n                            \"gradient_checkpointing\": false,\n                            \"hidden_act\": \"gelu\",\n                            \"hidden_dropout_prob\": 0.1,\n                            \"hidden_size\": 768,\n                            \"id2label\": {\n                                \"0\": \"LABEL_0\"\n                            },\n                            \"initializer_range\": 0.02,\n                            \"intermediate_size\": 3072,\n                            \"label2id\": {\n                                \"LABEL_0\": 0\n                            },\n                            \"layer_norm_eps\": 1e-12,\n                            \"max_position_embeddings\": 512,\n                            \"model_type\": \"bert\",\n                            \"num_attention_heads\": 12,\n                            \"num_hidden_layers\": 12,\n                            \"pad_token_id\": 0,\n                            \"position_embedding_type\": \"absolute\",\n                            \"torch_dtype\": \"float32\",\n                            \"transformers_version\": \"4.30.2\",\n                            \"type_vocab_size\": 2,\n                            \"use_cache\": true,\n                            \"vocab_size\": 30522\n                        },\n                        \"tokenizer_config\": {\n                            \"clean_up_tokenization_spaces\": true,\n                            \"cls_token\": \"[CLS]\",\n                            \"do_basic_tokenize\": true,\n                            \"do_lower_case\": true,\n                            \"mask_token\": \"[MASK]\",\n                            \"model_max_length\": 512,\n                            \"never_split\": null,\n                            \"pad_token\": \"[PAD]\",\n                            \"sep_token\": \"[SEP]\",\n                            \"strip_accents\": null,\n                            \"tokenize_chinese_chars\": true,\n                            \"tokenizer_class\": \"BertTokenizer\",\n                            \"unk_token\": \"[UNK]\"\n                        }\n                    }\n                },\n                \"import_info\": {\n                    \"params\": {\n                        \"model_name\": \"BAAI/bge-base-en\",\n                        \"pipeline_name\": \"feature-extraction\",\n                        \"tokenizer_name\": \"BAAI/bge-base-en\",\n                        \"toolkit\": \"HuggingFace\"\n                    }\n                }\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-embedder\",\n            \"task\": \"representation-learning\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"apparel-recognition\",\n            \"name\": \"apparel\",\n            \"created_at\": \"2016-12-15T01:29:04.622209Z\",\n            \"modified_at\": \"2023-05-23T12:34:15.093542Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"dc2cd6d9bff5425a80bfe0c4105583c1\",\n                \"created_at\": \"2016-12-15T01:29:04.622209Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 112,\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"concepts\": \"softmax\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"display_name\": \"apparel-visual-classifier\",\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-classifier\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"description\": \"AI model for identifying fashion-related and clothing concepts, hats, jewelry, handbags, etc. in images and video.\",\n            \"metadata\": {\n                \"presetInputs\": [\n                    {\n                        \"alt\": \"Clarifai apparel model featuring woman black turtleneck.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/apparel-woman-black-turtleneck.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai apparel model featuring yellow boots.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/apparel-yellow-boots.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai apparel model featuring black white striped socks.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/apparel-black-white-striped-socks.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai apparel model featuring sunglasses.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/apparel-sunglasses.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai apparel model featuring dog in a dog carrier.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/apparel-dog-in-a-dog-carrier.jpg\"\n                    }\n                ]\n            },\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"15b0041cc2cd848a0d8b45f8b83c1d7d\",\n            \"name\": \"CLIP\",\n            \"created_at\": \"2021-12-14T18:07:40.983254Z\",\n            \"modified_at\": \"2023-04-27T20:45:27.183474Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"97f20cc96b7c4bec8f3b96e284ba1173\",\n                \"created_at\": \"2021-12-14T18:07:41.268867Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"embeddings\": \"embeddings\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-embedder\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"notes\": \"This model has been deprecated. Please use `multilingual-multimodal-clip-embed` instead.\",\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"text-translation-english-spanish\",\n            \"name\": \"Helsinki-NLP/opus-mt-en-es\",\n            \"created_at\": \"2023-02-22T22:44:16.825059Z\",\n            \"modified_at\": \"2023-02-22T22:44:16.825059Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"643f30558de34013aff72b0e21f244f5\",\n                \"created_at\": \"2023-02-23T00:39:20.611092Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {\n                    \"params\": {\n                        \"model_config\": {\n                            \"_name_or_path\": \"Helsinki-NLP/opus-mt-en-es\",\n                            \"activation_dropout\": 0,\n                            \"activation_function\": \"swish\",\n                            \"add_bias_logits\": false,\n                            \"add_final_layer_norm\": false,\n                            \"architectures\": [\n                                \"MarianMTModel\"\n                            ],\n                            \"attention_dropout\": 0,\n                            \"bad_words_ids\": [\n                                [\n                                    65000\n                                ]\n                            ],\n                            \"bos_token_id\": 0,\n                            \"classif_dropout\": 0,\n                            \"classifier_dropout\": 0,\n                            \"d_model\": 512,\n                            \"decoder_attention_heads\": 8,\n                            \"decoder_ffn_dim\": 2048,\n                            \"decoder_layerdrop\": 0,\n                            \"decoder_layers\": 6,\n                            \"decoder_start_token_id\": 65000,\n                            \"dropout\": 0.1,\n                            \"encoder_attention_heads\": 8,\n                            \"encoder_ffn_dim\": 2048,\n                            \"encoder_layerdrop\": 0,\n                            \"encoder_layers\": 6,\n                            \"eos_token_id\": 0,\n                            \"extra_pos_embeddings\": 65001,\n                            \"force_bos_token_to_be_generated\": false,\n                            \"forced_eos_token_id\": 0,\n                            \"gradient_checkpointing\": false,\n                            \"id2label\": {\n                                \"0\": \"LABEL_0\",\n                                \"1\": \"LABEL_1\",\n                                \"2\": \"LABEL_2\"\n                            },\n                            \"init_std\": 0.02,\n                            \"is_encoder_decoder\": true,\n                            \"label2id\": {\n                                \"LABEL_0\": 0,\n                                \"LABEL_1\": 1,\n                                \"LABEL_2\": 2\n                            },\n                            \"max_length\": 512,\n                            \"max_position_embeddings\": 512,\n                            \"model_type\": \"marian\",\n                            \"normalize_before\": false,\n                            \"normalize_embedding\": false,\n                            \"num_beams\": 4,\n                            \"num_hidden_layers\": 6,\n                            \"pad_token_id\": 65000,\n                            \"scale_embedding\": true,\n                            \"static_position_embeddings\": true,\n                            \"torch_dtype\": \"float32\",\n                            \"transformers_version\": \"4.16.0\",\n                            \"use_cache\": true,\n                            \"vocab_size\": 65001\n                        },\n                        \"tokenizer_config\": {\n                            \"eos_token\": \"</s>\",\n                            \"model_max_length\": 512,\n                            \"name_or_path\": \"Helsinki-NLP/opus-mt-en-es\",\n                            \"pad_token\": \"<pad>\",\n                            \"source_lang\": \"eng\",\n                            \"sp_model_kwargs\": {},\n                            \"special_tokens_map_file\": null,\n                            \"target_lang\": \"spa\",\n                            \"tokenizer_class\": \"MarianTokenizer\",\n                            \"tokenizer_file\": null,\n                            \"unk_token\": \"<unk>\"\n                        }\n                    }\n                },\n                \"import_info\": {\n                    \"params\": {\n                        \"model_name\": \"Helsinki-NLP/opus-mt-en-es\",\n                        \"pipeline_name\": \"translation_xx_to_yy\",\n                        \"tokenizer_name\": \"Helsinki-NLP/opus-mt-en-es\",\n                        \"toolkit\": \"HuggingFace\"\n                    }\n                }\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-to-text\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"description\": \"Text translation model from English to Spanish using sentence piece-based segmentation\",\n            \"metadata\": {},\n            \"notes\": \"\\n # Helsinki-NLP - English to Spanish                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n ## Introduction                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n The Helsinki-NLP models are used to translate text from one language to another. As such, the model takes a block text as its input, and outputs the translated block of text. This particular model takes in English text as it's input and outputs Spanish text.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n ## Limitations                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n The usage of random capitalization and punctuation may result in erroneous translations grammatically speaking. If you are using this model in a workflow and find grammar issues, you can try utilizing aggregators to minimize errors.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n **More Info**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n * Original Repository: [GitHub](https://github.com/Helsinki-NLP/Tatoeba-Challenge)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \\n * Helsinki-NLP Opus: [eng-spa](https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/master/models/eng-spa)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \\n * Hugging Face docs: [Helsinki-NLP/opus-mt-en-es](https://huggingface.co/Helsinki-NLP/opus-mt-en-es)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n ## Paper                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n [Natural language processing for similar languages, varieties, and dialects: A survey](https://helda.helsinki.fi/bitstream/handle/10138/330117/natural_language_processing_for_similar_languages_varieties_and_dialects_a_survey.pdf)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n Authors: Marcos Zampieri, Preslav Nakov, Yves Scherrer                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n **Abstract**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n There has been a lot of recent interest in the natural language processing (NLP) community in the computational processing of language varieties and dialects, with the aim to improve the performance of applications such as machine translation, speech recognition, and dialogue systems. Here, we attempt to survey this growing field of research, with focus on computational methods for processing similar languages, varieties, and dialects. In particular, we discuss the most important challenges when dealing with diatopic language variation, and we present some of the available datasets, the process of data collection, and the most common data collection strategies used to compile datasets for similar languages, varieties, and dialects. We further present a number of studies on computational methods developed and/or adapted for preprocessing, normalization, part-of-speech tagging, and parsing similar languages, language varieties, and dialects. Finally, we discuss relevant applications such as language and dialect identification and machine translation for closely related languages, language varieties, and dialects.                                     \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n ## Risks, Limitations, and Biases                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n **CONTENT WARNING: Readers should be aware this section contains content that is disturbing, offensive, and can propagate historical and current stereotypes.**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n There has been significant research exploring bias and fairness issues with language models. Some important papers in this field include:                                                   # Helsinki-NLP - English to Spanish                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n ## Introduction                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n The Helsinki-NLP models are used to translate text from one language to another. As such, the model takes a block text as its input, and outputs the translated block of text. This particular model takes in English text as it's input and outputs Spanish text.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n ## Limitations                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n The usage of random capitalization and punctuation may result in erroneous translations grammatically speaking. If you are using this model in a workflow and find grammar issues, you can try utilizing aggregators to minimize errors.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n **More Info**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n * Original Repository: [GitHub](https://github.com/Helsinki-NLP/Tatoeba-Challenge)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \\n * Helsinki-NLP Opus: [eng-spa](https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/master/models/eng-spa)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \\n * Hugging Face docs: [Helsinki-NLP/opus-mt-en-es](https://huggingface.co/Helsinki-NLP/opus-mt-en-es)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n ## Paper                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n [Natural language processing for similar languages, varieties, and dialects: A survey](https://helda.helsinki.fi/bitstream/handle/10138/330117/natural_language_processing_for_similar_languages_varieties_and_dialects_a_survey.pdf)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n Authors: Marcos Zampieri, Preslav Nakov, Yves Scherrer                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n **Abstract**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n There has been a lot of recent interest in the natural language processing (NLP) community in the computational processing of language varieties and dialects, with the aim to improve the performance of applications such as machine translation, speech recognition, and dialogue systems. Here, we attempt to survey this growing field of research, with focus on computational methods for processing similar languages, varieties, and dialects. In particular, we discuss the most important challenges when dealing with diatopic language variation, and we present some of the available datasets, the process of data collection, and the most common data collection strategies used to compile datasets for similar languages, varieties, and dialects. We further present a number of studies on computational methods developed and/or adapted for preprocessing, normalization, part-of-speech tagging, and parsing similar languages, language varieties, and dialects. Finally, we discuss relevant applications such as language and dialect identification and machine translation for closely related languages, language varieties, and dialects.                                     \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n ## Risks, Limitations, and Biases                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n **CONTENT WARNING: Readers should be aware this section contains content that is disturbing, offensive, and can propagate historical and current stereotypes.**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n There has been significant research exploring bias and fairness issues with language models. Some important papers in this field include:                                                  \\n * [Societal Biases in Language Generation: Progress and Challenges](https://aclanthology.org/2021.acl-long.330.pdf)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \\n     * Authors: Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, Nanyun Peng                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \\n     * Abstract: Technology for language generation has advanced rapidly, spurred by advancements in pre-training large models on massive amounts of data and the need for intelligent agents to communicate in a natural manner. While techniques can effectively generate fluent text, they can also produce undesirable societal biases that can have a disproportionately negative impact on marginalized populations. Language generation presents unique challenges for biases in terms of direct user interaction and the structure of decoding techniques. To better understand these challenges, we present a survey on societal biases in language generation, focusing on how data and techniques contribute to biases and progress towards reducing biases. Motivated by a lack of studies on biases from decoding techniques, we also conduct experiments to quantify the effects of these techniques. By further discussing general trends and open challenges, we call to attention promising directions for research and the importance of fairness and inclusivity considerations for language generation applications.<br /><br />                                                          \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n * [On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \\n     * Authors: Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, Shmargaret Shmitchell                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \\n     * Abstract: The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n ## Benchmarks                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n The following benchmarks are for the **opus-2021-02-19** weights.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n | testset                        | BLEU | chr-F | #sent | #words | BP    |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\n | ------------------------------ | ---- | ----- | ----- | ------ | ----- |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\n | newssyscomb2009-engspa.eng.spa | 31.3 | 0.583 | 502   | 12506  | 0.990 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\n | news-test2008-engspa.eng.spa   | 29.6 | 0.564 | 2051  | 52596  | 1.000 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\n | newstest2009-engspa.eng.spa    | 30.2 | 0.578 | 2525  | 68114  | 1.000 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\n | newstest2010-engspa.eng.spa    | 36.9 | 0.620 | 2489  | 65522  | 1.000 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\n | newstest2011-engspa.eng.spa    | 38.3 | 0.620 | 3003  | 79476  | 0.984 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\n | newstest2012-engspa.eng.spa    | 39.1 | 0.626 | 3003  | 79006  | 0.969 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\n | newstest2013-engspa.eng.spa    | 35.1 | 0.598 | 3000  | 70528  | 0.960 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\n | Tatoeba-test.eng.spa           | 55.1 | 0.721 | 10000 | 77311  | 0.978 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\n | tico19-test.eng-spa            | 50.4 | 0.727 | 2100  | 66591  | 0.959 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n ## Additional Info                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n * Data set: Opus                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \\n * Model: Transformer                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \\n * Source Language(s): en (English)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \\n * Target Language(s): es (Spanish)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \\n * Pre-processing: Normalization  [SentencePiece](https://github.com/google/sentencepiece) (spm32k, spm32k)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \\n * Download original weights: [opus-2021-02-19.zip](https://object.pouta.csc.fi/Tatoeba-MT-models/eng-spa/opus-2021-02-19.zip)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \\n * Test set translations: [opus-2021-02-19.test.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/eng-spa/opus-2021-02-19.test.txt)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \\n * Test set scores: [opus-2021-02-19.eval.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/eng-spa/opus-2021-02-19.eval.txt)\\n\",\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"ocr_model_v3-1677100451\",\n            \"name\": \"ocr_model_v3-1677100451\",\n            \"created_at\": \"2023-02-22T21:14:10.921823Z\",\n            \"modified_at\": \"2023-02-22T21:14:10.921823Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"22894138385843978aaa97cae37780fb\",\n                \"created_at\": \"2023-02-22T21:14:10.928773Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\",\n                        \"regions[...].value\": \"predicted_det_scores\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {\n                \"presetInputs\": [\n                    {\n                        \"alt\": \"Stop Sign.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/9/9d/Stop_sign_light_red.svg/1200px-Stop_sign_light_red.svg.png\"\n                    }\n                ]\n            },\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"ocr-scene-english-paddleocr\",\n            \"name\": \"OCR Scene English PaddleOCR\",\n            \"created_at\": \"2023-02-22T15:48:10.066388Z\",\n            \"modified_at\": \"2023-02-22T15:48:10.066388Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"40dbb2c9cde44a27af226782e7157006\",\n                \"created_at\": \"2023-02-22T15:49:55.126424Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"description\": \"An OCR model for detecting and recognizing English text in images that are more complex than scans of a page.\",\n            \"metadata\": {\n                \"presetInputs\": [\n                    {\n                        \"type\": \"image\",\n                        \"url\": \"https://samples.clarifai.com/featured-models/ocr-woman-holding-sold-sign.jpg\"\n                    },\n                    {\n                        \"type\": \"image\",\n                        \"url\": \"https://samples.clarifai.com/featured-models/paddleocrs/ocr-scene-english-paddleocr-1.jpg\"\n                    },\n                    {\n                        \"type\": \"image\",\n                        \"url\": \"https://samples.clarifai.com/featured-models/paddleocrs/ocr-scene-english-paddleocr-2.jpg\"\n                    },\n                    {\n                        \"type\": \"image\",\n                        \"url\": \"https://samples.clarifai.com/featured-models/paddleocrs/ocr-scene-english-paddleocr-3.png\"\n                    }\n                ]\n            },\n            \"notes\": \"\\n # Introduction                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n PaddleOCR aims to create multilingual, awesome, leading, and practical OCR tools that help users train better models and apply them into practice. The information in this summary is taken from their [Github.](https://github.com/PaddlePaddle/PaddleOCR)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n Release PP-OCRv3: With comparable speed, the effect of Chinese scene is further improved by 5% compared with PP-OCRv2, the effect of English scene is improved by 11%, and the average recognition accuracy of 80 language multilingual models is improved by more than 5%.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n <iframe width=\\\"560\\\" height=\\\"315\\\" src=\\\"https://www.youtube.com/embed/ITTtqGKtS54\\\" title=\\\"YouTube video player\\\" frameborder=\\\"0\\\" allow=\\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\\\" allowfullscreen></iframe>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n # Features                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n PaddleOCR support a variety of cutting-edge algorithms related to OCR, and developed industrial featured models/solution [PP-OCR](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.5/doc/doc_en/ppocr_introduction_en.md) and [PP-Structure](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.5/ppstructure/README.md) on this basis, and get through the whole process of data production, model training, compression, inference and deployment.                                                                                                                                                                                                                                                                                                  \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n ## PP-OCR Series Model List - This model is the English ultra-lightweight PP-OCRv3 model (13.4M) on the second row.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n | Model introduction                                           | Model name                   | Recommended scene | Detection model                                              | Direction classifier                                         | Recognition model                                            |                                                                                                                                                                                                                                                                                                                                                                                                                                                    \\n | ------------------------------------------------------------ | ---------------------------- | ----------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |                                                                                                                                                                                                                                                                                                                                                                                                                                                    \\n | Chinese and English ultra-lightweight PP-OCRv3 model（16.2M）     | ch_PP-OCRv3_xx          | Mobile & Server | [inference model](https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_det_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_det_distill_train.tar) | [inference model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_train.tar) | [inference model](https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_rec_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_rec_train.tar) |                                 \\n | English ultra-lightweight PP-OCRv3 model（13.4M）     | en_PP-OCRv3_xx          | Mobile & Server | [inference model](https://paddleocr.bj.bcebos.com/PP-OCRv3/english/en_PP-OCRv3_det_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/PP-OCRv3/english/en_PP-OCRv3_det_distill_train.tar) | [inference model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_train.tar) | [inference model](https://paddleocr.bj.bcebos.com/PP-OCRv3/english/en_PP-OCRv3_rec_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/PP-OCRv3/english/en_PP-OCRv3_rec_train.tar) |                                             \\n | Chinese and English ultra-lightweight PP-OCRv2 model（11.6M） |  ch_PP-OCRv2_xx |Mobile & Server|[inference model](https://paddleocr.bj.bcebos.com/PP-OCRv2/chinese/ch_PP-OCRv2_det_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/PP-OCRv2/chinese/ch_PP-OCRv2_det_distill_train.tar)| [inference model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_train.tar) |[inference model](https://paddleocr.bj.bcebos.com/PP-OCRv2/chinese/ch_PP-OCRv2_rec_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/PP-OCRv2/chinese/ch_PP-OCRv2_rec_train.tar)|                                                   \\n | Chinese and English ultra-lightweight PP-OCR model (9.4M)       | ch_ppocr_mobile_v2.0_xx      | Mobile & server   |[inference model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_det_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_det_train.tar)|[inference model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_train.tar) |[inference model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_rec_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_rec_train.tar)      |   \\n | Chinese and English general PP-OCR model (143.4M)               | ch_ppocr_server_v2.0_xx      | Server            |[inference model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_server_v2.0_det_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_server_v2.0_det_train.tar)    |[inference model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_train.tar)    |[inference model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_server_v2.0_rec_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_server_v2.0_rec_train.tar)  |\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n - For more model downloads (including multiple languages), please refer to [PP-OCR series model downloads](./doc/doc_en/models_list_en.md).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \\n - For a new language request, please refer to [Guideline for new language_requests](#language_requests).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \\n - For structural document analysis models, please refer to [PP-Structure models](./ppstructure/docs/models_list_en.md).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n # PP-OCRv3 English model                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \\n ![](https://github.com/PaddlePaddle/PaddleOCR/raw/release/2.5/doc/imgs_results/PP-OCRv3/en/en_1.png)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n # PP-OCRv3 Chinese model                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \\n ![](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.5/doc/imgs_results/PP-OCRv3/ch/PP-OCRv3-pic003.jpg?raw=true)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n # PP-OCRv3 Multilingual model                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \\n ![](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.5/doc/imgs_results/PP-OCRv3/multi_lang/korean_1.jpg?raw=true)\",\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"ocr-scene-chinese-english-paddleocr\",\n            \"name\": \"ocr-scene-chinese-english-paddleocr\",\n            \"created_at\": \"2022-08-10T21:27:40.359110Z\",\n            \"modified_at\": \"2023-02-09T23:37:22.184921Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"67104613bc7245b594d6a38eb7e34974\",\n                \"created_at\": \"2022-08-10T21:27:40.889145Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {\n                \"presetInputs\": [\n                    {\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/general-shirts-bags-shoes-computer.jpg\"\n                    }\n                ]\n            },\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"multilingual-multimodal-clip-embed\",\n            \"name\": \"Multilingual Multimodal Clip Embedder\",\n            \"created_at\": \"2023-01-30T17:46:05.745974Z\",\n            \"modified_at\": \"2023-01-30T17:46:05.745974Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"e3289fa66be4419eb2958ba74b6e9fee\",\n                \"created_at\": \"2023-01-30T17:46:05.745974Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"train_stats\": {},\n                \"completed_at\": \"2023-01-30T17:46:05.745974Z\",\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"embeddings\": \"embeddings\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\",\n                        \"text\": \"text\"\n                    },\n                    \"params\": {\n                        \"text_token_warning_limit\": 77\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"multimodal-embedder\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"description\": \"CLIP-based multilingual multimodal embedding model.\",\n            \"metadata\": {},\n            \"notes\": \"##Multilingual CLIP\",\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"ccfc0043fa804de4a586005f72582e00\",\n            \"name\": \"Multimodal Clip Clusterer\",\n            \"created_at\": \"2022-11-16T14:51:43.695740Z\",\n            \"modified_at\": \"2022-11-16T14:51:43.695740Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"4b134b9fb5f24e2bb09b7493560cc922\",\n                \"created_at\": \"2022-11-16T14:51:43.695740Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"metrics\": {\n                    \"status\": {\n                        \"code\": 21300,\n                        \"description\": \"Model was successfully evaluated.\"\n                    },\n                    \"summary\": {\n                        \"macro_avg_roc_auc\": 0,\n                        \"macro_std_roc_auc\": 0,\n                        \"macro_avg_f1_score\": 0,\n                        \"macro_std_f1_score\": 0,\n                        \"macro_avg_precision\": 0,\n                        \"macro_avg_recall\": 0,\n                        \"lopq_metrics\": [\n                            {\n                                \"k\": 10,\n                                \"recall_vs_brute_force\": 0.95100015,\n                                \"kendall_tau_vs_brute_force\": 1,\n                                \"most_frequent_code_percent\": 0.41158637,\n                                \"lopq_ndcg\": 0,\n                                \"brute_force_ndcg\": 0\n                            },\n                            {\n                                \"k\": 20,\n                                \"recall_vs_brute_force\": 0.93349975,\n                                \"kendall_tau_vs_brute_force\": 0.99947363,\n                                \"most_frequent_code_percent\": 0.41158637,\n                                \"lopq_ndcg\": 0,\n                                \"brute_force_ndcg\": 0\n                            },\n                            {\n                                \"k\": 50,\n                                \"recall_vs_brute_force\": 0.9118,\n                                \"kendall_tau_vs_brute_force\": 0.99697524,\n                                \"most_frequent_code_percent\": 0.41158637,\n                                \"lopq_ndcg\": 0,\n                                \"brute_force_ndcg\": 0\n                            },\n                            {\n                                \"k\": 100,\n                                \"recall_vs_brute_force\": 0.8869,\n                                \"kendall_tau_vs_brute_force\": 0.9947445,\n                                \"most_frequent_code_percent\": 0.41158637,\n                                \"lopq_ndcg\": 0,\n                                \"brute_force_ndcg\": 0\n                            },\n                            {\n                                \"k\": 200,\n                                \"recall_vs_brute_force\": 0.8432002,\n                                \"kendall_tau_vs_brute_force\": 0.9947241,\n                                \"most_frequent_code_percent\": 0.41158637,\n                                \"lopq_ndcg\": 0,\n                                \"brute_force_ndcg\": 0\n                            }\n                        ]\n                    }\n                },\n                \"total_input_count\": 9527727,\n                \"train_stats\": {},\n                \"completed_at\": \"2022-11-16T14:51:43.695740Z\",\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\"\n                },\n                \"input_info\": {\n                    \"base_embed_model\": {\n                        \"id\": \"multimodal-clip-embed\",\n                        \"app_id\": \"main\",\n                        \"model_version\": {\n                            \"id\": \"9fe2c8962c104327bc87b8f8104b161a\"\n                        },\n                        \"user_id\": \"clarifai\",\n                        \"model_type_id\": \"multimodal-embedder\",\n                        \"toolkits\": [],\n                        \"use_cases\": [],\n                        \"languages\": [],\n                        \"languages_full\": [],\n                        \"check_consents\": []\n                    }\n                },\n                \"train_info\": {\n                    \"params\": {\n                        \"beta\": 1,\n                        \"coarse_clusters\": 125,\n                        \"dataset_id\": \"\",\n                        \"dataset_version_id\": \"ee243135d683462eaa1060c4f5c63725\",\n                        \"eval_holdout_fraction\": 0.2,\n                        \"max_num_query_embeddings\": 100,\n                        \"max_visited\": 1562,\n                        \"num_results_per_query\": [\n                            10,\n                            20,\n                            50,\n                            100,\n                            200\n                        ],\n                        \"query_holdout_fraction\": 0.1,\n                        \"quota\": 10000,\n                        \"to_be_indexed_queries_fraction\": 0.25,\n                        \"train_iters\": 1,\n                        \"training_timeout\": 86400\n                    }\n                },\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"clusterer\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {},\n            \"notes\": \"##CLIP\",\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"multimodal-clip-embed\",\n            \"name\": \"multimodal-clip\",\n            \"created_at\": \"2022-11-07T17:47:19.112250Z\",\n            \"modified_at\": \"2022-11-07T17:47:19.112250Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"9fe2c8962c104327bc87b8f8104b161a\",\n                \"created_at\": \"2022-11-07T17:47:19.123181Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"embeddings\": \"embeddings\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\",\n                        \"text\": \"text\"\n                    },\n                    \"params\": {\n                        \"text_token_warning_limit\": 77\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"multimodal-embedder\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"da94111b740547aeae38ba9668f998a3\",\n            \"name\": \"ocr-scene-devanagari-paddleocr\",\n            \"created_at\": \"2022-08-10T19:52:53.761109Z\",\n            \"modified_at\": \"2022-08-10T19:52:53.761109Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"5e35e10fb7814f5c9223ccb3c3afebec\",\n                \"created_at\": \"2022-08-10T19:52:53.956768Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"5f42b31a4589d672152e9668d02eb471\",\n            \"name\": \"ocr-scene-cyrillic-paddleocr\",\n            \"created_at\": \"2022-08-10T19:52:53.009976Z\",\n            \"modified_at\": \"2022-08-10T19:52:53.009976Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"bbb2a719af92447ebeaabc88d3f41123\",\n                \"created_at\": \"2022-08-10T19:52:53.252145Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"e1e3b6f78fd2d55c830c631434ba83a5\",\n            \"name\": \"ocr-scene-arabic-paddleocr\",\n            \"created_at\": \"2022-08-10T19:52:51.994687Z\",\n            \"modified_at\": \"2022-08-10T19:52:51.994687Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"4b33b79b4b2e42b4b9ee07c844f1bb56\",\n                \"created_at\": \"2022-08-10T19:52:52.548327Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"c54e41dd13c9669630426078d36718ec\",\n            \"name\": \"ocr-scene-latin-paddleocr\",\n            \"created_at\": \"2022-08-10T19:52:51.331917Z\",\n            \"modified_at\": \"2022-08-10T19:52:51.331917Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"a82c91d715f74c789df25c78e811dc6a\",\n                \"created_at\": \"2022-08-10T19:52:51.487313Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"e3fde27570dc04aff3b997a223601ac8\",\n            \"name\": \"ocr-scene-kannada-paddleocr\",\n            \"created_at\": \"2022-08-10T19:52:50.570402Z\",\n            \"modified_at\": \"2022-08-10T19:52:50.570402Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"324f4a4d0bc64d81ab7be56c21748328\",\n                \"created_at\": \"2022-08-10T19:52:50.803339Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"1d5cbe1f275acaa8ac109dec6e07120b\",\n            \"name\": \"ocr-scene-telugu-paddleocr\",\n            \"created_at\": \"2022-08-10T19:52:49.858398Z\",\n            \"modified_at\": \"2022-08-10T19:52:49.858398Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"004b78b3cc3b4e2b9fe53f3cda5ff001\",\n                \"created_at\": \"2022-08-10T19:52:50.014584Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"019c84b9cae1c41e7bf661a037f6ac12\",\n            \"name\": \"ocr-scene-tamil-paddleocr\",\n            \"created_at\": \"2022-08-10T19:52:49.207422Z\",\n            \"modified_at\": \"2022-08-10T19:52:49.207422Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"88bf32db4fdb488593303bc5fb309754\",\n                \"created_at\": \"2022-08-10T19:52:49.353280Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"6748476e7c5ae4df9bc2019dd6bd5892\",\n            \"name\": \"ocr-scene-japanese-paddleocr\",\n            \"created_at\": \"2022-08-10T19:52:48.506276Z\",\n            \"modified_at\": \"2022-08-10T19:52:48.506276Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"b3a22a7bc99943b998de94c1e3c5d420\",\n                \"created_at\": \"2022-08-10T19:52:48.703164Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"bf08cee902d18405ca6ac58b68b3e2a6\",\n            \"name\": \"ocr-scene-korean-paddleocr\",\n            \"created_at\": \"2022-08-10T19:52:47.804473Z\",\n            \"modified_at\": \"2022-08-10T19:52:47.804473Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"f242bc5057b4402dab82ca0dbaf1be1e\",\n                \"created_at\": \"2022-08-10T19:52:47.982674Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"general-image-detection\",\n            \"name\": \"Image Detection\",\n            \"created_at\": \"2020-09-02T13:49:26.221543Z\",\n            \"modified_at\": \"2022-06-13T17:34:27.250292Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"1586f39b8d8040dda41537f5e47d68f0\",\n                \"created_at\": \"2020-10-20T16:16:33.521041Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 601,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0.25\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"description\": \"Detects a variety of common objects and the location and generates regions of an image that may contain that object.\",\n            \"metadata\": {\n                \"presetInputs\": [\n                    {\n                        \"alt\": \"Clarifai general model featuring shirts bags shoes computer.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/general-shirts-bags-shoes-computer.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai general model featuring elephants.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/general-elephants.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai food model featuring hamburgers bacon cheese.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/food-hamburgers-bacon-cheese.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai general model featuring car dashboard steering wheel.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/general-car-dashboard-steering-wheel.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai general model featuring city buildings skyscraper.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/general-city-buildings-skyscraper.jpg\"\n                    }\n                ]\n            },\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"general-image-recognition\",\n            \"name\": \"Image Recognition\",\n            \"created_at\": \"2016-03-19T04:14:25.007149Z\",\n            \"modified_at\": \"2022-06-13T17:34:27.240649Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"aa7f35c01e0642fda5cf400f543e7c40\",\n                \"created_at\": \"2018-03-06T19:43:54Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 9098,\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"concepts\": \"softmax\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"display_name\": \"general-visual-classifier\",\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-classifier\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"description\": \"Identifies a variety of concepts in images and video including objects, themes, and more. Trained with over 10,000 concepts and 20M images.\",\n            \"metadata\": {\n                \"presetInputs\": [\n                    {\n                        \"alt\": \"Clarifai general model featuring shirts bags shoes computer.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/general-shirts-bags-shoes-computer.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai general model featuring elephants.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/general-elephants.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai food model featuring hamburgers bacon cheese.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/food-hamburgers-bacon-cheese.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai general model featuring car dashboard steering wheel.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/general-car-dashboard-steering-wheel.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai general model featuring city buildings skyscraper.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/general-city-buildings-skyscraper.jpg\"\n                    }\n                ]\n            },\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"celebrity-face-detection\",\n            \"name\": \"Celebrity Face Detection\",\n            \"created_at\": \"2016-10-25T19:02:38.845777Z\",\n            \"modified_at\": \"2022-06-13T17:34:27.225442Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"bc50aad2bf4b403b94656aa9f64d1454\",\n                \"created_at\": \"2019-04-27T15:52:36.026793Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 10553,\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\"\n                },\n                \"input_info\": {},\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"description\": \"Detects whether images or video contain celebrity faces. Trained with over 10,000 recognized celebrities.\",\n            \"metadata\": {\n                \"presetInputs\": [\n                    {\n                        \"alt\": \"Clarifai celebrity model featuring morgan freeman.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/celebrity-morgan-freeman.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai celebrity model featuring kim kardashian.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/celebrity-kim-kardashian.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai celebrity model featuring matt damon.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/celebrity-matt-damon.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai celebrity model featuring aziz ansari.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/celebrity-aziz-ansari.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai celebrity model featuring angelina jolie.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/celebrity-angelina-jolie.jpg\"\n                    }\n                ]\n            },\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [\n                \"pii\"\n            ],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"moderation-recognition\",\n            \"name\": \"Image Moderation\",\n            \"created_at\": \"2017-05-12T21:28:00.471607Z\",\n            \"modified_at\": \"2022-06-13T17:34:27.216052Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"aa8be956dbaa4b7a858826a84253cab9\",\n                \"created_at\": \"2017-10-26T20:29:09.263232Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 5,\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"concepts\": \"softmax\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-classifier\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"description\": \"Recognizes inappropriate content in images and video containing concepts: gore, drug, explicit, suggestive, and safe.\",\n            \"metadata\": {\n                \"presetInputs\": [\n                    {\n                        \"alt\": \"Clarifai moderation model featuring medical syringe cocaine.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/moderation-medical-syringe-cocaine.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai moderation model featuring woman breast feeding.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/moderation-woman-breast-feeding.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai moderation model featuring cutting red meet blood.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/moderation-cutting-red-meet-blood.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai moderation model featuring woman man no shirts.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/moderation-woman-man-no-shirts.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai moderation model featuring try your own text.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/moderation-try-your-own-text.jpg\"\n                    }\n                ]\n            },\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"96298cb700d5ec33176b40eee032f6df\",\n            \"name\": \"army-crada-5\",\n            \"created_at\": \"2022-04-28T01:41:20.604596Z\",\n            \"modified_at\": \"2022-04-28T01:41:20.604596Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"fdf1903eca5d47a8aa80d80fb0be6187\",\n                \"created_at\": \"2022-04-28T01:41:20.638136Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 25,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0,\n                        \"max_concepts\": 20,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"df518ae2dcd8c2ef72c13e1a06f4ef52\",\n            \"name\": \"BLIP\",\n            \"created_at\": \"2022-04-21T23:11:06.152830Z\",\n            \"modified_at\": \"2022-04-21T23:11:06.152830Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"463f6b55c0b84128b97f6af550386aaa\",\n                \"created_at\": \"2022-04-21T23:11:06.167104Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"embeddings\": \"embeddings\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-embedder\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"3dfd7edda8a9cfc4ae9feab7194274a6\",\n            \"name\": \"BLIP\",\n            \"created_at\": \"2022-04-21T23:10:58.431676Z\",\n            \"modified_at\": \"2022-04-21T23:10:58.431676Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"d25dfc5c081342cbb09c60672b06b18f\",\n                \"created_at\": \"2022-04-21T23:10:58.444450Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"embeddings\": \"embeddings\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-embedder\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"7c2f2dfc80f0286b72b689379e9e81e0\",\n            \"name\": \"general-english-image-caption-blip\",\n            \"created_at\": \"2022-04-18T18:55:56.785Z\",\n            \"modified_at\": \"2022-04-18T18:55:56.785Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"d66b4cd5b5b249489c98543632f13078\",\n                \"created_at\": \"2022-04-18T18:55:56.802386Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"image-to-text\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"2a34e347777e5744496bf6ae76e01e91\",\n            \"name\": \"ocr-document-english-printed-trocr-large\",\n            \"created_at\": \"2022-04-11T17:35:23.575656Z\",\n            \"modified_at\": \"2022-04-11T17:35:23.575656Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"761de02719ce48aaafaa8ac16e3e4117\",\n                \"created_at\": \"2022-04-11T17:35:23.596706Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"ocr-scene-french-paddleocr\",\n            \"name\": \"ocr-scene-french-paddleocr\",\n            \"created_at\": \"2022-02-16T19:46:42.443570Z\",\n            \"modified_at\": \"2022-04-05T12:42:47.955693Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"e1932c6028db4a428462ceeaa8a06ba7\",\n                \"created_at\": \"2022-02-16T19:46:42.458876Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"c4d4284d2d52988de50c95791f161e63\",\n            \"name\": \"logos-yolov5\",\n            \"created_at\": \"2022-03-29T23:13:34.162269Z\",\n            \"modified_at\": \"2022-03-31T20:09:51.482312Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"244f2a67c17d42fda8483a116edc9d3f\",\n                \"created_at\": \"2022-03-31T20:09:51.490755Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 3464,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0,\n                        \"max_concepts\": 20,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"ner-english\",\n            \"name\": \"ner_english_v2\",\n            \"created_at\": \"2022-03-16T13:48:57.921631Z\",\n            \"modified_at\": \"2022-03-22T08:06:44.455695Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"a813ff5b362c41f790c506b871e7dea4\",\n                \"created_at\": \"2022-03-16T13:48:57.946788Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 9,\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"\": \"start\",\n                        \"regions[...].data.concepts[...].id\": \"labels\",\n                        \"regions[...].data.concepts[...].value\": \"scores\"\n                    },\n                    \"params\": {\n                        \"max_concepts\": 20,\n                        \"min_value\": 0,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {\n                    \"params\": {\n                        \"model_config\": {\n                            \"_name_or_path\": \"dbmdz/bert-large-cased-finetuned-conll03-english\",\n                            \"_num_labels\": 9,\n                            \"architectures\": [\n                                \"BertForTokenClassification\"\n                            ],\n                            \"attention_probs_dropout_prob\": 0.1,\n                            \"classifier_dropout\": null,\n                            \"directionality\": \"bidi\",\n                            \"hidden_act\": \"gelu\",\n                            \"hidden_dropout_prob\": 0.1,\n                            \"hidden_size\": 1024,\n                            \"id2label\": {\n                                \"0\": \"O\",\n                                \"1\": \"B-MISC\",\n                                \"2\": \"I-MISC\",\n                                \"3\": \"B-PER\",\n                                \"4\": \"I-PER\",\n                                \"5\": \"B-ORG\",\n                                \"6\": \"I-ORG\",\n                                \"7\": \"B-LOC\",\n                                \"8\": \"I-LOC\"\n                            },\n                            \"initializer_range\": 0.02,\n                            \"intermediate_size\": 4096,\n                            \"label2id\": {\n                                \"B-LOC\": 7,\n                                \"B-MISC\": 1,\n                                \"B-ORG\": 5,\n                                \"B-PER\": 3,\n                                \"I-LOC\": 8,\n                                \"I-MISC\": 2,\n                                \"I-ORG\": 6,\n                                \"I-PER\": 4,\n                                \"O\": 0\n                            },\n                            \"layer_norm_eps\": 1e-12,\n                            \"max_position_embeddings\": 512,\n                            \"model_type\": \"bert\",\n                            \"num_attention_heads\": 16,\n                            \"num_hidden_layers\": 24,\n                            \"pad_token_id\": 0,\n                            \"pooler_fc_size\": 768,\n                            \"pooler_num_attention_heads\": 12,\n                            \"pooler_num_fc_layers\": 3,\n                            \"pooler_size_per_head\": 128,\n                            \"pooler_type\": \"first_token_transform\",\n                            \"position_embedding_type\": \"absolute\",\n                            \"torch_dtype\": \"float32\",\n                            \"transformers_version\": \"4.16.0\",\n                            \"type_vocab_size\": 2,\n                            \"use_cache\": true,\n                            \"vocab_size\": 28996\n                        },\n                        \"tokenizer_config\": {\n                            \"cls_token\": \"[CLS]\",\n                            \"do_basic_tokenize\": true,\n                            \"do_lower_case\": false,\n                            \"mask_token\": \"[MASK]\",\n                            \"max_len\": 512,\n                            \"name_or_path\": \"dbmdz/bert-large-cased-finetuned-conll03-english\",\n                            \"never_split\": null,\n                            \"pad_token\": \"[PAD]\",\n                            \"sep_token\": \"[SEP]\",\n                            \"special_tokens_map_file\": null,\n                            \"strip_accents\": null,\n                            \"tokenize_chinese_chars\": true,\n                            \"tokenizer_class\": \"BertTokenizer\",\n                            \"unk_token\": \"[UNK]\"\n                        }\n                    }\n                },\n                \"import_info\": {\n                    \"params\": {\n                        \"model_name\": \"dbmdz/bert-large-cased-finetuned-conll03-english\",\n                        \"pipeline_name\": \"ner\",\n                        \"tokenizer_name\": \"dbmdz/bert-large-cased-finetuned-conll03-english\",\n                        \"toolkit\": \"HuggingFace\"\n                    }\n                }\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-token-classifier\",\n            \"task\": \"named-entity-recognition\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {},\n            \"notes\": \"dbmdz/bert-large-cased-finetuned-conll03-english\",\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"4828602e14c60157672b103bc4174b6a\",\n            \"name\": \"ocr-scene-devanagari-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:05.691047Z\",\n            \"modified_at\": \"2022-02-16T19:51:05.691047Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"9676ee4c210f42aabb032be9e077e34e\",\n                \"created_at\": \"2022-02-16T19:51:05.695789Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"79ae4202a52a10bf910158747e2b0bd6\",\n            \"name\": \"ocr-scene-cyrillic-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:05.332230Z\",\n            \"modified_at\": \"2022-02-16T19:51:05.332230Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"eecd3d4159e447f982df39f4efc6e7a3\",\n                \"created_at\": \"2022-02-16T19:51:05.337055Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"ce1ec795e7613564611373788f719e76\",\n            \"name\": \"ocr-scene-latin-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:04.953704Z\",\n            \"modified_at\": \"2022-02-16T19:51:04.953704Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"fc43e001904541188ae7c2fc9ff1d7b8\",\n                \"created_at\": \"2022-02-16T19:51:04.961006Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"a440094009739c7f8470208af640d72b\",\n            \"name\": \"ocr-scene-tamil-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:04.601929Z\",\n            \"modified_at\": \"2022-02-16T19:51:04.601929Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"3850ab217dc048dfa92bb99c612445ed\",\n                \"created_at\": \"2022-02-16T19:51:04.607237Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"b6e3b7e79bc7d1870ee895ea802da533\",\n            \"name\": \"ocr-scene-kannada-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:04.222500Z\",\n            \"modified_at\": \"2022-02-16T19:51:04.222500Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"1552d0c25c4c453983931e02f762ad40\",\n                \"created_at\": \"2022-02-16T19:51:04.227503Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"7d4b60ad1f153879defaa4ecfd25d4f4\",\n            \"name\": \"ocr-scene-telugu-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:03.816522Z\",\n            \"modified_at\": \"2022-02-16T19:51:03.816522Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"ac7029e22f054edd809ea197524c7fc8\",\n                \"created_at\": \"2022-02-16T19:51:03.821861Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"abb6ab21bf2395f88eacb3c32fbb5fc0\",\n            \"name\": \"ocr-scene-belarusian-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:03.371226Z\",\n            \"modified_at\": \"2022-02-16T19:51:03.371226Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"6be6fdaa01854f2aa3c75a31cc9e9193\",\n                \"created_at\": \"2022-02-16T19:51:03.377576Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"819928b81ec7e0b9cb18c26175447b42\",\n            \"name\": \"ocr-scene-ukranian-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:02.987649Z\",\n            \"modified_at\": \"2022-02-16T19:51:02.987649Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"8d4bf420524a47d0a476d6e86fee65dd\",\n                \"created_at\": \"2022-02-16T19:51:02.992349Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"3953adcdf93719125977186c1e2d3f8d\",\n            \"name\": \"ocr-scene-bulgarian-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:02.616626Z\",\n            \"modified_at\": \"2022-02-16T19:51:02.616626Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"c960deec8cb34e07a800d9cc237e46f5\",\n                \"created_at\": \"2022-02-16T19:51:02.622189Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"79aa2ea6d2eaf696c93fda9eae8e360b\",\n            \"name\": \"ocr-scene-serbian-latin-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:02.186999Z\",\n            \"modified_at\": \"2022-02-16T19:51:02.186999Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"3d49fec2485f42b197d0183ba098b011\",\n                \"created_at\": \"2022-02-16T19:51:02.191990Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"d5a14580da35c5ad0b2d2bd27ce81928\",\n            \"name\": \"ocr-scene-serbian-cyrillic-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:01.799532Z\",\n            \"modified_at\": \"2022-02-16T19:51:01.799532Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"f4c110d928de45a2b9b289ec903b4a6a\",\n                \"created_at\": \"2022-02-16T19:51:01.804656Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"ca24b6b1b79b2288794ec0fd61dc43b5\",\n            \"name\": \"ocr-scene-nepali-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:01.454280Z\",\n            \"modified_at\": \"2022-02-16T19:51:01.454280Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"0cab8fcaaccd4c769dd885fc55f47214\",\n                \"created_at\": \"2022-02-16T19:51:01.458814Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"d36ee770992af4049520a0728ce0db08\",\n            \"name\": \"ocr-scene-marathi-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:01.034772Z\",\n            \"modified_at\": \"2022-02-16T19:51:01.034772Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"2f8faace78e144b3afc6f767b3fb8629\",\n                \"created_at\": \"2022-02-16T19:51:01.040656Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"84463f4cb26b41d899da8ad49189e47b\",\n            \"name\": \"ocr-scene-occitan-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:00.574910Z\",\n            \"modified_at\": \"2022-02-16T19:51:00.574910Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"e12bc12d8e254d239945e50eaf7c79ac\",\n                \"created_at\": \"2022-02-16T19:51:00.583308Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"7f9563940b3cc48321ae8d126daa0403\",\n            \"name\": \"ocr-scene-urdu-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:00.083220Z\",\n            \"modified_at\": \"2022-02-16T19:51:00.083220Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"2ff3a156373c4d1393754e945d85e7ec\",\n                \"created_at\": \"2022-02-16T19:51:00.108328Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"8f503aa63b60707e818d8a6a11a7dbe9\",\n            \"name\": \"ocr-scene-persian-paddleocr\",\n            \"created_at\": \"2022-02-16T19:50:59.643063Z\",\n            \"modified_at\": \"2022-02-16T19:50:59.643063Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"2d4cd9e9a3e04c199e27ca2493370c40\",\n                \"created_at\": \"2022-02-16T19:50:59.650272Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"6f7e3f2f2ed23a27897b898b93e8874a\",\n            \"name\": \"ocr-scene-uyghur-paddleocr\",\n            \"created_at\": \"2022-02-16T19:50:59.305322Z\",\n            \"modified_at\": \"2022-02-16T19:50:59.305322Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"6d38e8fed498460f811d5840caf7a19a\",\n                \"created_at\": \"2022-02-16T19:50:59.310025Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"594d939e0c9d8b19355f80cf1086b69d\",\n            \"name\": \"ocr-scene-hindi-paddleocr\",\n            \"created_at\": \"2022-02-16T19:50:58.946883Z\",\n            \"modified_at\": \"2022-02-16T19:50:58.946883Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"20c09506f61a4b389c15b086fedd66d5\",\n                \"created_at\": \"2022-02-16T19:50:58.953501Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"18189394090615325d5d783724ae290b\",\n            \"name\": \"ocr-scene-italian-paddleocr\",\n            \"created_at\": \"2022-02-16T19:50:58.539760Z\",\n            \"modified_at\": \"2022-02-16T19:50:58.539760Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"a56454cd3c2943a2a1a4b8840b73fda5\",\n                \"created_at\": \"2022-02-16T19:50:58.546364Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"2e47eda28e43cda4d9f5556924662bec\",\n            \"name\": \"ocr-scene-japanese-paddleocr\",\n            \"created_at\": \"2022-02-16T19:50:58.125647Z\",\n            \"modified_at\": \"2022-02-16T19:50:58.125647Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"413a870e2105418fbaadfbebc1b71b17\",\n                \"created_at\": \"2022-02-16T19:50:58.132649Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"e184d889c6e9737e6744e48df535d8f9\",\n            \"name\": \"ocr-scene-korean-paddleocr\",\n            \"created_at\": \"2022-02-16T19:50:57.685263Z\",\n            \"modified_at\": \"2022-02-16T19:50:57.685263Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"1b2985fac7d34e3cba5adfdfb1345705\",\n                \"created_at\": \"2022-02-16T19:50:57.695732Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"2b7192cc990a1a5a68d12f8326475ba6\",\n            \"name\": \"ocr-scene-german-paddleocr\",\n            \"created_at\": \"2022-02-16T19:50:57.289519Z\",\n            \"modified_at\": \"2022-02-16T19:50:57.289519Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"b10c4e125fb5423ca03f9f7600b2af6f\",\n                \"created_at\": \"2022-02-16T19:50:57.295199Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"952daadb10f7ace3433e8fc372c7c4d6\",\n            \"name\": \"ocr-scene-russian-paddleocr\",\n            \"created_at\": \"2022-02-16T19:50:56.894498Z\",\n            \"modified_at\": \"2022-02-16T19:50:56.894498Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"b02773fbacaf4f3eb5dcd6144ca01ee6\",\n                \"created_at\": \"2022-02-16T19:50:56.901399Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"60a98793f1db9dacb48e61fbb5aa61e1\",\n            \"name\": \"ocr-scene-portuguese-paddleocr\",\n            \"created_at\": \"2022-02-16T19:50:56.490797Z\",\n            \"modified_at\": \"2022-02-16T19:50:56.490797Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"d7c3be32a4bf488a987f97587cb9299d\",\n                \"created_at\": \"2022-02-16T19:50:56.500084Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"5560bcbc3d7ad8feb3999874b558b2a8\",\n            \"name\": \"ocr-scene-spanish-paddleocr\",\n            \"created_at\": \"2022-02-16T19:50:56.091771Z\",\n            \"modified_at\": \"2022-02-16T19:50:56.091771Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"a4bbcd9a06054acb8d44b3cc313dc6de\",\n                \"created_at\": \"2022-02-16T19:50:56.109454Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"fda611add70f1009c5c01d95f0fbe57b\",\n            \"name\": \"ocr-scene-arabic-paddleocr\",\n            \"created_at\": \"2022-02-16T19:50:55.696689Z\",\n            \"modified_at\": \"2022-02-16T19:50:55.696689Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"b1b8c11aee044f4a860c61d256bbd12e\",\n                \"created_at\": \"2022-02-16T19:50:55.707467Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"7acefe1233c41a8acab80619d79c6b56\",\n            \"name\": \"ocr-scene-chinese-traditional-paddleocr\",\n            \"created_at\": \"2022-02-16T19:50:55.240293Z\",\n            \"modified_at\": \"2022-02-16T19:50:55.240293Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"3b83168a4e764cd3b61c63e9662af235\",\n                \"created_at\": \"2022-02-16T19:50:55.255319Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"dc09ac965f64826410fbd8fea603abe6\",\n            \"name\": \"ocr-scene-chinese-english-paddleocr\",\n            \"created_at\": \"2022-01-25T01:53:05.944447Z\",\n            \"modified_at\": \"2022-01-25T01:53:05.944447Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"8aa1c6c2febf49f880cae2783097c2fa\",\n                \"created_at\": \"2022-01-25T01:53:05.952851Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"ba040818064ef61074e6d1cdec1c40c6\",\n            \"name\": \"paddleocr-english-chinese\",\n            \"created_at\": \"2022-01-25T00:30:04.159834Z\",\n            \"modified_at\": \"2022-01-25T00:30:04.159834Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"6daaabc3998440eeb1db18086d926bd8\",\n                \"created_at\": \"2022-01-25T00:30:04.183333Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"celebrity-face-recognition\",\n            \"name\": \"Celebrity\",\n            \"created_at\": \"2020-11-20T17:09:42.109453Z\",\n            \"modified_at\": \"2022-01-21T13:11:23.236298Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"0676ebddd5d6413ebdaa101570295a39\",\n                \"created_at\": \"2020-11-20T17:09:42.109453Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 10553,\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"concepts\": \"softmax\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-classifier\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"description\": \"AI model for recognizing celebrity faces in images or video.\",\n            \"metadata\": {\n                \"presetInputs\": [\n                    {\n                        \"alt\": \"Clarifai celebrity model featuring morgan freeman.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/celebrity-morgan-freeman.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai celebrity model featuring kim kardashian.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/celebrity-kim-kardashian.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai celebrity model featuring matt damon.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/celebrity-matt-damon.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai celebrity model featuring aziz ansari.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/celebrity-aziz-ansari.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai celebrity model featuring angelina jolie.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/celebrity-angelina-jolie.jpg\"\n                    }\n                ]\n            },\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [\n                \"pii\"\n            ],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"3b67f45ec89b4b9c6fb9db700120c91a\",\n            \"name\": \"advanced-det-handgun\",\n            \"created_at\": \"2022-01-06T20:27:53.965051Z\",\n            \"modified_at\": \"2022-01-06T20:27:53.965051Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"cecc3b705fe74a9ea88ee11e5ddd46f4\",\n                \"created_at\": \"2022-01-06T20:27:54.022071Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 2,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0,\n                        \"max_concepts\": 20,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"e4e42c048019241132ec2ecb583a3446\",\n            \"name\": \"microsoft/trocr-base-handwritten\",\n            \"created_at\": \"2021-11-10T17:18:34.067075Z\",\n            \"modified_at\": \"2021-12-29T16:56:15.937705Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"fd6f3dfd83854dd59cdeb8a5243e9e27\",\n                \"created_at\": \"2021-11-10T17:18:34.083675Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {},\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"86039c857a206810679f7f72b82fff54\",\n            \"name\": \"CLIP Prefix Captioning\",\n            \"created_at\": \"2021-12-09T04:32:31.377820Z\",\n            \"modified_at\": \"2021-12-09T04:32:31.377820Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"05fb71c53ff746f3834ab8333e401a1c\",\n                \"created_at\": \"2021-12-09T04:32:31.426529Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"image\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"image-to-text\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {},\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"bebd0da30b7090bb5250d5951960d96d\",\n            \"name\": \"CLIP\",\n            \"created_at\": \"2021-12-04T02:51:40.264878Z\",\n            \"modified_at\": \"2021-12-04T02:51:40.264878Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"5f6c752f8f964e56bef0c2eb32f3aca4\",\n                \"created_at\": \"2021-12-04T02:51:40.285448Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"embeddings\": \"embeddings\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-embedder\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"d05c045b95d85241c7d79e1ed3da3f8e\",\n            \"name\": \"PaddleOCR Multiplexed\",\n            \"created_at\": \"2021-11-05T04:36:50.693728Z\",\n            \"modified_at\": \"2021-11-05T04:36:50.693728Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"6da2f96ab8fb4255aa0ac53e6653345b\",\n                \"created_at\": \"2021-11-05T04:36:50.707810Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"fe000880ecc20921af7fbd46c485dbd2\",\n            \"name\": \"multiplexed easyocr\",\n            \"created_at\": \"2021-11-03T04:05:28.978761Z\",\n            \"modified_at\": \"2021-11-03T04:05:28.978761Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"19b30d2a3b064b13bbcd451a7b829f40\",\n                \"created_at\": \"2021-11-03T04:05:28.996684Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"c13d6411707faa5a5115b23f17957d82\",\n            \"name\": \"EasyOCR Multilingual Large\",\n            \"created_at\": \"2021-10-30T21:31:12.405449Z\",\n            \"modified_at\": \"2021-10-30T21:31:12.405449Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"7a4e17e73e144e5c8e9570aadc1cf0f5\",\n                \"created_at\": \"2021-10-30T21:31:12.411454Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"856fd858ed1dc14c741c15f0b9005cbb\",\n            \"name\": \"EasyOCR Multilingual\",\n            \"created_at\": \"2021-10-30T20:11:48.409877Z\",\n            \"modified_at\": \"2021-10-30T20:11:48.409877Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"5a77e2441d334d95859337454efbfd73\",\n                \"created_at\": \"2021-10-30T20:11:48.417474Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"23a5c4692f1a0449aef1c510be55b180\",\n            \"name\": \"facebook/wav2vec2-base-960h\",\n            \"created_at\": \"2021-10-05T17:43:10.985960Z\",\n            \"modified_at\": \"2021-10-27T13:18:19.573415Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"f4917f7f2e83453f9fe6d5eef4f598db\",\n                \"created_at\": \"2021-10-05T17:43:10.998245Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"audio\": \"audio\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"audio-to-text\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {},\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [\n                \"pii\"\n            ],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"2c1050ead1b24472be8033f5fd421f3d\",\n            \"name\": \"english\",\n            \"created_at\": \"2020-11-12T15:43:07.560737Z\",\n            \"modified_at\": \"2021-10-27T13:13:42.421644Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"dfe4776f79ee4c23a85d86d3b0649127\",\n                \"created_at\": \"2020-11-12T15:43:07.560737Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"text\": \"TRANSCRIPT\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"audio\": \"AUDIO_SIGNAL\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"audio-to-text\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {},\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"ba585a5a737771884d89972fea6c41f8\",\n            \"name\": \"language-script\",\n            \"created_at\": \"2021-10-20T18:05:49.263669Z\",\n            \"modified_at\": \"2021-10-20T18:05:49.263669Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"60fa287837dc4c10a3b5da6b92e062e2\",\n                \"created_at\": \"2021-10-20T18:05:49.328969Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 8,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"concepts\": \"softmax\"\n                    },\n                    \"params\": {\n                        \"max_concepts\": 20,\n                        \"min_value\": 0,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"logo-detection\",\n            \"name\": \"logo\",\n            \"created_at\": \"2017-03-06T18:38:13.025998Z\",\n            \"modified_at\": \"2021-10-19T12:21:30.579234Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"ef1b7237d28b415f910ca343a9145e99\",\n                \"created_at\": \"2017-03-06T18:38:13.025998Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 561,\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_cls_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_cls_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_cls_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0.05\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"display_name\": \"logo-visual-detector\",\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"description\": \"Logo detection model for locating logos of some of the most popular consumer brands within images and videos.\",\n            \"metadata\": {\n                \"presetInputs\": [\n                    {\n                        \"alt\": \"\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/logo-detection-black-bmw-silver-volkswagon.jpg\"\n                    },\n                    {\n                        \"alt\": \"\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/logo-detection-corona-extra-beer-bottle.jpg\"\n                    },\n                    {\n                        \"alt\": \"\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/logo-detection-northface-pink-jacket.jpg\"\n                    },\n                    {\n                        \"alt\": \"\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/logo-detection-pepsi-bottles-in-crate.jpg\"\n                    },\n                    {\n                        \"alt\": \"\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/logo-detection-tesla-steering-wheel.jpg\"\n                    }\n                ]\n            },\n            \"notes\": \"I can edit\",\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"food-item-v1-recognition\",\n            \"name\": \"food-items-v1.0\",\n            \"created_at\": \"2016-09-17T04:22:07.183747Z\",\n            \"modified_at\": \"2021-10-18T16:34:36.410436Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"dfebc169854e429086aceb8368662641\",\n                \"created_at\": \"2016-09-17T04:22:07.183747Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 970,\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"concepts\": \"softmax\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"display_name\": \"food-items-v1-visual-classifier\",\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-classifier\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"description\": \"AI model to recognize a wide variety of food items, including dishes and ingredients, in images and video.\",\n            \"metadata\": {\n                \"presetInputs\": [\n                    {\n                        \"alt\": \"Clarifai food model featuring pan of steamed clams.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/food-pan-of-steamed-clams.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai food model featuring waffle with strawberries blueberries.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/food-waffle-with-strawberries-blueberries.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai food model featuring hamburgers bacon cheese buns.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/food-hamburgers-bacon-cheese-buns.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai food model featuring pepperoni pizza.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/food-pepperoni-pizza.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai food model featuring tomato basil.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/food-tomato-basil.jpg\"\n                    }\n                ]\n            },\n            \"notes\": \"This is a food model note (wip) \",\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"9fe78b4150a52794f86f237770141b33\",\n            \"name\": \"english\",\n            \"created_at\": \"2021-02-04T05:24:54.250897Z\",\n            \"modified_at\": \"2021-10-18T15:46:17.997145Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"89961b2723e440abb49ec89a05b31219\",\n                \"created_at\": \"2021-09-30T14:43:29.569715Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"license\": \"BSD-2\",\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"image\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"image-to-text\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"description\": \"--\",\n            \"metadata\": {},\n            \"notes\": \"test model note now\",\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [\n                \"demographics\"\n            ],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"c3110dc5905447e410161091f0f95337\",\n            \"name\": \"anas/wav2vec2-large-xlsr-arabic\",\n            \"created_at\": \"2021-10-14T19:23:19.862284Z\",\n            \"modified_at\": \"2021-10-14T19:23:19.862284Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"f486dde2e2d046dabfd4c9e4db2c8e36\",\n                \"created_at\": \"2021-10-14T19:23:19.869018Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"audio\": \"audio\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"audio-to-text\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"68bbf91b9ad247921822a255ca381f11\",\n            \"name\": \"elgeish/wav2vec2-large-xlsr-53-arabic\",\n            \"created_at\": \"2021-10-14T18:44:04.938624Z\",\n            \"modified_at\": \"2021-10-14T18:44:04.938624Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"749a7a96fb404596bc11465dc41c3fb2\",\n                \"created_at\": \"2021-10-14T18:44:04.946902Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"audio\": \"audio\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"audio-to-text\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"face-detection\",\n            \"name\": \"Face\",\n            \"created_at\": \"2020-11-20T17:09:42.109453Z\",\n            \"modified_at\": \"2021-10-14T07:45:05.937031Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"fe995da8cb73490f8556416ecf25cea3\",\n                \"created_at\": \"2021-01-21T23:31:28.004422Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 2,\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0.9\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"display_name\": \"Face-visual-detector\",\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"description\": \"AI model for detecting the location of human faces in images and video.\",\n            \"metadata\": {\n                \"presetInputs\": [\n                    {\n                        \"alt\": \"Clarifai face model featuring little girl boy standing outside.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/face-little-girl-boy-standing-outside.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai face model featuring three men sitting in van.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/face-three-men-sitting-in-van.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai face model featuring family with light blue shirts.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/face-family-with-light-blue-shirts.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai face model featuring arfrican american man woman laughing.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/face-arfrican-american-man-woman-laughing.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai face model featuring crowd of monks orange robe.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/face-crowd-of-monks-orange-robe.jpg\"\n                    }\n                ]\n            },\n            \"notes\": \"test123\",\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [\n                \"faces\"\n            ],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [\n                \"pii\"\n            ],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"6a3dc529acf3f720a629cdc8c6ad41a9\",\n            \"name\": \"subject\",\n            \"created_at\": \"2021-04-26T09:20:01.359645Z\",\n            \"modified_at\": \"2021-10-12T12:36:06.443774Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"55b2051b75f14577b6fdd5a4fa3fd5a8\",\n                \"created_at\": \"2021-04-26T15:06:37.619220Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 2,\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].region_info.mask,regions[...].data.concepts\": \"output__0\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"input__0\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-segmenter\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"description\": \"test\",\n            \"metadata\": {},\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"beb9ed2d034b0126c6e24135ace76d8f\",\n            \"name\": \"prithivida/informal_to_formal_styletransfer\",\n            \"created_at\": \"2021-10-05T19:58:24.859525Z\",\n            \"modified_at\": \"2021-10-05T20:06:53.273373Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"a35db0e9b9b34f52ac0c83e1007db145\",\n                \"created_at\": \"2021-10-05T20:06:53.278139Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-to-text\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"69469f13c714cb6c68149db326d8c69a\",\n            \"name\": \"prithivida/formal_to_informal_styletransfer\",\n            \"created_at\": \"2021-10-05T19:45:07.925572Z\",\n            \"modified_at\": \"2021-10-05T19:45:07.925572Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"d34c3b68aadb408f9e770632183cb164\",\n                \"created_at\": \"2021-10-05T19:45:07.942034Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-to-text\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"b7e22ac73f924e2a6199af495724ddac\",\n            \"name\": \"facebook/wav2vec2-large-xlsr-53-french\",\n            \"created_at\": \"2021-10-05T17:18:44.727040Z\",\n            \"modified_at\": \"2021-10-05T17:18:44.727040Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"a648b13f7269461d9797e6bc58111a60\",\n                \"created_at\": \"2021-10-05T17:18:44.743898Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"audio\": \"audio\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"audio-to-text\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"6db1c2dc0d9c09d843c763bc0a05b989\",\n            \"name\": \"assembly\",\n            \"created_at\": \"2021-09-23T17:34:00.947876Z\",\n            \"modified_at\": \"2021-09-24T06:17:00.318253Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"5ef730b4ef014b508b31af5e5577386d\",\n                \"created_at\": \"2021-09-24T06:17:00.341875Z\",\n                \"status\": {\n                    \"code\": 99009,\n                    \"description\": \"Model upload timed out\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"embeddings\": \"embedding\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-embedder\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"2e099a9219da8fc580ac0dc54bf842fd\",\n            \"name\": \"paddleocr-multilingual-text-detector\",\n            \"created_at\": \"2021-08-19T00:40:40.789176Z\",\n            \"modified_at\": \"2021-09-22T19:50:23.797705Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"dff9af491f0d48449801decee0e2f136\",\n                \"created_at\": \"2021-09-22T19:50:23.809040Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 2,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0,\n                        \"max_concepts\": 20,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"6f96f8bf841280a388f8b52cb1868df4\",\n            \"name\": \"person-detector-yolov5x-libtorch\",\n            \"created_at\": \"2021-08-25T22:19:26.887486Z\",\n            \"modified_at\": \"2021-08-25T22:19:26.887486Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"8d1ac866905a4154b6d34ae2566503ad\",\n                \"created_at\": \"2021-08-25T22:19:26.962227Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 2,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0,\n                        \"max_concepts\": 20,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"a065882d92d66dbac3b5ebe108170197\",\n            \"name\": \"person-vehicle-detector-yolov5x-libtorch\",\n            \"created_at\": \"2021-08-25T17:56:25.508821Z\",\n            \"modified_at\": \"2021-08-25T21:51:12.368401Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"be2ea5c740f940429e6284826660da9a\",\n                \"created_at\": \"2021-08-25T21:51:12.377067Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 3,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0,\n                        \"max_concepts\": 20,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"2cf7739e65bfc63c1537f65e7ef3ae87\",\n            \"name\": \"person-vehicle-detector-yolov5s-libtorch\",\n            \"created_at\": \"2021-08-24T19:14:37.883020Z\",\n            \"modified_at\": \"2021-08-24T19:14:37.883020Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"3b6b338aa1b7433a8935d22b0915fc32\",\n                \"created_at\": \"2021-08-24T19:14:37.990646Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 3,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0,\n                        \"max_concepts\": 20,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"23aa4f9c9767a2fd61e63c55a73790ad\",\n            \"name\": \"person-detector-yolov5s-libtorch\",\n            \"created_at\": \"2021-08-24T18:52:53.766484Z\",\n            \"modified_at\": \"2021-08-24T18:52:53.766484Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"92b204309cce44209f1428e38e3406fb\",\n                \"created_at\": \"2023-02-24T05:41:57.516485Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 2,\n                \"description\": \"Yolov5\",\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0,\n                        \"max_concepts\": 20,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"bdcedc0f8da58c396b7df12f634ef923\",\n            \"name\": \"multilingual-moderation\",\n            \"created_at\": \"2021-01-13T16:45:48.370348Z\",\n            \"modified_at\": \"2021-07-13T15:12:21.536311Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"c56b46fd91ca4540823ba70496d008f9\",\n                \"created_at\": \"2021-01-13T20:45:50.641841Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 6,\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"concepts\": \"output__1\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"input__0\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-classifier\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {},\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"eac94da66f428ebf98dc2cae30030699\",\n            \"name\": \"hate-symbols\",\n            \"created_at\": \"2021-07-07T20:24:28.707682Z\",\n            \"modified_at\": \"2021-07-08T15:06:49.454931Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"ca52d78ed8cb4cfba4eacebadc38548b\",\n                \"created_at\": \"2021-07-08T15:06:49.459168Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 2,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0.05,\n                        \"max_concepts\": 20,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"bc6b2c89a5dc35ee5b5872612d0df25a\",\n            \"name\": \"EasyOCR (Turkish)\",\n            \"created_at\": \"2021-06-17T19:51:18.060645Z\",\n            \"modified_at\": \"2021-06-17T19:51:18.060645Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"17ca4640290f4c3e885ed74e757272df\",\n                \"created_at\": \"2021-06-17T19:51:18.066247Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"869245fe9708e30e6a0869c6e3dc3132\",\n            \"name\": \"EasyOCR (Swedish)\",\n            \"created_at\": \"2021-06-17T19:49:51.218255Z\",\n            \"modified_at\": \"2021-06-17T19:49:51.218255Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"8eb77eec58314083a0b9765047974d45\",\n                \"created_at\": \"2021-06-17T19:49:51.224781Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"b24a775f7b9e156f3518772206c342ef\",\n            \"name\": \"EasyOCR (Polish)\",\n            \"created_at\": \"2021-06-17T19:45:42.334798Z\",\n            \"modified_at\": \"2021-06-17T19:45:42.334798Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"ce2624cd1b84445e988b1b437dfe2a95\",\n                \"created_at\": \"2021-06-17T19:45:42.340957Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"e6bcd24eb84835a8de347b7b8a028f27\",\n            \"name\": \"EasyOCR (Dutch)\",\n            \"created_at\": \"2021-06-17T19:43:57.103955Z\",\n            \"modified_at\": \"2021-06-17T19:43:57.103955Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"22669f6ea56e428f87465b55c9296ca5\",\n                \"created_at\": \"2021-06-17T19:43:57.109610Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"f43080883661c6676779384149eb9249\",\n            \"name\": \"EasyOCR (Malay)\",\n            \"created_at\": \"2021-06-17T19:37:49.087245Z\",\n            \"modified_at\": \"2021-06-17T19:37:49.087245Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"650fde39145f417db48b6270bfa78c2a\",\n                \"created_at\": \"2021-06-17T19:37:49.096062Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"a34ea557d25cc1c7b3b6bf080381eb04\",\n            \"name\": \"EasyOCR (Vietnamese)\",\n            \"created_at\": \"2021-06-17T19:32:48.422165Z\",\n            \"modified_at\": \"2021-06-17T19:32:48.422165Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"9dcfa47d32374f2684e5ed167ad0337a\",\n                \"created_at\": \"2021-06-17T19:32:48.431675Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"edd250a6e1f82cbee84819e3550dbaf4\",\n            \"name\": \"Helsinki-NLP/opus-mt-nl-en\",\n            \"created_at\": \"2021-05-28T16:30:24.060377Z\",\n            \"modified_at\": \"2021-05-28T16:30:24.060377Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"1183f7daaadd48969be19c0e7ad1c5ec\",\n                \"created_at\": \"2021-05-28T16:30:24.070688Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-to-text\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"68a51a726f7033bbfcf57c905f09b7ca\",\n            \"name\": \"general\",\n            \"created_at\": \"2021-04-19T16:03:17.091357Z\",\n            \"modified_at\": \"2021-05-20T08:13:55.391070Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"df708d35d2fa4dea9e9d3f76ce842450\",\n                \"created_at\": \"2021-05-20T08:13:55.399476Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 183,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].region_info.mask,regions[...].data.concepts\": \"output__0\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"input__0\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-segmenter\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"19de30b6d9c38ed8a0478ac5103efebe\",\n            \"name\": \"person-vehicle\",\n            \"created_at\": \"2021-05-14T19:50:52.826338Z\",\n            \"modified_at\": \"2021-05-18T18:59:59.867143Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"f06017161d0843dca0c6cf962cf08a11\",\n                \"created_at\": \"2021-05-18T18:59:59.876468Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 3,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0,\n                        \"max_concepts\": 20,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"332956f015ea667f81cef1f37b1a20f3\",\n            \"name\": \"person-vehicle-lite\",\n            \"created_at\": \"2021-05-14T19:33:26.027753Z\",\n            \"modified_at\": \"2021-05-18T18:55:51.389600Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"ef042aa9117141079f579db19809b1d3\",\n                \"created_at\": \"2021-05-18T18:55:51.401858Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 3,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0,\n                        \"max_concepts\": 20,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"4236cc320afa91a7d6c53ec949b66785\",\n            \"name\": \"Helsinki-NLP/opus-mt-es-en\",\n            \"created_at\": \"2021-05-12T22:17:11.471812Z\",\n            \"modified_at\": \"2021-05-12T22:17:11.471812Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"6fff4d1143114416b47f278084f4ffc7\",\n                \"created_at\": \"2021-05-12T22:17:11.477501Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-to-text\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"31025e019a18970a1acc55ba6a184dc6\",\n            \"name\": \"face-sentiment\",\n            \"created_at\": \"2021-05-12T16:02:43.390981Z\",\n            \"modified_at\": \"2021-05-12T16:02:43.390981Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"edcf31cfa67e426a8b12cd889453f0c3\",\n                \"created_at\": \"2021-05-12T16:02:43.563574Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 7,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"concepts\": \"softmax\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"17a76b5162066195dad4c0437e66be80\",\n            \"name\": \"objectness-detector\",\n            \"created_at\": \"2021-05-11T17:35:00.699972Z\",\n            \"modified_at\": \"2021-05-11T17:35:00.699972Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"a488dd0eb0b94e16b22aa35656f4dd31\",\n                \"created_at\": \"2021-05-11T17:35:00.734676Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 1,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0.6\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"e609405a6ced78aa8a9eff2288f6edc6\",\n            \"name\": \"tank-rodeo\",\n            \"created_at\": \"2021-05-04T15:39:36.312115Z\",\n            \"modified_at\": \"2021-05-04T15:39:36.312115Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"745c6724c6a3456bb41ab0814070e191\",\n                \"created_at\": \"2021-05-04T15:39:36.537311Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 14,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0.1\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"385c7fd117d77553962b39629659d51a\",\n            \"name\": \"apparel\",\n            \"created_at\": \"2021-04-23T20:11:59.736603Z\",\n            \"modified_at\": \"2021-04-26T18:20:03.699907Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"c318a0b6769540e6bfe684af83560a9f\",\n                \"created_at\": \"2021-04-26T18:20:03.705751Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 192,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"concepts\": \"softmax\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"c833800b94175363881d4db8b55e4a52\",\n            \"name\": \"test20_ner\",\n            \"created_at\": \"2021-04-23T18:19:00.421074Z\",\n            \"modified_at\": \"2021-04-23T18:19:00.421074Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"aa16e51cc0ef427ab728928932fed6f6\",\n                \"created_at\": \"2021-04-23T18:19:00.654472Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 9,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"labels\",\n                        \"regions[...].data.concepts[...].value\": \"scores\",\n                        \"regions[...].data.text\": \"tokens\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-token-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"9de6872fa62a3118ce66c313e5c7d567\",\n            \"name\": \"test4_bert_base_ner\",\n            \"created_at\": \"2021-04-22T01:34:00.926953Z\",\n            \"modified_at\": \"2021-04-22T01:34:00.926953Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"b263587fd32f48f4a89a80271b5ce18f\",\n                \"created_at\": \"2021-04-22T01:34:01.026938Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 9,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"labels\",\n                        \"regions[...].data.concepts[...].value\": \"scores\",\n                        \"regions[...].data.text\": \"tokens\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-token-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"33128485c45b681a11380cea3933789a\",\n            \"name\": \"test2_bert_base_ner\",\n            \"created_at\": \"2021-04-21T20:34:07.150108Z\",\n            \"modified_at\": \"2021-04-21T20:34:07.150108Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"da4b266f7b654679b6b4ba8c8691c326\",\n                \"created_at\": \"2021-04-21T20:34:07.278954Z\",\n                \"status\": {\n                    \"code\": 99009,\n                    \"description\": \"Training Unknown Error\"\n                },\n                \"active_concept_count\": 9,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"labels\",\n                        \"regions[...].data.concepts[...].value\": \"scores\",\n                        \"regions[...].data.text\": \"tokens\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-token-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"48f9e31a431a24754f7db8bf221e1e41\",\n            \"name\": \"test_bert_base_ner\",\n            \"created_at\": \"2021-04-21T15:22:05.347218Z\",\n            \"modified_at\": \"2021-04-21T15:22:05.347218Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"e9c0b45944a24b6a9c3e77f0a10a836f\",\n                \"created_at\": \"2021-04-21T15:22:05.487673Z\",\n                \"status\": {\n                    \"code\": 99009,\n                    \"description\": \"Training Unknown Error\"\n                },\n                \"active_concept_count\": 9,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"labels\",\n                        \"regions[...].data.concepts[...].value\": \"scores\",\n                        \"regions[...].data.text\": \"tokens\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-token-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"b5632bca03e3aca4e0c948303935fb0d\",\n            \"name\": \"dslim/bert-base-NER\",\n            \"created_at\": \"2021-04-20T21:52:32.656339Z\",\n            \"modified_at\": \"2021-04-20T21:52:32.656339Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"ff092f6185c94800902d62ff68775a6e\",\n                \"created_at\": \"2021-04-20T21:52:32.810261Z\",\n                \"status\": {\n                    \"code\": 99009,\n                    \"description\": \"Training Unknown Error\"\n                },\n                \"active_concept_count\": 9,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"labels\",\n                        \"regions[...].data.concepts[...].value\": \"scores\",\n                        \"regions[...].data.text\": \"tokens\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-token-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"8db409c46a3a9153becc3565f1d99022\",\n            \"name\": \"Test Triton GPU Landmarks\",\n            \"created_at\": \"2021-01-15T21:31:47.307624Z\",\n            \"modified_at\": \"2021-04-12T20:07:18.744912Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"cf33eda7f56f4d389ffac719cdf82da4\",\n                \"created_at\": \"2021-04-12T20:07:18.749574Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 1,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts,regions[...].region_info.keypoint_locations\": \"output__0\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"input__0\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-keypointer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        }\n    ]\n}"
                  },
                  "example-1": {
                    "summary": "List Models in User/Org",
                    "value": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"dd2303b0cb6fe8859693ae9e8f4adfa7\"\n    },\n    \"models\": [\n        {\n            \"id\": \"openai-create-image-9\",\n            \"name\": \"newnewnewname\",\n            \"created_at\": \"2023-11-23T11:16:25.775735Z\",\n            \"modified_at\": \"2023-11-23T11:19:38.173686Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"text-to-text\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"test-model-with-hyperparms-1700738311\",\n            \"name\": \"test-model-with-hyperparms-1700738311\",\n            \"created_at\": \"2023-11-23T11:18:31.353199Z\",\n            \"modified_at\": \"2023-11-23T11:18:31.353199Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"embedding-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"test-model-1700738282\",\n            \"name\": \"test-model-1700738282\",\n            \"created_at\": \"2023-11-23T11:18:01.807941Z\",\n            \"modified_at\": \"2023-11-23T11:18:01.807941Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"embedding-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"test-model-1700738263\",\n            \"name\": \"test-model-1700738263\",\n            \"created_at\": \"2023-11-23T11:17:43.761741Z\",\n            \"modified_at\": \"2023-11-23T11:17:43.761741Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"embedding-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"test-model-1700738240\",\n            \"name\": \"test-model-1700738240\",\n            \"created_at\": \"2023-11-23T11:17:20.719370Z\",\n            \"modified_at\": \"2023-11-23T11:17:20.719370Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"image-crop\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"test-model-1700738220\",\n            \"name\": \"test-model-1700738220\",\n            \"created_at\": \"2023-11-23T11:17:00.314086Z\",\n            \"modified_at\": \"2023-11-23T11:17:00.314086Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"embedding-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"profanity-filter-new2\",\n            \"name\": \"profanity-filter-new2\",\n            \"created_at\": \"2023-11-23T11:16:45.317604Z\",\n            \"modified_at\": \"2023-11-23T11:16:45.317604Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"keyword-filter-operator\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"named-entity-recognition-diseases-english-text\",\n            \"name\": \"burgerz\",\n            \"created_at\": \"2023-11-23T11:15:32.431806Z\",\n            \"modified_at\": \"2023-11-23T11:16:02.900299Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"embedding-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"custom-config\",\n            \"name\": \"custom-config\",\n            \"created_at\": \"2023-11-23T09:41:08.002419Z\",\n            \"modified_at\": \"2023-11-23T09:41:08.002419Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"model_version\": {\n                \"id\": \"1e4c121974f849209abb658cdf682585\",\n                \"created_at\": \"2023-11-23T09:41:18.470087Z\",\n                \"status\": {\n                    \"code\": 21110,\n                    \"description\": \"datasets.dataset.DataBatchEmpty: No databatch found in train set's file directory\\nFailed to create a training dataset, because there are no appropriately annotated inputs. Expected annotations with concepts for model type id text-classifier. \"\n                },\n                \"active_concept_count\": 6,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"test-app-1700638575-empty\",\n                \"user_id\": \"a0btrubbaefn\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"params\": {\n                        \"max_concepts\": 20,\n                        \"min_value\": 0,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {},\n                \"train_info\": {\n                    \"params\": {\n                        \"dataset_id\": \"\",\n                        \"dataset_version_id\": \"\",\n                        \"invalid_data_tolerance_percent\": 5,\n                        \"model_config\": {\n                            \"pretrained_model_name\": \"EleutherAI/gpt-neo-125m\"\n                        },\n                        \"num_gpus\": 1,\n                        \"peft_config\": {\n                            \"peft_type\": \"LORA\"\n                        },\n                        \"template\": \"HF_GPTNeo_125m_lora\",\n                        \"tokenizer_config\": {},\n                        \"trainer_config\": {\n                            \"auto_find_batch_size\": true,\n                            \"num_train_epochs\": 20,\n                            \"output_dir\": \"checkpoint\"\n                        }\n                    }\n                },\n                \"import_info\": {}\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"text-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"imagecl1\",\n            \"name\": \"imagecl1\",\n            \"created_at\": \"2023-11-23T07:58:15.786698Z\",\n            \"modified_at\": \"2023-11-23T07:58:15.786698Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"embedding-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"imagecl\",\n            \"name\": \"imagecl\",\n            \"created_at\": \"2023-11-23T07:57:56.370980Z\",\n            \"modified_at\": \"2023-11-23T07:57:56.370980Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"embedding-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"deep_cls_bg1\",\n            \"name\": \"deep_cls_bg1\",\n            \"created_at\": \"2023-11-23T07:27:32.857722Z\",\n            \"modified_at\": \"2023-11-23T07:27:32.857722Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"model_version\": {\n                \"id\": \"00668896e0a64cf5b37302c000e96f23\",\n                \"created_at\": \"2023-11-23T08:06:12.577745Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 16,\n                \"metrics\": {\n                    \"status\": {\n                        \"code\": 21300,\n                        \"description\": \"Model was successfully evaluated.\"\n                    },\n                    \"summary\": {\n                        \"macro_avg_roc_auc\": 0.52151275,\n                        \"macro_std_roc_auc\": 0.34620512,\n                        \"macro_avg_f1_score\": 0.40380955,\n                        \"macro_std_f1_score\": 0.17236254,\n                        \"macro_avg_precision\": 0.09772728,\n                        \"macro_avg_recall\": 0.52380955\n                    }\n                },\n                \"completed_at\": \"2023-11-23T08:21:08.303040Z\",\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"test-app-1700638575-empty\",\n                \"user_id\": \"a0btrubbaefn\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"concepts\": \"probs\"\n                    },\n                    \"params\": {\n                        \"max_concepts\": 20,\n                        \"min_value\": 0,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"input\"\n                    }\n                },\n                \"train_info\": {\n                    \"params\": {\n                        \"batch_size\": 64,\n                        \"concepts_mutually_exclusive\": false,\n                        \"dataset_id\": \"\",\n                        \"dataset_version_id\": \"\",\n                        \"flip_direction\": \"horizontal\",\n                        \"flip_probability\": 0.5,\n                        \"image_size\": 224,\n                        \"invalid_data_tolerance_percent\": 5,\n                        \"num_epochs\": 60,\n                        \"num_gpus\": 1,\n                        \"per_item_lrate\": 0.00001953125,\n                        \"per_item_min_lrate\": 1.5625e-08,\n                        \"pretrained_weights\": \"ImageNet-1k\",\n                        \"seed\": -1,\n                        \"template\": \"MMClassification_ResNet_50_RSB_A1\",\n                        \"warmup_iters\": 100,\n                        \"warmup_ratio\": 0.0001,\n                        \"weight_decay\": 0.01\n                    }\n                },\n                \"import_info\": {}\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"visual-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"deep_cls_bg\",\n            \"name\": \"deep_cls_bg\",\n            \"created_at\": \"2023-11-23T07:12:59.578284Z\",\n            \"modified_at\": \"2023-11-23T07:12:59.578284Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"visual-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"profanity-filter-new\",\n            \"name\": \"profanity-filter-new\",\n            \"created_at\": \"2023-11-23T07:08:10.016262Z\",\n            \"modified_at\": \"2023-11-23T07:08:10.016262Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"model_version\": {\n                \"id\": \"b56d87c6dc80484291586d10f2feaacd\",\n                \"created_at\": \"2023-11-23T07:08:17.041706Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"test-app-1700638575-empty\",\n                \"user_id\": \"a0btrubbaefn\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"params\": {\n                        \"case_sensitive\": false,\n                        \"keywords\": [\n                            \"\"\n                        ]\n                    }\n                },\n                \"input_info\": {},\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"keyword-filter-operator\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"tiny-random-gpt2\",\n            \"name\": \"tiny-random-gpt2\",\n            \"created_at\": \"2023-11-17T21:31:10.384113Z\",\n            \"modified_at\": \"2023-11-17T21:31:10.384113Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"214531acb40a4af8b56ca79103390466\",\n                \"created_at\": \"2023-11-17T21:31:10.384113Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {\n                    \"params\": {\n                        \"model_config\": {\n                            \"_name_or_path\": \"hf-internal-testing/tiny-random-gpt2\",\n                            \"activation_function\": \"gelu_new\",\n                            \"architectures\": [\n                                \"GPT2LMHeadModel\"\n                            ],\n                            \"attention_probs_dropout_prob\": 0.1,\n                            \"attn_pdrop\": 0.1,\n                            \"bos_token_id\": 98,\n                            \"embd_pdrop\": 0.1,\n                            \"eos_token_id\": 98,\n                            \"gradient_checkpointing\": false,\n                            \"hidden_act\": \"gelu\",\n                            \"hidden_dropout_prob\": 0.1,\n                            \"initializer_range\": 0.02,\n                            \"intermediate_size\": 37,\n                            \"layer_norm_epsilon\": 0.00001,\n                            \"model_type\": \"gpt2\",\n                            \"n_ctx\": 512,\n                            \"n_embd\": 32,\n                            \"n_head\": 4,\n                            \"n_inner\": null,\n                            \"n_layer\": 5,\n                            \"n_positions\": 512,\n                            \"pad_token_id\": 98,\n                            \"reorder_and_upcast_attn\": false,\n                            \"resid_pdrop\": 0.1,\n                            \"scale_attn_by_inverse_layer_idx\": false,\n                            \"scale_attn_weights\": true,\n                            \"summary_activation\": null,\n                            \"summary_first_dropout\": 0.1,\n                            \"summary_proj_to_labels\": true,\n                            \"summary_type\": \"cls_index\",\n                            \"summary_use_proj\": true,\n                            \"torch_dtype\": \"float32\",\n                            \"transformers_version\": \"4.32.1\",\n                            \"type_vocab_size\": 16,\n                            \"use_cache\": true,\n                            \"vocab_size\": 1000\n                        },\n                        \"tokenizer_config\": {\n                            \"add_prefix_space\": false,\n                            \"bos_token\": \"<|endoftext|>\",\n                            \"clean_up_tokenization_spaces\": true,\n                            \"eos_token\": \"<|endoftext|>\",\n                            \"model_max_length\": 512,\n                            \"tokenizer_class\": \"GPT2Tokenizer\",\n                            \"unk_token\": \"<|endoftext|>\"\n                        }\n                    }\n                },\n                \"import_info\": {\n                    \"params\": {\n                        \"model_name\": \"hf-internal-testing/tiny-random-gpt2\",\n                        \"pipeline_name\": \"text-generation\",\n                        \"toolkit\": \"HuggingFace\"\n                    }\n                }\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-to-text\",\n            \"task\": \"text-generation\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"BAAI-bge-base-en-v15\",\n            \"name\": \"BAAI-bge-base-en-v15\",\n            \"created_at\": \"2023-11-01T12:53:30.339664Z\",\n            \"modified_at\": \"2023-11-01T12:53:30.339664Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"23cf79f86459491cb31fd7f0273c9fff\",\n                \"created_at\": \"2023-11-01T12:53:30.339664Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"embeddings\": \"embedding\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    },\n                    \"params\": {\n                        \"text_token_warning_limit\": 512\n                    }\n                },\n                \"train_info\": {\n                    \"params\": {\n                        \"model_config\": {\n                            \"_name_or_path\": \"BAAI/bge-base-en-v1.5\",\n                            \"architectures\": [\n                                \"BertModel\"\n                            ],\n                            \"attention_probs_dropout_prob\": 0.1,\n                            \"classifier_dropout\": null,\n                            \"gradient_checkpointing\": false,\n                            \"hidden_act\": \"gelu\",\n                            \"hidden_dropout_prob\": 0.1,\n                            \"hidden_size\": 768,\n                            \"id2label\": {\n                                \"0\": \"LABEL_0\"\n                            },\n                            \"initializer_range\": 0.02,\n                            \"intermediate_size\": 3072,\n                            \"label2id\": {\n                                \"LABEL_0\": 0\n                            },\n                            \"layer_norm_eps\": 1e-12,\n                            \"max_position_embeddings\": 512,\n                            \"model_type\": \"bert\",\n                            \"num_attention_heads\": 12,\n                            \"num_hidden_layers\": 12,\n                            \"pad_token_id\": 0,\n                            \"position_embedding_type\": \"absolute\",\n                            \"torch_dtype\": \"float32\",\n                            \"transformers_version\": \"4.32.1\",\n                            \"type_vocab_size\": 2,\n                            \"use_cache\": true,\n                            \"vocab_size\": 30522\n                        },\n                        \"tokenizer_config\": {\n                            \"clean_up_tokenization_spaces\": true,\n                            \"cls_token\": \"[CLS]\",\n                            \"do_basic_tokenize\": true,\n                            \"do_lower_case\": true,\n                            \"mask_token\": \"[MASK]\",\n                            \"model_max_length\": 512,\n                            \"never_split\": null,\n                            \"pad_token\": \"[PAD]\",\n                            \"sep_token\": \"[SEP]\",\n                            \"strip_accents\": null,\n                            \"tokenize_chinese_chars\": true,\n                            \"tokenizer_class\": \"BertTokenizer\",\n                            \"unk_token\": \"[UNK]\"\n                        }\n                    }\n                },\n                \"import_info\": {\n                    \"params\": {\n                        \"model_name\": \"BAAI/bge-base-en-v1.5\",\n                        \"pipeline_name\": \"feature-extraction\",\n                        \"toolkit\": \"HuggingFace\"\n                    }\n                }\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-embedder\",\n            \"task\": \"representation-learning\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"BAAI-bge-base-en-cluster\",\n            \"name\": \"BAAI-bge-base-en-cluster\",\n            \"created_at\": \"2023-08-15T14:21:18.083130Z\",\n            \"modified_at\": \"2023-08-15T14:21:18.083130Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"4a47a75c931c4b0784cebc2cd45bc5a2\",\n                \"created_at\": \"2023-09-18T11:00:41.832176Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"metrics\": {\n                    \"status\": {\n                        \"code\": 21300,\n                        \"description\": \"Model was successfully evaluated.\"\n                    }\n                },\n                \"total_input_count\": 293849,\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\"\n                },\n                \"input_info\": {},\n                \"train_info\": {\n                    \"params\": {\n                        \"beta\": 1,\n                        \"coarse_clusters\": 128,\n                        \"dataset_id\": \"quora-dataset-corpus-2\",\n                        \"dataset_version_id\": \"dataset-version-1692900595413\",\n                        \"eval_holdout_fraction\": 0.2,\n                        \"max_num_query_embeddings\": 100,\n                        \"max_visited\": 32,\n                        \"num_results_per_query\": [\n                            1,\n                            5,\n                            10,\n                            20\n                        ],\n                        \"query_holdout_fraction\": 0.1,\n                        \"quota\": 1000,\n                        \"to_be_indexed_queries_fraction\": 0.25,\n                        \"train_iters\": 1,\n                        \"training_timeout\": 72000\n                    },\n                    \"dataset\": {\n                        \"id\": \"quora-dataset-corpus-2\",\n                        \"created_at\": \"2023-08-24T07:40:45.232142Z\",\n                        \"modified_at\": \"2023-08-24T18:09:55.799396Z\",\n                        \"app_id\": \"quora-dataset\",\n                        \"user_id\": \"isaac\",\n                        \"metadata\": {},\n                        \"visibility\": {\n                            \"gettable\": 10\n                        },\n                        \"version\": {\n                            \"id\": \"dataset-version-1692900595413\",\n                            \"created_at\": \"0001-01-01T00:00:00Z\",\n                            \"modified_at\": \"0001-01-01T00:00:00Z\",\n                            \"app_id\": \"quora-dataset\",\n                            \"user_id\": \"isaac\",\n                            \"dataset_id\": \"quora-dataset-corpus-2\",\n                            \"status\": {\n                                \"code\": 99009,\n                                \"description\": \"Internal error\"\n                            },\n                            \"metadata\": {}\n                        }\n                    }\n                },\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"clusterer\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"BAAI-bge-base-en\",\n            \"name\": \"BAAI-bge-base-en\",\n            \"created_at\": \"2023-08-15T11:36:23.145658Z\",\n            \"modified_at\": \"2023-08-15T11:36:23.145658Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"b55d165cc3c64ed4bab3090c7b402188\",\n                \"created_at\": \"2023-08-15T11:36:23.145658Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"embeddings\": \"embedding\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {\n                    \"params\": {\n                        \"model_config\": {\n                            \"_name_or_path\": \"BAAI/bge-base-en\",\n                            \"architectures\": [\n                                \"BertModel\"\n                            ],\n                            \"attention_probs_dropout_prob\": 0.1,\n                            \"classifier_dropout\": null,\n                            \"gradient_checkpointing\": false,\n                            \"hidden_act\": \"gelu\",\n                            \"hidden_dropout_prob\": 0.1,\n                            \"hidden_size\": 768,\n                            \"id2label\": {\n                                \"0\": \"LABEL_0\"\n                            },\n                            \"initializer_range\": 0.02,\n                            \"intermediate_size\": 3072,\n                            \"label2id\": {\n                                \"LABEL_0\": 0\n                            },\n                            \"layer_norm_eps\": 1e-12,\n                            \"max_position_embeddings\": 512,\n                            \"model_type\": \"bert\",\n                            \"num_attention_heads\": 12,\n                            \"num_hidden_layers\": 12,\n                            \"pad_token_id\": 0,\n                            \"position_embedding_type\": \"absolute\",\n                            \"torch_dtype\": \"float32\",\n                            \"transformers_version\": \"4.30.2\",\n                            \"type_vocab_size\": 2,\n                            \"use_cache\": true,\n                            \"vocab_size\": 30522\n                        },\n                        \"tokenizer_config\": {\n                            \"clean_up_tokenization_spaces\": true,\n                            \"cls_token\": \"[CLS]\",\n                            \"do_basic_tokenize\": true,\n                            \"do_lower_case\": true,\n                            \"mask_token\": \"[MASK]\",\n                            \"model_max_length\": 512,\n                            \"never_split\": null,\n                            \"pad_token\": \"[PAD]\",\n                            \"sep_token\": \"[SEP]\",\n                            \"strip_accents\": null,\n                            \"tokenize_chinese_chars\": true,\n                            \"tokenizer_class\": \"BertTokenizer\",\n                            \"unk_token\": \"[UNK]\"\n                        }\n                    }\n                },\n                \"import_info\": {\n                    \"params\": {\n                        \"model_name\": \"BAAI/bge-base-en\",\n                        \"pipeline_name\": \"feature-extraction\",\n                        \"tokenizer_name\": \"BAAI/bge-base-en\",\n                        \"toolkit\": \"HuggingFace\"\n                    }\n                }\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-embedder\",\n            \"task\": \"representation-learning\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"apparel-recognition\",\n            \"name\": \"apparel\",\n            \"created_at\": \"2016-12-15T01:29:04.622209Z\",\n            \"modified_at\": \"2023-05-23T12:34:15.093542Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"dc2cd6d9bff5425a80bfe0c4105583c1\",\n                \"created_at\": \"2016-12-15T01:29:04.622209Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 112,\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"concepts\": \"softmax\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"display_name\": \"apparel-visual-classifier\",\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-classifier\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"description\": \"AI model for identifying fashion-related and clothing concepts, hats, jewelry, handbags, etc. in images and video.\",\n            \"metadata\": {\n                \"presetInputs\": [\n                    {\n                        \"alt\": \"Clarifai apparel model featuring woman black turtleneck.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/apparel-woman-black-turtleneck.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai apparel model featuring yellow boots.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/apparel-yellow-boots.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai apparel model featuring black white striped socks.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/apparel-black-white-striped-socks.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai apparel model featuring sunglasses.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/apparel-sunglasses.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai apparel model featuring dog in a dog carrier.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/apparel-dog-in-a-dog-carrier.jpg\"\n                    }\n                ]\n            },\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"15b0041cc2cd848a0d8b45f8b83c1d7d\",\n            \"name\": \"CLIP\",\n            \"created_at\": \"2021-12-14T18:07:40.983254Z\",\n            \"modified_at\": \"2023-04-27T20:45:27.183474Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"97f20cc96b7c4bec8f3b96e284ba1173\",\n                \"created_at\": \"2021-12-14T18:07:41.268867Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"embeddings\": \"embeddings\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-embedder\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"notes\": \"This model has been deprecated. Please use `multilingual-multimodal-clip-embed` instead.\",\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"text-translation-english-spanish\",\n            \"name\": \"Helsinki-NLP/opus-mt-en-es\",\n            \"created_at\": \"2023-02-22T22:44:16.825059Z\",\n            \"modified_at\": \"2023-02-22T22:44:16.825059Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"643f30558de34013aff72b0e21f244f5\",\n                \"created_at\": \"2023-02-23T00:39:20.611092Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {\n                    \"params\": {\n                        \"model_config\": {\n                            \"_name_or_path\": \"Helsinki-NLP/opus-mt-en-es\",\n                            \"activation_dropout\": 0,\n                            \"activation_function\": \"swish\",\n                            \"add_bias_logits\": false,\n                            \"add_final_layer_norm\": false,\n                            \"architectures\": [\n                                \"MarianMTModel\"\n                            ],\n                            \"attention_dropout\": 0,\n                            \"bad_words_ids\": [\n                                [\n                                    65000\n                                ]\n                            ],\n                            \"bos_token_id\": 0,\n                            \"classif_dropout\": 0,\n                            \"classifier_dropout\": 0,\n                            \"d_model\": 512,\n                            \"decoder_attention_heads\": 8,\n                            \"decoder_ffn_dim\": 2048,\n                            \"decoder_layerdrop\": 0,\n                            \"decoder_layers\": 6,\n                            \"decoder_start_token_id\": 65000,\n                            \"dropout\": 0.1,\n                            \"encoder_attention_heads\": 8,\n                            \"encoder_ffn_dim\": 2048,\n                            \"encoder_layerdrop\": 0,\n                            \"encoder_layers\": 6,\n                            \"eos_token_id\": 0,\n                            \"extra_pos_embeddings\": 65001,\n                            \"force_bos_token_to_be_generated\": false,\n                            \"forced_eos_token_id\": 0,\n                            \"gradient_checkpointing\": false,\n                            \"id2label\": {\n                                \"0\": \"LABEL_0\",\n                                \"1\": \"LABEL_1\",\n                                \"2\": \"LABEL_2\"\n                            },\n                            \"init_std\": 0.02,\n                            \"is_encoder_decoder\": true,\n                            \"label2id\": {\n                                \"LABEL_0\": 0,\n                                \"LABEL_1\": 1,\n                                \"LABEL_2\": 2\n                            },\n                            \"max_length\": 512,\n                            \"max_position_embeddings\": 512,\n                            \"model_type\": \"marian\",\n                            \"normalize_before\": false,\n                            \"normalize_embedding\": false,\n                            \"num_beams\": 4,\n                            \"num_hidden_layers\": 6,\n                            \"pad_token_id\": 65000,\n                            \"scale_embedding\": true,\n                            \"static_position_embeddings\": true,\n                            \"torch_dtype\": \"float32\",\n                            \"transformers_version\": \"4.16.0\",\n                            \"use_cache\": true,\n                            \"vocab_size\": 65001\n                        },\n                        \"tokenizer_config\": {\n                            \"eos_token\": \"</s>\",\n                            \"model_max_length\": 512,\n                            \"name_or_path\": \"Helsinki-NLP/opus-mt-en-es\",\n                            \"pad_token\": \"<pad>\",\n                            \"source_lang\": \"eng\",\n                            \"sp_model_kwargs\": {},\n                            \"special_tokens_map_file\": null,\n                            \"target_lang\": \"spa\",\n                            \"tokenizer_class\": \"MarianTokenizer\",\n                            \"tokenizer_file\": null,\n                            \"unk_token\": \"<unk>\"\n                        }\n                    }\n                },\n                \"import_info\": {\n                    \"params\": {\n                        \"model_name\": \"Helsinki-NLP/opus-mt-en-es\",\n                        \"pipeline_name\": \"translation_xx_to_yy\",\n                        \"tokenizer_name\": \"Helsinki-NLP/opus-mt-en-es\",\n                        \"toolkit\": \"HuggingFace\"\n                    }\n                }\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-to-text\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"description\": \"Text translation model from English to Spanish using sentence piece-based segmentation\",\n            \"metadata\": {},\n            \"notes\": \"\\n # Helsinki-NLP - English to Spanish                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n ## Introduction                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n The Helsinki-NLP models are used to translate text from one language to another. As such, the model takes a block text as its input, and outputs the translated block of text. This particular model takes in English text as it's input and outputs Spanish text.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n ## Limitations                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n The usage of random capitalization and punctuation may result in erroneous translations grammatically speaking. If you are using this model in a workflow and find grammar issues, you can try utilizing aggregators to minimize errors.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n **More Info**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n * Original Repository: [GitHub](https://github.com/Helsinki-NLP/Tatoeba-Challenge)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \\n * Helsinki-NLP Opus: [eng-spa](https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/master/models/eng-spa)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \\n * Hugging Face docs: [Helsinki-NLP/opus-mt-en-es](https://huggingface.co/Helsinki-NLP/opus-mt-en-es)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n ## Paper                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n [Natural language processing for similar languages, varieties, and dialects: A survey](https://helda.helsinki.fi/bitstream/handle/10138/330117/natural_language_processing_for_similar_languages_varieties_and_dialects_a_survey.pdf)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n Authors: Marcos Zampieri, Preslav Nakov, Yves Scherrer                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n **Abstract**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n There has been a lot of recent interest in the natural language processing (NLP) community in the computational processing of language varieties and dialects, with the aim to improve the performance of applications such as machine translation, speech recognition, and dialogue systems. Here, we attempt to survey this growing field of research, with focus on computational methods for processing similar languages, varieties, and dialects. In particular, we discuss the most important challenges when dealing with diatopic language variation, and we present some of the available datasets, the process of data collection, and the most common data collection strategies used to compile datasets for similar languages, varieties, and dialects. We further present a number of studies on computational methods developed and/or adapted for preprocessing, normalization, part-of-speech tagging, and parsing similar languages, language varieties, and dialects. Finally, we discuss relevant applications such as language and dialect identification and machine translation for closely related languages, language varieties, and dialects.                                     \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n ## Risks, Limitations, and Biases                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n **CONTENT WARNING: Readers should be aware this section contains content that is disturbing, offensive, and can propagate historical and current stereotypes.**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n There has been significant research exploring bias and fairness issues with language models. Some important papers in this field include:                                                   # Helsinki-NLP - English to Spanish                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n ## Introduction                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n The Helsinki-NLP models are used to translate text from one language to another. As such, the model takes a block text as its input, and outputs the translated block of text. This particular model takes in English text as it's input and outputs Spanish text.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n ## Limitations                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n The usage of random capitalization and punctuation may result in erroneous translations grammatically speaking. If you are using this model in a workflow and find grammar issues, you can try utilizing aggregators to minimize errors.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n **More Info**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n * Original Repository: [GitHub](https://github.com/Helsinki-NLP/Tatoeba-Challenge)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \\n * Helsinki-NLP Opus: [eng-spa](https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/master/models/eng-spa)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \\n * Hugging Face docs: [Helsinki-NLP/opus-mt-en-es](https://huggingface.co/Helsinki-NLP/opus-mt-en-es)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n ## Paper                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n [Natural language processing for similar languages, varieties, and dialects: A survey](https://helda.helsinki.fi/bitstream/handle/10138/330117/natural_language_processing_for_similar_languages_varieties_and_dialects_a_survey.pdf)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n Authors: Marcos Zampieri, Preslav Nakov, Yves Scherrer                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n **Abstract**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n There has been a lot of recent interest in the natural language processing (NLP) community in the computational processing of language varieties and dialects, with the aim to improve the performance of applications such as machine translation, speech recognition, and dialogue systems. Here, we attempt to survey this growing field of research, with focus on computational methods for processing similar languages, varieties, and dialects. In particular, we discuss the most important challenges when dealing with diatopic language variation, and we present some of the available datasets, the process of data collection, and the most common data collection strategies used to compile datasets for similar languages, varieties, and dialects. We further present a number of studies on computational methods developed and/or adapted for preprocessing, normalization, part-of-speech tagging, and parsing similar languages, language varieties, and dialects. Finally, we discuss relevant applications such as language and dialect identification and machine translation for closely related languages, language varieties, and dialects.                                     \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n ## Risks, Limitations, and Biases                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n **CONTENT WARNING: Readers should be aware this section contains content that is disturbing, offensive, and can propagate historical and current stereotypes.**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n There has been significant research exploring bias and fairness issues with language models. Some important papers in this field include:                                                  \\n * [Societal Biases in Language Generation: Progress and Challenges](https://aclanthology.org/2021.acl-long.330.pdf)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \\n     * Authors: Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, Nanyun Peng                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \\n     * Abstract: Technology for language generation has advanced rapidly, spurred by advancements in pre-training large models on massive amounts of data and the need for intelligent agents to communicate in a natural manner. While techniques can effectively generate fluent text, they can also produce undesirable societal biases that can have a disproportionately negative impact on marginalized populations. Language generation presents unique challenges for biases in terms of direct user interaction and the structure of decoding techniques. To better understand these challenges, we present a survey on societal biases in language generation, focusing on how data and techniques contribute to biases and progress towards reducing biases. Motivated by a lack of studies on biases from decoding techniques, we also conduct experiments to quantify the effects of these techniques. By further discussing general trends and open challenges, we call to attention promising directions for research and the importance of fairness and inclusivity considerations for language generation applications.<br /><br />                                                          \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n * [On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \\n     * Authors: Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, Shmargaret Shmitchell                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \\n     * Abstract: The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n ## Benchmarks                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n The following benchmarks are for the **opus-2021-02-19** weights.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n | testset                        | BLEU | chr-F | #sent | #words | BP    |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\n | ------------------------------ | ---- | ----- | ----- | ------ | ----- |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\n | newssyscomb2009-engspa.eng.spa | 31.3 | 0.583 | 502   | 12506  | 0.990 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\n | news-test2008-engspa.eng.spa   | 29.6 | 0.564 | 2051  | 52596  | 1.000 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\n | newstest2009-engspa.eng.spa    | 30.2 | 0.578 | 2525  | 68114  | 1.000 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\n | newstest2010-engspa.eng.spa    | 36.9 | 0.620 | 2489  | 65522  | 1.000 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\n | newstest2011-engspa.eng.spa    | 38.3 | 0.620 | 3003  | 79476  | 0.984 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\n | newstest2012-engspa.eng.spa    | 39.1 | 0.626 | 3003  | 79006  | 0.969 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\n | newstest2013-engspa.eng.spa    | 35.1 | 0.598 | 3000  | 70528  | 0.960 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\n | Tatoeba-test.eng.spa           | 55.1 | 0.721 | 10000 | 77311  | 0.978 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\n | tico19-test.eng-spa            | 50.4 | 0.727 | 2100  | 66591  | 0.959 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n ## Additional Info                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n * Data set: Opus                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \\n * Model: Transformer                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \\n * Source Language(s): en (English)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \\n * Target Language(s): es (Spanish)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \\n * Pre-processing: Normalization  [SentencePiece](https://github.com/google/sentencepiece) (spm32k, spm32k)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \\n * Download original weights: [opus-2021-02-19.zip](https://object.pouta.csc.fi/Tatoeba-MT-models/eng-spa/opus-2021-02-19.zip)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \\n * Test set translations: [opus-2021-02-19.test.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/eng-spa/opus-2021-02-19.test.txt)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \\n * Test set scores: [opus-2021-02-19.eval.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/eng-spa/opus-2021-02-19.eval.txt)\\n\",\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"ocr_model_v3-1677100451\",\n            \"name\": \"ocr_model_v3-1677100451\",\n            \"created_at\": \"2023-02-22T21:14:10.921823Z\",\n            \"modified_at\": \"2023-02-22T21:14:10.921823Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"22894138385843978aaa97cae37780fb\",\n                \"created_at\": \"2023-02-22T21:14:10.928773Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\",\n                        \"regions[...].value\": \"predicted_det_scores\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {\n                \"presetInputs\": [\n                    {\n                        \"alt\": \"Stop Sign.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/9/9d/Stop_sign_light_red.svg/1200px-Stop_sign_light_red.svg.png\"\n                    }\n                ]\n            },\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"ocr-scene-english-paddleocr\",\n            \"name\": \"OCR Scene English PaddleOCR\",\n            \"created_at\": \"2023-02-22T15:48:10.066388Z\",\n            \"modified_at\": \"2023-02-22T15:48:10.066388Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"40dbb2c9cde44a27af226782e7157006\",\n                \"created_at\": \"2023-02-22T15:49:55.126424Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"description\": \"An OCR model for detecting and recognizing English text in images that are more complex than scans of a page.\",\n            \"metadata\": {\n                \"presetInputs\": [\n                    {\n                        \"type\": \"image\",\n                        \"url\": \"https://samples.clarifai.com/featured-models/ocr-woman-holding-sold-sign.jpg\"\n                    },\n                    {\n                        \"type\": \"image\",\n                        \"url\": \"https://samples.clarifai.com/featured-models/paddleocrs/ocr-scene-english-paddleocr-1.jpg\"\n                    },\n                    {\n                        \"type\": \"image\",\n                        \"url\": \"https://samples.clarifai.com/featured-models/paddleocrs/ocr-scene-english-paddleocr-2.jpg\"\n                    },\n                    {\n                        \"type\": \"image\",\n                        \"url\": \"https://samples.clarifai.com/featured-models/paddleocrs/ocr-scene-english-paddleocr-3.png\"\n                    }\n                ]\n            },\n            \"notes\": \"\\n # Introduction                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n PaddleOCR aims to create multilingual, awesome, leading, and practical OCR tools that help users train better models and apply them into practice. The information in this summary is taken from their [Github.](https://github.com/PaddlePaddle/PaddleOCR)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n Release PP-OCRv3: With comparable speed, the effect of Chinese scene is further improved by 5% compared with PP-OCRv2, the effect of English scene is improved by 11%, and the average recognition accuracy of 80 language multilingual models is improved by more than 5%.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n <iframe width=\\\"560\\\" height=\\\"315\\\" src=\\\"https://www.youtube.com/embed/ITTtqGKtS54\\\" title=\\\"YouTube video player\\\" frameborder=\\\"0\\\" allow=\\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\\\" allowfullscreen></iframe>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n # Features                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n PaddleOCR support a variety of cutting-edge algorithms related to OCR, and developed industrial featured models/solution [PP-OCR](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.5/doc/doc_en/ppocr_introduction_en.md) and [PP-Structure](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.5/ppstructure/README.md) on this basis, and get through the whole process of data production, model training, compression, inference and deployment.                                                                                                                                                                                                                                                                                                  \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n ## PP-OCR Series Model List - This model is the English ultra-lightweight PP-OCRv3 model (13.4M) on the second row.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n | Model introduction                                           | Model name                   | Recommended scene | Detection model                                              | Direction classifier                                         | Recognition model                                            |                                                                                                                                                                                                                                                                                                                                                                                                                                                    \\n | ------------------------------------------------------------ | ---------------------------- | ----------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |                                                                                                                                                                                                                                                                                                                                                                                                                                                    \\n | Chinese and English ultra-lightweight PP-OCRv3 model（16.2M）     | ch_PP-OCRv3_xx          | Mobile & Server | [inference model](https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_det_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_det_distill_train.tar) | [inference model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_train.tar) | [inference model](https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_rec_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_rec_train.tar) |                                 \\n | English ultra-lightweight PP-OCRv3 model（13.4M）     | en_PP-OCRv3_xx          | Mobile & Server | [inference model](https://paddleocr.bj.bcebos.com/PP-OCRv3/english/en_PP-OCRv3_det_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/PP-OCRv3/english/en_PP-OCRv3_det_distill_train.tar) | [inference model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_train.tar) | [inference model](https://paddleocr.bj.bcebos.com/PP-OCRv3/english/en_PP-OCRv3_rec_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/PP-OCRv3/english/en_PP-OCRv3_rec_train.tar) |                                             \\n | Chinese and English ultra-lightweight PP-OCRv2 model（11.6M） |  ch_PP-OCRv2_xx |Mobile & Server|[inference model](https://paddleocr.bj.bcebos.com/PP-OCRv2/chinese/ch_PP-OCRv2_det_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/PP-OCRv2/chinese/ch_PP-OCRv2_det_distill_train.tar)| [inference model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_train.tar) |[inference model](https://paddleocr.bj.bcebos.com/PP-OCRv2/chinese/ch_PP-OCRv2_rec_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/PP-OCRv2/chinese/ch_PP-OCRv2_rec_train.tar)|                                                   \\n | Chinese and English ultra-lightweight PP-OCR model (9.4M)       | ch_ppocr_mobile_v2.0_xx      | Mobile & server   |[inference model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_det_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_det_train.tar)|[inference model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_train.tar) |[inference model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_rec_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_rec_train.tar)      |   \\n | Chinese and English general PP-OCR model (143.4M)               | ch_ppocr_server_v2.0_xx      | Server            |[inference model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_server_v2.0_det_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_server_v2.0_det_train.tar)    |[inference model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_train.tar)    |[inference model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_server_v2.0_rec_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_server_v2.0_rec_train.tar)  |\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n - For more model downloads (including multiple languages), please refer to [PP-OCR series model downloads](./doc/doc_en/models_list_en.md).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \\n - For a new language request, please refer to [Guideline for new language_requests](#language_requests).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \\n - For structural document analysis models, please refer to [PP-Structure models](./ppstructure/docs/models_list_en.md).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n # PP-OCRv3 English model                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \\n ![](https://github.com/PaddlePaddle/PaddleOCR/raw/release/2.5/doc/imgs_results/PP-OCRv3/en/en_1.png)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n # PP-OCRv3 Chinese model                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \\n ![](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.5/doc/imgs_results/PP-OCRv3/ch/PP-OCRv3-pic003.jpg?raw=true)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n # PP-OCRv3 Multilingual model                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \\n ![](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.5/doc/imgs_results/PP-OCRv3/multi_lang/korean_1.jpg?raw=true)\",\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"ocr-scene-chinese-english-paddleocr\",\n            \"name\": \"ocr-scene-chinese-english-paddleocr\",\n            \"created_at\": \"2022-08-10T21:27:40.359110Z\",\n            \"modified_at\": \"2023-02-09T23:37:22.184921Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"67104613bc7245b594d6a38eb7e34974\",\n                \"created_at\": \"2022-08-10T21:27:40.889145Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {\n                \"presetInputs\": [\n                    {\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/general-shirts-bags-shoes-computer.jpg\"\n                    }\n                ]\n            },\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"multilingual-multimodal-clip-embed\",\n            \"name\": \"Multilingual Multimodal Clip Embedder\",\n            \"created_at\": \"2023-01-30T17:46:05.745974Z\",\n            \"modified_at\": \"2023-01-30T17:46:05.745974Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"e3289fa66be4419eb2958ba74b6e9fee\",\n                \"created_at\": \"2023-01-30T17:46:05.745974Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"train_stats\": {},\n                \"completed_at\": \"2023-01-30T17:46:05.745974Z\",\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"embeddings\": \"embeddings\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\",\n                        \"text\": \"text\"\n                    },\n                    \"params\": {\n                        \"text_token_warning_limit\": 77\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"multimodal-embedder\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"description\": \"CLIP-based multilingual multimodal embedding model.\",\n            \"metadata\": {},\n            \"notes\": \"##Multilingual CLIP\",\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"ccfc0043fa804de4a586005f72582e00\",\n            \"name\": \"Multimodal Clip Clusterer\",\n            \"created_at\": \"2022-11-16T14:51:43.695740Z\",\n            \"modified_at\": \"2022-11-16T14:51:43.695740Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"4b134b9fb5f24e2bb09b7493560cc922\",\n                \"created_at\": \"2022-11-16T14:51:43.695740Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"metrics\": {\n                    \"status\": {\n                        \"code\": 21300,\n                        \"description\": \"Model was successfully evaluated.\"\n                    },\n                    \"summary\": {\n                        \"macro_avg_roc_auc\": 0,\n                        \"macro_std_roc_auc\": 0,\n                        \"macro_avg_f1_score\": 0,\n                        \"macro_std_f1_score\": 0,\n                        \"macro_avg_precision\": 0,\n                        \"macro_avg_recall\": 0,\n                        \"lopq_metrics\": [\n                            {\n                                \"k\": 10,\n                                \"recall_vs_brute_force\": 0.95100015,\n                                \"kendall_tau_vs_brute_force\": 1,\n                                \"most_frequent_code_percent\": 0.41158637,\n                                \"lopq_ndcg\": 0,\n                                \"brute_force_ndcg\": 0\n                            },\n                            {\n                                \"k\": 20,\n                                \"recall_vs_brute_force\": 0.93349975,\n                                \"kendall_tau_vs_brute_force\": 0.99947363,\n                                \"most_frequent_code_percent\": 0.41158637,\n                                \"lopq_ndcg\": 0,\n                                \"brute_force_ndcg\": 0\n                            },\n                            {\n                                \"k\": 50,\n                                \"recall_vs_brute_force\": 0.9118,\n                                \"kendall_tau_vs_brute_force\": 0.99697524,\n                                \"most_frequent_code_percent\": 0.41158637,\n                                \"lopq_ndcg\": 0,\n                                \"brute_force_ndcg\": 0\n                            },\n                            {\n                                \"k\": 100,\n                                \"recall_vs_brute_force\": 0.8869,\n                                \"kendall_tau_vs_brute_force\": 0.9947445,\n                                \"most_frequent_code_percent\": 0.41158637,\n                                \"lopq_ndcg\": 0,\n                                \"brute_force_ndcg\": 0\n                            },\n                            {\n                                \"k\": 200,\n                                \"recall_vs_brute_force\": 0.8432002,\n                                \"kendall_tau_vs_brute_force\": 0.9947241,\n                                \"most_frequent_code_percent\": 0.41158637,\n                                \"lopq_ndcg\": 0,\n                                \"brute_force_ndcg\": 0\n                            }\n                        ]\n                    }\n                },\n                \"total_input_count\": 9527727,\n                \"train_stats\": {},\n                \"completed_at\": \"2022-11-16T14:51:43.695740Z\",\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\"\n                },\n                \"input_info\": {\n                    \"base_embed_model\": {\n                        \"id\": \"multimodal-clip-embed\",\n                        \"app_id\": \"main\",\n                        \"model_version\": {\n                            \"id\": \"9fe2c8962c104327bc87b8f8104b161a\"\n                        },\n                        \"user_id\": \"clarifai\",\n                        \"model_type_id\": \"multimodal-embedder\",\n                        \"toolkits\": [],\n                        \"use_cases\": [],\n                        \"languages\": [],\n                        \"languages_full\": [],\n                        \"check_consents\": []\n                    }\n                },\n                \"train_info\": {\n                    \"params\": {\n                        \"beta\": 1,\n                        \"coarse_clusters\": 125,\n                        \"dataset_id\": \"\",\n                        \"dataset_version_id\": \"ee243135d683462eaa1060c4f5c63725\",\n                        \"eval_holdout_fraction\": 0.2,\n                        \"max_num_query_embeddings\": 100,\n                        \"max_visited\": 1562,\n                        \"num_results_per_query\": [\n                            10,\n                            20,\n                            50,\n                            100,\n                            200\n                        ],\n                        \"query_holdout_fraction\": 0.1,\n                        \"quota\": 10000,\n                        \"to_be_indexed_queries_fraction\": 0.25,\n                        \"train_iters\": 1,\n                        \"training_timeout\": 86400\n                    }\n                },\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"clusterer\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {},\n            \"notes\": \"##CLIP\",\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"multimodal-clip-embed\",\n            \"name\": \"multimodal-clip\",\n            \"created_at\": \"2022-11-07T17:47:19.112250Z\",\n            \"modified_at\": \"2022-11-07T17:47:19.112250Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"9fe2c8962c104327bc87b8f8104b161a\",\n                \"created_at\": \"2022-11-07T17:47:19.123181Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"embeddings\": \"embeddings\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\",\n                        \"text\": \"text\"\n                    },\n                    \"params\": {\n                        \"text_token_warning_limit\": 77\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"multimodal-embedder\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"da94111b740547aeae38ba9668f998a3\",\n            \"name\": \"ocr-scene-devanagari-paddleocr\",\n            \"created_at\": \"2022-08-10T19:52:53.761109Z\",\n            \"modified_at\": \"2022-08-10T19:52:53.761109Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"5e35e10fb7814f5c9223ccb3c3afebec\",\n                \"created_at\": \"2022-08-10T19:52:53.956768Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"5f42b31a4589d672152e9668d02eb471\",\n            \"name\": \"ocr-scene-cyrillic-paddleocr\",\n            \"created_at\": \"2022-08-10T19:52:53.009976Z\",\n            \"modified_at\": \"2022-08-10T19:52:53.009976Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"bbb2a719af92447ebeaabc88d3f41123\",\n                \"created_at\": \"2022-08-10T19:52:53.252145Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"e1e3b6f78fd2d55c830c631434ba83a5\",\n            \"name\": \"ocr-scene-arabic-paddleocr\",\n            \"created_at\": \"2022-08-10T19:52:51.994687Z\",\n            \"modified_at\": \"2022-08-10T19:52:51.994687Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"4b33b79b4b2e42b4b9ee07c844f1bb56\",\n                \"created_at\": \"2022-08-10T19:52:52.548327Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"c54e41dd13c9669630426078d36718ec\",\n            \"name\": \"ocr-scene-latin-paddleocr\",\n            \"created_at\": \"2022-08-10T19:52:51.331917Z\",\n            \"modified_at\": \"2022-08-10T19:52:51.331917Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"a82c91d715f74c789df25c78e811dc6a\",\n                \"created_at\": \"2022-08-10T19:52:51.487313Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"e3fde27570dc04aff3b997a223601ac8\",\n            \"name\": \"ocr-scene-kannada-paddleocr\",\n            \"created_at\": \"2022-08-10T19:52:50.570402Z\",\n            \"modified_at\": \"2022-08-10T19:52:50.570402Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"324f4a4d0bc64d81ab7be56c21748328\",\n                \"created_at\": \"2022-08-10T19:52:50.803339Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"1d5cbe1f275acaa8ac109dec6e07120b\",\n            \"name\": \"ocr-scene-telugu-paddleocr\",\n            \"created_at\": \"2022-08-10T19:52:49.858398Z\",\n            \"modified_at\": \"2022-08-10T19:52:49.858398Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"004b78b3cc3b4e2b9fe53f3cda5ff001\",\n                \"created_at\": \"2022-08-10T19:52:50.014584Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"019c84b9cae1c41e7bf661a037f6ac12\",\n            \"name\": \"ocr-scene-tamil-paddleocr\",\n            \"created_at\": \"2022-08-10T19:52:49.207422Z\",\n            \"modified_at\": \"2022-08-10T19:52:49.207422Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"88bf32db4fdb488593303bc5fb309754\",\n                \"created_at\": \"2022-08-10T19:52:49.353280Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"6748476e7c5ae4df9bc2019dd6bd5892\",\n            \"name\": \"ocr-scene-japanese-paddleocr\",\n            \"created_at\": \"2022-08-10T19:52:48.506276Z\",\n            \"modified_at\": \"2022-08-10T19:52:48.506276Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"b3a22a7bc99943b998de94c1e3c5d420\",\n                \"created_at\": \"2022-08-10T19:52:48.703164Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"bf08cee902d18405ca6ac58b68b3e2a6\",\n            \"name\": \"ocr-scene-korean-paddleocr\",\n            \"created_at\": \"2022-08-10T19:52:47.804473Z\",\n            \"modified_at\": \"2022-08-10T19:52:47.804473Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"f242bc5057b4402dab82ca0dbaf1be1e\",\n                \"created_at\": \"2022-08-10T19:52:47.982674Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"general-image-detection\",\n            \"name\": \"Image Detection\",\n            \"created_at\": \"2020-09-02T13:49:26.221543Z\",\n            \"modified_at\": \"2022-06-13T17:34:27.250292Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"1586f39b8d8040dda41537f5e47d68f0\",\n                \"created_at\": \"2020-10-20T16:16:33.521041Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 601,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0.25\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"description\": \"Detects a variety of common objects and the location and generates regions of an image that may contain that object.\",\n            \"metadata\": {\n                \"presetInputs\": [\n                    {\n                        \"alt\": \"Clarifai general model featuring shirts bags shoes computer.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/general-shirts-bags-shoes-computer.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai general model featuring elephants.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/general-elephants.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai food model featuring hamburgers bacon cheese.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/food-hamburgers-bacon-cheese.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai general model featuring car dashboard steering wheel.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/general-car-dashboard-steering-wheel.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai general model featuring city buildings skyscraper.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/general-city-buildings-skyscraper.jpg\"\n                    }\n                ]\n            },\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"general-image-recognition\",\n            \"name\": \"Image Recognition\",\n            \"created_at\": \"2016-03-19T04:14:25.007149Z\",\n            \"modified_at\": \"2022-06-13T17:34:27.240649Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"aa7f35c01e0642fda5cf400f543e7c40\",\n                \"created_at\": \"2018-03-06T19:43:54Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 9098,\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"concepts\": \"softmax\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"display_name\": \"general-visual-classifier\",\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-classifier\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"description\": \"Identifies a variety of concepts in images and video including objects, themes, and more. Trained with over 10,000 concepts and 20M images.\",\n            \"metadata\": {\n                \"presetInputs\": [\n                    {\n                        \"alt\": \"Clarifai general model featuring shirts bags shoes computer.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/general-shirts-bags-shoes-computer.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai general model featuring elephants.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/general-elephants.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai food model featuring hamburgers bacon cheese.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/food-hamburgers-bacon-cheese.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai general model featuring car dashboard steering wheel.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/general-car-dashboard-steering-wheel.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai general model featuring city buildings skyscraper.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/general-city-buildings-skyscraper.jpg\"\n                    }\n                ]\n            },\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"celebrity-face-detection\",\n            \"name\": \"Celebrity Face Detection\",\n            \"created_at\": \"2016-10-25T19:02:38.845777Z\",\n            \"modified_at\": \"2022-06-13T17:34:27.225442Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"bc50aad2bf4b403b94656aa9f64d1454\",\n                \"created_at\": \"2019-04-27T15:52:36.026793Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 10553,\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\"\n                },\n                \"input_info\": {},\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"description\": \"Detects whether images or video contain celebrity faces. Trained with over 10,000 recognized celebrities.\",\n            \"metadata\": {\n                \"presetInputs\": [\n                    {\n                        \"alt\": \"Clarifai celebrity model featuring morgan freeman.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/celebrity-morgan-freeman.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai celebrity model featuring kim kardashian.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/celebrity-kim-kardashian.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai celebrity model featuring matt damon.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/celebrity-matt-damon.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai celebrity model featuring aziz ansari.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/celebrity-aziz-ansari.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai celebrity model featuring angelina jolie.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/celebrity-angelina-jolie.jpg\"\n                    }\n                ]\n            },\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [\n                \"pii\"\n            ],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"moderation-recognition\",\n            \"name\": \"Image Moderation\",\n            \"created_at\": \"2017-05-12T21:28:00.471607Z\",\n            \"modified_at\": \"2022-06-13T17:34:27.216052Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"aa8be956dbaa4b7a858826a84253cab9\",\n                \"created_at\": \"2017-10-26T20:29:09.263232Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 5,\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"concepts\": \"softmax\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-classifier\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"description\": \"Recognizes inappropriate content in images and video containing concepts: gore, drug, explicit, suggestive, and safe.\",\n            \"metadata\": {\n                \"presetInputs\": [\n                    {\n                        \"alt\": \"Clarifai moderation model featuring medical syringe cocaine.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/moderation-medical-syringe-cocaine.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai moderation model featuring woman breast feeding.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/moderation-woman-breast-feeding.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai moderation model featuring cutting red meet blood.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/moderation-cutting-red-meet-blood.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai moderation model featuring woman man no shirts.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/moderation-woman-man-no-shirts.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai moderation model featuring try your own text.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/moderation-try-your-own-text.jpg\"\n                    }\n                ]\n            },\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"96298cb700d5ec33176b40eee032f6df\",\n            \"name\": \"army-crada-5\",\n            \"created_at\": \"2022-04-28T01:41:20.604596Z\",\n            \"modified_at\": \"2022-04-28T01:41:20.604596Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"fdf1903eca5d47a8aa80d80fb0be6187\",\n                \"created_at\": \"2022-04-28T01:41:20.638136Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 25,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0,\n                        \"max_concepts\": 20,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"df518ae2dcd8c2ef72c13e1a06f4ef52\",\n            \"name\": \"BLIP\",\n            \"created_at\": \"2022-04-21T23:11:06.152830Z\",\n            \"modified_at\": \"2022-04-21T23:11:06.152830Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"463f6b55c0b84128b97f6af550386aaa\",\n                \"created_at\": \"2022-04-21T23:11:06.167104Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"embeddings\": \"embeddings\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-embedder\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"3dfd7edda8a9cfc4ae9feab7194274a6\",\n            \"name\": \"BLIP\",\n            \"created_at\": \"2022-04-21T23:10:58.431676Z\",\n            \"modified_at\": \"2022-04-21T23:10:58.431676Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"d25dfc5c081342cbb09c60672b06b18f\",\n                \"created_at\": \"2022-04-21T23:10:58.444450Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"embeddings\": \"embeddings\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-embedder\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"7c2f2dfc80f0286b72b689379e9e81e0\",\n            \"name\": \"general-english-image-caption-blip\",\n            \"created_at\": \"2022-04-18T18:55:56.785Z\",\n            \"modified_at\": \"2022-04-18T18:55:56.785Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"d66b4cd5b5b249489c98543632f13078\",\n                \"created_at\": \"2022-04-18T18:55:56.802386Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"image-to-text\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"2a34e347777e5744496bf6ae76e01e91\",\n            \"name\": \"ocr-document-english-printed-trocr-large\",\n            \"created_at\": \"2022-04-11T17:35:23.575656Z\",\n            \"modified_at\": \"2022-04-11T17:35:23.575656Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"761de02719ce48aaafaa8ac16e3e4117\",\n                \"created_at\": \"2022-04-11T17:35:23.596706Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"ocr-scene-french-paddleocr\",\n            \"name\": \"ocr-scene-french-paddleocr\",\n            \"created_at\": \"2022-02-16T19:46:42.443570Z\",\n            \"modified_at\": \"2022-04-05T12:42:47.955693Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"e1932c6028db4a428462ceeaa8a06ba7\",\n                \"created_at\": \"2022-02-16T19:46:42.458876Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"c4d4284d2d52988de50c95791f161e63\",\n            \"name\": \"logos-yolov5\",\n            \"created_at\": \"2022-03-29T23:13:34.162269Z\",\n            \"modified_at\": \"2022-03-31T20:09:51.482312Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"244f2a67c17d42fda8483a116edc9d3f\",\n                \"created_at\": \"2022-03-31T20:09:51.490755Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 3464,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0,\n                        \"max_concepts\": 20,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"ner-english\",\n            \"name\": \"ner_english_v2\",\n            \"created_at\": \"2022-03-16T13:48:57.921631Z\",\n            \"modified_at\": \"2022-03-22T08:06:44.455695Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"a813ff5b362c41f790c506b871e7dea4\",\n                \"created_at\": \"2022-03-16T13:48:57.946788Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 9,\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"\": \"start\",\n                        \"regions[...].data.concepts[...].id\": \"labels\",\n                        \"regions[...].data.concepts[...].value\": \"scores\"\n                    },\n                    \"params\": {\n                        \"max_concepts\": 20,\n                        \"min_value\": 0,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {\n                    \"params\": {\n                        \"model_config\": {\n                            \"_name_or_path\": \"dbmdz/bert-large-cased-finetuned-conll03-english\",\n                            \"_num_labels\": 9,\n                            \"architectures\": [\n                                \"BertForTokenClassification\"\n                            ],\n                            \"attention_probs_dropout_prob\": 0.1,\n                            \"classifier_dropout\": null,\n                            \"directionality\": \"bidi\",\n                            \"hidden_act\": \"gelu\",\n                            \"hidden_dropout_prob\": 0.1,\n                            \"hidden_size\": 1024,\n                            \"id2label\": {\n                                \"0\": \"O\",\n                                \"1\": \"B-MISC\",\n                                \"2\": \"I-MISC\",\n                                \"3\": \"B-PER\",\n                                \"4\": \"I-PER\",\n                                \"5\": \"B-ORG\",\n                                \"6\": \"I-ORG\",\n                                \"7\": \"B-LOC\",\n                                \"8\": \"I-LOC\"\n                            },\n                            \"initializer_range\": 0.02,\n                            \"intermediate_size\": 4096,\n                            \"label2id\": {\n                                \"B-LOC\": 7,\n                                \"B-MISC\": 1,\n                                \"B-ORG\": 5,\n                                \"B-PER\": 3,\n                                \"I-LOC\": 8,\n                                \"I-MISC\": 2,\n                                \"I-ORG\": 6,\n                                \"I-PER\": 4,\n                                \"O\": 0\n                            },\n                            \"layer_norm_eps\": 1e-12,\n                            \"max_position_embeddings\": 512,\n                            \"model_type\": \"bert\",\n                            \"num_attention_heads\": 16,\n                            \"num_hidden_layers\": 24,\n                            \"pad_token_id\": 0,\n                            \"pooler_fc_size\": 768,\n                            \"pooler_num_attention_heads\": 12,\n                            \"pooler_num_fc_layers\": 3,\n                            \"pooler_size_per_head\": 128,\n                            \"pooler_type\": \"first_token_transform\",\n                            \"position_embedding_type\": \"absolute\",\n                            \"torch_dtype\": \"float32\",\n                            \"transformers_version\": \"4.16.0\",\n                            \"type_vocab_size\": 2,\n                            \"use_cache\": true,\n                            \"vocab_size\": 28996\n                        },\n                        \"tokenizer_config\": {\n                            \"cls_token\": \"[CLS]\",\n                            \"do_basic_tokenize\": true,\n                            \"do_lower_case\": false,\n                            \"mask_token\": \"[MASK]\",\n                            \"max_len\": 512,\n                            \"name_or_path\": \"dbmdz/bert-large-cased-finetuned-conll03-english\",\n                            \"never_split\": null,\n                            \"pad_token\": \"[PAD]\",\n                            \"sep_token\": \"[SEP]\",\n                            \"special_tokens_map_file\": null,\n                            \"strip_accents\": null,\n                            \"tokenize_chinese_chars\": true,\n                            \"tokenizer_class\": \"BertTokenizer\",\n                            \"unk_token\": \"[UNK]\"\n                        }\n                    }\n                },\n                \"import_info\": {\n                    \"params\": {\n                        \"model_name\": \"dbmdz/bert-large-cased-finetuned-conll03-english\",\n                        \"pipeline_name\": \"ner\",\n                        \"tokenizer_name\": \"dbmdz/bert-large-cased-finetuned-conll03-english\",\n                        \"toolkit\": \"HuggingFace\"\n                    }\n                }\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-token-classifier\",\n            \"task\": \"named-entity-recognition\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {},\n            \"notes\": \"dbmdz/bert-large-cased-finetuned-conll03-english\",\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"4828602e14c60157672b103bc4174b6a\",\n            \"name\": \"ocr-scene-devanagari-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:05.691047Z\",\n            \"modified_at\": \"2022-02-16T19:51:05.691047Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"9676ee4c210f42aabb032be9e077e34e\",\n                \"created_at\": \"2022-02-16T19:51:05.695789Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"79ae4202a52a10bf910158747e2b0bd6\",\n            \"name\": \"ocr-scene-cyrillic-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:05.332230Z\",\n            \"modified_at\": \"2022-02-16T19:51:05.332230Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"eecd3d4159e447f982df39f4efc6e7a3\",\n                \"created_at\": \"2022-02-16T19:51:05.337055Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"ce1ec795e7613564611373788f719e76\",\n            \"name\": \"ocr-scene-latin-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:04.953704Z\",\n            \"modified_at\": \"2022-02-16T19:51:04.953704Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"fc43e001904541188ae7c2fc9ff1d7b8\",\n                \"created_at\": \"2022-02-16T19:51:04.961006Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"a440094009739c7f8470208af640d72b\",\n            \"name\": \"ocr-scene-tamil-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:04.601929Z\",\n            \"modified_at\": \"2022-02-16T19:51:04.601929Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"3850ab217dc048dfa92bb99c612445ed\",\n                \"created_at\": \"2022-02-16T19:51:04.607237Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"b6e3b7e79bc7d1870ee895ea802da533\",\n            \"name\": \"ocr-scene-kannada-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:04.222500Z\",\n            \"modified_at\": \"2022-02-16T19:51:04.222500Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"1552d0c25c4c453983931e02f762ad40\",\n                \"created_at\": \"2022-02-16T19:51:04.227503Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"7d4b60ad1f153879defaa4ecfd25d4f4\",\n            \"name\": \"ocr-scene-telugu-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:03.816522Z\",\n            \"modified_at\": \"2022-02-16T19:51:03.816522Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"ac7029e22f054edd809ea197524c7fc8\",\n                \"created_at\": \"2022-02-16T19:51:03.821861Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"abb6ab21bf2395f88eacb3c32fbb5fc0\",\n            \"name\": \"ocr-scene-belarusian-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:03.371226Z\",\n            \"modified_at\": \"2022-02-16T19:51:03.371226Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"6be6fdaa01854f2aa3c75a31cc9e9193\",\n                \"created_at\": \"2022-02-16T19:51:03.377576Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"819928b81ec7e0b9cb18c26175447b42\",\n            \"name\": \"ocr-scene-ukranian-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:02.987649Z\",\n            \"modified_at\": \"2022-02-16T19:51:02.987649Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"8d4bf420524a47d0a476d6e86fee65dd\",\n                \"created_at\": \"2022-02-16T19:51:02.992349Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"3953adcdf93719125977186c1e2d3f8d\",\n            \"name\": \"ocr-scene-bulgarian-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:02.616626Z\",\n            \"modified_at\": \"2022-02-16T19:51:02.616626Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"c960deec8cb34e07a800d9cc237e46f5\",\n                \"created_at\": \"2022-02-16T19:51:02.622189Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"79aa2ea6d2eaf696c93fda9eae8e360b\",\n            \"name\": \"ocr-scene-serbian-latin-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:02.186999Z\",\n            \"modified_at\": \"2022-02-16T19:51:02.186999Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"3d49fec2485f42b197d0183ba098b011\",\n                \"created_at\": \"2022-02-16T19:51:02.191990Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"d5a14580da35c5ad0b2d2bd27ce81928\",\n            \"name\": \"ocr-scene-serbian-cyrillic-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:01.799532Z\",\n            \"modified_at\": \"2022-02-16T19:51:01.799532Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"f4c110d928de45a2b9b289ec903b4a6a\",\n                \"created_at\": \"2022-02-16T19:51:01.804656Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"ca24b6b1b79b2288794ec0fd61dc43b5\",\n            \"name\": \"ocr-scene-nepali-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:01.454280Z\",\n            \"modified_at\": \"2022-02-16T19:51:01.454280Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"0cab8fcaaccd4c769dd885fc55f47214\",\n                \"created_at\": \"2022-02-16T19:51:01.458814Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"d36ee770992af4049520a0728ce0db08\",\n            \"name\": \"ocr-scene-marathi-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:01.034772Z\",\n            \"modified_at\": \"2022-02-16T19:51:01.034772Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"2f8faace78e144b3afc6f767b3fb8629\",\n                \"created_at\": \"2022-02-16T19:51:01.040656Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"84463f4cb26b41d899da8ad49189e47b\",\n            \"name\": \"ocr-scene-occitan-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:00.574910Z\",\n            \"modified_at\": \"2022-02-16T19:51:00.574910Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"e12bc12d8e254d239945e50eaf7c79ac\",\n                \"created_at\": \"2022-02-16T19:51:00.583308Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"7f9563940b3cc48321ae8d126daa0403\",\n            \"name\": \"ocr-scene-urdu-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:00.083220Z\",\n            \"modified_at\": \"2022-02-16T19:51:00.083220Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"2ff3a156373c4d1393754e945d85e7ec\",\n                \"created_at\": \"2022-02-16T19:51:00.108328Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"8f503aa63b60707e818d8a6a11a7dbe9\",\n            \"name\": \"ocr-scene-persian-paddleocr\",\n            \"created_at\": \"2022-02-16T19:50:59.643063Z\",\n            \"modified_at\": \"2022-02-16T19:50:59.643063Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"2d4cd9e9a3e04c199e27ca2493370c40\",\n                \"created_at\": \"2022-02-16T19:50:59.650272Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"6f7e3f2f2ed23a27897b898b93e8874a\",\n            \"name\": \"ocr-scene-uyghur-paddleocr\",\n            \"created_at\": \"2022-02-16T19:50:59.305322Z\",\n            \"modified_at\": \"2022-02-16T19:50:59.305322Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"6d38e8fed498460f811d5840caf7a19a\",\n                \"created_at\": \"2022-02-16T19:50:59.310025Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"594d939e0c9d8b19355f80cf1086b69d\",\n            \"name\": \"ocr-scene-hindi-paddleocr\",\n            \"created_at\": \"2022-02-16T19:50:58.946883Z\",\n            \"modified_at\": \"2022-02-16T19:50:58.946883Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"20c09506f61a4b389c15b086fedd66d5\",\n                \"created_at\": \"2022-02-16T19:50:58.953501Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"18189394090615325d5d783724ae290b\",\n            \"name\": \"ocr-scene-italian-paddleocr\",\n            \"created_at\": \"2022-02-16T19:50:58.539760Z\",\n            \"modified_at\": \"2022-02-16T19:50:58.539760Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"a56454cd3c2943a2a1a4b8840b73fda5\",\n                \"created_at\": \"2022-02-16T19:50:58.546364Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"2e47eda28e43cda4d9f5556924662bec\",\n            \"name\": \"ocr-scene-japanese-paddleocr\",\n            \"created_at\": \"2022-02-16T19:50:58.125647Z\",\n            \"modified_at\": \"2022-02-16T19:50:58.125647Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"413a870e2105418fbaadfbebc1b71b17\",\n                \"created_at\": \"2022-02-16T19:50:58.132649Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"e184d889c6e9737e6744e48df535d8f9\",\n            \"name\": \"ocr-scene-korean-paddleocr\",\n            \"created_at\": \"2022-02-16T19:50:57.685263Z\",\n            \"modified_at\": \"2022-02-16T19:50:57.685263Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"1b2985fac7d34e3cba5adfdfb1345705\",\n                \"created_at\": \"2022-02-16T19:50:57.695732Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"2b7192cc990a1a5a68d12f8326475ba6\",\n            \"name\": \"ocr-scene-german-paddleocr\",\n            \"created_at\": \"2022-02-16T19:50:57.289519Z\",\n            \"modified_at\": \"2022-02-16T19:50:57.289519Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"b10c4e125fb5423ca03f9f7600b2af6f\",\n                \"created_at\": \"2022-02-16T19:50:57.295199Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"952daadb10f7ace3433e8fc372c7c4d6\",\n            \"name\": \"ocr-scene-russian-paddleocr\",\n            \"created_at\": \"2022-02-16T19:50:56.894498Z\",\n            \"modified_at\": \"2022-02-16T19:50:56.894498Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"b02773fbacaf4f3eb5dcd6144ca01ee6\",\n                \"created_at\": \"2022-02-16T19:50:56.901399Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"60a98793f1db9dacb48e61fbb5aa61e1\",\n            \"name\": \"ocr-scene-portuguese-paddleocr\",\n            \"created_at\": \"2022-02-16T19:50:56.490797Z\",\n            \"modified_at\": \"2022-02-16T19:50:56.490797Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"d7c3be32a4bf488a987f97587cb9299d\",\n                \"created_at\": \"2022-02-16T19:50:56.500084Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"5560bcbc3d7ad8feb3999874b558b2a8\",\n            \"name\": \"ocr-scene-spanish-paddleocr\",\n            \"created_at\": \"2022-02-16T19:50:56.091771Z\",\n            \"modified_at\": \"2022-02-16T19:50:56.091771Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"a4bbcd9a06054acb8d44b3cc313dc6de\",\n                \"created_at\": \"2022-02-16T19:50:56.109454Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"fda611add70f1009c5c01d95f0fbe57b\",\n            \"name\": \"ocr-scene-arabic-paddleocr\",\n            \"created_at\": \"2022-02-16T19:50:55.696689Z\",\n            \"modified_at\": \"2022-02-16T19:50:55.696689Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"b1b8c11aee044f4a860c61d256bbd12e\",\n                \"created_at\": \"2022-02-16T19:50:55.707467Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"7acefe1233c41a8acab80619d79c6b56\",\n            \"name\": \"ocr-scene-chinese-traditional-paddleocr\",\n            \"created_at\": \"2022-02-16T19:50:55.240293Z\",\n            \"modified_at\": \"2022-02-16T19:50:55.240293Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"3b83168a4e764cd3b61c63e9662af235\",\n                \"created_at\": \"2022-02-16T19:50:55.255319Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"dc09ac965f64826410fbd8fea603abe6\",\n            \"name\": \"ocr-scene-chinese-english-paddleocr\",\n            \"created_at\": \"2022-01-25T01:53:05.944447Z\",\n            \"modified_at\": \"2022-01-25T01:53:05.944447Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"8aa1c6c2febf49f880cae2783097c2fa\",\n                \"created_at\": \"2022-01-25T01:53:05.952851Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"ba040818064ef61074e6d1cdec1c40c6\",\n            \"name\": \"paddleocr-english-chinese\",\n            \"created_at\": \"2022-01-25T00:30:04.159834Z\",\n            \"modified_at\": \"2022-01-25T00:30:04.159834Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"6daaabc3998440eeb1db18086d926bd8\",\n                \"created_at\": \"2022-01-25T00:30:04.183333Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"celebrity-face-recognition\",\n            \"name\": \"Celebrity\",\n            \"created_at\": \"2020-11-20T17:09:42.109453Z\",\n            \"modified_at\": \"2022-01-21T13:11:23.236298Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"0676ebddd5d6413ebdaa101570295a39\",\n                \"created_at\": \"2020-11-20T17:09:42.109453Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 10553,\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"concepts\": \"softmax\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-classifier\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"description\": \"AI model for recognizing celebrity faces in images or video.\",\n            \"metadata\": {\n                \"presetInputs\": [\n                    {\n                        \"alt\": \"Clarifai celebrity model featuring morgan freeman.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/celebrity-morgan-freeman.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai celebrity model featuring kim kardashian.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/celebrity-kim-kardashian.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai celebrity model featuring matt damon.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/celebrity-matt-damon.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai celebrity model featuring aziz ansari.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/celebrity-aziz-ansari.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai celebrity model featuring angelina jolie.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/celebrity-angelina-jolie.jpg\"\n                    }\n                ]\n            },\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [\n                \"pii\"\n            ],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"3b67f45ec89b4b9c6fb9db700120c91a\",\n            \"name\": \"advanced-det-handgun\",\n            \"created_at\": \"2022-01-06T20:27:53.965051Z\",\n            \"modified_at\": \"2022-01-06T20:27:53.965051Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"cecc3b705fe74a9ea88ee11e5ddd46f4\",\n                \"created_at\": \"2022-01-06T20:27:54.022071Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 2,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0,\n                        \"max_concepts\": 20,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"e4e42c048019241132ec2ecb583a3446\",\n            \"name\": \"microsoft/trocr-base-handwritten\",\n            \"created_at\": \"2021-11-10T17:18:34.067075Z\",\n            \"modified_at\": \"2021-12-29T16:56:15.937705Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"fd6f3dfd83854dd59cdeb8a5243e9e27\",\n                \"created_at\": \"2021-11-10T17:18:34.083675Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {},\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"86039c857a206810679f7f72b82fff54\",\n            \"name\": \"CLIP Prefix Captioning\",\n            \"created_at\": \"2021-12-09T04:32:31.377820Z\",\n            \"modified_at\": \"2021-12-09T04:32:31.377820Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"05fb71c53ff746f3834ab8333e401a1c\",\n                \"created_at\": \"2021-12-09T04:32:31.426529Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"image\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"image-to-text\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {},\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"bebd0da30b7090bb5250d5951960d96d\",\n            \"name\": \"CLIP\",\n            \"created_at\": \"2021-12-04T02:51:40.264878Z\",\n            \"modified_at\": \"2021-12-04T02:51:40.264878Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"5f6c752f8f964e56bef0c2eb32f3aca4\",\n                \"created_at\": \"2021-12-04T02:51:40.285448Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"embeddings\": \"embeddings\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-embedder\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"d05c045b95d85241c7d79e1ed3da3f8e\",\n            \"name\": \"PaddleOCR Multiplexed\",\n            \"created_at\": \"2021-11-05T04:36:50.693728Z\",\n            \"modified_at\": \"2021-11-05T04:36:50.693728Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"6da2f96ab8fb4255aa0ac53e6653345b\",\n                \"created_at\": \"2021-11-05T04:36:50.707810Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"fe000880ecc20921af7fbd46c485dbd2\",\n            \"name\": \"multiplexed easyocr\",\n            \"created_at\": \"2021-11-03T04:05:28.978761Z\",\n            \"modified_at\": \"2021-11-03T04:05:28.978761Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"19b30d2a3b064b13bbcd451a7b829f40\",\n                \"created_at\": \"2021-11-03T04:05:28.996684Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"c13d6411707faa5a5115b23f17957d82\",\n            \"name\": \"EasyOCR Multilingual Large\",\n            \"created_at\": \"2021-10-30T21:31:12.405449Z\",\n            \"modified_at\": \"2021-10-30T21:31:12.405449Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"7a4e17e73e144e5c8e9570aadc1cf0f5\",\n                \"created_at\": \"2021-10-30T21:31:12.411454Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"856fd858ed1dc14c741c15f0b9005cbb\",\n            \"name\": \"EasyOCR Multilingual\",\n            \"created_at\": \"2021-10-30T20:11:48.409877Z\",\n            \"modified_at\": \"2021-10-30T20:11:48.409877Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"5a77e2441d334d95859337454efbfd73\",\n                \"created_at\": \"2021-10-30T20:11:48.417474Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"23a5c4692f1a0449aef1c510be55b180\",\n            \"name\": \"facebook/wav2vec2-base-960h\",\n            \"created_at\": \"2021-10-05T17:43:10.985960Z\",\n            \"modified_at\": \"2021-10-27T13:18:19.573415Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"f4917f7f2e83453f9fe6d5eef4f598db\",\n                \"created_at\": \"2021-10-05T17:43:10.998245Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"audio\": \"audio\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"audio-to-text\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {},\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [\n                \"pii\"\n            ],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"2c1050ead1b24472be8033f5fd421f3d\",\n            \"name\": \"english\",\n            \"created_at\": \"2020-11-12T15:43:07.560737Z\",\n            \"modified_at\": \"2021-10-27T13:13:42.421644Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"dfe4776f79ee4c23a85d86d3b0649127\",\n                \"created_at\": \"2020-11-12T15:43:07.560737Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"text\": \"TRANSCRIPT\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"audio\": \"AUDIO_SIGNAL\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"audio-to-text\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {},\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"ba585a5a737771884d89972fea6c41f8\",\n            \"name\": \"language-script\",\n            \"created_at\": \"2021-10-20T18:05:49.263669Z\",\n            \"modified_at\": \"2021-10-20T18:05:49.263669Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"60fa287837dc4c10a3b5da6b92e062e2\",\n                \"created_at\": \"2021-10-20T18:05:49.328969Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 8,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"concepts\": \"softmax\"\n                    },\n                    \"params\": {\n                        \"max_concepts\": 20,\n                        \"min_value\": 0,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"logo-detection\",\n            \"name\": \"logo\",\n            \"created_at\": \"2017-03-06T18:38:13.025998Z\",\n            \"modified_at\": \"2021-10-19T12:21:30.579234Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"ef1b7237d28b415f910ca343a9145e99\",\n                \"created_at\": \"2017-03-06T18:38:13.025998Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 561,\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_cls_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_cls_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_cls_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0.05\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"display_name\": \"logo-visual-detector\",\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"description\": \"Logo detection model for locating logos of some of the most popular consumer brands within images and videos.\",\n            \"metadata\": {\n                \"presetInputs\": [\n                    {\n                        \"alt\": \"\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/logo-detection-black-bmw-silver-volkswagon.jpg\"\n                    },\n                    {\n                        \"alt\": \"\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/logo-detection-corona-extra-beer-bottle.jpg\"\n                    },\n                    {\n                        \"alt\": \"\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/logo-detection-northface-pink-jacket.jpg\"\n                    },\n                    {\n                        \"alt\": \"\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/logo-detection-pepsi-bottles-in-crate.jpg\"\n                    },\n                    {\n                        \"alt\": \"\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/logo-detection-tesla-steering-wheel.jpg\"\n                    }\n                ]\n            },\n            \"notes\": \"I can edit\",\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"food-item-v1-recognition\",\n            \"name\": \"food-items-v1.0\",\n            \"created_at\": \"2016-09-17T04:22:07.183747Z\",\n            \"modified_at\": \"2021-10-18T16:34:36.410436Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"dfebc169854e429086aceb8368662641\",\n                \"created_at\": \"2016-09-17T04:22:07.183747Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 970,\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"concepts\": \"softmax\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"display_name\": \"food-items-v1-visual-classifier\",\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-classifier\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"description\": \"AI model to recognize a wide variety of food items, including dishes and ingredients, in images and video.\",\n            \"metadata\": {\n                \"presetInputs\": [\n                    {\n                        \"alt\": \"Clarifai food model featuring pan of steamed clams.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/food-pan-of-steamed-clams.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai food model featuring waffle with strawberries blueberries.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/food-waffle-with-strawberries-blueberries.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai food model featuring hamburgers bacon cheese buns.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/food-hamburgers-bacon-cheese-buns.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai food model featuring pepperoni pizza.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/food-pepperoni-pizza.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai food model featuring tomato basil.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/food-tomato-basil.jpg\"\n                    }\n                ]\n            },\n            \"notes\": \"This is a food model note (wip) \",\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"9fe78b4150a52794f86f237770141b33\",\n            \"name\": \"english\",\n            \"created_at\": \"2021-02-04T05:24:54.250897Z\",\n            \"modified_at\": \"2021-10-18T15:46:17.997145Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"89961b2723e440abb49ec89a05b31219\",\n                \"created_at\": \"2021-09-30T14:43:29.569715Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"license\": \"BSD-2\",\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"image\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"image-to-text\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"description\": \"--\",\n            \"metadata\": {},\n            \"notes\": \"test model note now\",\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [\n                \"demographics\"\n            ],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"c3110dc5905447e410161091f0f95337\",\n            \"name\": \"anas/wav2vec2-large-xlsr-arabic\",\n            \"created_at\": \"2021-10-14T19:23:19.862284Z\",\n            \"modified_at\": \"2021-10-14T19:23:19.862284Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"f486dde2e2d046dabfd4c9e4db2c8e36\",\n                \"created_at\": \"2021-10-14T19:23:19.869018Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"audio\": \"audio\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"audio-to-text\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"68bbf91b9ad247921822a255ca381f11\",\n            \"name\": \"elgeish/wav2vec2-large-xlsr-53-arabic\",\n            \"created_at\": \"2021-10-14T18:44:04.938624Z\",\n            \"modified_at\": \"2021-10-14T18:44:04.938624Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"749a7a96fb404596bc11465dc41c3fb2\",\n                \"created_at\": \"2021-10-14T18:44:04.946902Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"audio\": \"audio\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"audio-to-text\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"face-detection\",\n            \"name\": \"Face\",\n            \"created_at\": \"2020-11-20T17:09:42.109453Z\",\n            \"modified_at\": \"2021-10-14T07:45:05.937031Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"fe995da8cb73490f8556416ecf25cea3\",\n                \"created_at\": \"2021-01-21T23:31:28.004422Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 2,\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0.9\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"display_name\": \"Face-visual-detector\",\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"description\": \"AI model for detecting the location of human faces in images and video.\",\n            \"metadata\": {\n                \"presetInputs\": [\n                    {\n                        \"alt\": \"Clarifai face model featuring little girl boy standing outside.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/face-little-girl-boy-standing-outside.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai face model featuring three men sitting in van.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/face-three-men-sitting-in-van.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai face model featuring family with light blue shirts.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/face-family-with-light-blue-shirts.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai face model featuring arfrican american man woman laughing.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/face-arfrican-american-man-woman-laughing.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai face model featuring crowd of monks orange robe.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/face-crowd-of-monks-orange-robe.jpg\"\n                    }\n                ]\n            },\n            \"notes\": \"test123\",\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [\n                \"faces\"\n            ],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [\n                \"pii\"\n            ],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"6a3dc529acf3f720a629cdc8c6ad41a9\",\n            \"name\": \"subject\",\n            \"created_at\": \"2021-04-26T09:20:01.359645Z\",\n            \"modified_at\": \"2021-10-12T12:36:06.443774Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"55b2051b75f14577b6fdd5a4fa3fd5a8\",\n                \"created_at\": \"2021-04-26T15:06:37.619220Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 2,\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].region_info.mask,regions[...].data.concepts\": \"output__0\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"input__0\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-segmenter\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"description\": \"test\",\n            \"metadata\": {},\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"beb9ed2d034b0126c6e24135ace76d8f\",\n            \"name\": \"prithivida/informal_to_formal_styletransfer\",\n            \"created_at\": \"2021-10-05T19:58:24.859525Z\",\n            \"modified_at\": \"2021-10-05T20:06:53.273373Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"a35db0e9b9b34f52ac0c83e1007db145\",\n                \"created_at\": \"2021-10-05T20:06:53.278139Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-to-text\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"69469f13c714cb6c68149db326d8c69a\",\n            \"name\": \"prithivida/formal_to_informal_styletransfer\",\n            \"created_at\": \"2021-10-05T19:45:07.925572Z\",\n            \"modified_at\": \"2021-10-05T19:45:07.925572Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"d34c3b68aadb408f9e770632183cb164\",\n                \"created_at\": \"2021-10-05T19:45:07.942034Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-to-text\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"b7e22ac73f924e2a6199af495724ddac\",\n            \"name\": \"facebook/wav2vec2-large-xlsr-53-french\",\n            \"created_at\": \"2021-10-05T17:18:44.727040Z\",\n            \"modified_at\": \"2021-10-05T17:18:44.727040Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"a648b13f7269461d9797e6bc58111a60\",\n                \"created_at\": \"2021-10-05T17:18:44.743898Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"audio\": \"audio\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"audio-to-text\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"6db1c2dc0d9c09d843c763bc0a05b989\",\n            \"name\": \"assembly\",\n            \"created_at\": \"2021-09-23T17:34:00.947876Z\",\n            \"modified_at\": \"2021-09-24T06:17:00.318253Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"5ef730b4ef014b508b31af5e5577386d\",\n                \"created_at\": \"2021-09-24T06:17:00.341875Z\",\n                \"status\": {\n                    \"code\": 99009,\n                    \"description\": \"Model upload timed out\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"embeddings\": \"embedding\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-embedder\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"2e099a9219da8fc580ac0dc54bf842fd\",\n            \"name\": \"paddleocr-multilingual-text-detector\",\n            \"created_at\": \"2021-08-19T00:40:40.789176Z\",\n            \"modified_at\": \"2021-09-22T19:50:23.797705Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"dff9af491f0d48449801decee0e2f136\",\n                \"created_at\": \"2021-09-22T19:50:23.809040Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 2,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0,\n                        \"max_concepts\": 20,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"6f96f8bf841280a388f8b52cb1868df4\",\n            \"name\": \"person-detector-yolov5x-libtorch\",\n            \"created_at\": \"2021-08-25T22:19:26.887486Z\",\n            \"modified_at\": \"2021-08-25T22:19:26.887486Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"8d1ac866905a4154b6d34ae2566503ad\",\n                \"created_at\": \"2021-08-25T22:19:26.962227Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 2,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0,\n                        \"max_concepts\": 20,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"a065882d92d66dbac3b5ebe108170197\",\n            \"name\": \"person-vehicle-detector-yolov5x-libtorch\",\n            \"created_at\": \"2021-08-25T17:56:25.508821Z\",\n            \"modified_at\": \"2021-08-25T21:51:12.368401Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"be2ea5c740f940429e6284826660da9a\",\n                \"created_at\": \"2021-08-25T21:51:12.377067Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 3,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0,\n                        \"max_concepts\": 20,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"2cf7739e65bfc63c1537f65e7ef3ae87\",\n            \"name\": \"person-vehicle-detector-yolov5s-libtorch\",\n            \"created_at\": \"2021-08-24T19:14:37.883020Z\",\n            \"modified_at\": \"2021-08-24T19:14:37.883020Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"3b6b338aa1b7433a8935d22b0915fc32\",\n                \"created_at\": \"2021-08-24T19:14:37.990646Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 3,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0,\n                        \"max_concepts\": 20,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"23aa4f9c9767a2fd61e63c55a73790ad\",\n            \"name\": \"person-detector-yolov5s-libtorch\",\n            \"created_at\": \"2021-08-24T18:52:53.766484Z\",\n            \"modified_at\": \"2021-08-24T18:52:53.766484Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"92b204309cce44209f1428e38e3406fb\",\n                \"created_at\": \"2023-02-24T05:41:57.516485Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 2,\n                \"description\": \"Yolov5\",\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0,\n                        \"max_concepts\": 20,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"bdcedc0f8da58c396b7df12f634ef923\",\n            \"name\": \"multilingual-moderation\",\n            \"created_at\": \"2021-01-13T16:45:48.370348Z\",\n            \"modified_at\": \"2021-07-13T15:12:21.536311Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"c56b46fd91ca4540823ba70496d008f9\",\n                \"created_at\": \"2021-01-13T20:45:50.641841Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 6,\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"concepts\": \"output__1\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"input__0\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-classifier\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {},\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"eac94da66f428ebf98dc2cae30030699\",\n            \"name\": \"hate-symbols\",\n            \"created_at\": \"2021-07-07T20:24:28.707682Z\",\n            \"modified_at\": \"2021-07-08T15:06:49.454931Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"ca52d78ed8cb4cfba4eacebadc38548b\",\n                \"created_at\": \"2021-07-08T15:06:49.459168Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 2,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0.05,\n                        \"max_concepts\": 20,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"bc6b2c89a5dc35ee5b5872612d0df25a\",\n            \"name\": \"EasyOCR (Turkish)\",\n            \"created_at\": \"2021-06-17T19:51:18.060645Z\",\n            \"modified_at\": \"2021-06-17T19:51:18.060645Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"17ca4640290f4c3e885ed74e757272df\",\n                \"created_at\": \"2021-06-17T19:51:18.066247Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"869245fe9708e30e6a0869c6e3dc3132\",\n            \"name\": \"EasyOCR (Swedish)\",\n            \"created_at\": \"2021-06-17T19:49:51.218255Z\",\n            \"modified_at\": \"2021-06-17T19:49:51.218255Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"8eb77eec58314083a0b9765047974d45\",\n                \"created_at\": \"2021-06-17T19:49:51.224781Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"b24a775f7b9e156f3518772206c342ef\",\n            \"name\": \"EasyOCR (Polish)\",\n            \"created_at\": \"2021-06-17T19:45:42.334798Z\",\n            \"modified_at\": \"2021-06-17T19:45:42.334798Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"ce2624cd1b84445e988b1b437dfe2a95\",\n                \"created_at\": \"2021-06-17T19:45:42.340957Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"e6bcd24eb84835a8de347b7b8a028f27\",\n            \"name\": \"EasyOCR (Dutch)\",\n            \"created_at\": \"2021-06-17T19:43:57.103955Z\",\n            \"modified_at\": \"2021-06-17T19:43:57.103955Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"22669f6ea56e428f87465b55c9296ca5\",\n                \"created_at\": \"2021-06-17T19:43:57.109610Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"f43080883661c6676779384149eb9249\",\n            \"name\": \"EasyOCR (Malay)\",\n            \"created_at\": \"2021-06-17T19:37:49.087245Z\",\n            \"modified_at\": \"2021-06-17T19:37:49.087245Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"650fde39145f417db48b6270bfa78c2a\",\n                \"created_at\": \"2021-06-17T19:37:49.096062Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"a34ea557d25cc1c7b3b6bf080381eb04\",\n            \"name\": \"EasyOCR (Vietnamese)\",\n            \"created_at\": \"2021-06-17T19:32:48.422165Z\",\n            \"modified_at\": \"2021-06-17T19:32:48.422165Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"9dcfa47d32374f2684e5ed167ad0337a\",\n                \"created_at\": \"2021-06-17T19:32:48.431675Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"edd250a6e1f82cbee84819e3550dbaf4\",\n            \"name\": \"Helsinki-NLP/opus-mt-nl-en\",\n            \"created_at\": \"2021-05-28T16:30:24.060377Z\",\n            \"modified_at\": \"2021-05-28T16:30:24.060377Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"1183f7daaadd48969be19c0e7ad1c5ec\",\n                \"created_at\": \"2021-05-28T16:30:24.070688Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-to-text\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"68a51a726f7033bbfcf57c905f09b7ca\",\n            \"name\": \"general\",\n            \"created_at\": \"2021-04-19T16:03:17.091357Z\",\n            \"modified_at\": \"2021-05-20T08:13:55.391070Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"df708d35d2fa4dea9e9d3f76ce842450\",\n                \"created_at\": \"2021-05-20T08:13:55.399476Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 183,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].region_info.mask,regions[...].data.concepts\": \"output__0\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"input__0\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-segmenter\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"19de30b6d9c38ed8a0478ac5103efebe\",\n            \"name\": \"person-vehicle\",\n            \"created_at\": \"2021-05-14T19:50:52.826338Z\",\n            \"modified_at\": \"2021-05-18T18:59:59.867143Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"f06017161d0843dca0c6cf962cf08a11\",\n                \"created_at\": \"2021-05-18T18:59:59.876468Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 3,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0,\n                        \"max_concepts\": 20,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"332956f015ea667f81cef1f37b1a20f3\",\n            \"name\": \"person-vehicle-lite\",\n            \"created_at\": \"2021-05-14T19:33:26.027753Z\",\n            \"modified_at\": \"2021-05-18T18:55:51.389600Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"ef042aa9117141079f579db19809b1d3\",\n                \"created_at\": \"2021-05-18T18:55:51.401858Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 3,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0,\n                        \"max_concepts\": 20,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"4236cc320afa91a7d6c53ec949b66785\",\n            \"name\": \"Helsinki-NLP/opus-mt-es-en\",\n            \"created_at\": \"2021-05-12T22:17:11.471812Z\",\n            \"modified_at\": \"2021-05-12T22:17:11.471812Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"6fff4d1143114416b47f278084f4ffc7\",\n                \"created_at\": \"2021-05-12T22:17:11.477501Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-to-text\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"31025e019a18970a1acc55ba6a184dc6\",\n            \"name\": \"face-sentiment\",\n            \"created_at\": \"2021-05-12T16:02:43.390981Z\",\n            \"modified_at\": \"2021-05-12T16:02:43.390981Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"edcf31cfa67e426a8b12cd889453f0c3\",\n                \"created_at\": \"2021-05-12T16:02:43.563574Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 7,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"concepts\": \"softmax\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"17a76b5162066195dad4c0437e66be80\",\n            \"name\": \"objectness-detector\",\n            \"created_at\": \"2021-05-11T17:35:00.699972Z\",\n            \"modified_at\": \"2021-05-11T17:35:00.699972Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"a488dd0eb0b94e16b22aa35656f4dd31\",\n                \"created_at\": \"2021-05-11T17:35:00.734676Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 1,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0.6\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"e609405a6ced78aa8a9eff2288f6edc6\",\n            \"name\": \"tank-rodeo\",\n            \"created_at\": \"2021-05-04T15:39:36.312115Z\",\n            \"modified_at\": \"2021-05-04T15:39:36.312115Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"745c6724c6a3456bb41ab0814070e191\",\n                \"created_at\": \"2021-05-04T15:39:36.537311Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 14,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0.1\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"385c7fd117d77553962b39629659d51a\",\n            \"name\": \"apparel\",\n            \"created_at\": \"2021-04-23T20:11:59.736603Z\",\n            \"modified_at\": \"2021-04-26T18:20:03.699907Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"c318a0b6769540e6bfe684af83560a9f\",\n                \"created_at\": \"2021-04-26T18:20:03.705751Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 192,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"concepts\": \"softmax\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"c833800b94175363881d4db8b55e4a52\",\n            \"name\": \"test20_ner\",\n            \"created_at\": \"2021-04-23T18:19:00.421074Z\",\n            \"modified_at\": \"2021-04-23T18:19:00.421074Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"aa16e51cc0ef427ab728928932fed6f6\",\n                \"created_at\": \"2021-04-23T18:19:00.654472Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 9,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"labels\",\n                        \"regions[...].data.concepts[...].value\": \"scores\",\n                        \"regions[...].data.text\": \"tokens\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-token-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"9de6872fa62a3118ce66c313e5c7d567\",\n            \"name\": \"test4_bert_base_ner\",\n            \"created_at\": \"2021-04-22T01:34:00.926953Z\",\n            \"modified_at\": \"2021-04-22T01:34:00.926953Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"b263587fd32f48f4a89a80271b5ce18f\",\n                \"created_at\": \"2021-04-22T01:34:01.026938Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 9,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"labels\",\n                        \"regions[...].data.concepts[...].value\": \"scores\",\n                        \"regions[...].data.text\": \"tokens\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-token-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"33128485c45b681a11380cea3933789a\",\n            \"name\": \"test2_bert_base_ner\",\n            \"created_at\": \"2021-04-21T20:34:07.150108Z\",\n            \"modified_at\": \"2021-04-21T20:34:07.150108Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"da4b266f7b654679b6b4ba8c8691c326\",\n                \"created_at\": \"2021-04-21T20:34:07.278954Z\",\n                \"status\": {\n                    \"code\": 99009,\n                    \"description\": \"Training Unknown Error\"\n                },\n                \"active_concept_count\": 9,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"labels\",\n                        \"regions[...].data.concepts[...].value\": \"scores\",\n                        \"regions[...].data.text\": \"tokens\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-token-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"48f9e31a431a24754f7db8bf221e1e41\",\n            \"name\": \"test_bert_base_ner\",\n            \"created_at\": \"2021-04-21T15:22:05.347218Z\",\n            \"modified_at\": \"2021-04-21T15:22:05.347218Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"e9c0b45944a24b6a9c3e77f0a10a836f\",\n                \"created_at\": \"2021-04-21T15:22:05.487673Z\",\n                \"status\": {\n                    \"code\": 99009,\n                    \"description\": \"Training Unknown Error\"\n                },\n                \"active_concept_count\": 9,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"labels\",\n                        \"regions[...].data.concepts[...].value\": \"scores\",\n                        \"regions[...].data.text\": \"tokens\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-token-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"b5632bca03e3aca4e0c948303935fb0d\",\n            \"name\": \"dslim/bert-base-NER\",\n            \"created_at\": \"2021-04-20T21:52:32.656339Z\",\n            \"modified_at\": \"2021-04-20T21:52:32.656339Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"ff092f6185c94800902d62ff68775a6e\",\n                \"created_at\": \"2021-04-20T21:52:32.810261Z\",\n                \"status\": {\n                    \"code\": 99009,\n                    \"description\": \"Training Unknown Error\"\n                },\n                \"active_concept_count\": 9,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"labels\",\n                        \"regions[...].data.concepts[...].value\": \"scores\",\n                        \"regions[...].data.text\": \"tokens\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-token-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"8db409c46a3a9153becc3565f1d99022\",\n            \"name\": \"Test Triton GPU Landmarks\",\n            \"created_at\": \"2021-01-15T21:31:47.307624Z\",\n            \"modified_at\": \"2021-04-12T20:07:18.744912Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"cf33eda7f56f4d389ffac719cdf82da4\",\n                \"created_at\": \"2021-04-12T20:07:18.749574Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 1,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts,regions[...].region_info.keypoint_locations\": \"output__0\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"input__0\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-keypointer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        }\n    ]\n}"
                  },
                  "example-2": {
                    "summary": "List Models in Community",
                    "value": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"dd2303b0cb6fe8859693ae9e8f4adfa7\"\n    },\n    \"models\": [\n        {\n            \"id\": \"openai-create-image-9\",\n            \"name\": \"newnewnewname\",\n            \"created_at\": \"2023-11-23T11:16:25.775735Z\",\n            \"modified_at\": \"2023-11-23T11:19:38.173686Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"text-to-text\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"test-model-with-hyperparms-1700738311\",\n            \"name\": \"test-model-with-hyperparms-1700738311\",\n            \"created_at\": \"2023-11-23T11:18:31.353199Z\",\n            \"modified_at\": \"2023-11-23T11:18:31.353199Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"embedding-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"test-model-1700738282\",\n            \"name\": \"test-model-1700738282\",\n            \"created_at\": \"2023-11-23T11:18:01.807941Z\",\n            \"modified_at\": \"2023-11-23T11:18:01.807941Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"embedding-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"test-model-1700738263\",\n            \"name\": \"test-model-1700738263\",\n            \"created_at\": \"2023-11-23T11:17:43.761741Z\",\n            \"modified_at\": \"2023-11-23T11:17:43.761741Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"embedding-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"test-model-1700738240\",\n            \"name\": \"test-model-1700738240\",\n            \"created_at\": \"2023-11-23T11:17:20.719370Z\",\n            \"modified_at\": \"2023-11-23T11:17:20.719370Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"image-crop\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"test-model-1700738220\",\n            \"name\": \"test-model-1700738220\",\n            \"created_at\": \"2023-11-23T11:17:00.314086Z\",\n            \"modified_at\": \"2023-11-23T11:17:00.314086Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"embedding-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"profanity-filter-new2\",\n            \"name\": \"profanity-filter-new2\",\n            \"created_at\": \"2023-11-23T11:16:45.317604Z\",\n            \"modified_at\": \"2023-11-23T11:16:45.317604Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"keyword-filter-operator\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"named-entity-recognition-diseases-english-text\",\n            \"name\": \"burgerz\",\n            \"created_at\": \"2023-11-23T11:15:32.431806Z\",\n            \"modified_at\": \"2023-11-23T11:16:02.900299Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"embedding-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"custom-config\",\n            \"name\": \"custom-config\",\n            \"created_at\": \"2023-11-23T09:41:08.002419Z\",\n            \"modified_at\": \"2023-11-23T09:41:08.002419Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"model_version\": {\n                \"id\": \"1e4c121974f849209abb658cdf682585\",\n                \"created_at\": \"2023-11-23T09:41:18.470087Z\",\n                \"status\": {\n                    \"code\": 21110,\n                    \"description\": \"datasets.dataset.DataBatchEmpty: No databatch found in train set's file directory\\nFailed to create a training dataset, because there are no appropriately annotated inputs. Expected annotations with concepts for model type id text-classifier. \"\n                },\n                \"active_concept_count\": 6,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"test-app-1700638575-empty\",\n                \"user_id\": \"a0btrubbaefn\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"params\": {\n                        \"max_concepts\": 20,\n                        \"min_value\": 0,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {},\n                \"train_info\": {\n                    \"params\": {\n                        \"dataset_id\": \"\",\n                        \"dataset_version_id\": \"\",\n                        \"invalid_data_tolerance_percent\": 5,\n                        \"model_config\": {\n                            \"pretrained_model_name\": \"EleutherAI/gpt-neo-125m\"\n                        },\n                        \"num_gpus\": 1,\n                        \"peft_config\": {\n                            \"peft_type\": \"LORA\"\n                        },\n                        \"template\": \"HF_GPTNeo_125m_lora\",\n                        \"tokenizer_config\": {},\n                        \"trainer_config\": {\n                            \"auto_find_batch_size\": true,\n                            \"num_train_epochs\": 20,\n                            \"output_dir\": \"checkpoint\"\n                        }\n                    }\n                },\n                \"import_info\": {}\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"text-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"imagecl1\",\n            \"name\": \"imagecl1\",\n            \"created_at\": \"2023-11-23T07:58:15.786698Z\",\n            \"modified_at\": \"2023-11-23T07:58:15.786698Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"embedding-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"imagecl\",\n            \"name\": \"imagecl\",\n            \"created_at\": \"2023-11-23T07:57:56.370980Z\",\n            \"modified_at\": \"2023-11-23T07:57:56.370980Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"embedding-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"deep_cls_bg1\",\n            \"name\": \"deep_cls_bg1\",\n            \"created_at\": \"2023-11-23T07:27:32.857722Z\",\n            \"modified_at\": \"2023-11-23T07:27:32.857722Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"model_version\": {\n                \"id\": \"00668896e0a64cf5b37302c000e96f23\",\n                \"created_at\": \"2023-11-23T08:06:12.577745Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 16,\n                \"metrics\": {\n                    \"status\": {\n                        \"code\": 21300,\n                        \"description\": \"Model was successfully evaluated.\"\n                    },\n                    \"summary\": {\n                        \"macro_avg_roc_auc\": 0.52151275,\n                        \"macro_std_roc_auc\": 0.34620512,\n                        \"macro_avg_f1_score\": 0.40380955,\n                        \"macro_std_f1_score\": 0.17236254,\n                        \"macro_avg_precision\": 0.09772728,\n                        \"macro_avg_recall\": 0.52380955\n                    }\n                },\n                \"completed_at\": \"2023-11-23T08:21:08.303040Z\",\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"test-app-1700638575-empty\",\n                \"user_id\": \"a0btrubbaefn\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"concepts\": \"probs\"\n                    },\n                    \"params\": {\n                        \"max_concepts\": 20,\n                        \"min_value\": 0,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"input\"\n                    }\n                },\n                \"train_info\": {\n                    \"params\": {\n                        \"batch_size\": 64,\n                        \"concepts_mutually_exclusive\": false,\n                        \"dataset_id\": \"\",\n                        \"dataset_version_id\": \"\",\n                        \"flip_direction\": \"horizontal\",\n                        \"flip_probability\": 0.5,\n                        \"image_size\": 224,\n                        \"invalid_data_tolerance_percent\": 5,\n                        \"num_epochs\": 60,\n                        \"num_gpus\": 1,\n                        \"per_item_lrate\": 0.00001953125,\n                        \"per_item_min_lrate\": 1.5625e-08,\n                        \"pretrained_weights\": \"ImageNet-1k\",\n                        \"seed\": -1,\n                        \"template\": \"MMClassification_ResNet_50_RSB_A1\",\n                        \"warmup_iters\": 100,\n                        \"warmup_ratio\": 0.0001,\n                        \"weight_decay\": 0.01\n                    }\n                },\n                \"import_info\": {}\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"visual-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"deep_cls_bg\",\n            \"name\": \"deep_cls_bg\",\n            \"created_at\": \"2023-11-23T07:12:59.578284Z\",\n            \"modified_at\": \"2023-11-23T07:12:59.578284Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"visual-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"profanity-filter-new\",\n            \"name\": \"profanity-filter-new\",\n            \"created_at\": \"2023-11-23T07:08:10.016262Z\",\n            \"modified_at\": \"2023-11-23T07:08:10.016262Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"model_version\": {\n                \"id\": \"b56d87c6dc80484291586d10f2feaacd\",\n                \"created_at\": \"2023-11-23T07:08:17.041706Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"test-app-1700638575-empty\",\n                \"user_id\": \"a0btrubbaefn\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"params\": {\n                        \"case_sensitive\": false,\n                        \"keywords\": [\n                            \"\"\n                        ]\n                    }\n                },\n                \"input_info\": {},\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"keyword-filter-operator\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"tiny-random-gpt2\",\n            \"name\": \"tiny-random-gpt2\",\n            \"created_at\": \"2023-11-17T21:31:10.384113Z\",\n            \"modified_at\": \"2023-11-17T21:31:10.384113Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"214531acb40a4af8b56ca79103390466\",\n                \"created_at\": \"2023-11-17T21:31:10.384113Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {\n                    \"params\": {\n                        \"model_config\": {\n                            \"_name_or_path\": \"hf-internal-testing/tiny-random-gpt2\",\n                            \"activation_function\": \"gelu_new\",\n                            \"architectures\": [\n                                \"GPT2LMHeadModel\"\n                            ],\n                            \"attention_probs_dropout_prob\": 0.1,\n                            \"attn_pdrop\": 0.1,\n                            \"bos_token_id\": 98,\n                            \"embd_pdrop\": 0.1,\n                            \"eos_token_id\": 98,\n                            \"gradient_checkpointing\": false,\n                            \"hidden_act\": \"gelu\",\n                            \"hidden_dropout_prob\": 0.1,\n                            \"initializer_range\": 0.02,\n                            \"intermediate_size\": 37,\n                            \"layer_norm_epsilon\": 0.00001,\n                            \"model_type\": \"gpt2\",\n                            \"n_ctx\": 512,\n                            \"n_embd\": 32,\n                            \"n_head\": 4,\n                            \"n_inner\": null,\n                            \"n_layer\": 5,\n                            \"n_positions\": 512,\n                            \"pad_token_id\": 98,\n                            \"reorder_and_upcast_attn\": false,\n                            \"resid_pdrop\": 0.1,\n                            \"scale_attn_by_inverse_layer_idx\": false,\n                            \"scale_attn_weights\": true,\n                            \"summary_activation\": null,\n                            \"summary_first_dropout\": 0.1,\n                            \"summary_proj_to_labels\": true,\n                            \"summary_type\": \"cls_index\",\n                            \"summary_use_proj\": true,\n                            \"torch_dtype\": \"float32\",\n                            \"transformers_version\": \"4.32.1\",\n                            \"type_vocab_size\": 16,\n                            \"use_cache\": true,\n                            \"vocab_size\": 1000\n                        },\n                        \"tokenizer_config\": {\n                            \"add_prefix_space\": false,\n                            \"bos_token\": \"<|endoftext|>\",\n                            \"clean_up_tokenization_spaces\": true,\n                            \"eos_token\": \"<|endoftext|>\",\n                            \"model_max_length\": 512,\n                            \"tokenizer_class\": \"GPT2Tokenizer\",\n                            \"unk_token\": \"<|endoftext|>\"\n                        }\n                    }\n                },\n                \"import_info\": {\n                    \"params\": {\n                        \"model_name\": \"hf-internal-testing/tiny-random-gpt2\",\n                        \"pipeline_name\": \"text-generation\",\n                        \"toolkit\": \"HuggingFace\"\n                    }\n                }\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-to-text\",\n            \"task\": \"text-generation\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"BAAI-bge-base-en-v15\",\n            \"name\": \"BAAI-bge-base-en-v15\",\n            \"created_at\": \"2023-11-01T12:53:30.339664Z\",\n            \"modified_at\": \"2023-11-01T12:53:30.339664Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"23cf79f86459491cb31fd7f0273c9fff\",\n                \"created_at\": \"2023-11-01T12:53:30.339664Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"embeddings\": \"embedding\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    },\n                    \"params\": {\n                        \"text_token_warning_limit\": 512\n                    }\n                },\n                \"train_info\": {\n                    \"params\": {\n                        \"model_config\": {\n                            \"_name_or_path\": \"BAAI/bge-base-en-v1.5\",\n                            \"architectures\": [\n                                \"BertModel\"\n                            ],\n                            \"attention_probs_dropout_prob\": 0.1,\n                            \"classifier_dropout\": null,\n                            \"gradient_checkpointing\": false,\n                            \"hidden_act\": \"gelu\",\n                            \"hidden_dropout_prob\": 0.1,\n                            \"hidden_size\": 768,\n                            \"id2label\": {\n                                \"0\": \"LABEL_0\"\n                            },\n                            \"initializer_range\": 0.02,\n                            \"intermediate_size\": 3072,\n                            \"label2id\": {\n                                \"LABEL_0\": 0\n                            },\n                            \"layer_norm_eps\": 1e-12,\n                            \"max_position_embeddings\": 512,\n                            \"model_type\": \"bert\",\n                            \"num_attention_heads\": 12,\n                            \"num_hidden_layers\": 12,\n                            \"pad_token_id\": 0,\n                            \"position_embedding_type\": \"absolute\",\n                            \"torch_dtype\": \"float32\",\n                            \"transformers_version\": \"4.32.1\",\n                            \"type_vocab_size\": 2,\n                            \"use_cache\": true,\n                            \"vocab_size\": 30522\n                        },\n                        \"tokenizer_config\": {\n                            \"clean_up_tokenization_spaces\": true,\n                            \"cls_token\": \"[CLS]\",\n                            \"do_basic_tokenize\": true,\n                            \"do_lower_case\": true,\n                            \"mask_token\": \"[MASK]\",\n                            \"model_max_length\": 512,\n                            \"never_split\": null,\n                            \"pad_token\": \"[PAD]\",\n                            \"sep_token\": \"[SEP]\",\n                            \"strip_accents\": null,\n                            \"tokenize_chinese_chars\": true,\n                            \"tokenizer_class\": \"BertTokenizer\",\n                            \"unk_token\": \"[UNK]\"\n                        }\n                    }\n                },\n                \"import_info\": {\n                    \"params\": {\n                        \"model_name\": \"BAAI/bge-base-en-v1.5\",\n                        \"pipeline_name\": \"feature-extraction\",\n                        \"toolkit\": \"HuggingFace\"\n                    }\n                }\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-embedder\",\n            \"task\": \"representation-learning\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"BAAI-bge-base-en-cluster\",\n            \"name\": \"BAAI-bge-base-en-cluster\",\n            \"created_at\": \"2023-08-15T14:21:18.083130Z\",\n            \"modified_at\": \"2023-08-15T14:21:18.083130Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"4a47a75c931c4b0784cebc2cd45bc5a2\",\n                \"created_at\": \"2023-09-18T11:00:41.832176Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"metrics\": {\n                    \"status\": {\n                        \"code\": 21300,\n                        \"description\": \"Model was successfully evaluated.\"\n                    }\n                },\n                \"total_input_count\": 293849,\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\"\n                },\n                \"input_info\": {},\n                \"train_info\": {\n                    \"params\": {\n                        \"beta\": 1,\n                        \"coarse_clusters\": 128,\n                        \"dataset_id\": \"quora-dataset-corpus-2\",\n                        \"dataset_version_id\": \"dataset-version-1692900595413\",\n                        \"eval_holdout_fraction\": 0.2,\n                        \"max_num_query_embeddings\": 100,\n                        \"max_visited\": 32,\n                        \"num_results_per_query\": [\n                            1,\n                            5,\n                            10,\n                            20\n                        ],\n                        \"query_holdout_fraction\": 0.1,\n                        \"quota\": 1000,\n                        \"to_be_indexed_queries_fraction\": 0.25,\n                        \"train_iters\": 1,\n                        \"training_timeout\": 72000\n                    },\n                    \"dataset\": {\n                        \"id\": \"quora-dataset-corpus-2\",\n                        \"created_at\": \"2023-08-24T07:40:45.232142Z\",\n                        \"modified_at\": \"2023-08-24T18:09:55.799396Z\",\n                        \"app_id\": \"quora-dataset\",\n                        \"user_id\": \"isaac\",\n                        \"metadata\": {},\n                        \"visibility\": {\n                            \"gettable\": 10\n                        },\n                        \"version\": {\n                            \"id\": \"dataset-version-1692900595413\",\n                            \"created_at\": \"0001-01-01T00:00:00Z\",\n                            \"modified_at\": \"0001-01-01T00:00:00Z\",\n                            \"app_id\": \"quora-dataset\",\n                            \"user_id\": \"isaac\",\n                            \"dataset_id\": \"quora-dataset-corpus-2\",\n                            \"status\": {\n                                \"code\": 99009,\n                                \"description\": \"Internal error\"\n                            },\n                            \"metadata\": {}\n                        }\n                    }\n                },\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"clusterer\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"BAAI-bge-base-en\",\n            \"name\": \"BAAI-bge-base-en\",\n            \"created_at\": \"2023-08-15T11:36:23.145658Z\",\n            \"modified_at\": \"2023-08-15T11:36:23.145658Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"b55d165cc3c64ed4bab3090c7b402188\",\n                \"created_at\": \"2023-08-15T11:36:23.145658Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"embeddings\": \"embedding\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {\n                    \"params\": {\n                        \"model_config\": {\n                            \"_name_or_path\": \"BAAI/bge-base-en\",\n                            \"architectures\": [\n                                \"BertModel\"\n                            ],\n                            \"attention_probs_dropout_prob\": 0.1,\n                            \"classifier_dropout\": null,\n                            \"gradient_checkpointing\": false,\n                            \"hidden_act\": \"gelu\",\n                            \"hidden_dropout_prob\": 0.1,\n                            \"hidden_size\": 768,\n                            \"id2label\": {\n                                \"0\": \"LABEL_0\"\n                            },\n                            \"initializer_range\": 0.02,\n                            \"intermediate_size\": 3072,\n                            \"label2id\": {\n                                \"LABEL_0\": 0\n                            },\n                            \"layer_norm_eps\": 1e-12,\n                            \"max_position_embeddings\": 512,\n                            \"model_type\": \"bert\",\n                            \"num_attention_heads\": 12,\n                            \"num_hidden_layers\": 12,\n                            \"pad_token_id\": 0,\n                            \"position_embedding_type\": \"absolute\",\n                            \"torch_dtype\": \"float32\",\n                            \"transformers_version\": \"4.30.2\",\n                            \"type_vocab_size\": 2,\n                            \"use_cache\": true,\n                            \"vocab_size\": 30522\n                        },\n                        \"tokenizer_config\": {\n                            \"clean_up_tokenization_spaces\": true,\n                            \"cls_token\": \"[CLS]\",\n                            \"do_basic_tokenize\": true,\n                            \"do_lower_case\": true,\n                            \"mask_token\": \"[MASK]\",\n                            \"model_max_length\": 512,\n                            \"never_split\": null,\n                            \"pad_token\": \"[PAD]\",\n                            \"sep_token\": \"[SEP]\",\n                            \"strip_accents\": null,\n                            \"tokenize_chinese_chars\": true,\n                            \"tokenizer_class\": \"BertTokenizer\",\n                            \"unk_token\": \"[UNK]\"\n                        }\n                    }\n                },\n                \"import_info\": {\n                    \"params\": {\n                        \"model_name\": \"BAAI/bge-base-en\",\n                        \"pipeline_name\": \"feature-extraction\",\n                        \"tokenizer_name\": \"BAAI/bge-base-en\",\n                        \"toolkit\": \"HuggingFace\"\n                    }\n                }\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-embedder\",\n            \"task\": \"representation-learning\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"apparel-recognition\",\n            \"name\": \"apparel\",\n            \"created_at\": \"2016-12-15T01:29:04.622209Z\",\n            \"modified_at\": \"2023-05-23T12:34:15.093542Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"dc2cd6d9bff5425a80bfe0c4105583c1\",\n                \"created_at\": \"2016-12-15T01:29:04.622209Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 112,\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"concepts\": \"softmax\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"display_name\": \"apparel-visual-classifier\",\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-classifier\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"description\": \"AI model for identifying fashion-related and clothing concepts, hats, jewelry, handbags, etc. in images and video.\",\n            \"metadata\": {\n                \"presetInputs\": [\n                    {\n                        \"alt\": \"Clarifai apparel model featuring woman black turtleneck.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/apparel-woman-black-turtleneck.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai apparel model featuring yellow boots.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/apparel-yellow-boots.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai apparel model featuring black white striped socks.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/apparel-black-white-striped-socks.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai apparel model featuring sunglasses.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/apparel-sunglasses.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai apparel model featuring dog in a dog carrier.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/apparel-dog-in-a-dog-carrier.jpg\"\n                    }\n                ]\n            },\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"15b0041cc2cd848a0d8b45f8b83c1d7d\",\n            \"name\": \"CLIP\",\n            \"created_at\": \"2021-12-14T18:07:40.983254Z\",\n            \"modified_at\": \"2023-04-27T20:45:27.183474Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"97f20cc96b7c4bec8f3b96e284ba1173\",\n                \"created_at\": \"2021-12-14T18:07:41.268867Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"embeddings\": \"embeddings\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-embedder\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"notes\": \"This model has been deprecated. Please use `multilingual-multimodal-clip-embed` instead.\",\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"text-translation-english-spanish\",\n            \"name\": \"Helsinki-NLP/opus-mt-en-es\",\n            \"created_at\": \"2023-02-22T22:44:16.825059Z\",\n            \"modified_at\": \"2023-02-22T22:44:16.825059Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"643f30558de34013aff72b0e21f244f5\",\n                \"created_at\": \"2023-02-23T00:39:20.611092Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {\n                    \"params\": {\n                        \"model_config\": {\n                            \"_name_or_path\": \"Helsinki-NLP/opus-mt-en-es\",\n                            \"activation_dropout\": 0,\n                            \"activation_function\": \"swish\",\n                            \"add_bias_logits\": false,\n                            \"add_final_layer_norm\": false,\n                            \"architectures\": [\n                                \"MarianMTModel\"\n                            ],\n                            \"attention_dropout\": 0,\n                            \"bad_words_ids\": [\n                                [\n                                    65000\n                                ]\n                            ],\n                            \"bos_token_id\": 0,\n                            \"classif_dropout\": 0,\n                            \"classifier_dropout\": 0,\n                            \"d_model\": 512,\n                            \"decoder_attention_heads\": 8,\n                            \"decoder_ffn_dim\": 2048,\n                            \"decoder_layerdrop\": 0,\n                            \"decoder_layers\": 6,\n                            \"decoder_start_token_id\": 65000,\n                            \"dropout\": 0.1,\n                            \"encoder_attention_heads\": 8,\n                            \"encoder_ffn_dim\": 2048,\n                            \"encoder_layerdrop\": 0,\n                            \"encoder_layers\": 6,\n                            \"eos_token_id\": 0,\n                            \"extra_pos_embeddings\": 65001,\n                            \"force_bos_token_to_be_generated\": false,\n                            \"forced_eos_token_id\": 0,\n                            \"gradient_checkpointing\": false,\n                            \"id2label\": {\n                                \"0\": \"LABEL_0\",\n                                \"1\": \"LABEL_1\",\n                                \"2\": \"LABEL_2\"\n                            },\n                            \"init_std\": 0.02,\n                            \"is_encoder_decoder\": true,\n                            \"label2id\": {\n                                \"LABEL_0\": 0,\n                                \"LABEL_1\": 1,\n                                \"LABEL_2\": 2\n                            },\n                            \"max_length\": 512,\n                            \"max_position_embeddings\": 512,\n                            \"model_type\": \"marian\",\n                            \"normalize_before\": false,\n                            \"normalize_embedding\": false,\n                            \"num_beams\": 4,\n                            \"num_hidden_layers\": 6,\n                            \"pad_token_id\": 65000,\n                            \"scale_embedding\": true,\n                            \"static_position_embeddings\": true,\n                            \"torch_dtype\": \"float32\",\n                            \"transformers_version\": \"4.16.0\",\n                            \"use_cache\": true,\n                            \"vocab_size\": 65001\n                        },\n                        \"tokenizer_config\": {\n                            \"eos_token\": \"</s>\",\n                            \"model_max_length\": 512,\n                            \"name_or_path\": \"Helsinki-NLP/opus-mt-en-es\",\n                            \"pad_token\": \"<pad>\",\n                            \"source_lang\": \"eng\",\n                            \"sp_model_kwargs\": {},\n                            \"special_tokens_map_file\": null,\n                            \"target_lang\": \"spa\",\n                            \"tokenizer_class\": \"MarianTokenizer\",\n                            \"tokenizer_file\": null,\n                            \"unk_token\": \"<unk>\"\n                        }\n                    }\n                },\n                \"import_info\": {\n                    \"params\": {\n                        \"model_name\": \"Helsinki-NLP/opus-mt-en-es\",\n                        \"pipeline_name\": \"translation_xx_to_yy\",\n                        \"tokenizer_name\": \"Helsinki-NLP/opus-mt-en-es\",\n                        \"toolkit\": \"HuggingFace\"\n                    }\n                }\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-to-text\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"description\": \"Text translation model from English to Spanish using sentence piece-based segmentation\",\n            \"metadata\": {},\n            \"notes\": \"\\n # Helsinki-NLP - English to Spanish                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n ## Introduction                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n The Helsinki-NLP models are used to translate text from one language to another. As such, the model takes a block text as its input, and outputs the translated block of text. This particular model takes in English text as it's input and outputs Spanish text.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n ## Limitations                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n The usage of random capitalization and punctuation may result in erroneous translations grammatically speaking. If you are using this model in a workflow and find grammar issues, you can try utilizing aggregators to minimize errors.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n **More Info**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n * Original Repository: [GitHub](https://github.com/Helsinki-NLP/Tatoeba-Challenge)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \\n * Helsinki-NLP Opus: [eng-spa](https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/master/models/eng-spa)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \\n * Hugging Face docs: [Helsinki-NLP/opus-mt-en-es](https://huggingface.co/Helsinki-NLP/opus-mt-en-es)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n ## Paper                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n [Natural language processing for similar languages, varieties, and dialects: A survey](https://helda.helsinki.fi/bitstream/handle/10138/330117/natural_language_processing_for_similar_languages_varieties_and_dialects_a_survey.pdf)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n Authors: Marcos Zampieri, Preslav Nakov, Yves Scherrer                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n **Abstract**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n There has been a lot of recent interest in the natural language processing (NLP) community in the computational processing of language varieties and dialects, with the aim to improve the performance of applications such as machine translation, speech recognition, and dialogue systems. Here, we attempt to survey this growing field of research, with focus on computational methods for processing similar languages, varieties, and dialects. In particular, we discuss the most important challenges when dealing with diatopic language variation, and we present some of the available datasets, the process of data collection, and the most common data collection strategies used to compile datasets for similar languages, varieties, and dialects. We further present a number of studies on computational methods developed and/or adapted for preprocessing, normalization, part-of-speech tagging, and parsing similar languages, language varieties, and dialects. Finally, we discuss relevant applications such as language and dialect identification and machine translation for closely related languages, language varieties, and dialects.                                     \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n ## Risks, Limitations, and Biases                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n **CONTENT WARNING: Readers should be aware this section contains content that is disturbing, offensive, and can propagate historical and current stereotypes.**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n There has been significant research exploring bias and fairness issues with language models. Some important papers in this field include:                                                   # Helsinki-NLP - English to Spanish                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n ## Introduction                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n The Helsinki-NLP models are used to translate text from one language to another. As such, the model takes a block text as its input, and outputs the translated block of text. This particular model takes in English text as it's input and outputs Spanish text.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n ## Limitations                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n The usage of random capitalization and punctuation may result in erroneous translations grammatically speaking. If you are using this model in a workflow and find grammar issues, you can try utilizing aggregators to minimize errors.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n **More Info**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n * Original Repository: [GitHub](https://github.com/Helsinki-NLP/Tatoeba-Challenge)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \\n * Helsinki-NLP Opus: [eng-spa](https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/master/models/eng-spa)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \\n * Hugging Face docs: [Helsinki-NLP/opus-mt-en-es](https://huggingface.co/Helsinki-NLP/opus-mt-en-es)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n ## Paper                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n [Natural language processing for similar languages, varieties, and dialects: A survey](https://helda.helsinki.fi/bitstream/handle/10138/330117/natural_language_processing_for_similar_languages_varieties_and_dialects_a_survey.pdf)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n Authors: Marcos Zampieri, Preslav Nakov, Yves Scherrer                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n **Abstract**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n There has been a lot of recent interest in the natural language processing (NLP) community in the computational processing of language varieties and dialects, with the aim to improve the performance of applications such as machine translation, speech recognition, and dialogue systems. Here, we attempt to survey this growing field of research, with focus on computational methods for processing similar languages, varieties, and dialects. In particular, we discuss the most important challenges when dealing with diatopic language variation, and we present some of the available datasets, the process of data collection, and the most common data collection strategies used to compile datasets for similar languages, varieties, and dialects. We further present a number of studies on computational methods developed and/or adapted for preprocessing, normalization, part-of-speech tagging, and parsing similar languages, language varieties, and dialects. Finally, we discuss relevant applications such as language and dialect identification and machine translation for closely related languages, language varieties, and dialects.                                     \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n ## Risks, Limitations, and Biases                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n **CONTENT WARNING: Readers should be aware this section contains content that is disturbing, offensive, and can propagate historical and current stereotypes.**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n There has been significant research exploring bias and fairness issues with language models. Some important papers in this field include:                                                  \\n * [Societal Biases in Language Generation: Progress and Challenges](https://aclanthology.org/2021.acl-long.330.pdf)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \\n     * Authors: Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, Nanyun Peng                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \\n     * Abstract: Technology for language generation has advanced rapidly, spurred by advancements in pre-training large models on massive amounts of data and the need for intelligent agents to communicate in a natural manner. While techniques can effectively generate fluent text, they can also produce undesirable societal biases that can have a disproportionately negative impact on marginalized populations. Language generation presents unique challenges for biases in terms of direct user interaction and the structure of decoding techniques. To better understand these challenges, we present a survey on societal biases in language generation, focusing on how data and techniques contribute to biases and progress towards reducing biases. Motivated by a lack of studies on biases from decoding techniques, we also conduct experiments to quantify the effects of these techniques. By further discussing general trends and open challenges, we call to attention promising directions for research and the importance of fairness and inclusivity considerations for language generation applications.<br /><br />                                                          \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n * [On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \\n     * Authors: Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, Shmargaret Shmitchell                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \\n     * Abstract: The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n ## Benchmarks                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n The following benchmarks are for the **opus-2021-02-19** weights.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n | testset                        | BLEU | chr-F | #sent | #words | BP    |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\n | ------------------------------ | ---- | ----- | ----- | ------ | ----- |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\n | newssyscomb2009-engspa.eng.spa | 31.3 | 0.583 | 502   | 12506  | 0.990 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\n | news-test2008-engspa.eng.spa   | 29.6 | 0.564 | 2051  | 52596  | 1.000 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\n | newstest2009-engspa.eng.spa    | 30.2 | 0.578 | 2525  | 68114  | 1.000 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\n | newstest2010-engspa.eng.spa    | 36.9 | 0.620 | 2489  | 65522  | 1.000 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\n | newstest2011-engspa.eng.spa    | 38.3 | 0.620 | 3003  | 79476  | 0.984 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\n | newstest2012-engspa.eng.spa    | 39.1 | 0.626 | 3003  | 79006  | 0.969 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\n | newstest2013-engspa.eng.spa    | 35.1 | 0.598 | 3000  | 70528  | 0.960 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\n | Tatoeba-test.eng.spa           | 55.1 | 0.721 | 10000 | 77311  | 0.978 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\n | tico19-test.eng-spa            | 50.4 | 0.727 | 2100  | 66591  | 0.959 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n ## Additional Info                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n * Data set: Opus                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \\n * Model: Transformer                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \\n * Source Language(s): en (English)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \\n * Target Language(s): es (Spanish)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \\n * Pre-processing: Normalization  [SentencePiece](https://github.com/google/sentencepiece) (spm32k, spm32k)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \\n * Download original weights: [opus-2021-02-19.zip](https://object.pouta.csc.fi/Tatoeba-MT-models/eng-spa/opus-2021-02-19.zip)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \\n * Test set translations: [opus-2021-02-19.test.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/eng-spa/opus-2021-02-19.test.txt)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \\n * Test set scores: [opus-2021-02-19.eval.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/eng-spa/opus-2021-02-19.eval.txt)\\n\",\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"ocr_model_v3-1677100451\",\n            \"name\": \"ocr_model_v3-1677100451\",\n            \"created_at\": \"2023-02-22T21:14:10.921823Z\",\n            \"modified_at\": \"2023-02-22T21:14:10.921823Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"22894138385843978aaa97cae37780fb\",\n                \"created_at\": \"2023-02-22T21:14:10.928773Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\",\n                        \"regions[...].value\": \"predicted_det_scores\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {\n                \"presetInputs\": [\n                    {\n                        \"alt\": \"Stop Sign.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/9/9d/Stop_sign_light_red.svg/1200px-Stop_sign_light_red.svg.png\"\n                    }\n                ]\n            },\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"ocr-scene-english-paddleocr\",\n            \"name\": \"OCR Scene English PaddleOCR\",\n            \"created_at\": \"2023-02-22T15:48:10.066388Z\",\n            \"modified_at\": \"2023-02-22T15:48:10.066388Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"40dbb2c9cde44a27af226782e7157006\",\n                \"created_at\": \"2023-02-22T15:49:55.126424Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"description\": \"An OCR model for detecting and recognizing English text in images that are more complex than scans of a page.\",\n            \"metadata\": {\n                \"presetInputs\": [\n                    {\n                        \"type\": \"image\",\n                        \"url\": \"https://samples.clarifai.com/featured-models/ocr-woman-holding-sold-sign.jpg\"\n                    },\n                    {\n                        \"type\": \"image\",\n                        \"url\": \"https://samples.clarifai.com/featured-models/paddleocrs/ocr-scene-english-paddleocr-1.jpg\"\n                    },\n                    {\n                        \"type\": \"image\",\n                        \"url\": \"https://samples.clarifai.com/featured-models/paddleocrs/ocr-scene-english-paddleocr-2.jpg\"\n                    },\n                    {\n                        \"type\": \"image\",\n                        \"url\": \"https://samples.clarifai.com/featured-models/paddleocrs/ocr-scene-english-paddleocr-3.png\"\n                    }\n                ]\n            },\n            \"notes\": \"\\n # Introduction                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n PaddleOCR aims to create multilingual, awesome, leading, and practical OCR tools that help users train better models and apply them into practice. The information in this summary is taken from their [Github.](https://github.com/PaddlePaddle/PaddleOCR)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n Release PP-OCRv3: With comparable speed, the effect of Chinese scene is further improved by 5% compared with PP-OCRv2, the effect of English scene is improved by 11%, and the average recognition accuracy of 80 language multilingual models is improved by more than 5%.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n <iframe width=\\\"560\\\" height=\\\"315\\\" src=\\\"https://www.youtube.com/embed/ITTtqGKtS54\\\" title=\\\"YouTube video player\\\" frameborder=\\\"0\\\" allow=\\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\\\" allowfullscreen></iframe>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n # Features                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n PaddleOCR support a variety of cutting-edge algorithms related to OCR, and developed industrial featured models/solution [PP-OCR](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.5/doc/doc_en/ppocr_introduction_en.md) and [PP-Structure](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.5/ppstructure/README.md) on this basis, and get through the whole process of data production, model training, compression, inference and deployment.                                                                                                                                                                                                                                                                                                  \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n ## PP-OCR Series Model List - This model is the English ultra-lightweight PP-OCRv3 model (13.4M) on the second row.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n | Model introduction                                           | Model name                   | Recommended scene | Detection model                                              | Direction classifier                                         | Recognition model                                            |                                                                                                                                                                                                                                                                                                                                                                                                                                                    \\n | ------------------------------------------------------------ | ---------------------------- | ----------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |                                                                                                                                                                                                                                                                                                                                                                                                                                                    \\n | Chinese and English ultra-lightweight PP-OCRv3 model（16.2M）     | ch_PP-OCRv3_xx          | Mobile & Server | [inference model](https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_det_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_det_distill_train.tar) | [inference model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_train.tar) | [inference model](https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_rec_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_rec_train.tar) |                                 \\n | English ultra-lightweight PP-OCRv3 model（13.4M）     | en_PP-OCRv3_xx          | Mobile & Server | [inference model](https://paddleocr.bj.bcebos.com/PP-OCRv3/english/en_PP-OCRv3_det_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/PP-OCRv3/english/en_PP-OCRv3_det_distill_train.tar) | [inference model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_train.tar) | [inference model](https://paddleocr.bj.bcebos.com/PP-OCRv3/english/en_PP-OCRv3_rec_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/PP-OCRv3/english/en_PP-OCRv3_rec_train.tar) |                                             \\n | Chinese and English ultra-lightweight PP-OCRv2 model（11.6M） |  ch_PP-OCRv2_xx |Mobile & Server|[inference model](https://paddleocr.bj.bcebos.com/PP-OCRv2/chinese/ch_PP-OCRv2_det_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/PP-OCRv2/chinese/ch_PP-OCRv2_det_distill_train.tar)| [inference model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_train.tar) |[inference model](https://paddleocr.bj.bcebos.com/PP-OCRv2/chinese/ch_PP-OCRv2_rec_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/PP-OCRv2/chinese/ch_PP-OCRv2_rec_train.tar)|                                                   \\n | Chinese and English ultra-lightweight PP-OCR model (9.4M)       | ch_ppocr_mobile_v2.0_xx      | Mobile & server   |[inference model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_det_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_det_train.tar)|[inference model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_train.tar) |[inference model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_rec_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_rec_train.tar)      |   \\n | Chinese and English general PP-OCR model (143.4M)               | ch_ppocr_server_v2.0_xx      | Server            |[inference model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_server_v2.0_det_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_server_v2.0_det_train.tar)    |[inference model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_train.tar)    |[inference model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_server_v2.0_rec_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_server_v2.0_rec_train.tar)  |\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n - For more model downloads (including multiple languages), please refer to [PP-OCR series model downloads](./doc/doc_en/models_list_en.md).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \\n - For a new language request, please refer to [Guideline for new language_requests](#language_requests).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \\n - For structural document analysis models, please refer to [PP-Structure models](./ppstructure/docs/models_list_en.md).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n # PP-OCRv3 English model                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \\n ![](https://github.com/PaddlePaddle/PaddleOCR/raw/release/2.5/doc/imgs_results/PP-OCRv3/en/en_1.png)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n # PP-OCRv3 Chinese model                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \\n ![](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.5/doc/imgs_results/PP-OCRv3/ch/PP-OCRv3-pic003.jpg?raw=true)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n # PP-OCRv3 Multilingual model                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \\n ![](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.5/doc/imgs_results/PP-OCRv3/multi_lang/korean_1.jpg?raw=true)\",\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"ocr-scene-chinese-english-paddleocr\",\n            \"name\": \"ocr-scene-chinese-english-paddleocr\",\n            \"created_at\": \"2022-08-10T21:27:40.359110Z\",\n            \"modified_at\": \"2023-02-09T23:37:22.184921Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"67104613bc7245b594d6a38eb7e34974\",\n                \"created_at\": \"2022-08-10T21:27:40.889145Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {\n                \"presetInputs\": [\n                    {\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/general-shirts-bags-shoes-computer.jpg\"\n                    }\n                ]\n            },\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"multilingual-multimodal-clip-embed\",\n            \"name\": \"Multilingual Multimodal Clip Embedder\",\n            \"created_at\": \"2023-01-30T17:46:05.745974Z\",\n            \"modified_at\": \"2023-01-30T17:46:05.745974Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"e3289fa66be4419eb2958ba74b6e9fee\",\n                \"created_at\": \"2023-01-30T17:46:05.745974Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"train_stats\": {},\n                \"completed_at\": \"2023-01-30T17:46:05.745974Z\",\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"embeddings\": \"embeddings\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\",\n                        \"text\": \"text\"\n                    },\n                    \"params\": {\n                        \"text_token_warning_limit\": 77\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"multimodal-embedder\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"description\": \"CLIP-based multilingual multimodal embedding model.\",\n            \"metadata\": {},\n            \"notes\": \"##Multilingual CLIP\",\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"ccfc0043fa804de4a586005f72582e00\",\n            \"name\": \"Multimodal Clip Clusterer\",\n            \"created_at\": \"2022-11-16T14:51:43.695740Z\",\n            \"modified_at\": \"2022-11-16T14:51:43.695740Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"4b134b9fb5f24e2bb09b7493560cc922\",\n                \"created_at\": \"2022-11-16T14:51:43.695740Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"metrics\": {\n                    \"status\": {\n                        \"code\": 21300,\n                        \"description\": \"Model was successfully evaluated.\"\n                    },\n                    \"summary\": {\n                        \"macro_avg_roc_auc\": 0,\n                        \"macro_std_roc_auc\": 0,\n                        \"macro_avg_f1_score\": 0,\n                        \"macro_std_f1_score\": 0,\n                        \"macro_avg_precision\": 0,\n                        \"macro_avg_recall\": 0,\n                        \"lopq_metrics\": [\n                            {\n                                \"k\": 10,\n                                \"recall_vs_brute_force\": 0.95100015,\n                                \"kendall_tau_vs_brute_force\": 1,\n                                \"most_frequent_code_percent\": 0.41158637,\n                                \"lopq_ndcg\": 0,\n                                \"brute_force_ndcg\": 0\n                            },\n                            {\n                                \"k\": 20,\n                                \"recall_vs_brute_force\": 0.93349975,\n                                \"kendall_tau_vs_brute_force\": 0.99947363,\n                                \"most_frequent_code_percent\": 0.41158637,\n                                \"lopq_ndcg\": 0,\n                                \"brute_force_ndcg\": 0\n                            },\n                            {\n                                \"k\": 50,\n                                \"recall_vs_brute_force\": 0.9118,\n                                \"kendall_tau_vs_brute_force\": 0.99697524,\n                                \"most_frequent_code_percent\": 0.41158637,\n                                \"lopq_ndcg\": 0,\n                                \"brute_force_ndcg\": 0\n                            },\n                            {\n                                \"k\": 100,\n                                \"recall_vs_brute_force\": 0.8869,\n                                \"kendall_tau_vs_brute_force\": 0.9947445,\n                                \"most_frequent_code_percent\": 0.41158637,\n                                \"lopq_ndcg\": 0,\n                                \"brute_force_ndcg\": 0\n                            },\n                            {\n                                \"k\": 200,\n                                \"recall_vs_brute_force\": 0.8432002,\n                                \"kendall_tau_vs_brute_force\": 0.9947241,\n                                \"most_frequent_code_percent\": 0.41158637,\n                                \"lopq_ndcg\": 0,\n                                \"brute_force_ndcg\": 0\n                            }\n                        ]\n                    }\n                },\n                \"total_input_count\": 9527727,\n                \"train_stats\": {},\n                \"completed_at\": \"2022-11-16T14:51:43.695740Z\",\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\"\n                },\n                \"input_info\": {\n                    \"base_embed_model\": {\n                        \"id\": \"multimodal-clip-embed\",\n                        \"app_id\": \"main\",\n                        \"model_version\": {\n                            \"id\": \"9fe2c8962c104327bc87b8f8104b161a\"\n                        },\n                        \"user_id\": \"clarifai\",\n                        \"model_type_id\": \"multimodal-embedder\",\n                        \"toolkits\": [],\n                        \"use_cases\": [],\n                        \"languages\": [],\n                        \"languages_full\": [],\n                        \"check_consents\": []\n                    }\n                },\n                \"train_info\": {\n                    \"params\": {\n                        \"beta\": 1,\n                        \"coarse_clusters\": 125,\n                        \"dataset_id\": \"\",\n                        \"dataset_version_id\": \"ee243135d683462eaa1060c4f5c63725\",\n                        \"eval_holdout_fraction\": 0.2,\n                        \"max_num_query_embeddings\": 100,\n                        \"max_visited\": 1562,\n                        \"num_results_per_query\": [\n                            10,\n                            20,\n                            50,\n                            100,\n                            200\n                        ],\n                        \"query_holdout_fraction\": 0.1,\n                        \"quota\": 10000,\n                        \"to_be_indexed_queries_fraction\": 0.25,\n                        \"train_iters\": 1,\n                        \"training_timeout\": 86400\n                    }\n                },\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"clusterer\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {},\n            \"notes\": \"##CLIP\",\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"multimodal-clip-embed\",\n            \"name\": \"multimodal-clip\",\n            \"created_at\": \"2022-11-07T17:47:19.112250Z\",\n            \"modified_at\": \"2022-11-07T17:47:19.112250Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"9fe2c8962c104327bc87b8f8104b161a\",\n                \"created_at\": \"2022-11-07T17:47:19.123181Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"embeddings\": \"embeddings\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\",\n                        \"text\": \"text\"\n                    },\n                    \"params\": {\n                        \"text_token_warning_limit\": 77\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"multimodal-embedder\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"da94111b740547aeae38ba9668f998a3\",\n            \"name\": \"ocr-scene-devanagari-paddleocr\",\n            \"created_at\": \"2022-08-10T19:52:53.761109Z\",\n            \"modified_at\": \"2022-08-10T19:52:53.761109Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"5e35e10fb7814f5c9223ccb3c3afebec\",\n                \"created_at\": \"2022-08-10T19:52:53.956768Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"5f42b31a4589d672152e9668d02eb471\",\n            \"name\": \"ocr-scene-cyrillic-paddleocr\",\n            \"created_at\": \"2022-08-10T19:52:53.009976Z\",\n            \"modified_at\": \"2022-08-10T19:52:53.009976Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"bbb2a719af92447ebeaabc88d3f41123\",\n                \"created_at\": \"2022-08-10T19:52:53.252145Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"e1e3b6f78fd2d55c830c631434ba83a5\",\n            \"name\": \"ocr-scene-arabic-paddleocr\",\n            \"created_at\": \"2022-08-10T19:52:51.994687Z\",\n            \"modified_at\": \"2022-08-10T19:52:51.994687Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"4b33b79b4b2e42b4b9ee07c844f1bb56\",\n                \"created_at\": \"2022-08-10T19:52:52.548327Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"c54e41dd13c9669630426078d36718ec\",\n            \"name\": \"ocr-scene-latin-paddleocr\",\n            \"created_at\": \"2022-08-10T19:52:51.331917Z\",\n            \"modified_at\": \"2022-08-10T19:52:51.331917Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"a82c91d715f74c789df25c78e811dc6a\",\n                \"created_at\": \"2022-08-10T19:52:51.487313Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"e3fde27570dc04aff3b997a223601ac8\",\n            \"name\": \"ocr-scene-kannada-paddleocr\",\n            \"created_at\": \"2022-08-10T19:52:50.570402Z\",\n            \"modified_at\": \"2022-08-10T19:52:50.570402Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"324f4a4d0bc64d81ab7be56c21748328\",\n                \"created_at\": \"2022-08-10T19:52:50.803339Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"1d5cbe1f275acaa8ac109dec6e07120b\",\n            \"name\": \"ocr-scene-telugu-paddleocr\",\n            \"created_at\": \"2022-08-10T19:52:49.858398Z\",\n            \"modified_at\": \"2022-08-10T19:52:49.858398Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"004b78b3cc3b4e2b9fe53f3cda5ff001\",\n                \"created_at\": \"2022-08-10T19:52:50.014584Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"019c84b9cae1c41e7bf661a037f6ac12\",\n            \"name\": \"ocr-scene-tamil-paddleocr\",\n            \"created_at\": \"2022-08-10T19:52:49.207422Z\",\n            \"modified_at\": \"2022-08-10T19:52:49.207422Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"88bf32db4fdb488593303bc5fb309754\",\n                \"created_at\": \"2022-08-10T19:52:49.353280Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"6748476e7c5ae4df9bc2019dd6bd5892\",\n            \"name\": \"ocr-scene-japanese-paddleocr\",\n            \"created_at\": \"2022-08-10T19:52:48.506276Z\",\n            \"modified_at\": \"2022-08-10T19:52:48.506276Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"b3a22a7bc99943b998de94c1e3c5d420\",\n                \"created_at\": \"2022-08-10T19:52:48.703164Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"bf08cee902d18405ca6ac58b68b3e2a6\",\n            \"name\": \"ocr-scene-korean-paddleocr\",\n            \"created_at\": \"2022-08-10T19:52:47.804473Z\",\n            \"modified_at\": \"2022-08-10T19:52:47.804473Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"f242bc5057b4402dab82ca0dbaf1be1e\",\n                \"created_at\": \"2022-08-10T19:52:47.982674Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"general-image-detection\",\n            \"name\": \"Image Detection\",\n            \"created_at\": \"2020-09-02T13:49:26.221543Z\",\n            \"modified_at\": \"2022-06-13T17:34:27.250292Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"1586f39b8d8040dda41537f5e47d68f0\",\n                \"created_at\": \"2020-10-20T16:16:33.521041Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 601,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0.25\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"description\": \"Detects a variety of common objects and the location and generates regions of an image that may contain that object.\",\n            \"metadata\": {\n                \"presetInputs\": [\n                    {\n                        \"alt\": \"Clarifai general model featuring shirts bags shoes computer.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/general-shirts-bags-shoes-computer.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai general model featuring elephants.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/general-elephants.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai food model featuring hamburgers bacon cheese.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/food-hamburgers-bacon-cheese.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai general model featuring car dashboard steering wheel.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/general-car-dashboard-steering-wheel.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai general model featuring city buildings skyscraper.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/general-city-buildings-skyscraper.jpg\"\n                    }\n                ]\n            },\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"general-image-recognition\",\n            \"name\": \"Image Recognition\",\n            \"created_at\": \"2016-03-19T04:14:25.007149Z\",\n            \"modified_at\": \"2022-06-13T17:34:27.240649Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"aa7f35c01e0642fda5cf400f543e7c40\",\n                \"created_at\": \"2018-03-06T19:43:54Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 9098,\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"concepts\": \"softmax\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"display_name\": \"general-visual-classifier\",\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-classifier\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"description\": \"Identifies a variety of concepts in images and video including objects, themes, and more. Trained with over 10,000 concepts and 20M images.\",\n            \"metadata\": {\n                \"presetInputs\": [\n                    {\n                        \"alt\": \"Clarifai general model featuring shirts bags shoes computer.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/general-shirts-bags-shoes-computer.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai general model featuring elephants.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/general-elephants.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai food model featuring hamburgers bacon cheese.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/food-hamburgers-bacon-cheese.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai general model featuring car dashboard steering wheel.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/general-car-dashboard-steering-wheel.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai general model featuring city buildings skyscraper.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/general-city-buildings-skyscraper.jpg\"\n                    }\n                ]\n            },\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"celebrity-face-detection\",\n            \"name\": \"Celebrity Face Detection\",\n            \"created_at\": \"2016-10-25T19:02:38.845777Z\",\n            \"modified_at\": \"2022-06-13T17:34:27.225442Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"bc50aad2bf4b403b94656aa9f64d1454\",\n                \"created_at\": \"2019-04-27T15:52:36.026793Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 10553,\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\"\n                },\n                \"input_info\": {},\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"description\": \"Detects whether images or video contain celebrity faces. Trained with over 10,000 recognized celebrities.\",\n            \"metadata\": {\n                \"presetInputs\": [\n                    {\n                        \"alt\": \"Clarifai celebrity model featuring morgan freeman.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/celebrity-morgan-freeman.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai celebrity model featuring kim kardashian.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/celebrity-kim-kardashian.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai celebrity model featuring matt damon.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/celebrity-matt-damon.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai celebrity model featuring aziz ansari.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/celebrity-aziz-ansari.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai celebrity model featuring angelina jolie.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/celebrity-angelina-jolie.jpg\"\n                    }\n                ]\n            },\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [\n                \"pii\"\n            ],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"moderation-recognition\",\n            \"name\": \"Image Moderation\",\n            \"created_at\": \"2017-05-12T21:28:00.471607Z\",\n            \"modified_at\": \"2022-06-13T17:34:27.216052Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"aa8be956dbaa4b7a858826a84253cab9\",\n                \"created_at\": \"2017-10-26T20:29:09.263232Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 5,\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"concepts\": \"softmax\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-classifier\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"description\": \"Recognizes inappropriate content in images and video containing concepts: gore, drug, explicit, suggestive, and safe.\",\n            \"metadata\": {\n                \"presetInputs\": [\n                    {\n                        \"alt\": \"Clarifai moderation model featuring medical syringe cocaine.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/moderation-medical-syringe-cocaine.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai moderation model featuring woman breast feeding.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/moderation-woman-breast-feeding.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai moderation model featuring cutting red meet blood.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/moderation-cutting-red-meet-blood.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai moderation model featuring woman man no shirts.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/moderation-woman-man-no-shirts.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai moderation model featuring try your own text.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/moderation-try-your-own-text.jpg\"\n                    }\n                ]\n            },\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"96298cb700d5ec33176b40eee032f6df\",\n            \"name\": \"army-crada-5\",\n            \"created_at\": \"2022-04-28T01:41:20.604596Z\",\n            \"modified_at\": \"2022-04-28T01:41:20.604596Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"fdf1903eca5d47a8aa80d80fb0be6187\",\n                \"created_at\": \"2022-04-28T01:41:20.638136Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 25,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0,\n                        \"max_concepts\": 20,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"df518ae2dcd8c2ef72c13e1a06f4ef52\",\n            \"name\": \"BLIP\",\n            \"created_at\": \"2022-04-21T23:11:06.152830Z\",\n            \"modified_at\": \"2022-04-21T23:11:06.152830Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"463f6b55c0b84128b97f6af550386aaa\",\n                \"created_at\": \"2022-04-21T23:11:06.167104Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"embeddings\": \"embeddings\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-embedder\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"3dfd7edda8a9cfc4ae9feab7194274a6\",\n            \"name\": \"BLIP\",\n            \"created_at\": \"2022-04-21T23:10:58.431676Z\",\n            \"modified_at\": \"2022-04-21T23:10:58.431676Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"d25dfc5c081342cbb09c60672b06b18f\",\n                \"created_at\": \"2022-04-21T23:10:58.444450Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"embeddings\": \"embeddings\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-embedder\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"7c2f2dfc80f0286b72b689379e9e81e0\",\n            \"name\": \"general-english-image-caption-blip\",\n            \"created_at\": \"2022-04-18T18:55:56.785Z\",\n            \"modified_at\": \"2022-04-18T18:55:56.785Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"d66b4cd5b5b249489c98543632f13078\",\n                \"created_at\": \"2022-04-18T18:55:56.802386Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"image-to-text\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"2a34e347777e5744496bf6ae76e01e91\",\n            \"name\": \"ocr-document-english-printed-trocr-large\",\n            \"created_at\": \"2022-04-11T17:35:23.575656Z\",\n            \"modified_at\": \"2022-04-11T17:35:23.575656Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"761de02719ce48aaafaa8ac16e3e4117\",\n                \"created_at\": \"2022-04-11T17:35:23.596706Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"ocr-scene-french-paddleocr\",\n            \"name\": \"ocr-scene-french-paddleocr\",\n            \"created_at\": \"2022-02-16T19:46:42.443570Z\",\n            \"modified_at\": \"2022-04-05T12:42:47.955693Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"e1932c6028db4a428462ceeaa8a06ba7\",\n                \"created_at\": \"2022-02-16T19:46:42.458876Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"c4d4284d2d52988de50c95791f161e63\",\n            \"name\": \"logos-yolov5\",\n            \"created_at\": \"2022-03-29T23:13:34.162269Z\",\n            \"modified_at\": \"2022-03-31T20:09:51.482312Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"244f2a67c17d42fda8483a116edc9d3f\",\n                \"created_at\": \"2022-03-31T20:09:51.490755Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 3464,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0,\n                        \"max_concepts\": 20,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"ner-english\",\n            \"name\": \"ner_english_v2\",\n            \"created_at\": \"2022-03-16T13:48:57.921631Z\",\n            \"modified_at\": \"2022-03-22T08:06:44.455695Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"a813ff5b362c41f790c506b871e7dea4\",\n                \"created_at\": \"2022-03-16T13:48:57.946788Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 9,\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"\": \"start\",\n                        \"regions[...].data.concepts[...].id\": \"labels\",\n                        \"regions[...].data.concepts[...].value\": \"scores\"\n                    },\n                    \"params\": {\n                        \"max_concepts\": 20,\n                        \"min_value\": 0,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {\n                    \"params\": {\n                        \"model_config\": {\n                            \"_name_or_path\": \"dbmdz/bert-large-cased-finetuned-conll03-english\",\n                            \"_num_labels\": 9,\n                            \"architectures\": [\n                                \"BertForTokenClassification\"\n                            ],\n                            \"attention_probs_dropout_prob\": 0.1,\n                            \"classifier_dropout\": null,\n                            \"directionality\": \"bidi\",\n                            \"hidden_act\": \"gelu\",\n                            \"hidden_dropout_prob\": 0.1,\n                            \"hidden_size\": 1024,\n                            \"id2label\": {\n                                \"0\": \"O\",\n                                \"1\": \"B-MISC\",\n                                \"2\": \"I-MISC\",\n                                \"3\": \"B-PER\",\n                                \"4\": \"I-PER\",\n                                \"5\": \"B-ORG\",\n                                \"6\": \"I-ORG\",\n                                \"7\": \"B-LOC\",\n                                \"8\": \"I-LOC\"\n                            },\n                            \"initializer_range\": 0.02,\n                            \"intermediate_size\": 4096,\n                            \"label2id\": {\n                                \"B-LOC\": 7,\n                                \"B-MISC\": 1,\n                                \"B-ORG\": 5,\n                                \"B-PER\": 3,\n                                \"I-LOC\": 8,\n                                \"I-MISC\": 2,\n                                \"I-ORG\": 6,\n                                \"I-PER\": 4,\n                                \"O\": 0\n                            },\n                            \"layer_norm_eps\": 1e-12,\n                            \"max_position_embeddings\": 512,\n                            \"model_type\": \"bert\",\n                            \"num_attention_heads\": 16,\n                            \"num_hidden_layers\": 24,\n                            \"pad_token_id\": 0,\n                            \"pooler_fc_size\": 768,\n                            \"pooler_num_attention_heads\": 12,\n                            \"pooler_num_fc_layers\": 3,\n                            \"pooler_size_per_head\": 128,\n                            \"pooler_type\": \"first_token_transform\",\n                            \"position_embedding_type\": \"absolute\",\n                            \"torch_dtype\": \"float32\",\n                            \"transformers_version\": \"4.16.0\",\n                            \"type_vocab_size\": 2,\n                            \"use_cache\": true,\n                            \"vocab_size\": 28996\n                        },\n                        \"tokenizer_config\": {\n                            \"cls_token\": \"[CLS]\",\n                            \"do_basic_tokenize\": true,\n                            \"do_lower_case\": false,\n                            \"mask_token\": \"[MASK]\",\n                            \"max_len\": 512,\n                            \"name_or_path\": \"dbmdz/bert-large-cased-finetuned-conll03-english\",\n                            \"never_split\": null,\n                            \"pad_token\": \"[PAD]\",\n                            \"sep_token\": \"[SEP]\",\n                            \"special_tokens_map_file\": null,\n                            \"strip_accents\": null,\n                            \"tokenize_chinese_chars\": true,\n                            \"tokenizer_class\": \"BertTokenizer\",\n                            \"unk_token\": \"[UNK]\"\n                        }\n                    }\n                },\n                \"import_info\": {\n                    \"params\": {\n                        \"model_name\": \"dbmdz/bert-large-cased-finetuned-conll03-english\",\n                        \"pipeline_name\": \"ner\",\n                        \"tokenizer_name\": \"dbmdz/bert-large-cased-finetuned-conll03-english\",\n                        \"toolkit\": \"HuggingFace\"\n                    }\n                }\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-token-classifier\",\n            \"task\": \"named-entity-recognition\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {},\n            \"notes\": \"dbmdz/bert-large-cased-finetuned-conll03-english\",\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"4828602e14c60157672b103bc4174b6a\",\n            \"name\": \"ocr-scene-devanagari-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:05.691047Z\",\n            \"modified_at\": \"2022-02-16T19:51:05.691047Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"9676ee4c210f42aabb032be9e077e34e\",\n                \"created_at\": \"2022-02-16T19:51:05.695789Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"79ae4202a52a10bf910158747e2b0bd6\",\n            \"name\": \"ocr-scene-cyrillic-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:05.332230Z\",\n            \"modified_at\": \"2022-02-16T19:51:05.332230Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"eecd3d4159e447f982df39f4efc6e7a3\",\n                \"created_at\": \"2022-02-16T19:51:05.337055Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"ce1ec795e7613564611373788f719e76\",\n            \"name\": \"ocr-scene-latin-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:04.953704Z\",\n            \"modified_at\": \"2022-02-16T19:51:04.953704Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"fc43e001904541188ae7c2fc9ff1d7b8\",\n                \"created_at\": \"2022-02-16T19:51:04.961006Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"a440094009739c7f8470208af640d72b\",\n            \"name\": \"ocr-scene-tamil-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:04.601929Z\",\n            \"modified_at\": \"2022-02-16T19:51:04.601929Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"3850ab217dc048dfa92bb99c612445ed\",\n                \"created_at\": \"2022-02-16T19:51:04.607237Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"b6e3b7e79bc7d1870ee895ea802da533\",\n            \"name\": \"ocr-scene-kannada-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:04.222500Z\",\n            \"modified_at\": \"2022-02-16T19:51:04.222500Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"1552d0c25c4c453983931e02f762ad40\",\n                \"created_at\": \"2022-02-16T19:51:04.227503Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"7d4b60ad1f153879defaa4ecfd25d4f4\",\n            \"name\": \"ocr-scene-telugu-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:03.816522Z\",\n            \"modified_at\": \"2022-02-16T19:51:03.816522Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"ac7029e22f054edd809ea197524c7fc8\",\n                \"created_at\": \"2022-02-16T19:51:03.821861Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"abb6ab21bf2395f88eacb3c32fbb5fc0\",\n            \"name\": \"ocr-scene-belarusian-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:03.371226Z\",\n            \"modified_at\": \"2022-02-16T19:51:03.371226Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"6be6fdaa01854f2aa3c75a31cc9e9193\",\n                \"created_at\": \"2022-02-16T19:51:03.377576Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"819928b81ec7e0b9cb18c26175447b42\",\n            \"name\": \"ocr-scene-ukranian-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:02.987649Z\",\n            \"modified_at\": \"2022-02-16T19:51:02.987649Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"8d4bf420524a47d0a476d6e86fee65dd\",\n                \"created_at\": \"2022-02-16T19:51:02.992349Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"3953adcdf93719125977186c1e2d3f8d\",\n            \"name\": \"ocr-scene-bulgarian-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:02.616626Z\",\n            \"modified_at\": \"2022-02-16T19:51:02.616626Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"c960deec8cb34e07a800d9cc237e46f5\",\n                \"created_at\": \"2022-02-16T19:51:02.622189Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"79aa2ea6d2eaf696c93fda9eae8e360b\",\n            \"name\": \"ocr-scene-serbian-latin-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:02.186999Z\",\n            \"modified_at\": \"2022-02-16T19:51:02.186999Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"3d49fec2485f42b197d0183ba098b011\",\n                \"created_at\": \"2022-02-16T19:51:02.191990Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"d5a14580da35c5ad0b2d2bd27ce81928\",\n            \"name\": \"ocr-scene-serbian-cyrillic-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:01.799532Z\",\n            \"modified_at\": \"2022-02-16T19:51:01.799532Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"f4c110d928de45a2b9b289ec903b4a6a\",\n                \"created_at\": \"2022-02-16T19:51:01.804656Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"ca24b6b1b79b2288794ec0fd61dc43b5\",\n            \"name\": \"ocr-scene-nepali-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:01.454280Z\",\n            \"modified_at\": \"2022-02-16T19:51:01.454280Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"0cab8fcaaccd4c769dd885fc55f47214\",\n                \"created_at\": \"2022-02-16T19:51:01.458814Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"d36ee770992af4049520a0728ce0db08\",\n            \"name\": \"ocr-scene-marathi-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:01.034772Z\",\n            \"modified_at\": \"2022-02-16T19:51:01.034772Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"2f8faace78e144b3afc6f767b3fb8629\",\n                \"created_at\": \"2022-02-16T19:51:01.040656Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"84463f4cb26b41d899da8ad49189e47b\",\n            \"name\": \"ocr-scene-occitan-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:00.574910Z\",\n            \"modified_at\": \"2022-02-16T19:51:00.574910Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"e12bc12d8e254d239945e50eaf7c79ac\",\n                \"created_at\": \"2022-02-16T19:51:00.583308Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"7f9563940b3cc48321ae8d126daa0403\",\n            \"name\": \"ocr-scene-urdu-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:00.083220Z\",\n            \"modified_at\": \"2022-02-16T19:51:00.083220Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"2ff3a156373c4d1393754e945d85e7ec\",\n                \"created_at\": \"2022-02-16T19:51:00.108328Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"8f503aa63b60707e818d8a6a11a7dbe9\",\n            \"name\": \"ocr-scene-persian-paddleocr\",\n            \"created_at\": \"2022-02-16T19:50:59.643063Z\",\n            \"modified_at\": \"2022-02-16T19:50:59.643063Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"2d4cd9e9a3e04c199e27ca2493370c40\",\n                \"created_at\": \"2022-02-16T19:50:59.650272Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"6f7e3f2f2ed23a27897b898b93e8874a\",\n            \"name\": \"ocr-scene-uyghur-paddleocr\",\n            \"created_at\": \"2022-02-16T19:50:59.305322Z\",\n            \"modified_at\": \"2022-02-16T19:50:59.305322Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"6d38e8fed498460f811d5840caf7a19a\",\n                \"created_at\": \"2022-02-16T19:50:59.310025Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"594d939e0c9d8b19355f80cf1086b69d\",\n            \"name\": \"ocr-scene-hindi-paddleocr\",\n            \"created_at\": \"2022-02-16T19:50:58.946883Z\",\n            \"modified_at\": \"2022-02-16T19:50:58.946883Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"20c09506f61a4b389c15b086fedd66d5\",\n                \"created_at\": \"2022-02-16T19:50:58.953501Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"18189394090615325d5d783724ae290b\",\n            \"name\": \"ocr-scene-italian-paddleocr\",\n            \"created_at\": \"2022-02-16T19:50:58.539760Z\",\n            \"modified_at\": \"2022-02-16T19:50:58.539760Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"a56454cd3c2943a2a1a4b8840b73fda5\",\n                \"created_at\": \"2022-02-16T19:50:58.546364Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"2e47eda28e43cda4d9f5556924662bec\",\n            \"name\": \"ocr-scene-japanese-paddleocr\",\n            \"created_at\": \"2022-02-16T19:50:58.125647Z\",\n            \"modified_at\": \"2022-02-16T19:50:58.125647Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"413a870e2105418fbaadfbebc1b71b17\",\n                \"created_at\": \"2022-02-16T19:50:58.132649Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"e184d889c6e9737e6744e48df535d8f9\",\n            \"name\": \"ocr-scene-korean-paddleocr\",\n            \"created_at\": \"2022-02-16T19:50:57.685263Z\",\n            \"modified_at\": \"2022-02-16T19:50:57.685263Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"1b2985fac7d34e3cba5adfdfb1345705\",\n                \"created_at\": \"2022-02-16T19:50:57.695732Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"2b7192cc990a1a5a68d12f8326475ba6\",\n            \"name\": \"ocr-scene-german-paddleocr\",\n            \"created_at\": \"2022-02-16T19:50:57.289519Z\",\n            \"modified_at\": \"2022-02-16T19:50:57.289519Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"b10c4e125fb5423ca03f9f7600b2af6f\",\n                \"created_at\": \"2022-02-16T19:50:57.295199Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"952daadb10f7ace3433e8fc372c7c4d6\",\n            \"name\": \"ocr-scene-russian-paddleocr\",\n            \"created_at\": \"2022-02-16T19:50:56.894498Z\",\n            \"modified_at\": \"2022-02-16T19:50:56.894498Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"b02773fbacaf4f3eb5dcd6144ca01ee6\",\n                \"created_at\": \"2022-02-16T19:50:56.901399Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"60a98793f1db9dacb48e61fbb5aa61e1\",\n            \"name\": \"ocr-scene-portuguese-paddleocr\",\n            \"created_at\": \"2022-02-16T19:50:56.490797Z\",\n            \"modified_at\": \"2022-02-16T19:50:56.490797Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"d7c3be32a4bf488a987f97587cb9299d\",\n                \"created_at\": \"2022-02-16T19:50:56.500084Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"5560bcbc3d7ad8feb3999874b558b2a8\",\n            \"name\": \"ocr-scene-spanish-paddleocr\",\n            \"created_at\": \"2022-02-16T19:50:56.091771Z\",\n            \"modified_at\": \"2022-02-16T19:50:56.091771Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"a4bbcd9a06054acb8d44b3cc313dc6de\",\n                \"created_at\": \"2022-02-16T19:50:56.109454Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"fda611add70f1009c5c01d95f0fbe57b\",\n            \"name\": \"ocr-scene-arabic-paddleocr\",\n            \"created_at\": \"2022-02-16T19:50:55.696689Z\",\n            \"modified_at\": \"2022-02-16T19:50:55.696689Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"b1b8c11aee044f4a860c61d256bbd12e\",\n                \"created_at\": \"2022-02-16T19:50:55.707467Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"7acefe1233c41a8acab80619d79c6b56\",\n            \"name\": \"ocr-scene-chinese-traditional-paddleocr\",\n            \"created_at\": \"2022-02-16T19:50:55.240293Z\",\n            \"modified_at\": \"2022-02-16T19:50:55.240293Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"3b83168a4e764cd3b61c63e9662af235\",\n                \"created_at\": \"2022-02-16T19:50:55.255319Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"dc09ac965f64826410fbd8fea603abe6\",\n            \"name\": \"ocr-scene-chinese-english-paddleocr\",\n            \"created_at\": \"2022-01-25T01:53:05.944447Z\",\n            \"modified_at\": \"2022-01-25T01:53:05.944447Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"8aa1c6c2febf49f880cae2783097c2fa\",\n                \"created_at\": \"2022-01-25T01:53:05.952851Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"ba040818064ef61074e6d1cdec1c40c6\",\n            \"name\": \"paddleocr-english-chinese\",\n            \"created_at\": \"2022-01-25T00:30:04.159834Z\",\n            \"modified_at\": \"2022-01-25T00:30:04.159834Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"6daaabc3998440eeb1db18086d926bd8\",\n                \"created_at\": \"2022-01-25T00:30:04.183333Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"celebrity-face-recognition\",\n            \"name\": \"Celebrity\",\n            \"created_at\": \"2020-11-20T17:09:42.109453Z\",\n            \"modified_at\": \"2022-01-21T13:11:23.236298Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"0676ebddd5d6413ebdaa101570295a39\",\n                \"created_at\": \"2020-11-20T17:09:42.109453Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 10553,\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"concepts\": \"softmax\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-classifier\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"description\": \"AI model for recognizing celebrity faces in images or video.\",\n            \"metadata\": {\n                \"presetInputs\": [\n                    {\n                        \"alt\": \"Clarifai celebrity model featuring morgan freeman.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/celebrity-morgan-freeman.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai celebrity model featuring kim kardashian.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/celebrity-kim-kardashian.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai celebrity model featuring matt damon.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/celebrity-matt-damon.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai celebrity model featuring aziz ansari.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/celebrity-aziz-ansari.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai celebrity model featuring angelina jolie.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/celebrity-angelina-jolie.jpg\"\n                    }\n                ]\n            },\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [\n                \"pii\"\n            ],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"3b67f45ec89b4b9c6fb9db700120c91a\",\n            \"name\": \"advanced-det-handgun\",\n            \"created_at\": \"2022-01-06T20:27:53.965051Z\",\n            \"modified_at\": \"2022-01-06T20:27:53.965051Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"cecc3b705fe74a9ea88ee11e5ddd46f4\",\n                \"created_at\": \"2022-01-06T20:27:54.022071Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 2,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0,\n                        \"max_concepts\": 20,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"e4e42c048019241132ec2ecb583a3446\",\n            \"name\": \"microsoft/trocr-base-handwritten\",\n            \"created_at\": \"2021-11-10T17:18:34.067075Z\",\n            \"modified_at\": \"2021-12-29T16:56:15.937705Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"fd6f3dfd83854dd59cdeb8a5243e9e27\",\n                \"created_at\": \"2021-11-10T17:18:34.083675Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {},\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"86039c857a206810679f7f72b82fff54\",\n            \"name\": \"CLIP Prefix Captioning\",\n            \"created_at\": \"2021-12-09T04:32:31.377820Z\",\n            \"modified_at\": \"2021-12-09T04:32:31.377820Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"05fb71c53ff746f3834ab8333e401a1c\",\n                \"created_at\": \"2021-12-09T04:32:31.426529Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"image\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"image-to-text\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {},\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"bebd0da30b7090bb5250d5951960d96d\",\n            \"name\": \"CLIP\",\n            \"created_at\": \"2021-12-04T02:51:40.264878Z\",\n            \"modified_at\": \"2021-12-04T02:51:40.264878Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"5f6c752f8f964e56bef0c2eb32f3aca4\",\n                \"created_at\": \"2021-12-04T02:51:40.285448Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"embeddings\": \"embeddings\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-embedder\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"d05c045b95d85241c7d79e1ed3da3f8e\",\n            \"name\": \"PaddleOCR Multiplexed\",\n            \"created_at\": \"2021-11-05T04:36:50.693728Z\",\n            \"modified_at\": \"2021-11-05T04:36:50.693728Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"6da2f96ab8fb4255aa0ac53e6653345b\",\n                \"created_at\": \"2021-11-05T04:36:50.707810Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"fe000880ecc20921af7fbd46c485dbd2\",\n            \"name\": \"multiplexed easyocr\",\n            \"created_at\": \"2021-11-03T04:05:28.978761Z\",\n            \"modified_at\": \"2021-11-03T04:05:28.978761Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"19b30d2a3b064b13bbcd451a7b829f40\",\n                \"created_at\": \"2021-11-03T04:05:28.996684Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"c13d6411707faa5a5115b23f17957d82\",\n            \"name\": \"EasyOCR Multilingual Large\",\n            \"created_at\": \"2021-10-30T21:31:12.405449Z\",\n            \"modified_at\": \"2021-10-30T21:31:12.405449Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"7a4e17e73e144e5c8e9570aadc1cf0f5\",\n                \"created_at\": \"2021-10-30T21:31:12.411454Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"856fd858ed1dc14c741c15f0b9005cbb\",\n            \"name\": \"EasyOCR Multilingual\",\n            \"created_at\": \"2021-10-30T20:11:48.409877Z\",\n            \"modified_at\": \"2021-10-30T20:11:48.409877Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"5a77e2441d334d95859337454efbfd73\",\n                \"created_at\": \"2021-10-30T20:11:48.417474Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"23a5c4692f1a0449aef1c510be55b180\",\n            \"name\": \"facebook/wav2vec2-base-960h\",\n            \"created_at\": \"2021-10-05T17:43:10.985960Z\",\n            \"modified_at\": \"2021-10-27T13:18:19.573415Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"f4917f7f2e83453f9fe6d5eef4f598db\",\n                \"created_at\": \"2021-10-05T17:43:10.998245Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"audio\": \"audio\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"audio-to-text\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {},\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [\n                \"pii\"\n            ],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"2c1050ead1b24472be8033f5fd421f3d\",\n            \"name\": \"english\",\n            \"created_at\": \"2020-11-12T15:43:07.560737Z\",\n            \"modified_at\": \"2021-10-27T13:13:42.421644Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"dfe4776f79ee4c23a85d86d3b0649127\",\n                \"created_at\": \"2020-11-12T15:43:07.560737Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"text\": \"TRANSCRIPT\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"audio\": \"AUDIO_SIGNAL\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"audio-to-text\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {},\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"ba585a5a737771884d89972fea6c41f8\",\n            \"name\": \"language-script\",\n            \"created_at\": \"2021-10-20T18:05:49.263669Z\",\n            \"modified_at\": \"2021-10-20T18:05:49.263669Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"60fa287837dc4c10a3b5da6b92e062e2\",\n                \"created_at\": \"2021-10-20T18:05:49.328969Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 8,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"concepts\": \"softmax\"\n                    },\n                    \"params\": {\n                        \"max_concepts\": 20,\n                        \"min_value\": 0,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"logo-detection\",\n            \"name\": \"logo\",\n            \"created_at\": \"2017-03-06T18:38:13.025998Z\",\n            \"modified_at\": \"2021-10-19T12:21:30.579234Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"ef1b7237d28b415f910ca343a9145e99\",\n                \"created_at\": \"2017-03-06T18:38:13.025998Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 561,\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_cls_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_cls_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_cls_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0.05\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"display_name\": \"logo-visual-detector\",\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"description\": \"Logo detection model for locating logos of some of the most popular consumer brands within images and videos.\",\n            \"metadata\": {\n                \"presetInputs\": [\n                    {\n                        \"alt\": \"\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/logo-detection-black-bmw-silver-volkswagon.jpg\"\n                    },\n                    {\n                        \"alt\": \"\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/logo-detection-corona-extra-beer-bottle.jpg\"\n                    },\n                    {\n                        \"alt\": \"\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/logo-detection-northface-pink-jacket.jpg\"\n                    },\n                    {\n                        \"alt\": \"\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/logo-detection-pepsi-bottles-in-crate.jpg\"\n                    },\n                    {\n                        \"alt\": \"\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/logo-detection-tesla-steering-wheel.jpg\"\n                    }\n                ]\n            },\n            \"notes\": \"I can edit\",\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"food-item-v1-recognition\",\n            \"name\": \"food-items-v1.0\",\n            \"created_at\": \"2016-09-17T04:22:07.183747Z\",\n            \"modified_at\": \"2021-10-18T16:34:36.410436Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"dfebc169854e429086aceb8368662641\",\n                \"created_at\": \"2016-09-17T04:22:07.183747Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 970,\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"concepts\": \"softmax\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"display_name\": \"food-items-v1-visual-classifier\",\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-classifier\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"description\": \"AI model to recognize a wide variety of food items, including dishes and ingredients, in images and video.\",\n            \"metadata\": {\n                \"presetInputs\": [\n                    {\n                        \"alt\": \"Clarifai food model featuring pan of steamed clams.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/food-pan-of-steamed-clams.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai food model featuring waffle with strawberries blueberries.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/food-waffle-with-strawberries-blueberries.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai food model featuring hamburgers bacon cheese buns.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/food-hamburgers-bacon-cheese-buns.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai food model featuring pepperoni pizza.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/food-pepperoni-pizza.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai food model featuring tomato basil.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/food-tomato-basil.jpg\"\n                    }\n                ]\n            },\n            \"notes\": \"This is a food model note (wip) \",\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"9fe78b4150a52794f86f237770141b33\",\n            \"name\": \"english\",\n            \"created_at\": \"2021-02-04T05:24:54.250897Z\",\n            \"modified_at\": \"2021-10-18T15:46:17.997145Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"89961b2723e440abb49ec89a05b31219\",\n                \"created_at\": \"2021-09-30T14:43:29.569715Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"license\": \"BSD-2\",\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"image\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"image-to-text\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"description\": \"--\",\n            \"metadata\": {},\n            \"notes\": \"test model note now\",\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [\n                \"demographics\"\n            ],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"c3110dc5905447e410161091f0f95337\",\n            \"name\": \"anas/wav2vec2-large-xlsr-arabic\",\n            \"created_at\": \"2021-10-14T19:23:19.862284Z\",\n            \"modified_at\": \"2021-10-14T19:23:19.862284Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"f486dde2e2d046dabfd4c9e4db2c8e36\",\n                \"created_at\": \"2021-10-14T19:23:19.869018Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"audio\": \"audio\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"audio-to-text\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"68bbf91b9ad247921822a255ca381f11\",\n            \"name\": \"elgeish/wav2vec2-large-xlsr-53-arabic\",\n            \"created_at\": \"2021-10-14T18:44:04.938624Z\",\n            \"modified_at\": \"2021-10-14T18:44:04.938624Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"749a7a96fb404596bc11465dc41c3fb2\",\n                \"created_at\": \"2021-10-14T18:44:04.946902Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"audio\": \"audio\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"audio-to-text\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"face-detection\",\n            \"name\": \"Face\",\n            \"created_at\": \"2020-11-20T17:09:42.109453Z\",\n            \"modified_at\": \"2021-10-14T07:45:05.937031Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"fe995da8cb73490f8556416ecf25cea3\",\n                \"created_at\": \"2021-01-21T23:31:28.004422Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 2,\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0.9\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"display_name\": \"Face-visual-detector\",\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"description\": \"AI model for detecting the location of human faces in images and video.\",\n            \"metadata\": {\n                \"presetInputs\": [\n                    {\n                        \"alt\": \"Clarifai face model featuring little girl boy standing outside.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/face-little-girl-boy-standing-outside.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai face model featuring three men sitting in van.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/face-three-men-sitting-in-van.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai face model featuring family with light blue shirts.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/face-family-with-light-blue-shirts.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai face model featuring arfrican american man woman laughing.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/face-arfrican-american-man-woman-laughing.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai face model featuring crowd of monks orange robe.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/face-crowd-of-monks-orange-robe.jpg\"\n                    }\n                ]\n            },\n            \"notes\": \"test123\",\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [\n                \"faces\"\n            ],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [\n                \"pii\"\n            ],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"6a3dc529acf3f720a629cdc8c6ad41a9\",\n            \"name\": \"subject\",\n            \"created_at\": \"2021-04-26T09:20:01.359645Z\",\n            \"modified_at\": \"2021-10-12T12:36:06.443774Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"55b2051b75f14577b6fdd5a4fa3fd5a8\",\n                \"created_at\": \"2021-04-26T15:06:37.619220Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 2,\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].region_info.mask,regions[...].data.concepts\": \"output__0\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"input__0\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-segmenter\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"description\": \"test\",\n            \"metadata\": {},\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"beb9ed2d034b0126c6e24135ace76d8f\",\n            \"name\": \"prithivida/informal_to_formal_styletransfer\",\n            \"created_at\": \"2021-10-05T19:58:24.859525Z\",\n            \"modified_at\": \"2021-10-05T20:06:53.273373Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"a35db0e9b9b34f52ac0c83e1007db145\",\n                \"created_at\": \"2021-10-05T20:06:53.278139Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-to-text\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"69469f13c714cb6c68149db326d8c69a\",\n            \"name\": \"prithivida/formal_to_informal_styletransfer\",\n            \"created_at\": \"2021-10-05T19:45:07.925572Z\",\n            \"modified_at\": \"2021-10-05T19:45:07.925572Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"d34c3b68aadb408f9e770632183cb164\",\n                \"created_at\": \"2021-10-05T19:45:07.942034Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-to-text\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"b7e22ac73f924e2a6199af495724ddac\",\n            \"name\": \"facebook/wav2vec2-large-xlsr-53-french\",\n            \"created_at\": \"2021-10-05T17:18:44.727040Z\",\n            \"modified_at\": \"2021-10-05T17:18:44.727040Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"a648b13f7269461d9797e6bc58111a60\",\n                \"created_at\": \"2021-10-05T17:18:44.743898Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"audio\": \"audio\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"audio-to-text\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"6db1c2dc0d9c09d843c763bc0a05b989\",\n            \"name\": \"assembly\",\n            \"created_at\": \"2021-09-23T17:34:00.947876Z\",\n            \"modified_at\": \"2021-09-24T06:17:00.318253Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"5ef730b4ef014b508b31af5e5577386d\",\n                \"created_at\": \"2021-09-24T06:17:00.341875Z\",\n                \"status\": {\n                    \"code\": 99009,\n                    \"description\": \"Model upload timed out\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"embeddings\": \"embedding\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-embedder\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"2e099a9219da8fc580ac0dc54bf842fd\",\n            \"name\": \"paddleocr-multilingual-text-detector\",\n            \"created_at\": \"2021-08-19T00:40:40.789176Z\",\n            \"modified_at\": \"2021-09-22T19:50:23.797705Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"dff9af491f0d48449801decee0e2f136\",\n                \"created_at\": \"2021-09-22T19:50:23.809040Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 2,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0,\n                        \"max_concepts\": 20,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"6f96f8bf841280a388f8b52cb1868df4\",\n            \"name\": \"person-detector-yolov5x-libtorch\",\n            \"created_at\": \"2021-08-25T22:19:26.887486Z\",\n            \"modified_at\": \"2021-08-25T22:19:26.887486Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"8d1ac866905a4154b6d34ae2566503ad\",\n                \"created_at\": \"2021-08-25T22:19:26.962227Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 2,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0,\n                        \"max_concepts\": 20,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"a065882d92d66dbac3b5ebe108170197\",\n            \"name\": \"person-vehicle-detector-yolov5x-libtorch\",\n            \"created_at\": \"2021-08-25T17:56:25.508821Z\",\n            \"modified_at\": \"2021-08-25T21:51:12.368401Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"be2ea5c740f940429e6284826660da9a\",\n                \"created_at\": \"2021-08-25T21:51:12.377067Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 3,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0,\n                        \"max_concepts\": 20,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"2cf7739e65bfc63c1537f65e7ef3ae87\",\n            \"name\": \"person-vehicle-detector-yolov5s-libtorch\",\n            \"created_at\": \"2021-08-24T19:14:37.883020Z\",\n            \"modified_at\": \"2021-08-24T19:14:37.883020Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"3b6b338aa1b7433a8935d22b0915fc32\",\n                \"created_at\": \"2021-08-24T19:14:37.990646Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 3,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0,\n                        \"max_concepts\": 20,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"23aa4f9c9767a2fd61e63c55a73790ad\",\n            \"name\": \"person-detector-yolov5s-libtorch\",\n            \"created_at\": \"2021-08-24T18:52:53.766484Z\",\n            \"modified_at\": \"2021-08-24T18:52:53.766484Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"92b204309cce44209f1428e38e3406fb\",\n                \"created_at\": \"2023-02-24T05:41:57.516485Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 2,\n                \"description\": \"Yolov5\",\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0,\n                        \"max_concepts\": 20,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"bdcedc0f8da58c396b7df12f634ef923\",\n            \"name\": \"multilingual-moderation\",\n            \"created_at\": \"2021-01-13T16:45:48.370348Z\",\n            \"modified_at\": \"2021-07-13T15:12:21.536311Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"c56b46fd91ca4540823ba70496d008f9\",\n                \"created_at\": \"2021-01-13T20:45:50.641841Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 6,\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"concepts\": \"output__1\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"input__0\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-classifier\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {},\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"eac94da66f428ebf98dc2cae30030699\",\n            \"name\": \"hate-symbols\",\n            \"created_at\": \"2021-07-07T20:24:28.707682Z\",\n            \"modified_at\": \"2021-07-08T15:06:49.454931Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"ca52d78ed8cb4cfba4eacebadc38548b\",\n                \"created_at\": \"2021-07-08T15:06:49.459168Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 2,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0.05,\n                        \"max_concepts\": 20,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"bc6b2c89a5dc35ee5b5872612d0df25a\",\n            \"name\": \"EasyOCR (Turkish)\",\n            \"created_at\": \"2021-06-17T19:51:18.060645Z\",\n            \"modified_at\": \"2021-06-17T19:51:18.060645Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"17ca4640290f4c3e885ed74e757272df\",\n                \"created_at\": \"2021-06-17T19:51:18.066247Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"869245fe9708e30e6a0869c6e3dc3132\",\n            \"name\": \"EasyOCR (Swedish)\",\n            \"created_at\": \"2021-06-17T19:49:51.218255Z\",\n            \"modified_at\": \"2021-06-17T19:49:51.218255Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"8eb77eec58314083a0b9765047974d45\",\n                \"created_at\": \"2021-06-17T19:49:51.224781Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"b24a775f7b9e156f3518772206c342ef\",\n            \"name\": \"EasyOCR (Polish)\",\n            \"created_at\": \"2021-06-17T19:45:42.334798Z\",\n            \"modified_at\": \"2021-06-17T19:45:42.334798Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"ce2624cd1b84445e988b1b437dfe2a95\",\n                \"created_at\": \"2021-06-17T19:45:42.340957Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"e6bcd24eb84835a8de347b7b8a028f27\",\n            \"name\": \"EasyOCR (Dutch)\",\n            \"created_at\": \"2021-06-17T19:43:57.103955Z\",\n            \"modified_at\": \"2021-06-17T19:43:57.103955Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"22669f6ea56e428f87465b55c9296ca5\",\n                \"created_at\": \"2021-06-17T19:43:57.109610Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"f43080883661c6676779384149eb9249\",\n            \"name\": \"EasyOCR (Malay)\",\n            \"created_at\": \"2021-06-17T19:37:49.087245Z\",\n            \"modified_at\": \"2021-06-17T19:37:49.087245Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"650fde39145f417db48b6270bfa78c2a\",\n                \"created_at\": \"2021-06-17T19:37:49.096062Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"a34ea557d25cc1c7b3b6bf080381eb04\",\n            \"name\": \"EasyOCR (Vietnamese)\",\n            \"created_at\": \"2021-06-17T19:32:48.422165Z\",\n            \"modified_at\": \"2021-06-17T19:32:48.422165Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"9dcfa47d32374f2684e5ed167ad0337a\",\n                \"created_at\": \"2021-06-17T19:32:48.431675Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"edd250a6e1f82cbee84819e3550dbaf4\",\n            \"name\": \"Helsinki-NLP/opus-mt-nl-en\",\n            \"created_at\": \"2021-05-28T16:30:24.060377Z\",\n            \"modified_at\": \"2021-05-28T16:30:24.060377Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"1183f7daaadd48969be19c0e7ad1c5ec\",\n                \"created_at\": \"2021-05-28T16:30:24.070688Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-to-text\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"68a51a726f7033bbfcf57c905f09b7ca\",\n            \"name\": \"general\",\n            \"created_at\": \"2021-04-19T16:03:17.091357Z\",\n            \"modified_at\": \"2021-05-20T08:13:55.391070Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"df708d35d2fa4dea9e9d3f76ce842450\",\n                \"created_at\": \"2021-05-20T08:13:55.399476Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 183,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].region_info.mask,regions[...].data.concepts\": \"output__0\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"input__0\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-segmenter\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"19de30b6d9c38ed8a0478ac5103efebe\",\n            \"name\": \"person-vehicle\",\n            \"created_at\": \"2021-05-14T19:50:52.826338Z\",\n            \"modified_at\": \"2021-05-18T18:59:59.867143Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"f06017161d0843dca0c6cf962cf08a11\",\n                \"created_at\": \"2021-05-18T18:59:59.876468Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 3,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0,\n                        \"max_concepts\": 20,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"332956f015ea667f81cef1f37b1a20f3\",\n            \"name\": \"person-vehicle-lite\",\n            \"created_at\": \"2021-05-14T19:33:26.027753Z\",\n            \"modified_at\": \"2021-05-18T18:55:51.389600Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"ef042aa9117141079f579db19809b1d3\",\n                \"created_at\": \"2021-05-18T18:55:51.401858Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 3,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0,\n                        \"max_concepts\": 20,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"4236cc320afa91a7d6c53ec949b66785\",\n            \"name\": \"Helsinki-NLP/opus-mt-es-en\",\n            \"created_at\": \"2021-05-12T22:17:11.471812Z\",\n            \"modified_at\": \"2021-05-12T22:17:11.471812Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"6fff4d1143114416b47f278084f4ffc7\",\n                \"created_at\": \"2021-05-12T22:17:11.477501Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-to-text\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"31025e019a18970a1acc55ba6a184dc6\",\n            \"name\": \"face-sentiment\",\n            \"created_at\": \"2021-05-12T16:02:43.390981Z\",\n            \"modified_at\": \"2021-05-12T16:02:43.390981Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"edcf31cfa67e426a8b12cd889453f0c3\",\n                \"created_at\": \"2021-05-12T16:02:43.563574Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 7,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"concepts\": \"softmax\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"17a76b5162066195dad4c0437e66be80\",\n            \"name\": \"objectness-detector\",\n            \"created_at\": \"2021-05-11T17:35:00.699972Z\",\n            \"modified_at\": \"2021-05-11T17:35:00.699972Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"a488dd0eb0b94e16b22aa35656f4dd31\",\n                \"created_at\": \"2021-05-11T17:35:00.734676Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 1,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0.6\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"e609405a6ced78aa8a9eff2288f6edc6\",\n            \"name\": \"tank-rodeo\",\n            \"created_at\": \"2021-05-04T15:39:36.312115Z\",\n            \"modified_at\": \"2021-05-04T15:39:36.312115Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"745c6724c6a3456bb41ab0814070e191\",\n                \"created_at\": \"2021-05-04T15:39:36.537311Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 14,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0.1\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"385c7fd117d77553962b39629659d51a\",\n            \"name\": \"apparel\",\n            \"created_at\": \"2021-04-23T20:11:59.736603Z\",\n            \"modified_at\": \"2021-04-26T18:20:03.699907Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"c318a0b6769540e6bfe684af83560a9f\",\n                \"created_at\": \"2021-04-26T18:20:03.705751Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 192,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"concepts\": \"softmax\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"c833800b94175363881d4db8b55e4a52\",\n            \"name\": \"test20_ner\",\n            \"created_at\": \"2021-04-23T18:19:00.421074Z\",\n            \"modified_at\": \"2021-04-23T18:19:00.421074Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"aa16e51cc0ef427ab728928932fed6f6\",\n                \"created_at\": \"2021-04-23T18:19:00.654472Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 9,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"labels\",\n                        \"regions[...].data.concepts[...].value\": \"scores\",\n                        \"regions[...].data.text\": \"tokens\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-token-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"9de6872fa62a3118ce66c313e5c7d567\",\n            \"name\": \"test4_bert_base_ner\",\n            \"created_at\": \"2021-04-22T01:34:00.926953Z\",\n            \"modified_at\": \"2021-04-22T01:34:00.926953Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"b263587fd32f48f4a89a80271b5ce18f\",\n                \"created_at\": \"2021-04-22T01:34:01.026938Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 9,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"labels\",\n                        \"regions[...].data.concepts[...].value\": \"scores\",\n                        \"regions[...].data.text\": \"tokens\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-token-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"33128485c45b681a11380cea3933789a\",\n            \"name\": \"test2_bert_base_ner\",\n            \"created_at\": \"2021-04-21T20:34:07.150108Z\",\n            \"modified_at\": \"2021-04-21T20:34:07.150108Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"da4b266f7b654679b6b4ba8c8691c326\",\n                \"created_at\": \"2021-04-21T20:34:07.278954Z\",\n                \"status\": {\n                    \"code\": 99009,\n                    \"description\": \"Training Unknown Error\"\n                },\n                \"active_concept_count\": 9,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"labels\",\n                        \"regions[...].data.concepts[...].value\": \"scores\",\n                        \"regions[...].data.text\": \"tokens\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-token-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"48f9e31a431a24754f7db8bf221e1e41\",\n            \"name\": \"test_bert_base_ner\",\n            \"created_at\": \"2021-04-21T15:22:05.347218Z\",\n            \"modified_at\": \"2021-04-21T15:22:05.347218Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"e9c0b45944a24b6a9c3e77f0a10a836f\",\n                \"created_at\": \"2021-04-21T15:22:05.487673Z\",\n                \"status\": {\n                    \"code\": 99009,\n                    \"description\": \"Training Unknown Error\"\n                },\n                \"active_concept_count\": 9,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"labels\",\n                        \"regions[...].data.concepts[...].value\": \"scores\",\n                        \"regions[...].data.text\": \"tokens\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-token-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"b5632bca03e3aca4e0c948303935fb0d\",\n            \"name\": \"dslim/bert-base-NER\",\n            \"created_at\": \"2021-04-20T21:52:32.656339Z\",\n            \"modified_at\": \"2021-04-20T21:52:32.656339Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"ff092f6185c94800902d62ff68775a6e\",\n                \"created_at\": \"2021-04-20T21:52:32.810261Z\",\n                \"status\": {\n                    \"code\": 99009,\n                    \"description\": \"Training Unknown Error\"\n                },\n                \"active_concept_count\": 9,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"labels\",\n                        \"regions[...].data.concepts[...].value\": \"scores\",\n                        \"regions[...].data.text\": \"tokens\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-token-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"8db409c46a3a9153becc3565f1d99022\",\n            \"name\": \"Test Triton GPU Landmarks\",\n            \"created_at\": \"2021-01-15T21:31:47.307624Z\",\n            \"modified_at\": \"2021-04-12T20:07:18.744912Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"cf33eda7f56f4d389ffac719cdf82da4\",\n                \"created_at\": \"2021-04-12T20:07:18.749574Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 1,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts,regions[...].region_info.keypoint_locations\": \"output__0\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"input__0\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-keypointer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        }\n    ]\n}"
                  },
                  "example-3": {
                    "summary": "List Models in App  ( show only model type ids)",
                    "value": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"b764066ba8509971a7d20ad73533f255\"\n    },\n    \"models\": [\n        {\n            \"id\": \"openai-create-image-9\",\n            \"name\": \"newnewnewname\",\n            \"created_at\": \"2023-11-23T11:16:25.775735Z\",\n            \"modified_at\": \"2023-11-23T11:19:38.173686Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"text-to-text\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"test-model-with-hyperparms-1700738311\",\n            \"name\": \"test-model-with-hyperparms-1700738311\",\n            \"created_at\": \"2023-11-23T11:18:31.353199Z\",\n            \"modified_at\": \"2023-11-23T11:18:31.353199Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"embedding-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"test-model-1700738282\",\n            \"name\": \"test-model-1700738282\",\n            \"created_at\": \"2023-11-23T11:18:01.807941Z\",\n            \"modified_at\": \"2023-11-23T11:18:01.807941Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"embedding-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"test-model-1700738263\",\n            \"name\": \"test-model-1700738263\",\n            \"created_at\": \"2023-11-23T11:17:43.761741Z\",\n            \"modified_at\": \"2023-11-23T11:17:43.761741Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"embedding-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"test-model-1700738240\",\n            \"name\": \"test-model-1700738240\",\n            \"created_at\": \"2023-11-23T11:17:20.719370Z\",\n            \"modified_at\": \"2023-11-23T11:17:20.719370Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"image-crop\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"test-model-1700738220\",\n            \"name\": \"test-model-1700738220\",\n            \"created_at\": \"2023-11-23T11:17:00.314086Z\",\n            \"modified_at\": \"2023-11-23T11:17:00.314086Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"embedding-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"profanity-filter-new2\",\n            \"name\": \"profanity-filter-new2\",\n            \"created_at\": \"2023-11-23T11:16:45.317604Z\",\n            \"modified_at\": \"2023-11-23T11:16:45.317604Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"keyword-filter-operator\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"named-entity-recognition-diseases-english-text\",\n            \"name\": \"burgerz\",\n            \"created_at\": \"2023-11-23T11:15:32.431806Z\",\n            \"modified_at\": \"2023-11-23T11:16:02.900299Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"embedding-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"custom-config\",\n            \"name\": \"custom-config\",\n            \"created_at\": \"2023-11-23T09:41:08.002419Z\",\n            \"modified_at\": \"2023-11-23T09:41:08.002419Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"model_version\": {\n                \"id\": \"1e4c121974f849209abb658cdf682585\",\n                \"created_at\": \"2023-11-23T09:41:18.470087Z\",\n                \"status\": {\n                    \"code\": 21110,\n                    \"description\": \"datasets.dataset.DataBatchEmpty: No databatch found in train set's file directory\\nFailed to create a training dataset, because there are no appropriately annotated inputs. Expected annotations with concepts for model type id text-classifier. \"\n                },\n                \"active_concept_count\": 6,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"test-app-1700638575-empty\",\n                \"user_id\": \"a0btrubbaefn\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"params\": {\n                        \"max_concepts\": 20,\n                        \"min_value\": 0,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {},\n                \"train_info\": {\n                    \"params\": {\n                        \"dataset_id\": \"\",\n                        \"dataset_version_id\": \"\",\n                        \"invalid_data_tolerance_percent\": 5,\n                        \"model_config\": {\n                            \"pretrained_model_name\": \"EleutherAI/gpt-neo-125m\"\n                        },\n                        \"num_gpus\": 1,\n                        \"peft_config\": {\n                            \"peft_type\": \"LORA\"\n                        },\n                        \"template\": \"HF_GPTNeo_125m_lora\",\n                        \"tokenizer_config\": {},\n                        \"trainer_config\": {\n                            \"auto_find_batch_size\": true,\n                            \"num_train_epochs\": 20,\n                            \"output_dir\": \"checkpoint\"\n                        }\n                    }\n                },\n                \"import_info\": {}\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"text-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"imagecl1\",\n            \"name\": \"imagecl1\",\n            \"created_at\": \"2023-11-23T07:58:15.786698Z\",\n            \"modified_at\": \"2023-11-23T07:58:15.786698Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"embedding-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"imagecl\",\n            \"name\": \"imagecl\",\n            \"created_at\": \"2023-11-23T07:57:56.370980Z\",\n            \"modified_at\": \"2023-11-23T07:57:56.370980Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"embedding-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"deep_cls_bg1\",\n            \"name\": \"deep_cls_bg1\",\n            \"created_at\": \"2023-11-23T07:27:32.857722Z\",\n            \"modified_at\": \"2023-11-23T07:27:32.857722Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"model_version\": {\n                \"id\": \"00668896e0a64cf5b37302c000e96f23\",\n                \"created_at\": \"2023-11-23T08:06:12.577745Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 16,\n                \"metrics\": {\n                    \"status\": {\n                        \"code\": 21300,\n                        \"description\": \"Model was successfully evaluated.\"\n                    },\n                    \"summary\": {\n                        \"macro_avg_roc_auc\": 0.52151275,\n                        \"macro_std_roc_auc\": 0.34620512,\n                        \"macro_avg_f1_score\": 0.40380955,\n                        \"macro_std_f1_score\": 0.17236254,\n                        \"macro_avg_precision\": 0.09772728,\n                        \"macro_avg_recall\": 0.52380955\n                    }\n                },\n                \"completed_at\": \"2023-11-23T08:21:08.303040Z\",\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"test-app-1700638575-empty\",\n                \"user_id\": \"a0btrubbaefn\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"concepts\": \"probs\"\n                    },\n                    \"params\": {\n                        \"max_concepts\": 20,\n                        \"min_value\": 0,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"input\"\n                    }\n                },\n                \"train_info\": {\n                    \"params\": {\n                        \"batch_size\": 64,\n                        \"concepts_mutually_exclusive\": false,\n                        \"dataset_id\": \"\",\n                        \"dataset_version_id\": \"\",\n                        \"flip_direction\": \"horizontal\",\n                        \"flip_probability\": 0.5,\n                        \"image_size\": 224,\n                        \"invalid_data_tolerance_percent\": 5,\n                        \"num_epochs\": 60,\n                        \"num_gpus\": 1,\n                        \"per_item_lrate\": 0.00001953125,\n                        \"per_item_min_lrate\": 1.5625e-08,\n                        \"pretrained_weights\": \"ImageNet-1k\",\n                        \"seed\": -1,\n                        \"template\": \"MMClassification_ResNet_50_RSB_A1\",\n                        \"warmup_iters\": 100,\n                        \"warmup_ratio\": 0.0001,\n                        \"weight_decay\": 0.01\n                    }\n                },\n                \"import_info\": {}\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"visual-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"deep_cls_bg\",\n            \"name\": \"deep_cls_bg\",\n            \"created_at\": \"2023-11-23T07:12:59.578284Z\",\n            \"modified_at\": \"2023-11-23T07:12:59.578284Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"visual-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"profanity-filter-new\",\n            \"name\": \"profanity-filter-new\",\n            \"created_at\": \"2023-11-23T07:08:10.016262Z\",\n            \"modified_at\": \"2023-11-23T07:08:10.016262Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"model_version\": {\n                \"id\": \"b56d87c6dc80484291586d10f2feaacd\",\n                \"created_at\": \"2023-11-23T07:08:17.041706Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"test-app-1700638575-empty\",\n                \"user_id\": \"a0btrubbaefn\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"params\": {\n                        \"case_sensitive\": false,\n                        \"keywords\": [\n                            \"\"\n                        ]\n                    }\n                },\n                \"input_info\": {},\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"keyword-filter-operator\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"tiny-random-gpt2\",\n            \"name\": \"tiny-random-gpt2\",\n            \"created_at\": \"2023-11-17T21:31:10.384113Z\",\n            \"modified_at\": \"2023-11-17T21:31:10.384113Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"214531acb40a4af8b56ca79103390466\",\n                \"created_at\": \"2023-11-17T21:31:10.384113Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {\n                    \"params\": {\n                        \"model_config\": {\n                            \"_name_or_path\": \"hf-internal-testing/tiny-random-gpt2\",\n                            \"activation_function\": \"gelu_new\",\n                            \"architectures\": [\n                                \"GPT2LMHeadModel\"\n                            ],\n                            \"attention_probs_dropout_prob\": 0.1,\n                            \"attn_pdrop\": 0.1,\n                            \"bos_token_id\": 98,\n                            \"embd_pdrop\": 0.1,\n                            \"eos_token_id\": 98,\n                            \"gradient_checkpointing\": false,\n                            \"hidden_act\": \"gelu\",\n                            \"hidden_dropout_prob\": 0.1,\n                            \"initializer_range\": 0.02,\n                            \"intermediate_size\": 37,\n                            \"layer_norm_epsilon\": 0.00001,\n                            \"model_type\": \"gpt2\",\n                            \"n_ctx\": 512,\n                            \"n_embd\": 32,\n                            \"n_head\": 4,\n                            \"n_inner\": null,\n                            \"n_layer\": 5,\n                            \"n_positions\": 512,\n                            \"pad_token_id\": 98,\n                            \"reorder_and_upcast_attn\": false,\n                            \"resid_pdrop\": 0.1,\n                            \"scale_attn_by_inverse_layer_idx\": false,\n                            \"scale_attn_weights\": true,\n                            \"summary_activation\": null,\n                            \"summary_first_dropout\": 0.1,\n                            \"summary_proj_to_labels\": true,\n                            \"summary_type\": \"cls_index\",\n                            \"summary_use_proj\": true,\n                            \"torch_dtype\": \"float32\",\n                            \"transformers_version\": \"4.32.1\",\n                            \"type_vocab_size\": 16,\n                            \"use_cache\": true,\n                            \"vocab_size\": 1000\n                        },\n                        \"tokenizer_config\": {\n                            \"add_prefix_space\": false,\n                            \"bos_token\": \"<|endoftext|>\",\n                            \"clean_up_tokenization_spaces\": true,\n                            \"eos_token\": \"<|endoftext|>\",\n                            \"model_max_length\": 512,\n                            \"tokenizer_class\": \"GPT2Tokenizer\",\n                            \"unk_token\": \"<|endoftext|>\"\n                        }\n                    }\n                },\n                \"import_info\": {\n                    \"params\": {\n                        \"model_name\": \"hf-internal-testing/tiny-random-gpt2\",\n                        \"pipeline_name\": \"text-generation\",\n                        \"toolkit\": \"HuggingFace\"\n                    }\n                }\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-to-text\",\n            \"task\": \"text-generation\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"BAAI-bge-base-en-v15\",\n            \"name\": \"BAAI-bge-base-en-v15\",\n            \"created_at\": \"2023-11-01T12:53:30.339664Z\",\n            \"modified_at\": \"2023-11-01T12:53:30.339664Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"23cf79f86459491cb31fd7f0273c9fff\",\n                \"created_at\": \"2023-11-01T12:53:30.339664Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"embeddings\": \"embedding\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    },\n                    \"params\": {\n                        \"text_token_warning_limit\": 512\n                    }\n                },\n                \"train_info\": {\n                    \"params\": {\n                        \"model_config\": {\n                            \"_name_or_path\": \"BAAI/bge-base-en-v1.5\",\n                            \"architectures\": [\n                                \"BertModel\"\n                            ],\n                            \"attention_probs_dropout_prob\": 0.1,\n                            \"classifier_dropout\": null,\n                            \"gradient_checkpointing\": false,\n                            \"hidden_act\": \"gelu\",\n                            \"hidden_dropout_prob\": 0.1,\n                            \"hidden_size\": 768,\n                            \"id2label\": {\n                                \"0\": \"LABEL_0\"\n                            },\n                            \"initializer_range\": 0.02,\n                            \"intermediate_size\": 3072,\n                            \"label2id\": {\n                                \"LABEL_0\": 0\n                            },\n                            \"layer_norm_eps\": 1e-12,\n                            \"max_position_embeddings\": 512,\n                            \"model_type\": \"bert\",\n                            \"num_attention_heads\": 12,\n                            \"num_hidden_layers\": 12,\n                            \"pad_token_id\": 0,\n                            \"position_embedding_type\": \"absolute\",\n                            \"torch_dtype\": \"float32\",\n                            \"transformers_version\": \"4.32.1\",\n                            \"type_vocab_size\": 2,\n                            \"use_cache\": true,\n                            \"vocab_size\": 30522\n                        },\n                        \"tokenizer_config\": {\n                            \"clean_up_tokenization_spaces\": true,\n                            \"cls_token\": \"[CLS]\",\n                            \"do_basic_tokenize\": true,\n                            \"do_lower_case\": true,\n                            \"mask_token\": \"[MASK]\",\n                            \"model_max_length\": 512,\n                            \"never_split\": null,\n                            \"pad_token\": \"[PAD]\",\n                            \"sep_token\": \"[SEP]\",\n                            \"strip_accents\": null,\n                            \"tokenize_chinese_chars\": true,\n                            \"tokenizer_class\": \"BertTokenizer\",\n                            \"unk_token\": \"[UNK]\"\n                        }\n                    }\n                },\n                \"import_info\": {\n                    \"params\": {\n                        \"model_name\": \"BAAI/bge-base-en-v1.5\",\n                        \"pipeline_name\": \"feature-extraction\",\n                        \"toolkit\": \"HuggingFace\"\n                    }\n                }\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-embedder\",\n            \"task\": \"representation-learning\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"BAAI-bge-base-en-cluster\",\n            \"name\": \"BAAI-bge-base-en-cluster\",\n            \"created_at\": \"2023-08-15T14:21:18.083130Z\",\n            \"modified_at\": \"2023-08-15T14:21:18.083130Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"4a47a75c931c4b0784cebc2cd45bc5a2\",\n                \"created_at\": \"2023-09-18T11:00:41.832176Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"metrics\": {\n                    \"status\": {\n                        \"code\": 21300,\n                        \"description\": \"Model was successfully evaluated.\"\n                    }\n                },\n                \"total_input_count\": 293849,\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\"\n                },\n                \"input_info\": {},\n                \"train_info\": {\n                    \"params\": {\n                        \"beta\": 1,\n                        \"coarse_clusters\": 128,\n                        \"dataset_id\": \"quora-dataset-corpus-2\",\n                        \"dataset_version_id\": \"dataset-version-1692900595413\",\n                        \"eval_holdout_fraction\": 0.2,\n                        \"max_num_query_embeddings\": 100,\n                        \"max_visited\": 32,\n                        \"num_results_per_query\": [\n                            1,\n                            5,\n                            10,\n                            20\n                        ],\n                        \"query_holdout_fraction\": 0.1,\n                        \"quota\": 1000,\n                        \"to_be_indexed_queries_fraction\": 0.25,\n                        \"train_iters\": 1,\n                        \"training_timeout\": 72000\n                    },\n                    \"dataset\": {\n                        \"id\": \"quora-dataset-corpus-2\",\n                        \"created_at\": \"2023-08-24T07:40:45.232142Z\",\n                        \"modified_at\": \"2023-08-24T18:09:55.799396Z\",\n                        \"app_id\": \"quora-dataset\",\n                        \"user_id\": \"isaac\",\n                        \"metadata\": {},\n                        \"visibility\": {\n                            \"gettable\": 10\n                        },\n                        \"version\": {\n                            \"id\": \"dataset-version-1692900595413\",\n                            \"created_at\": \"0001-01-01T00:00:00Z\",\n                            \"modified_at\": \"0001-01-01T00:00:00Z\",\n                            \"app_id\": \"quora-dataset\",\n                            \"user_id\": \"isaac\",\n                            \"dataset_id\": \"quora-dataset-corpus-2\",\n                            \"status\": {\n                                \"code\": 99009,\n                                \"description\": \"Internal error\"\n                            },\n                            \"metadata\": {}\n                        }\n                    }\n                },\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"clusterer\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"BAAI-bge-base-en\",\n            \"name\": \"BAAI-bge-base-en\",\n            \"created_at\": \"2023-08-15T11:36:23.145658Z\",\n            \"modified_at\": \"2023-08-15T11:36:23.145658Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"b55d165cc3c64ed4bab3090c7b402188\",\n                \"created_at\": \"2023-08-15T11:36:23.145658Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"embeddings\": \"embedding\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {\n                    \"params\": {\n                        \"model_config\": {\n                            \"_name_or_path\": \"BAAI/bge-base-en\",\n                            \"architectures\": [\n                                \"BertModel\"\n                            ],\n                            \"attention_probs_dropout_prob\": 0.1,\n                            \"classifier_dropout\": null,\n                            \"gradient_checkpointing\": false,\n                            \"hidden_act\": \"gelu\",\n                            \"hidden_dropout_prob\": 0.1,\n                            \"hidden_size\": 768,\n                            \"id2label\": {\n                                \"0\": \"LABEL_0\"\n                            },\n                            \"initializer_range\": 0.02,\n                            \"intermediate_size\": 3072,\n                            \"label2id\": {\n                                \"LABEL_0\": 0\n                            },\n                            \"layer_norm_eps\": 1e-12,\n                            \"max_position_embeddings\": 512,\n                            \"model_type\": \"bert\",\n                            \"num_attention_heads\": 12,\n                            \"num_hidden_layers\": 12,\n                            \"pad_token_id\": 0,\n                            \"position_embedding_type\": \"absolute\",\n                            \"torch_dtype\": \"float32\",\n                            \"transformers_version\": \"4.30.2\",\n                            \"type_vocab_size\": 2,\n                            \"use_cache\": true,\n                            \"vocab_size\": 30522\n                        },\n                        \"tokenizer_config\": {\n                            \"clean_up_tokenization_spaces\": true,\n                            \"cls_token\": \"[CLS]\",\n                            \"do_basic_tokenize\": true,\n                            \"do_lower_case\": true,\n                            \"mask_token\": \"[MASK]\",\n                            \"model_max_length\": 512,\n                            \"never_split\": null,\n                            \"pad_token\": \"[PAD]\",\n                            \"sep_token\": \"[SEP]\",\n                            \"strip_accents\": null,\n                            \"tokenize_chinese_chars\": true,\n                            \"tokenizer_class\": \"BertTokenizer\",\n                            \"unk_token\": \"[UNK]\"\n                        }\n                    }\n                },\n                \"import_info\": {\n                    \"params\": {\n                        \"model_name\": \"BAAI/bge-base-en\",\n                        \"pipeline_name\": \"feature-extraction\",\n                        \"tokenizer_name\": \"BAAI/bge-base-en\",\n                        \"toolkit\": \"HuggingFace\"\n                    }\n                }\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-embedder\",\n            \"task\": \"representation-learning\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"apparel-recognition\",\n            \"name\": \"apparel\",\n            \"created_at\": \"2016-12-15T01:29:04.622209Z\",\n            \"modified_at\": \"2023-05-23T12:34:15.093542Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"dc2cd6d9bff5425a80bfe0c4105583c1\",\n                \"created_at\": \"2016-12-15T01:29:04.622209Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 112,\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"concepts\": \"softmax\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"display_name\": \"apparel-visual-classifier\",\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-classifier\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"description\": \"AI model for identifying fashion-related and clothing concepts, hats, jewelry, handbags, etc. in images and video.\",\n            \"metadata\": {\n                \"presetInputs\": [\n                    {\n                        \"alt\": \"Clarifai apparel model featuring woman black turtleneck.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/apparel-woman-black-turtleneck.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai apparel model featuring yellow boots.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/apparel-yellow-boots.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai apparel model featuring black white striped socks.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/apparel-black-white-striped-socks.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai apparel model featuring sunglasses.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/apparel-sunglasses.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai apparel model featuring dog in a dog carrier.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/apparel-dog-in-a-dog-carrier.jpg\"\n                    }\n                ]\n            },\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"15b0041cc2cd848a0d8b45f8b83c1d7d\",\n            \"name\": \"CLIP\",\n            \"created_at\": \"2021-12-14T18:07:40.983254Z\",\n            \"modified_at\": \"2023-04-27T20:45:27.183474Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"97f20cc96b7c4bec8f3b96e284ba1173\",\n                \"created_at\": \"2021-12-14T18:07:41.268867Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"embeddings\": \"embeddings\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-embedder\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"notes\": \"This model has been deprecated. Please use `multilingual-multimodal-clip-embed` instead.\",\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"text-translation-english-spanish\",\n            \"name\": \"Helsinki-NLP/opus-mt-en-es\",\n            \"created_at\": \"2023-02-22T22:44:16.825059Z\",\n            \"modified_at\": \"2023-02-22T22:44:16.825059Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"643f30558de34013aff72b0e21f244f5\",\n                \"created_at\": \"2023-02-23T00:39:20.611092Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {\n                    \"params\": {\n                        \"model_config\": {\n                            \"_name_or_path\": \"Helsinki-NLP/opus-mt-en-es\",\n                            \"activation_dropout\": 0,\n                            \"activation_function\": \"swish\",\n                            \"add_bias_logits\": false,\n                            \"add_final_layer_norm\": false,\n                            \"architectures\": [\n                                \"MarianMTModel\"\n                            ],\n                            \"attention_dropout\": 0,\n                            \"bad_words_ids\": [\n                                [\n                                    65000\n                                ]\n                            ],\n                            \"bos_token_id\": 0,\n                            \"classif_dropout\": 0,\n                            \"classifier_dropout\": 0,\n                            \"d_model\": 512,\n                            \"decoder_attention_heads\": 8,\n                            \"decoder_ffn_dim\": 2048,\n                            \"decoder_layerdrop\": 0,\n                            \"decoder_layers\": 6,\n                            \"decoder_start_token_id\": 65000,\n                            \"dropout\": 0.1,\n                            \"encoder_attention_heads\": 8,\n                            \"encoder_ffn_dim\": 2048,\n                            \"encoder_layerdrop\": 0,\n                            \"encoder_layers\": 6,\n                            \"eos_token_id\": 0,\n                            \"extra_pos_embeddings\": 65001,\n                            \"force_bos_token_to_be_generated\": false,\n                            \"forced_eos_token_id\": 0,\n                            \"gradient_checkpointing\": false,\n                            \"id2label\": {\n                                \"0\": \"LABEL_0\",\n                                \"1\": \"LABEL_1\",\n                                \"2\": \"LABEL_2\"\n                            },\n                            \"init_std\": 0.02,\n                            \"is_encoder_decoder\": true,\n                            \"label2id\": {\n                                \"LABEL_0\": 0,\n                                \"LABEL_1\": 1,\n                                \"LABEL_2\": 2\n                            },\n                            \"max_length\": 512,\n                            \"max_position_embeddings\": 512,\n                            \"model_type\": \"marian\",\n                            \"normalize_before\": false,\n                            \"normalize_embedding\": false,\n                            \"num_beams\": 4,\n                            \"num_hidden_layers\": 6,\n                            \"pad_token_id\": 65000,\n                            \"scale_embedding\": true,\n                            \"static_position_embeddings\": true,\n                            \"torch_dtype\": \"float32\",\n                            \"transformers_version\": \"4.16.0\",\n                            \"use_cache\": true,\n                            \"vocab_size\": 65001\n                        },\n                        \"tokenizer_config\": {\n                            \"eos_token\": \"</s>\",\n                            \"model_max_length\": 512,\n                            \"name_or_path\": \"Helsinki-NLP/opus-mt-en-es\",\n                            \"pad_token\": \"<pad>\",\n                            \"source_lang\": \"eng\",\n                            \"sp_model_kwargs\": {},\n                            \"special_tokens_map_file\": null,\n                            \"target_lang\": \"spa\",\n                            \"tokenizer_class\": \"MarianTokenizer\",\n                            \"tokenizer_file\": null,\n                            \"unk_token\": \"<unk>\"\n                        }\n                    }\n                },\n                \"import_info\": {\n                    \"params\": {\n                        \"model_name\": \"Helsinki-NLP/opus-mt-en-es\",\n                        \"pipeline_name\": \"translation_xx_to_yy\",\n                        \"tokenizer_name\": \"Helsinki-NLP/opus-mt-en-es\",\n                        \"toolkit\": \"HuggingFace\"\n                    }\n                }\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-to-text\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"description\": \"Text translation model from English to Spanish using sentence piece-based segmentation\",\n            \"metadata\": {},\n            \"notes\": \"\\n # Helsinki-NLP - English to Spanish                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n ## Introduction                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n The Helsinki-NLP models are used to translate text from one language to another. As such, the model takes a block text as its input, and outputs the translated block of text. This particular model takes in English text as it's input and outputs Spanish text.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n ## Limitations                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n The usage of random capitalization and punctuation may result in erroneous translations grammatically speaking. If you are using this model in a workflow and find grammar issues, you can try utilizing aggregators to minimize errors.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n **More Info**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n * Original Repository: [GitHub](https://github.com/Helsinki-NLP/Tatoeba-Challenge)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \\n * Helsinki-NLP Opus: [eng-spa](https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/master/models/eng-spa)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \\n * Hugging Face docs: [Helsinki-NLP/opus-mt-en-es](https://huggingface.co/Helsinki-NLP/opus-mt-en-es)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n ## Paper                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n [Natural language processing for similar languages, varieties, and dialects: A survey](https://helda.helsinki.fi/bitstream/handle/10138/330117/natural_language_processing_for_similar_languages_varieties_and_dialects_a_survey.pdf)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n Authors: Marcos Zampieri, Preslav Nakov, Yves Scherrer                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n **Abstract**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n There has been a lot of recent interest in the natural language processing (NLP) community in the computational processing of language varieties and dialects, with the aim to improve the performance of applications such as machine translation, speech recognition, and dialogue systems. Here, we attempt to survey this growing field of research, with focus on computational methods for processing similar languages, varieties, and dialects. In particular, we discuss the most important challenges when dealing with diatopic language variation, and we present some of the available datasets, the process of data collection, and the most common data collection strategies used to compile datasets for similar languages, varieties, and dialects. We further present a number of studies on computational methods developed and/or adapted for preprocessing, normalization, part-of-speech tagging, and parsing similar languages, language varieties, and dialects. Finally, we discuss relevant applications such as language and dialect identification and machine translation for closely related languages, language varieties, and dialects.                                     \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n ## Risks, Limitations, and Biases                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n **CONTENT WARNING: Readers should be aware this section contains content that is disturbing, offensive, and can propagate historical and current stereotypes.**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n There has been significant research exploring bias and fairness issues with language models. Some important papers in this field include:                                                   # Helsinki-NLP - English to Spanish                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n ## Introduction                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n The Helsinki-NLP models are used to translate text from one language to another. As such, the model takes a block text as its input, and outputs the translated block of text. This particular model takes in English text as it's input and outputs Spanish text.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n ## Limitations                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n The usage of random capitalization and punctuation may result in erroneous translations grammatically speaking. If you are using this model in a workflow and find grammar issues, you can try utilizing aggregators to minimize errors.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n **More Info**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n * Original Repository: [GitHub](https://github.com/Helsinki-NLP/Tatoeba-Challenge)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \\n * Helsinki-NLP Opus: [eng-spa](https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/master/models/eng-spa)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \\n * Hugging Face docs: [Helsinki-NLP/opus-mt-en-es](https://huggingface.co/Helsinki-NLP/opus-mt-en-es)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n ## Paper                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n [Natural language processing for similar languages, varieties, and dialects: A survey](https://helda.helsinki.fi/bitstream/handle/10138/330117/natural_language_processing_for_similar_languages_varieties_and_dialects_a_survey.pdf)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n Authors: Marcos Zampieri, Preslav Nakov, Yves Scherrer                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n **Abstract**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n There has been a lot of recent interest in the natural language processing (NLP) community in the computational processing of language varieties and dialects, with the aim to improve the performance of applications such as machine translation, speech recognition, and dialogue systems. Here, we attempt to survey this growing field of research, with focus on computational methods for processing similar languages, varieties, and dialects. In particular, we discuss the most important challenges when dealing with diatopic language variation, and we present some of the available datasets, the process of data collection, and the most common data collection strategies used to compile datasets for similar languages, varieties, and dialects. We further present a number of studies on computational methods developed and/or adapted for preprocessing, normalization, part-of-speech tagging, and parsing similar languages, language varieties, and dialects. Finally, we discuss relevant applications such as language and dialect identification and machine translation for closely related languages, language varieties, and dialects.                                     \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n ## Risks, Limitations, and Biases                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n **CONTENT WARNING: Readers should be aware this section contains content that is disturbing, offensive, and can propagate historical and current stereotypes.**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n There has been significant research exploring bias and fairness issues with language models. Some important papers in this field include:                                                  \\n * [Societal Biases in Language Generation: Progress and Challenges](https://aclanthology.org/2021.acl-long.330.pdf)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \\n     * Authors: Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, Nanyun Peng                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \\n     * Abstract: Technology for language generation has advanced rapidly, spurred by advancements in pre-training large models on massive amounts of data and the need for intelligent agents to communicate in a natural manner. While techniques can effectively generate fluent text, they can also produce undesirable societal biases that can have a disproportionately negative impact on marginalized populations. Language generation presents unique challenges for biases in terms of direct user interaction and the structure of decoding techniques. To better understand these challenges, we present a survey on societal biases in language generation, focusing on how data and techniques contribute to biases and progress towards reducing biases. Motivated by a lack of studies on biases from decoding techniques, we also conduct experiments to quantify the effects of these techniques. By further discussing general trends and open challenges, we call to attention promising directions for research and the importance of fairness and inclusivity considerations for language generation applications.<br /><br />                                                          \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n * [On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \\n     * Authors: Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, Shmargaret Shmitchell                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \\n     * Abstract: The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n ## Benchmarks                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n The following benchmarks are for the **opus-2021-02-19** weights.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n | testset                        | BLEU | chr-F | #sent | #words | BP    |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\n | ------------------------------ | ---- | ----- | ----- | ------ | ----- |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\n | newssyscomb2009-engspa.eng.spa | 31.3 | 0.583 | 502   | 12506  | 0.990 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\n | news-test2008-engspa.eng.spa   | 29.6 | 0.564 | 2051  | 52596  | 1.000 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\n | newstest2009-engspa.eng.spa    | 30.2 | 0.578 | 2525  | 68114  | 1.000 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\n | newstest2010-engspa.eng.spa    | 36.9 | 0.620 | 2489  | 65522  | 1.000 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\n | newstest2011-engspa.eng.spa    | 38.3 | 0.620 | 3003  | 79476  | 0.984 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\n | newstest2012-engspa.eng.spa    | 39.1 | 0.626 | 3003  | 79006  | 0.969 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\n | newstest2013-engspa.eng.spa    | 35.1 | 0.598 | 3000  | 70528  | 0.960 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\n | Tatoeba-test.eng.spa           | 55.1 | 0.721 | 10000 | 77311  | 0.978 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\n | tico19-test.eng-spa            | 50.4 | 0.727 | 2100  | 66591  | 0.959 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n ## Additional Info                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\n * Data set: Opus                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \\n * Model: Transformer                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \\n * Source Language(s): en (English)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \\n * Target Language(s): es (Spanish)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \\n * Pre-processing: Normalization  [SentencePiece](https://github.com/google/sentencepiece) (spm32k, spm32k)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \\n * Download original weights: [opus-2021-02-19.zip](https://object.pouta.csc.fi/Tatoeba-MT-models/eng-spa/opus-2021-02-19.zip)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \\n * Test set translations: [opus-2021-02-19.test.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/eng-spa/opus-2021-02-19.test.txt)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \\n * Test set scores: [opus-2021-02-19.eval.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/eng-spa/opus-2021-02-19.eval.txt)\\n\",\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"ocr_model_v3-1677100451\",\n            \"name\": \"ocr_model_v3-1677100451\",\n            \"created_at\": \"2023-02-22T21:14:10.921823Z\",\n            \"modified_at\": \"2023-02-22T21:14:10.921823Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"22894138385843978aaa97cae37780fb\",\n                \"created_at\": \"2023-02-22T21:14:10.928773Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\",\n                        \"regions[...].value\": \"predicted_det_scores\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {\n                \"presetInputs\": [\n                    {\n                        \"alt\": \"Stop Sign.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/9/9d/Stop_sign_light_red.svg/1200px-Stop_sign_light_red.svg.png\"\n                    }\n                ]\n            },\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"ocr-scene-english-paddleocr\",\n            \"name\": \"OCR Scene English PaddleOCR\",\n            \"created_at\": \"2023-02-22T15:48:10.066388Z\",\n            \"modified_at\": \"2023-02-22T15:48:10.066388Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"40dbb2c9cde44a27af226782e7157006\",\n                \"created_at\": \"2023-02-22T15:49:55.126424Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"description\": \"An OCR model for detecting and recognizing English text in images that are more complex than scans of a page.\",\n            \"metadata\": {\n                \"presetInputs\": [\n                    {\n                        \"type\": \"image\",\n                        \"url\": \"https://samples.clarifai.com/featured-models/ocr-woman-holding-sold-sign.jpg\"\n                    },\n                    {\n                        \"type\": \"image\",\n                        \"url\": \"https://samples.clarifai.com/featured-models/paddleocrs/ocr-scene-english-paddleocr-1.jpg\"\n                    },\n                    {\n                        \"type\": \"image\",\n                        \"url\": \"https://samples.clarifai.com/featured-models/paddleocrs/ocr-scene-english-paddleocr-2.jpg\"\n                    },\n                    {\n                        \"type\": \"image\",\n                        \"url\": \"https://samples.clarifai.com/featured-models/paddleocrs/ocr-scene-english-paddleocr-3.png\"\n                    }\n                ]\n            },\n            \"notes\": \"\\n # Introduction                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n PaddleOCR aims to create multilingual, awesome, leading, and practical OCR tools that help users train better models and apply them into practice. The information in this summary is taken from their [Github.](https://github.com/PaddlePaddle/PaddleOCR)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n Release PP-OCRv3: With comparable speed, the effect of Chinese scene is further improved by 5% compared with PP-OCRv2, the effect of English scene is improved by 11%, and the average recognition accuracy of 80 language multilingual models is improved by more than 5%.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n <iframe width=\\\"560\\\" height=\\\"315\\\" src=\\\"https://www.youtube.com/embed/ITTtqGKtS54\\\" title=\\\"YouTube video player\\\" frameborder=\\\"0\\\" allow=\\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\\\" allowfullscreen></iframe>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n # Features                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n PaddleOCR support a variety of cutting-edge algorithms related to OCR, and developed industrial featured models/solution [PP-OCR](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.5/doc/doc_en/ppocr_introduction_en.md) and [PP-Structure](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.5/ppstructure/README.md) on this basis, and get through the whole process of data production, model training, compression, inference and deployment.                                                                                                                                                                                                                                                                                                  \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n ## PP-OCR Series Model List - This model is the English ultra-lightweight PP-OCRv3 model (13.4M) on the second row.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n | Model introduction                                           | Model name                   | Recommended scene | Detection model                                              | Direction classifier                                         | Recognition model                                            |                                                                                                                                                                                                                                                                                                                                                                                                                                                    \\n | ------------------------------------------------------------ | ---------------------------- | ----------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |                                                                                                                                                                                                                                                                                                                                                                                                                                                    \\n | Chinese and English ultra-lightweight PP-OCRv3 model（16.2M）     | ch_PP-OCRv3_xx          | Mobile & Server | [inference model](https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_det_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_det_distill_train.tar) | [inference model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_train.tar) | [inference model](https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_rec_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_rec_train.tar) |                                 \\n | English ultra-lightweight PP-OCRv3 model（13.4M）     | en_PP-OCRv3_xx          | Mobile & Server | [inference model](https://paddleocr.bj.bcebos.com/PP-OCRv3/english/en_PP-OCRv3_det_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/PP-OCRv3/english/en_PP-OCRv3_det_distill_train.tar) | [inference model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_train.tar) | [inference model](https://paddleocr.bj.bcebos.com/PP-OCRv3/english/en_PP-OCRv3_rec_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/PP-OCRv3/english/en_PP-OCRv3_rec_train.tar) |                                             \\n | Chinese and English ultra-lightweight PP-OCRv2 model（11.6M） |  ch_PP-OCRv2_xx |Mobile & Server|[inference model](https://paddleocr.bj.bcebos.com/PP-OCRv2/chinese/ch_PP-OCRv2_det_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/PP-OCRv2/chinese/ch_PP-OCRv2_det_distill_train.tar)| [inference model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_train.tar) |[inference model](https://paddleocr.bj.bcebos.com/PP-OCRv2/chinese/ch_PP-OCRv2_rec_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/PP-OCRv2/chinese/ch_PP-OCRv2_rec_train.tar)|                                                   \\n | Chinese and English ultra-lightweight PP-OCR model (9.4M)       | ch_ppocr_mobile_v2.0_xx      | Mobile & server   |[inference model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_det_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_det_train.tar)|[inference model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_train.tar) |[inference model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_rec_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_rec_train.tar)      |   \\n | Chinese and English general PP-OCR model (143.4M)               | ch_ppocr_server_v2.0_xx      | Server            |[inference model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_server_v2.0_det_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_server_v2.0_det_train.tar)    |[inference model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_train.tar)    |[inference model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_server_v2.0_rec_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_server_v2.0_rec_train.tar)  |\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n - For more model downloads (including multiple languages), please refer to [PP-OCR series model downloads](./doc/doc_en/models_list_en.md).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \\n - For a new language request, please refer to [Guideline for new language_requests](#language_requests).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \\n - For structural document analysis models, please refer to [PP-Structure models](./ppstructure/docs/models_list_en.md).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n # PP-OCRv3 English model                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \\n ![](https://github.com/PaddlePaddle/PaddleOCR/raw/release/2.5/doc/imgs_results/PP-OCRv3/en/en_1.png)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n # PP-OCRv3 Chinese model                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \\n ![](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.5/doc/imgs_results/PP-OCRv3/ch/PP-OCRv3-pic003.jpg?raw=true)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n # PP-OCRv3 Multilingual model                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \\n ![](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.5/doc/imgs_results/PP-OCRv3/multi_lang/korean_1.jpg?raw=true)\",\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"ocr-scene-chinese-english-paddleocr\",\n            \"name\": \"ocr-scene-chinese-english-paddleocr\",\n            \"created_at\": \"2022-08-10T21:27:40.359110Z\",\n            \"modified_at\": \"2023-02-09T23:37:22.184921Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"67104613bc7245b594d6a38eb7e34974\",\n                \"created_at\": \"2022-08-10T21:27:40.889145Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {\n                \"presetInputs\": [\n                    {\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/general-shirts-bags-shoes-computer.jpg\"\n                    }\n                ]\n            },\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"multilingual-multimodal-clip-embed\",\n            \"name\": \"Multilingual Multimodal Clip Embedder\",\n            \"created_at\": \"2023-01-30T17:46:05.745974Z\",\n            \"modified_at\": \"2023-01-30T17:46:05.745974Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"e3289fa66be4419eb2958ba74b6e9fee\",\n                \"created_at\": \"2023-01-30T17:46:05.745974Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"train_stats\": {},\n                \"completed_at\": \"2023-01-30T17:46:05.745974Z\",\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"embeddings\": \"embeddings\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\",\n                        \"text\": \"text\"\n                    },\n                    \"params\": {\n                        \"text_token_warning_limit\": 77\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"multimodal-embedder\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"description\": \"CLIP-based multilingual multimodal embedding model.\",\n            \"metadata\": {},\n            \"notes\": \"##Multilingual CLIP\",\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"ccfc0043fa804de4a586005f72582e00\",\n            \"name\": \"Multimodal Clip Clusterer\",\n            \"created_at\": \"2022-11-16T14:51:43.695740Z\",\n            \"modified_at\": \"2022-11-16T14:51:43.695740Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"4b134b9fb5f24e2bb09b7493560cc922\",\n                \"created_at\": \"2022-11-16T14:51:43.695740Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"metrics\": {\n                    \"status\": {\n                        \"code\": 21300,\n                        \"description\": \"Model was successfully evaluated.\"\n                    },\n                    \"summary\": {\n                        \"macro_avg_roc_auc\": 0,\n                        \"macro_std_roc_auc\": 0,\n                        \"macro_avg_f1_score\": 0,\n                        \"macro_std_f1_score\": 0,\n                        \"macro_avg_precision\": 0,\n                        \"macro_avg_recall\": 0,\n                        \"lopq_metrics\": [\n                            {\n                                \"k\": 10,\n                                \"recall_vs_brute_force\": 0.95100015,\n                                \"kendall_tau_vs_brute_force\": 1,\n                                \"most_frequent_code_percent\": 0.41158637,\n                                \"lopq_ndcg\": 0,\n                                \"brute_force_ndcg\": 0\n                            },\n                            {\n                                \"k\": 20,\n                                \"recall_vs_brute_force\": 0.93349975,\n                                \"kendall_tau_vs_brute_force\": 0.99947363,\n                                \"most_frequent_code_percent\": 0.41158637,\n                                \"lopq_ndcg\": 0,\n                                \"brute_force_ndcg\": 0\n                            },\n                            {\n                                \"k\": 50,\n                                \"recall_vs_brute_force\": 0.9118,\n                                \"kendall_tau_vs_brute_force\": 0.99697524,\n                                \"most_frequent_code_percent\": 0.41158637,\n                                \"lopq_ndcg\": 0,\n                                \"brute_force_ndcg\": 0\n                            },\n                            {\n                                \"k\": 100,\n                                \"recall_vs_brute_force\": 0.8869,\n                                \"kendall_tau_vs_brute_force\": 0.9947445,\n                                \"most_frequent_code_percent\": 0.41158637,\n                                \"lopq_ndcg\": 0,\n                                \"brute_force_ndcg\": 0\n                            },\n                            {\n                                \"k\": 200,\n                                \"recall_vs_brute_force\": 0.8432002,\n                                \"kendall_tau_vs_brute_force\": 0.9947241,\n                                \"most_frequent_code_percent\": 0.41158637,\n                                \"lopq_ndcg\": 0,\n                                \"brute_force_ndcg\": 0\n                            }\n                        ]\n                    }\n                },\n                \"total_input_count\": 9527727,\n                \"train_stats\": {},\n                \"completed_at\": \"2022-11-16T14:51:43.695740Z\",\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\"\n                },\n                \"input_info\": {\n                    \"base_embed_model\": {\n                        \"id\": \"multimodal-clip-embed\",\n                        \"app_id\": \"main\",\n                        \"model_version\": {\n                            \"id\": \"9fe2c8962c104327bc87b8f8104b161a\"\n                        },\n                        \"user_id\": \"clarifai\",\n                        \"model_type_id\": \"multimodal-embedder\",\n                        \"toolkits\": [],\n                        \"use_cases\": [],\n                        \"languages\": [],\n                        \"languages_full\": [],\n                        \"check_consents\": []\n                    }\n                },\n                \"train_info\": {\n                    \"params\": {\n                        \"beta\": 1,\n                        \"coarse_clusters\": 125,\n                        \"dataset_id\": \"\",\n                        \"dataset_version_id\": \"ee243135d683462eaa1060c4f5c63725\",\n                        \"eval_holdout_fraction\": 0.2,\n                        \"max_num_query_embeddings\": 100,\n                        \"max_visited\": 1562,\n                        \"num_results_per_query\": [\n                            10,\n                            20,\n                            50,\n                            100,\n                            200\n                        ],\n                        \"query_holdout_fraction\": 0.1,\n                        \"quota\": 10000,\n                        \"to_be_indexed_queries_fraction\": 0.25,\n                        \"train_iters\": 1,\n                        \"training_timeout\": 86400\n                    }\n                },\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"clusterer\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {},\n            \"notes\": \"##CLIP\",\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"multimodal-clip-embed\",\n            \"name\": \"multimodal-clip\",\n            \"created_at\": \"2022-11-07T17:47:19.112250Z\",\n            \"modified_at\": \"2022-11-07T17:47:19.112250Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"9fe2c8962c104327bc87b8f8104b161a\",\n                \"created_at\": \"2022-11-07T17:47:19.123181Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"embeddings\": \"embeddings\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\",\n                        \"text\": \"text\"\n                    },\n                    \"params\": {\n                        \"text_token_warning_limit\": 77\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"multimodal-embedder\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"da94111b740547aeae38ba9668f998a3\",\n            \"name\": \"ocr-scene-devanagari-paddleocr\",\n            \"created_at\": \"2022-08-10T19:52:53.761109Z\",\n            \"modified_at\": \"2022-08-10T19:52:53.761109Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"5e35e10fb7814f5c9223ccb3c3afebec\",\n                \"created_at\": \"2022-08-10T19:52:53.956768Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"5f42b31a4589d672152e9668d02eb471\",\n            \"name\": \"ocr-scene-cyrillic-paddleocr\",\n            \"created_at\": \"2022-08-10T19:52:53.009976Z\",\n            \"modified_at\": \"2022-08-10T19:52:53.009976Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"bbb2a719af92447ebeaabc88d3f41123\",\n                \"created_at\": \"2022-08-10T19:52:53.252145Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"e1e3b6f78fd2d55c830c631434ba83a5\",\n            \"name\": \"ocr-scene-arabic-paddleocr\",\n            \"created_at\": \"2022-08-10T19:52:51.994687Z\",\n            \"modified_at\": \"2022-08-10T19:52:51.994687Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"4b33b79b4b2e42b4b9ee07c844f1bb56\",\n                \"created_at\": \"2022-08-10T19:52:52.548327Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"c54e41dd13c9669630426078d36718ec\",\n            \"name\": \"ocr-scene-latin-paddleocr\",\n            \"created_at\": \"2022-08-10T19:52:51.331917Z\",\n            \"modified_at\": \"2022-08-10T19:52:51.331917Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"a82c91d715f74c789df25c78e811dc6a\",\n                \"created_at\": \"2022-08-10T19:52:51.487313Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"e3fde27570dc04aff3b997a223601ac8\",\n            \"name\": \"ocr-scene-kannada-paddleocr\",\n            \"created_at\": \"2022-08-10T19:52:50.570402Z\",\n            \"modified_at\": \"2022-08-10T19:52:50.570402Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"324f4a4d0bc64d81ab7be56c21748328\",\n                \"created_at\": \"2022-08-10T19:52:50.803339Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"1d5cbe1f275acaa8ac109dec6e07120b\",\n            \"name\": \"ocr-scene-telugu-paddleocr\",\n            \"created_at\": \"2022-08-10T19:52:49.858398Z\",\n            \"modified_at\": \"2022-08-10T19:52:49.858398Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"004b78b3cc3b4e2b9fe53f3cda5ff001\",\n                \"created_at\": \"2022-08-10T19:52:50.014584Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"019c84b9cae1c41e7bf661a037f6ac12\",\n            \"name\": \"ocr-scene-tamil-paddleocr\",\n            \"created_at\": \"2022-08-10T19:52:49.207422Z\",\n            \"modified_at\": \"2022-08-10T19:52:49.207422Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"88bf32db4fdb488593303bc5fb309754\",\n                \"created_at\": \"2022-08-10T19:52:49.353280Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"6748476e7c5ae4df9bc2019dd6bd5892\",\n            \"name\": \"ocr-scene-japanese-paddleocr\",\n            \"created_at\": \"2022-08-10T19:52:48.506276Z\",\n            \"modified_at\": \"2022-08-10T19:52:48.506276Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"b3a22a7bc99943b998de94c1e3c5d420\",\n                \"created_at\": \"2022-08-10T19:52:48.703164Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"bf08cee902d18405ca6ac58b68b3e2a6\",\n            \"name\": \"ocr-scene-korean-paddleocr\",\n            \"created_at\": \"2022-08-10T19:52:47.804473Z\",\n            \"modified_at\": \"2022-08-10T19:52:47.804473Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"f242bc5057b4402dab82ca0dbaf1be1e\",\n                \"created_at\": \"2022-08-10T19:52:47.982674Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"general-image-detection\",\n            \"name\": \"Image Detection\",\n            \"created_at\": \"2020-09-02T13:49:26.221543Z\",\n            \"modified_at\": \"2022-06-13T17:34:27.250292Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"1586f39b8d8040dda41537f5e47d68f0\",\n                \"created_at\": \"2020-10-20T16:16:33.521041Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 601,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0.25\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"description\": \"Detects a variety of common objects and the location and generates regions of an image that may contain that object.\",\n            \"metadata\": {\n                \"presetInputs\": [\n                    {\n                        \"alt\": \"Clarifai general model featuring shirts bags shoes computer.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/general-shirts-bags-shoes-computer.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai general model featuring elephants.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/general-elephants.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai food model featuring hamburgers bacon cheese.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/food-hamburgers-bacon-cheese.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai general model featuring car dashboard steering wheel.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/general-car-dashboard-steering-wheel.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai general model featuring city buildings skyscraper.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/general-city-buildings-skyscraper.jpg\"\n                    }\n                ]\n            },\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"general-image-recognition\",\n            \"name\": \"Image Recognition\",\n            \"created_at\": \"2016-03-19T04:14:25.007149Z\",\n            \"modified_at\": \"2022-06-13T17:34:27.240649Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"aa7f35c01e0642fda5cf400f543e7c40\",\n                \"created_at\": \"2018-03-06T19:43:54Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 9098,\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"concepts\": \"softmax\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"display_name\": \"general-visual-classifier\",\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-classifier\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"description\": \"Identifies a variety of concepts in images and video including objects, themes, and more. Trained with over 10,000 concepts and 20M images.\",\n            \"metadata\": {\n                \"presetInputs\": [\n                    {\n                        \"alt\": \"Clarifai general model featuring shirts bags shoes computer.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/general-shirts-bags-shoes-computer.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai general model featuring elephants.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/general-elephants.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai food model featuring hamburgers bacon cheese.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/food-hamburgers-bacon-cheese.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai general model featuring car dashboard steering wheel.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/general-car-dashboard-steering-wheel.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai general model featuring city buildings skyscraper.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/general-city-buildings-skyscraper.jpg\"\n                    }\n                ]\n            },\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"celebrity-face-detection\",\n            \"name\": \"Celebrity Face Detection\",\n            \"created_at\": \"2016-10-25T19:02:38.845777Z\",\n            \"modified_at\": \"2022-06-13T17:34:27.225442Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"bc50aad2bf4b403b94656aa9f64d1454\",\n                \"created_at\": \"2019-04-27T15:52:36.026793Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 10553,\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\"\n                },\n                \"input_info\": {},\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"description\": \"Detects whether images or video contain celebrity faces. Trained with over 10,000 recognized celebrities.\",\n            \"metadata\": {\n                \"presetInputs\": [\n                    {\n                        \"alt\": \"Clarifai celebrity model featuring morgan freeman.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/celebrity-morgan-freeman.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai celebrity model featuring kim kardashian.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/celebrity-kim-kardashian.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai celebrity model featuring matt damon.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/celebrity-matt-damon.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai celebrity model featuring aziz ansari.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/celebrity-aziz-ansari.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai celebrity model featuring angelina jolie.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/celebrity-angelina-jolie.jpg\"\n                    }\n                ]\n            },\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [\n                \"pii\"\n            ],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"moderation-recognition\",\n            \"name\": \"Image Moderation\",\n            \"created_at\": \"2017-05-12T21:28:00.471607Z\",\n            \"modified_at\": \"2022-06-13T17:34:27.216052Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"aa8be956dbaa4b7a858826a84253cab9\",\n                \"created_at\": \"2017-10-26T20:29:09.263232Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 5,\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"concepts\": \"softmax\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-classifier\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"description\": \"Recognizes inappropriate content in images and video containing concepts: gore, drug, explicit, suggestive, and safe.\",\n            \"metadata\": {\n                \"presetInputs\": [\n                    {\n                        \"alt\": \"Clarifai moderation model featuring medical syringe cocaine.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/moderation-medical-syringe-cocaine.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai moderation model featuring woman breast feeding.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/moderation-woman-breast-feeding.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai moderation model featuring cutting red meet blood.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/moderation-cutting-red-meet-blood.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai moderation model featuring woman man no shirts.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/moderation-woman-man-no-shirts.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai moderation model featuring try your own text.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/moderation-try-your-own-text.jpg\"\n                    }\n                ]\n            },\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"96298cb700d5ec33176b40eee032f6df\",\n            \"name\": \"army-crada-5\",\n            \"created_at\": \"2022-04-28T01:41:20.604596Z\",\n            \"modified_at\": \"2022-04-28T01:41:20.604596Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"fdf1903eca5d47a8aa80d80fb0be6187\",\n                \"created_at\": \"2022-04-28T01:41:20.638136Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 25,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0,\n                        \"max_concepts\": 20,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"df518ae2dcd8c2ef72c13e1a06f4ef52\",\n            \"name\": \"BLIP\",\n            \"created_at\": \"2022-04-21T23:11:06.152830Z\",\n            \"modified_at\": \"2022-04-21T23:11:06.152830Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"463f6b55c0b84128b97f6af550386aaa\",\n                \"created_at\": \"2022-04-21T23:11:06.167104Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"embeddings\": \"embeddings\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-embedder\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"3dfd7edda8a9cfc4ae9feab7194274a6\",\n            \"name\": \"BLIP\",\n            \"created_at\": \"2022-04-21T23:10:58.431676Z\",\n            \"modified_at\": \"2022-04-21T23:10:58.431676Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"d25dfc5c081342cbb09c60672b06b18f\",\n                \"created_at\": \"2022-04-21T23:10:58.444450Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"embeddings\": \"embeddings\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-embedder\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"7c2f2dfc80f0286b72b689379e9e81e0\",\n            \"name\": \"general-english-image-caption-blip\",\n            \"created_at\": \"2022-04-18T18:55:56.785Z\",\n            \"modified_at\": \"2022-04-18T18:55:56.785Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"d66b4cd5b5b249489c98543632f13078\",\n                \"created_at\": \"2022-04-18T18:55:56.802386Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"image-to-text\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"2a34e347777e5744496bf6ae76e01e91\",\n            \"name\": \"ocr-document-english-printed-trocr-large\",\n            \"created_at\": \"2022-04-11T17:35:23.575656Z\",\n            \"modified_at\": \"2022-04-11T17:35:23.575656Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"761de02719ce48aaafaa8ac16e3e4117\",\n                \"created_at\": \"2022-04-11T17:35:23.596706Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"ocr-scene-french-paddleocr\",\n            \"name\": \"ocr-scene-french-paddleocr\",\n            \"created_at\": \"2022-02-16T19:46:42.443570Z\",\n            \"modified_at\": \"2022-04-05T12:42:47.955693Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"e1932c6028db4a428462ceeaa8a06ba7\",\n                \"created_at\": \"2022-02-16T19:46:42.458876Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"c4d4284d2d52988de50c95791f161e63\",\n            \"name\": \"logos-yolov5\",\n            \"created_at\": \"2022-03-29T23:13:34.162269Z\",\n            \"modified_at\": \"2022-03-31T20:09:51.482312Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"244f2a67c17d42fda8483a116edc9d3f\",\n                \"created_at\": \"2022-03-31T20:09:51.490755Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 3464,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0,\n                        \"max_concepts\": 20,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"ner-english\",\n            \"name\": \"ner_english_v2\",\n            \"created_at\": \"2022-03-16T13:48:57.921631Z\",\n            \"modified_at\": \"2022-03-22T08:06:44.455695Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"a813ff5b362c41f790c506b871e7dea4\",\n                \"created_at\": \"2022-03-16T13:48:57.946788Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 9,\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"\": \"start\",\n                        \"regions[...].data.concepts[...].id\": \"labels\",\n                        \"regions[...].data.concepts[...].value\": \"scores\"\n                    },\n                    \"params\": {\n                        \"max_concepts\": 20,\n                        \"min_value\": 0,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {\n                    \"params\": {\n                        \"model_config\": {\n                            \"_name_or_path\": \"dbmdz/bert-large-cased-finetuned-conll03-english\",\n                            \"_num_labels\": 9,\n                            \"architectures\": [\n                                \"BertForTokenClassification\"\n                            ],\n                            \"attention_probs_dropout_prob\": 0.1,\n                            \"classifier_dropout\": null,\n                            \"directionality\": \"bidi\",\n                            \"hidden_act\": \"gelu\",\n                            \"hidden_dropout_prob\": 0.1,\n                            \"hidden_size\": 1024,\n                            \"id2label\": {\n                                \"0\": \"O\",\n                                \"1\": \"B-MISC\",\n                                \"2\": \"I-MISC\",\n                                \"3\": \"B-PER\",\n                                \"4\": \"I-PER\",\n                                \"5\": \"B-ORG\",\n                                \"6\": \"I-ORG\",\n                                \"7\": \"B-LOC\",\n                                \"8\": \"I-LOC\"\n                            },\n                            \"initializer_range\": 0.02,\n                            \"intermediate_size\": 4096,\n                            \"label2id\": {\n                                \"B-LOC\": 7,\n                                \"B-MISC\": 1,\n                                \"B-ORG\": 5,\n                                \"B-PER\": 3,\n                                \"I-LOC\": 8,\n                                \"I-MISC\": 2,\n                                \"I-ORG\": 6,\n                                \"I-PER\": 4,\n                                \"O\": 0\n                            },\n                            \"layer_norm_eps\": 1e-12,\n                            \"max_position_embeddings\": 512,\n                            \"model_type\": \"bert\",\n                            \"num_attention_heads\": 16,\n                            \"num_hidden_layers\": 24,\n                            \"pad_token_id\": 0,\n                            \"pooler_fc_size\": 768,\n                            \"pooler_num_attention_heads\": 12,\n                            \"pooler_num_fc_layers\": 3,\n                            \"pooler_size_per_head\": 128,\n                            \"pooler_type\": \"first_token_transform\",\n                            \"position_embedding_type\": \"absolute\",\n                            \"torch_dtype\": \"float32\",\n                            \"transformers_version\": \"4.16.0\",\n                            \"type_vocab_size\": 2,\n                            \"use_cache\": true,\n                            \"vocab_size\": 28996\n                        },\n                        \"tokenizer_config\": {\n                            \"cls_token\": \"[CLS]\",\n                            \"do_basic_tokenize\": true,\n                            \"do_lower_case\": false,\n                            \"mask_token\": \"[MASK]\",\n                            \"max_len\": 512,\n                            \"name_or_path\": \"dbmdz/bert-large-cased-finetuned-conll03-english\",\n                            \"never_split\": null,\n                            \"pad_token\": \"[PAD]\",\n                            \"sep_token\": \"[SEP]\",\n                            \"special_tokens_map_file\": null,\n                            \"strip_accents\": null,\n                            \"tokenize_chinese_chars\": true,\n                            \"tokenizer_class\": \"BertTokenizer\",\n                            \"unk_token\": \"[UNK]\"\n                        }\n                    }\n                },\n                \"import_info\": {\n                    \"params\": {\n                        \"model_name\": \"dbmdz/bert-large-cased-finetuned-conll03-english\",\n                        \"pipeline_name\": \"ner\",\n                        \"tokenizer_name\": \"dbmdz/bert-large-cased-finetuned-conll03-english\",\n                        \"toolkit\": \"HuggingFace\"\n                    }\n                }\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-token-classifier\",\n            \"task\": \"named-entity-recognition\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {},\n            \"notes\": \"dbmdz/bert-large-cased-finetuned-conll03-english\",\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"4828602e14c60157672b103bc4174b6a\",\n            \"name\": \"ocr-scene-devanagari-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:05.691047Z\",\n            \"modified_at\": \"2022-02-16T19:51:05.691047Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"9676ee4c210f42aabb032be9e077e34e\",\n                \"created_at\": \"2022-02-16T19:51:05.695789Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"79ae4202a52a10bf910158747e2b0bd6\",\n            \"name\": \"ocr-scene-cyrillic-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:05.332230Z\",\n            \"modified_at\": \"2022-02-16T19:51:05.332230Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"eecd3d4159e447f982df39f4efc6e7a3\",\n                \"created_at\": \"2022-02-16T19:51:05.337055Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"ce1ec795e7613564611373788f719e76\",\n            \"name\": \"ocr-scene-latin-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:04.953704Z\",\n            \"modified_at\": \"2022-02-16T19:51:04.953704Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"fc43e001904541188ae7c2fc9ff1d7b8\",\n                \"created_at\": \"2022-02-16T19:51:04.961006Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"a440094009739c7f8470208af640d72b\",\n            \"name\": \"ocr-scene-tamil-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:04.601929Z\",\n            \"modified_at\": \"2022-02-16T19:51:04.601929Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"3850ab217dc048dfa92bb99c612445ed\",\n                \"created_at\": \"2022-02-16T19:51:04.607237Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"b6e3b7e79bc7d1870ee895ea802da533\",\n            \"name\": \"ocr-scene-kannada-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:04.222500Z\",\n            \"modified_at\": \"2022-02-16T19:51:04.222500Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"1552d0c25c4c453983931e02f762ad40\",\n                \"created_at\": \"2022-02-16T19:51:04.227503Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"7d4b60ad1f153879defaa4ecfd25d4f4\",\n            \"name\": \"ocr-scene-telugu-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:03.816522Z\",\n            \"modified_at\": \"2022-02-16T19:51:03.816522Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"ac7029e22f054edd809ea197524c7fc8\",\n                \"created_at\": \"2022-02-16T19:51:03.821861Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"abb6ab21bf2395f88eacb3c32fbb5fc0\",\n            \"name\": \"ocr-scene-belarusian-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:03.371226Z\",\n            \"modified_at\": \"2022-02-16T19:51:03.371226Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"6be6fdaa01854f2aa3c75a31cc9e9193\",\n                \"created_at\": \"2022-02-16T19:51:03.377576Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"819928b81ec7e0b9cb18c26175447b42\",\n            \"name\": \"ocr-scene-ukranian-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:02.987649Z\",\n            \"modified_at\": \"2022-02-16T19:51:02.987649Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"8d4bf420524a47d0a476d6e86fee65dd\",\n                \"created_at\": \"2022-02-16T19:51:02.992349Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"3953adcdf93719125977186c1e2d3f8d\",\n            \"name\": \"ocr-scene-bulgarian-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:02.616626Z\",\n            \"modified_at\": \"2022-02-16T19:51:02.616626Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"c960deec8cb34e07a800d9cc237e46f5\",\n                \"created_at\": \"2022-02-16T19:51:02.622189Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"79aa2ea6d2eaf696c93fda9eae8e360b\",\n            \"name\": \"ocr-scene-serbian-latin-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:02.186999Z\",\n            \"modified_at\": \"2022-02-16T19:51:02.186999Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"3d49fec2485f42b197d0183ba098b011\",\n                \"created_at\": \"2022-02-16T19:51:02.191990Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"d5a14580da35c5ad0b2d2bd27ce81928\",\n            \"name\": \"ocr-scene-serbian-cyrillic-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:01.799532Z\",\n            \"modified_at\": \"2022-02-16T19:51:01.799532Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"f4c110d928de45a2b9b289ec903b4a6a\",\n                \"created_at\": \"2022-02-16T19:51:01.804656Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"ca24b6b1b79b2288794ec0fd61dc43b5\",\n            \"name\": \"ocr-scene-nepali-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:01.454280Z\",\n            \"modified_at\": \"2022-02-16T19:51:01.454280Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"0cab8fcaaccd4c769dd885fc55f47214\",\n                \"created_at\": \"2022-02-16T19:51:01.458814Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"d36ee770992af4049520a0728ce0db08\",\n            \"name\": \"ocr-scene-marathi-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:01.034772Z\",\n            \"modified_at\": \"2022-02-16T19:51:01.034772Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"2f8faace78e144b3afc6f767b3fb8629\",\n                \"created_at\": \"2022-02-16T19:51:01.040656Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"84463f4cb26b41d899da8ad49189e47b\",\n            \"name\": \"ocr-scene-occitan-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:00.574910Z\",\n            \"modified_at\": \"2022-02-16T19:51:00.574910Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"e12bc12d8e254d239945e50eaf7c79ac\",\n                \"created_at\": \"2022-02-16T19:51:00.583308Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"7f9563940b3cc48321ae8d126daa0403\",\n            \"name\": \"ocr-scene-urdu-paddleocr\",\n            \"created_at\": \"2022-02-16T19:51:00.083220Z\",\n            \"modified_at\": \"2022-02-16T19:51:00.083220Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"2ff3a156373c4d1393754e945d85e7ec\",\n                \"created_at\": \"2022-02-16T19:51:00.108328Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"8f503aa63b60707e818d8a6a11a7dbe9\",\n            \"name\": \"ocr-scene-persian-paddleocr\",\n            \"created_at\": \"2022-02-16T19:50:59.643063Z\",\n            \"modified_at\": \"2022-02-16T19:50:59.643063Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"2d4cd9e9a3e04c199e27ca2493370c40\",\n                \"created_at\": \"2022-02-16T19:50:59.650272Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"6f7e3f2f2ed23a27897b898b93e8874a\",\n            \"name\": \"ocr-scene-uyghur-paddleocr\",\n            \"created_at\": \"2022-02-16T19:50:59.305322Z\",\n            \"modified_at\": \"2022-02-16T19:50:59.305322Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"6d38e8fed498460f811d5840caf7a19a\",\n                \"created_at\": \"2022-02-16T19:50:59.310025Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"594d939e0c9d8b19355f80cf1086b69d\",\n            \"name\": \"ocr-scene-hindi-paddleocr\",\n            \"created_at\": \"2022-02-16T19:50:58.946883Z\",\n            \"modified_at\": \"2022-02-16T19:50:58.946883Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"20c09506f61a4b389c15b086fedd66d5\",\n                \"created_at\": \"2022-02-16T19:50:58.953501Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"18189394090615325d5d783724ae290b\",\n            \"name\": \"ocr-scene-italian-paddleocr\",\n            \"created_at\": \"2022-02-16T19:50:58.539760Z\",\n            \"modified_at\": \"2022-02-16T19:50:58.539760Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"a56454cd3c2943a2a1a4b8840b73fda5\",\n                \"created_at\": \"2022-02-16T19:50:58.546364Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"2e47eda28e43cda4d9f5556924662bec\",\n            \"name\": \"ocr-scene-japanese-paddleocr\",\n            \"created_at\": \"2022-02-16T19:50:58.125647Z\",\n            \"modified_at\": \"2022-02-16T19:50:58.125647Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"413a870e2105418fbaadfbebc1b71b17\",\n                \"created_at\": \"2022-02-16T19:50:58.132649Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"e184d889c6e9737e6744e48df535d8f9\",\n            \"name\": \"ocr-scene-korean-paddleocr\",\n            \"created_at\": \"2022-02-16T19:50:57.685263Z\",\n            \"modified_at\": \"2022-02-16T19:50:57.685263Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"1b2985fac7d34e3cba5adfdfb1345705\",\n                \"created_at\": \"2022-02-16T19:50:57.695732Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"2b7192cc990a1a5a68d12f8326475ba6\",\n            \"name\": \"ocr-scene-german-paddleocr\",\n            \"created_at\": \"2022-02-16T19:50:57.289519Z\",\n            \"modified_at\": \"2022-02-16T19:50:57.289519Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"b10c4e125fb5423ca03f9f7600b2af6f\",\n                \"created_at\": \"2022-02-16T19:50:57.295199Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"952daadb10f7ace3433e8fc372c7c4d6\",\n            \"name\": \"ocr-scene-russian-paddleocr\",\n            \"created_at\": \"2022-02-16T19:50:56.894498Z\",\n            \"modified_at\": \"2022-02-16T19:50:56.894498Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"b02773fbacaf4f3eb5dcd6144ca01ee6\",\n                \"created_at\": \"2022-02-16T19:50:56.901399Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"60a98793f1db9dacb48e61fbb5aa61e1\",\n            \"name\": \"ocr-scene-portuguese-paddleocr\",\n            \"created_at\": \"2022-02-16T19:50:56.490797Z\",\n            \"modified_at\": \"2022-02-16T19:50:56.490797Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"d7c3be32a4bf488a987f97587cb9299d\",\n                \"created_at\": \"2022-02-16T19:50:56.500084Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"5560bcbc3d7ad8feb3999874b558b2a8\",\n            \"name\": \"ocr-scene-spanish-paddleocr\",\n            \"created_at\": \"2022-02-16T19:50:56.091771Z\",\n            \"modified_at\": \"2022-02-16T19:50:56.091771Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"a4bbcd9a06054acb8d44b3cc313dc6de\",\n                \"created_at\": \"2022-02-16T19:50:56.109454Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"fda611add70f1009c5c01d95f0fbe57b\",\n            \"name\": \"ocr-scene-arabic-paddleocr\",\n            \"created_at\": \"2022-02-16T19:50:55.696689Z\",\n            \"modified_at\": \"2022-02-16T19:50:55.696689Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"b1b8c11aee044f4a860c61d256bbd12e\",\n                \"created_at\": \"2022-02-16T19:50:55.707467Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"7acefe1233c41a8acab80619d79c6b56\",\n            \"name\": \"ocr-scene-chinese-traditional-paddleocr\",\n            \"created_at\": \"2022-02-16T19:50:55.240293Z\",\n            \"modified_at\": \"2022-02-16T19:50:55.240293Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"3b83168a4e764cd3b61c63e9662af235\",\n                \"created_at\": \"2022-02-16T19:50:55.255319Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"dc09ac965f64826410fbd8fea603abe6\",\n            \"name\": \"ocr-scene-chinese-english-paddleocr\",\n            \"created_at\": \"2022-01-25T01:53:05.944447Z\",\n            \"modified_at\": \"2022-01-25T01:53:05.944447Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"8aa1c6c2febf49f880cae2783097c2fa\",\n                \"created_at\": \"2022-01-25T01:53:05.952851Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"ba040818064ef61074e6d1cdec1c40c6\",\n            \"name\": \"paddleocr-english-chinese\",\n            \"created_at\": \"2022-01-25T00:30:04.159834Z\",\n            \"modified_at\": \"2022-01-25T00:30:04.159834Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"6daaabc3998440eeb1db18086d926bd8\",\n                \"created_at\": \"2022-01-25T00:30:04.183333Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"celebrity-face-recognition\",\n            \"name\": \"Celebrity\",\n            \"created_at\": \"2020-11-20T17:09:42.109453Z\",\n            \"modified_at\": \"2022-01-21T13:11:23.236298Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"0676ebddd5d6413ebdaa101570295a39\",\n                \"created_at\": \"2020-11-20T17:09:42.109453Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 10553,\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"concepts\": \"softmax\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-classifier\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"description\": \"AI model for recognizing celebrity faces in images or video.\",\n            \"metadata\": {\n                \"presetInputs\": [\n                    {\n                        \"alt\": \"Clarifai celebrity model featuring morgan freeman.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/celebrity-morgan-freeman.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai celebrity model featuring kim kardashian.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/celebrity-kim-kardashian.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai celebrity model featuring matt damon.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/celebrity-matt-damon.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai celebrity model featuring aziz ansari.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/celebrity-aziz-ansari.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai celebrity model featuring angelina jolie.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/celebrity-angelina-jolie.jpg\"\n                    }\n                ]\n            },\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [\n                \"pii\"\n            ],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"3b67f45ec89b4b9c6fb9db700120c91a\",\n            \"name\": \"advanced-det-handgun\",\n            \"created_at\": \"2022-01-06T20:27:53.965051Z\",\n            \"modified_at\": \"2022-01-06T20:27:53.965051Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"cecc3b705fe74a9ea88ee11e5ddd46f4\",\n                \"created_at\": \"2022-01-06T20:27:54.022071Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 2,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0,\n                        \"max_concepts\": 20,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"e4e42c048019241132ec2ecb583a3446\",\n            \"name\": \"microsoft/trocr-base-handwritten\",\n            \"created_at\": \"2021-11-10T17:18:34.067075Z\",\n            \"modified_at\": \"2021-12-29T16:56:15.937705Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"fd6f3dfd83854dd59cdeb8a5243e9e27\",\n                \"created_at\": \"2021-11-10T17:18:34.083675Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {},\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"86039c857a206810679f7f72b82fff54\",\n            \"name\": \"CLIP Prefix Captioning\",\n            \"created_at\": \"2021-12-09T04:32:31.377820Z\",\n            \"modified_at\": \"2021-12-09T04:32:31.377820Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"05fb71c53ff746f3834ab8333e401a1c\",\n                \"created_at\": \"2021-12-09T04:32:31.426529Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"image\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"image-to-text\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {},\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"bebd0da30b7090bb5250d5951960d96d\",\n            \"name\": \"CLIP\",\n            \"created_at\": \"2021-12-04T02:51:40.264878Z\",\n            \"modified_at\": \"2021-12-04T02:51:40.264878Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"5f6c752f8f964e56bef0c2eb32f3aca4\",\n                \"created_at\": \"2021-12-04T02:51:40.285448Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"embeddings\": \"embeddings\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-embedder\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"d05c045b95d85241c7d79e1ed3da3f8e\",\n            \"name\": \"PaddleOCR Multiplexed\",\n            \"created_at\": \"2021-11-05T04:36:50.693728Z\",\n            \"modified_at\": \"2021-11-05T04:36:50.693728Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"6da2f96ab8fb4255aa0ac53e6653345b\",\n                \"created_at\": \"2021-11-05T04:36:50.707810Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"fe000880ecc20921af7fbd46c485dbd2\",\n            \"name\": \"multiplexed easyocr\",\n            \"created_at\": \"2021-11-03T04:05:28.978761Z\",\n            \"modified_at\": \"2021-11-03T04:05:28.978761Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"19b30d2a3b064b13bbcd451a7b829f40\",\n                \"created_at\": \"2021-11-03T04:05:28.996684Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"c13d6411707faa5a5115b23f17957d82\",\n            \"name\": \"EasyOCR Multilingual Large\",\n            \"created_at\": \"2021-10-30T21:31:12.405449Z\",\n            \"modified_at\": \"2021-10-30T21:31:12.405449Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"7a4e17e73e144e5c8e9570aadc1cf0f5\",\n                \"created_at\": \"2021-10-30T21:31:12.411454Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"856fd858ed1dc14c741c15f0b9005cbb\",\n            \"name\": \"EasyOCR Multilingual\",\n            \"created_at\": \"2021-10-30T20:11:48.409877Z\",\n            \"modified_at\": \"2021-10-30T20:11:48.409877Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"5a77e2441d334d95859337454efbfd73\",\n                \"created_at\": \"2021-10-30T20:11:48.417474Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"23a5c4692f1a0449aef1c510be55b180\",\n            \"name\": \"facebook/wav2vec2-base-960h\",\n            \"created_at\": \"2021-10-05T17:43:10.985960Z\",\n            \"modified_at\": \"2021-10-27T13:18:19.573415Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"f4917f7f2e83453f9fe6d5eef4f598db\",\n                \"created_at\": \"2021-10-05T17:43:10.998245Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"audio\": \"audio\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"audio-to-text\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {},\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [\n                \"pii\"\n            ],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"2c1050ead1b24472be8033f5fd421f3d\",\n            \"name\": \"english\",\n            \"created_at\": \"2020-11-12T15:43:07.560737Z\",\n            \"modified_at\": \"2021-10-27T13:13:42.421644Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"dfe4776f79ee4c23a85d86d3b0649127\",\n                \"created_at\": \"2020-11-12T15:43:07.560737Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"text\": \"TRANSCRIPT\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"audio\": \"AUDIO_SIGNAL\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"audio-to-text\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {},\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"ba585a5a737771884d89972fea6c41f8\",\n            \"name\": \"language-script\",\n            \"created_at\": \"2021-10-20T18:05:49.263669Z\",\n            \"modified_at\": \"2021-10-20T18:05:49.263669Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"60fa287837dc4c10a3b5da6b92e062e2\",\n                \"created_at\": \"2021-10-20T18:05:49.328969Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 8,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"concepts\": \"softmax\"\n                    },\n                    \"params\": {\n                        \"max_concepts\": 20,\n                        \"min_value\": 0,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"logo-detection\",\n            \"name\": \"logo\",\n            \"created_at\": \"2017-03-06T18:38:13.025998Z\",\n            \"modified_at\": \"2021-10-19T12:21:30.579234Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"ef1b7237d28b415f910ca343a9145e99\",\n                \"created_at\": \"2017-03-06T18:38:13.025998Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 561,\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_cls_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_cls_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_cls_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0.05\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"display_name\": \"logo-visual-detector\",\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"description\": \"Logo detection model for locating logos of some of the most popular consumer brands within images and videos.\",\n            \"metadata\": {\n                \"presetInputs\": [\n                    {\n                        \"alt\": \"\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/logo-detection-black-bmw-silver-volkswagon.jpg\"\n                    },\n                    {\n                        \"alt\": \"\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/logo-detection-corona-extra-beer-bottle.jpg\"\n                    },\n                    {\n                        \"alt\": \"\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/logo-detection-northface-pink-jacket.jpg\"\n                    },\n                    {\n                        \"alt\": \"\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/logo-detection-pepsi-bottles-in-crate.jpg\"\n                    },\n                    {\n                        \"alt\": \"\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/logo-detection-tesla-steering-wheel.jpg\"\n                    }\n                ]\n            },\n            \"notes\": \"I can edit\",\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"food-item-v1-recognition\",\n            \"name\": \"food-items-v1.0\",\n            \"created_at\": \"2016-09-17T04:22:07.183747Z\",\n            \"modified_at\": \"2021-10-18T16:34:36.410436Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"dfebc169854e429086aceb8368662641\",\n                \"created_at\": \"2016-09-17T04:22:07.183747Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 970,\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"concepts\": \"softmax\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"display_name\": \"food-items-v1-visual-classifier\",\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-classifier\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"description\": \"AI model to recognize a wide variety of food items, including dishes and ingredients, in images and video.\",\n            \"metadata\": {\n                \"presetInputs\": [\n                    {\n                        \"alt\": \"Clarifai food model featuring pan of steamed clams.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/food-pan-of-steamed-clams.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai food model featuring waffle with strawberries blueberries.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/food-waffle-with-strawberries-blueberries.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai food model featuring hamburgers bacon cheese buns.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/food-hamburgers-bacon-cheese-buns.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai food model featuring pepperoni pizza.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/food-pepperoni-pizza.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai food model featuring tomato basil.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/food-tomato-basil.jpg\"\n                    }\n                ]\n            },\n            \"notes\": \"This is a food model note (wip) \",\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"9fe78b4150a52794f86f237770141b33\",\n            \"name\": \"english\",\n            \"created_at\": \"2021-02-04T05:24:54.250897Z\",\n            \"modified_at\": \"2021-10-18T15:46:17.997145Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"89961b2723e440abb49ec89a05b31219\",\n                \"created_at\": \"2021-09-30T14:43:29.569715Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"license\": \"BSD-2\",\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"image\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"image-to-text\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"description\": \"--\",\n            \"metadata\": {},\n            \"notes\": \"test model note now\",\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [\n                \"demographics\"\n            ],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"c3110dc5905447e410161091f0f95337\",\n            \"name\": \"anas/wav2vec2-large-xlsr-arabic\",\n            \"created_at\": \"2021-10-14T19:23:19.862284Z\",\n            \"modified_at\": \"2021-10-14T19:23:19.862284Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"f486dde2e2d046dabfd4c9e4db2c8e36\",\n                \"created_at\": \"2021-10-14T19:23:19.869018Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"audio\": \"audio\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"audio-to-text\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"68bbf91b9ad247921822a255ca381f11\",\n            \"name\": \"elgeish/wav2vec2-large-xlsr-53-arabic\",\n            \"created_at\": \"2021-10-14T18:44:04.938624Z\",\n            \"modified_at\": \"2021-10-14T18:44:04.938624Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"749a7a96fb404596bc11465dc41c3fb2\",\n                \"created_at\": \"2021-10-14T18:44:04.946902Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"audio\": \"audio\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"audio-to-text\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"face-detection\",\n            \"name\": \"Face\",\n            \"created_at\": \"2020-11-20T17:09:42.109453Z\",\n            \"modified_at\": \"2021-10-14T07:45:05.937031Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"fe995da8cb73490f8556416ecf25cea3\",\n                \"created_at\": \"2021-01-21T23:31:28.004422Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 2,\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0.9\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"display_name\": \"Face-visual-detector\",\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"description\": \"AI model for detecting the location of human faces in images and video.\",\n            \"metadata\": {\n                \"presetInputs\": [\n                    {\n                        \"alt\": \"Clarifai face model featuring little girl boy standing outside.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/face-little-girl-boy-standing-outside.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai face model featuring three men sitting in van.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/face-three-men-sitting-in-van.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai face model featuring family with light blue shirts.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/face-family-with-light-blue-shirts.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai face model featuring arfrican american man woman laughing.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/face-arfrican-american-man-woman-laughing.jpg\"\n                    },\n                    {\n                        \"alt\": \"Clarifai face model featuring crowd of monks orange robe.\",\n                        \"type\": \"image\",\n                        \"url\": \"https://s3.amazonaws.com/samples.clarifai.com/featured-models/face-crowd-of-monks-orange-robe.jpg\"\n                    }\n                ]\n            },\n            \"notes\": \"test123\",\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [\n                \"faces\"\n            ],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [\n                \"pii\"\n            ],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"6a3dc529acf3f720a629cdc8c6ad41a9\",\n            \"name\": \"subject\",\n            \"created_at\": \"2021-04-26T09:20:01.359645Z\",\n            \"modified_at\": \"2021-10-12T12:36:06.443774Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"55b2051b75f14577b6fdd5a4fa3fd5a8\",\n                \"created_at\": \"2021-04-26T15:06:37.619220Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 2,\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].region_info.mask,regions[...].data.concepts\": \"output__0\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"input__0\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-segmenter\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"description\": \"test\",\n            \"metadata\": {},\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"beb9ed2d034b0126c6e24135ace76d8f\",\n            \"name\": \"prithivida/informal_to_formal_styletransfer\",\n            \"created_at\": \"2021-10-05T19:58:24.859525Z\",\n            \"modified_at\": \"2021-10-05T20:06:53.273373Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"a35db0e9b9b34f52ac0c83e1007db145\",\n                \"created_at\": \"2021-10-05T20:06:53.278139Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-to-text\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"69469f13c714cb6c68149db326d8c69a\",\n            \"name\": \"prithivida/formal_to_informal_styletransfer\",\n            \"created_at\": \"2021-10-05T19:45:07.925572Z\",\n            \"modified_at\": \"2021-10-05T19:45:07.925572Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"d34c3b68aadb408f9e770632183cb164\",\n                \"created_at\": \"2021-10-05T19:45:07.942034Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-to-text\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"b7e22ac73f924e2a6199af495724ddac\",\n            \"name\": \"facebook/wav2vec2-large-xlsr-53-french\",\n            \"created_at\": \"2021-10-05T17:18:44.727040Z\",\n            \"modified_at\": \"2021-10-05T17:18:44.727040Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"a648b13f7269461d9797e6bc58111a60\",\n                \"created_at\": \"2021-10-05T17:18:44.743898Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"audio\": \"audio\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"audio-to-text\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"6db1c2dc0d9c09d843c763bc0a05b989\",\n            \"name\": \"assembly\",\n            \"created_at\": \"2021-09-23T17:34:00.947876Z\",\n            \"modified_at\": \"2021-09-24T06:17:00.318253Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"5ef730b4ef014b508b31af5e5577386d\",\n                \"created_at\": \"2021-09-24T06:17:00.341875Z\",\n                \"status\": {\n                    \"code\": 99009,\n                    \"description\": \"Model upload timed out\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"embeddings\": \"embedding\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-embedder\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"2e099a9219da8fc580ac0dc54bf842fd\",\n            \"name\": \"paddleocr-multilingual-text-detector\",\n            \"created_at\": \"2021-08-19T00:40:40.789176Z\",\n            \"modified_at\": \"2021-09-22T19:50:23.797705Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"dff9af491f0d48449801decee0e2f136\",\n                \"created_at\": \"2021-09-22T19:50:23.809040Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 2,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0,\n                        \"max_concepts\": 20,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"6f96f8bf841280a388f8b52cb1868df4\",\n            \"name\": \"person-detector-yolov5x-libtorch\",\n            \"created_at\": \"2021-08-25T22:19:26.887486Z\",\n            \"modified_at\": \"2021-08-25T22:19:26.887486Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"8d1ac866905a4154b6d34ae2566503ad\",\n                \"created_at\": \"2021-08-25T22:19:26.962227Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 2,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0,\n                        \"max_concepts\": 20,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"a065882d92d66dbac3b5ebe108170197\",\n            \"name\": \"person-vehicle-detector-yolov5x-libtorch\",\n            \"created_at\": \"2021-08-25T17:56:25.508821Z\",\n            \"modified_at\": \"2021-08-25T21:51:12.368401Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"be2ea5c740f940429e6284826660da9a\",\n                \"created_at\": \"2021-08-25T21:51:12.377067Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 3,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0,\n                        \"max_concepts\": 20,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"2cf7739e65bfc63c1537f65e7ef3ae87\",\n            \"name\": \"person-vehicle-detector-yolov5s-libtorch\",\n            \"created_at\": \"2021-08-24T19:14:37.883020Z\",\n            \"modified_at\": \"2021-08-24T19:14:37.883020Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"3b6b338aa1b7433a8935d22b0915fc32\",\n                \"created_at\": \"2021-08-24T19:14:37.990646Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 3,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0,\n                        \"max_concepts\": 20,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"23aa4f9c9767a2fd61e63c55a73790ad\",\n            \"name\": \"person-detector-yolov5s-libtorch\",\n            \"created_at\": \"2021-08-24T18:52:53.766484Z\",\n            \"modified_at\": \"2021-08-24T18:52:53.766484Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"92b204309cce44209f1428e38e3406fb\",\n                \"created_at\": \"2023-02-24T05:41:57.516485Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 2,\n                \"description\": \"Yolov5\",\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0,\n                        \"max_concepts\": 20,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"bdcedc0f8da58c396b7df12f634ef923\",\n            \"name\": \"multilingual-moderation\",\n            \"created_at\": \"2021-01-13T16:45:48.370348Z\",\n            \"modified_at\": \"2021-07-13T15:12:21.536311Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"c56b46fd91ca4540823ba70496d008f9\",\n                \"created_at\": \"2021-01-13T20:45:50.641841Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 6,\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"concepts\": \"output__1\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"input__0\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-classifier\",\n            \"visibility\": {\n                \"gettable\": 50\n            },\n            \"metadata\": {},\n            \"toolkits\": [\n                \"Clarifai\"\n            ],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"eac94da66f428ebf98dc2cae30030699\",\n            \"name\": \"hate-symbols\",\n            \"created_at\": \"2021-07-07T20:24:28.707682Z\",\n            \"modified_at\": \"2021-07-08T15:06:49.454931Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"ca52d78ed8cb4cfba4eacebadc38548b\",\n                \"created_at\": \"2021-07-08T15:06:49.459168Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 2,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0.05,\n                        \"max_concepts\": 20,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"bc6b2c89a5dc35ee5b5872612d0df25a\",\n            \"name\": \"EasyOCR (Turkish)\",\n            \"created_at\": \"2021-06-17T19:51:18.060645Z\",\n            \"modified_at\": \"2021-06-17T19:51:18.060645Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"17ca4640290f4c3e885ed74e757272df\",\n                \"created_at\": \"2021-06-17T19:51:18.066247Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"869245fe9708e30e6a0869c6e3dc3132\",\n            \"name\": \"EasyOCR (Swedish)\",\n            \"created_at\": \"2021-06-17T19:49:51.218255Z\",\n            \"modified_at\": \"2021-06-17T19:49:51.218255Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"8eb77eec58314083a0b9765047974d45\",\n                \"created_at\": \"2021-06-17T19:49:51.224781Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"b24a775f7b9e156f3518772206c342ef\",\n            \"name\": \"EasyOCR (Polish)\",\n            \"created_at\": \"2021-06-17T19:45:42.334798Z\",\n            \"modified_at\": \"2021-06-17T19:45:42.334798Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"ce2624cd1b84445e988b1b437dfe2a95\",\n                \"created_at\": \"2021-06-17T19:45:42.340957Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"e6bcd24eb84835a8de347b7b8a028f27\",\n            \"name\": \"EasyOCR (Dutch)\",\n            \"created_at\": \"2021-06-17T19:43:57.103955Z\",\n            \"modified_at\": \"2021-06-17T19:43:57.103955Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"22669f6ea56e428f87465b55c9296ca5\",\n                \"created_at\": \"2021-06-17T19:43:57.109610Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"f43080883661c6676779384149eb9249\",\n            \"name\": \"EasyOCR (Malay)\",\n            \"created_at\": \"2021-06-17T19:37:49.087245Z\",\n            \"modified_at\": \"2021-06-17T19:37:49.087245Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"650fde39145f417db48b6270bfa78c2a\",\n                \"created_at\": \"2021-06-17T19:37:49.096062Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"a34ea557d25cc1c7b3b6bf080381eb04\",\n            \"name\": \"EasyOCR (Vietnamese)\",\n            \"created_at\": \"2021-06-17T19:32:48.422165Z\",\n            \"modified_at\": \"2021-06-17T19:32:48.422165Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"9dcfa47d32374f2684e5ed167ad0337a\",\n                \"created_at\": \"2021-06-17T19:32:48.431675Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.text\": \"predicted_det_text\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"optical-character-recognizer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"edd250a6e1f82cbee84819e3550dbaf4\",\n            \"name\": \"Helsinki-NLP/opus-mt-nl-en\",\n            \"created_at\": \"2021-05-28T16:30:24.060377Z\",\n            \"modified_at\": \"2021-05-28T16:30:24.060377Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"1183f7daaadd48969be19c0e7ad1c5ec\",\n                \"created_at\": \"2021-05-28T16:30:24.070688Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-to-text\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"68a51a726f7033bbfcf57c905f09b7ca\",\n            \"name\": \"general\",\n            \"created_at\": \"2021-04-19T16:03:17.091357Z\",\n            \"modified_at\": \"2021-05-20T08:13:55.391070Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"df708d35d2fa4dea9e9d3f76ce842450\",\n                \"created_at\": \"2021-05-20T08:13:55.399476Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 183,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].region_info.mask,regions[...].data.concepts\": \"output__0\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"input__0\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-segmenter\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"19de30b6d9c38ed8a0478ac5103efebe\",\n            \"name\": \"person-vehicle\",\n            \"created_at\": \"2021-05-14T19:50:52.826338Z\",\n            \"modified_at\": \"2021-05-18T18:59:59.867143Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"f06017161d0843dca0c6cf962cf08a11\",\n                \"created_at\": \"2021-05-18T18:59:59.876468Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 3,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0,\n                        \"max_concepts\": 20,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"332956f015ea667f81cef1f37b1a20f3\",\n            \"name\": \"person-vehicle-lite\",\n            \"created_at\": \"2021-05-14T19:33:26.027753Z\",\n            \"modified_at\": \"2021-05-18T18:55:51.389600Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"ef042aa9117141079f579db19809b1d3\",\n                \"created_at\": \"2021-05-18T18:55:51.401858Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 3,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0,\n                        \"max_concepts\": 20,\n                        \"select_concepts\": []\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"4236cc320afa91a7d6c53ec949b66785\",\n            \"name\": \"Helsinki-NLP/opus-mt-es-en\",\n            \"created_at\": \"2021-05-12T22:17:11.471812Z\",\n            \"modified_at\": \"2021-05-12T22:17:11.471812Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"6fff4d1143114416b47f278084f4ffc7\",\n                \"created_at\": \"2021-05-12T22:17:11.477501Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-to-text\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"31025e019a18970a1acc55ba6a184dc6\",\n            \"name\": \"face-sentiment\",\n            \"created_at\": \"2021-05-12T16:02:43.390981Z\",\n            \"modified_at\": \"2021-05-12T16:02:43.390981Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"edcf31cfa67e426a8b12cd889453f0c3\",\n                \"created_at\": \"2021-05-12T16:02:43.563574Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 7,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"concepts\": \"softmax\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"17a76b5162066195dad4c0437e66be80\",\n            \"name\": \"objectness-detector\",\n            \"created_at\": \"2021-05-11T17:35:00.699972Z\",\n            \"modified_at\": \"2021-05-11T17:35:00.699972Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"a488dd0eb0b94e16b22aa35656f4dd31\",\n                \"created_at\": \"2021-05-11T17:35:00.734676Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 1,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0.6\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"e609405a6ced78aa8a9eff2288f6edc6\",\n            \"name\": \"tank-rodeo\",\n            \"created_at\": \"2021-05-04T15:39:36.312115Z\",\n            \"modified_at\": \"2021-05-04T15:39:36.312115Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"745c6724c6a3456bb41ab0814070e191\",\n                \"created_at\": \"2021-05-04T15:39:36.537311Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 14,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"predicted_det_labels\",\n                        \"regions[...].data.concepts[...].value\": \"predicted_det_scores\",\n                        \"regions[...].region_info.bounding_box\": \"predicted_det_bboxes\"\n                    },\n                    \"params\": {\n                        \"detection_threshold\": 0.1\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-detector\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"385c7fd117d77553962b39629659d51a\",\n            \"name\": \"apparel\",\n            \"created_at\": \"2021-04-23T20:11:59.736603Z\",\n            \"modified_at\": \"2021-04-26T18:20:03.699907Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"c318a0b6769540e6bfe684af83560a9f\",\n                \"created_at\": \"2021-04-26T18:20:03.705751Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 192,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"concepts\": \"softmax\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"images\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"c833800b94175363881d4db8b55e4a52\",\n            \"name\": \"test20_ner\",\n            \"created_at\": \"2021-04-23T18:19:00.421074Z\",\n            \"modified_at\": \"2021-04-23T18:19:00.421074Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"aa16e51cc0ef427ab728928932fed6f6\",\n                \"created_at\": \"2021-04-23T18:19:00.654472Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 9,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"labels\",\n                        \"regions[...].data.concepts[...].value\": \"scores\",\n                        \"regions[...].data.text\": \"tokens\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-token-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"9de6872fa62a3118ce66c313e5c7d567\",\n            \"name\": \"test4_bert_base_ner\",\n            \"created_at\": \"2021-04-22T01:34:00.926953Z\",\n            \"modified_at\": \"2021-04-22T01:34:00.926953Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"b263587fd32f48f4a89a80271b5ce18f\",\n                \"created_at\": \"2021-04-22T01:34:01.026938Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 9,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"labels\",\n                        \"regions[...].data.concepts[...].value\": \"scores\",\n                        \"regions[...].data.text\": \"tokens\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-token-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"33128485c45b681a11380cea3933789a\",\n            \"name\": \"test2_bert_base_ner\",\n            \"created_at\": \"2021-04-21T20:34:07.150108Z\",\n            \"modified_at\": \"2021-04-21T20:34:07.150108Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"da4b266f7b654679b6b4ba8c8691c326\",\n                \"created_at\": \"2021-04-21T20:34:07.278954Z\",\n                \"status\": {\n                    \"code\": 99009,\n                    \"description\": \"Training Unknown Error\"\n                },\n                \"active_concept_count\": 9,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"labels\",\n                        \"regions[...].data.concepts[...].value\": \"scores\",\n                        \"regions[...].data.text\": \"tokens\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-token-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"48f9e31a431a24754f7db8bf221e1e41\",\n            \"name\": \"test_bert_base_ner\",\n            \"created_at\": \"2021-04-21T15:22:05.347218Z\",\n            \"modified_at\": \"2021-04-21T15:22:05.347218Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"e9c0b45944a24b6a9c3e77f0a10a836f\",\n                \"created_at\": \"2021-04-21T15:22:05.487673Z\",\n                \"status\": {\n                    \"code\": 99009,\n                    \"description\": \"Training Unknown Error\"\n                },\n                \"active_concept_count\": 9,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"labels\",\n                        \"regions[...].data.concepts[...].value\": \"scores\",\n                        \"regions[...].data.text\": \"tokens\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-token-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"b5632bca03e3aca4e0c948303935fb0d\",\n            \"name\": \"dslim/bert-base-NER\",\n            \"created_at\": \"2021-04-20T21:52:32.656339Z\",\n            \"modified_at\": \"2021-04-20T21:52:32.656339Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"ff092f6185c94800902d62ff68775a6e\",\n                \"created_at\": \"2021-04-20T21:52:32.810261Z\",\n                \"status\": {\n                    \"code\": 99009,\n                    \"description\": \"Training Unknown Error\"\n                },\n                \"active_concept_count\": 9,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts[...].id\": \"labels\",\n                        \"regions[...].data.concepts[...].value\": \"scores\",\n                        \"regions[...].data.text\": \"tokens\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"text\": \"text\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"text-token-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        },\n        {\n            \"id\": \"8db409c46a3a9153becc3565f1d99022\",\n            \"name\": \"Test Triton GPU Landmarks\",\n            \"created_at\": \"2021-01-15T21:31:47.307624Z\",\n            \"modified_at\": \"2021-04-12T20:07:18.744912Z\",\n            \"app_id\": \"main\",\n            \"model_version\": {\n                \"id\": \"cf33eda7f56f4d389ffac719cdf82da4\",\n                \"created_at\": \"2021-04-12T20:07:18.749574Z\",\n                \"status\": {\n                    \"code\": 21100,\n                    \"description\": \"Model is trained and ready\"\n                },\n                \"active_concept_count\": 1,\n                \"visibility\": {\n                    \"gettable\": 10\n                },\n                \"app_id\": \"main\",\n                \"user_id\": \"clarifai\",\n                \"metadata\": {},\n                \"output_info\": {\n                    \"output_config\": {\n                        \"max_concepts\": 0,\n                        \"min_value\": 0\n                    },\n                    \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                    \"fields_map\": {\n                        \"regions[...].data.concepts,regions[...].region_info.keypoint_locations\": \"output__0\"\n                    }\n                },\n                \"input_info\": {\n                    \"fields_map\": {\n                        \"image\": \"input__0\"\n                    }\n                },\n                \"train_info\": {},\n                \"import_info\": {}\n            },\n            \"user_id\": \"clarifai\",\n            \"model_type_id\": \"visual-keypointer\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        }\n    ]\n}"
                  }
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/models' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Authorization: Key {{key}}'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/models/{model_id}/versions": {
      "post": {
        "tags": [
          "Walkthroughs > RAG"
        ],
        "summary": "Create Model Version",
        "description": "This endpoint allows users to create a model version by providing various parameters that help in training the model.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `model_id` | **string** | **Stores the Model ID** |\n| `model_version_id` | **string** | **Stores the model version** |\n| `dataset_id` | **string** | **Stores the dataset name** |\n| `num_gpus` | **string** | **Stores the number of GPU's to use for training** |\n| `pretrained_model_name` | **string** | **Stores the name of a pre-trained model** |\n| `template` | **string** | **Stores the template used for training** |\n| `concept_id` | **string** | **Stores the name of concepts** |",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "example": {
                  "model_versions": [
                    {
                      "output_info": {
                        "params": {
                          "min_score": 0.7,
                          "max_results": 5,
                          "prompt_template": "Context information is below:\n{data.hits}\nGiven the context information and not prior knowledge, answer the query.\nQuery: {data.text.raw}\nAnswer: "
                        }
                      }
                    }
                  ]
                }
              }
            }
          }
        },
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "model_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "200": {
            "description": "OK",
            "headers": {
              "Date": {
                "schema": {
                  "type": "string",
                  "example": "Thu, 02 May 2024 08:51:33 GMT"
                }
              },
              "Content-Type": {
                "schema": {
                  "type": "string",
                  "example": "application/json; charset=UTF-8"
                }
              },
              "Content-Length": {
                "schema": {
                  "type": "integer",
                  "example": "1167"
                }
              },
              "Connection": {
                "schema": {
                  "type": "string",
                  "example": "keep-alive"
                }
              },
              "Access-Control-Allow-Headers": {
                "schema": {
                  "type": "string",
                  "example": "Content-Type,Accept,X-Requested-With,Content-Type,Referer,Accept-Encoding,X-CSRF-Token,Authorization,X-Clarifai-Application-Id,X-Clarifai-REST-API-Key,X-Clarifai-Session-Token,X-Clarifai-Client,X-Clarifai-Site,X-RapidAPI-User,x-clarifai-request-id-prefix,x-request-id"
                }
              },
              "Access-Control-Allow-Methods": {
                "schema": {
                  "type": "string",
                  "example": "GET,HEAD,POST,PUT,PATCH,OPTIONS,DELETE"
                }
              },
              "Access-Control-Allow-Origin": {
                "schema": {
                  "type": "string",
                  "example": "*"
                }
              },
              "Grpc-Metadata-Content-Type": {
                "schema": {
                  "type": "string",
                  "example": "application/grpc"
                }
              },
              "X-Clarifai-Request-Id": {
                "schema": {
                  "type": "string",
                  "example": "1d349062b444477d888eeea83e7d5bbc"
                }
              }
            },
            "content": {
              "application/json": {
                "schema": {
                  "type": "object"
                },
                "example": {
                  "status": {
                    "code": 10000,
                    "description": "Ok",
                    "req_id": "1d349062b444477d888eeea83e7d5bbc"
                  },
                  "model": {
                    "id": "rag_prompter",
                    "name": "rag_prompter",
                    "created_at": "2024-05-02T08:48:42.363415Z",
                    "modified_at": "2024-05-02T08:48:42.363415Z",
                    "app_id": "rag-app-1714639564",
                    "model_version": {
                      "id": "6924fd5efffa49e1947a4c4b0b02bd52",
                      "created_at": "2024-05-02T08:51:33.206443200Z",
                      "status": {
                        "code": 21100,
                        "description": "Model is trained and ready for deployment"
                      },
                      "visibility": {
                        "gettable": 10
                      },
                      "app_id": "rag-app-1714639564",
                      "user_id": "clarifai",
                      "metadata": {},
                      "output_info": {
                        "output_config": {
                          "max_concepts": 0,
                          "min_value": 0
                        },
                        "message": "Show output_info with: GET /models/{model_id}/output_info",
                        "params": {
                          "max_results": 5,
                          "min_score": 0.7,
                          "prompt_template": "Context information is below:\n{data.hits}\nGiven the context information and not prior knowledge, answer the query.\nQuery: {data.text.raw}\nAnswer: "
                        }
                      },
                      "input_info": {
                        "params": {}
                      },
                      "train_info": {},
                      "import_info": {}
                    },
                    "user_id": "clarifai",
                    "model_type_id": "rag-prompter",
                    "visibility": {
                      "gettable": 10
                    },
                    "metadata": {},
                    "presets": {},
                    "toolkits": [],
                    "use_cases": [],
                    "languages": [],
                    "languages_full": [],
                    "check_consents": [],
                    "workflow_recommended": false
                  }
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/models/{model_id}/versions' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Content-Type: application/json' \\\n--data '{\n  \"model_versions\": [\n    {\n      \"output_info\": {\n        \"params\": {\n          \"min_score\": 0.7,\n          \"max_results\": 5,\n          \"prompt_template\": \"Context information is below:\\n{data.hits}\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: {data.text.raw}\\nAnswer: \"\n        }\n      }\n    }\n  ]\n}'"
          }
        ]
      },
      "get": {
        "tags": [
          "Models"
        ],
        "summary": "List Versions By Concepts",
        "description": "By using this request you can list model versions related to specific concepts.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application name** |\n| `model_id` | **string** | **Stores the Model ID** |",
        "parameters": [
          {
            "name": "Authorization",
            "in": "header",
            "schema": {
              "type": "string"
            },
            "example": "Key {{key}}"
          },
          {
            "name": "page",
            "in": "query",
            "schema": {
              "type": "integer"
            },
            "example": "1"
          },
          {
            "name": "per_page",
            "in": "query",
            "schema": {
              "type": "integer"
            },
            "example": "30"
          },
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "model_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"bf2ccc20d03a8f0a9405313e37971a5b\"\n    },\n    \"model_versions\": [\n        {\n            \"id\": \"1e4c121974f849209abb658cdf682585\",\n            \"created_at\": \"2023-11-23T09:41:18.470087Z\",\n            \"status\": {\n                \"code\": 21110,\n                \"description\": \"datasets.dataset.DataBatchEmpty: No databatch found in train set's file directory\\nFailed to create a training dataset, because there are no appropriately annotated inputs. Expected annotations with concepts for model type id text-classifier. \"\n            },\n            \"active_concept_count\": 6,\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"metadata\": {},\n            \"output_info\": {\n                \"output_config\": {\n                    \"max_concepts\": 0,\n                    \"min_value\": 0\n                },\n                \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                \"params\": {\n                    \"max_concepts\": 20,\n                    \"min_value\": 0,\n                    \"select_concepts\": []\n                }\n            },\n            \"input_info\": {},\n            \"train_info\": {\n                \"params\": {\n                    \"dataset_id\": \"\",\n                    \"dataset_version_id\": \"\",\n                    \"invalid_data_tolerance_percent\": 5,\n                    \"model_config\": {\n                        \"pretrained_model_name\": \"EleutherAI/gpt-neo-125m\"\n                    },\n                    \"num_gpus\": 1,\n                    \"peft_config\": {\n                        \"peft_type\": \"LORA\"\n                    },\n                    \"template\": \"HF_GPTNeo_125m_lora\",\n                    \"tokenizer_config\": {},\n                    \"trainer_config\": {\n                        \"auto_find_batch_size\": true,\n                        \"num_train_epochs\": 20,\n                        \"output_dir\": \"checkpoint\"\n                    }\n                }\n            },\n            \"import_info\": {}\n        }\n    ]\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/models/{model_id}/versions?page=1&per_page=30' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Authorization: Key {{key}}'"
          }
        ]
      }
    },
    "/v2/inputs": {
      "post": {
        "tags": [
          "Models > Create Text to Text LLM Model"
        ],
        "summary": "Add Text",
        "requestBody": {
          "content": {
            "*/*": {
              "schema": {
                "type": "string",
                "example": "\"{\\n    \\\"inputs\\\": [\\n        {\\n            \\\"data\\\": {\\n                \\\"text\\\": {\\n                    \\\"raw\\\": \\\"Marie is a published author.\\\",\\n                    \\\"allow_duplicate_url\\\": true\\n                }\\n            }\\n        },\\n        {\\n            \\\"data\\\": {\\n                \\\"text\\\": {\\n                    \\\"raw\\\": \\\"In three years, everyone will be happy.\\\",\\n                    \\\"allow_duplicate_url\\\": true\\n                }\\n            }\\n        },\\n        {\\n            \\\"data\\\": {\\n                \\\"text\\\": {\\n                    \\\"raw\\\": \\\"Nora Roberts is the most prolific romance writer the world has ever known.\\\",\\n                    \\\"allow_duplicate_url\\\": true\\n                }\\n            }\\n        },\\n        {\\n            \\\"data\\\": {\\n                \\\"text\\\": {\\n                    \\\"raw\\\": \\\"She has written more than 225 books.\\\",\\n                    \\\"allow_duplicate_url\\\": true\\n                }\\n            }\\n        },\\n        {\\n            \\\"data\\\": {\\n                \\\"text\\\": {\\n                    \\\"raw\\\": \\\"If you walk into Knoxville, you'll find a shop named Rala.\\\",\\n                    \\\"allow_duplicate_url\\\": true\\n                }\\n            }\\n        },\\n        {\\n            \\\"data\\\": {\\n                \\\"text\\\": {\\n                    \\\"raw\\\": \\\"There are more than 850 miles of hiking trails in the Great Smoky Mountains.\\\",\\n                    \\\"allow_duplicate_url\\\": true\\n                }\\n            }\\n        },\\n        {\\n            \\\"data\\\": {\\n                \\\"text\\\": {\\n                    \\\"raw\\\": \\\"Harrison Ford is 6'1\\\\\\\".\\\",\\n                    \\\"allow_duplicate_url\\\": true\\n                }\\n            }\\n        },\\n        {\\n            \\\"data\\\": {\\n                \\\"text\\\": {\\n                    \\\"raw\\\": \\\"According to Reader's Digest, in the original script of Return of The Jedi, Han Solo died.\\\",\\n                    \\\"allow_duplicate_url\\\": true\\n                }\\n            }\\n        },\\n        {\\n            \\\"data\\\": {\\n                \\\"text\\\": {\\n                    \\\"raw\\\": \\\"Kate travels to Doolin, Ireland every year for a writers' conference.\\\",\\n                    \\\"allow_duplicate_url\\\": true\\n                }\\n            }\\n        },\\n        {\\n            \\\"data\\\": {\\n                \\\"text\\\": {\\n                    \\\"raw\\\": \\\"Fort Stevens was decommissioned by the United States military in 1947.\\\",\\n                    \\\"allow_duplicate_url\\\": true\\n                }\\n            }\\n        },\\n        {\\n            \\\"data\\\": {\\n                \\\"text\\\": {\\n                    \\\"url\\\": \\\"https://samples.clarifai.com/negative_sentence_1.txt\\\",\\n                    \\\"allow_duplicate_url\\\": true\\n                }\\n            }\\n        },\\n        {\\n            \\\"data\\\": {\\n                \\\"text\\\": {\\n                    \\\"url\\\": \\\"https://samples.clarifai.com/negative_sentence_2.txt\\\",\\n                    \\\"allow_duplicate_url\\\": true\\n                }\\n            }\\n        },\\n        {\\n            \\\"data\\\": {\\n                \\\"text\\\": {\\n                    \\\"url\\\": \\\"https://samples.clarifai.com/negative_sentence_3.txt\\\",\\n                    \\\"allow_duplicate_url\\\": true\\n                }\\n            }\\n        },\\n        {\\n            \\\"data\\\": {\\n                \\\"text\\\": {\\n                    \\\"url\\\": \\\"https://samples.clarifai.com/negative_sentence_4.txt\\\",\\n                    \\\"allow_duplicate_url\\\": true\\n                }\\n            }\\n        },\\n        {\\n            \\\"data\\\": {\\n                \\\"text\\\": {\\n                    \\\"url\\\": \\\"https://samples.clarifai.com/negative_sentence_5.txt\\\",\\n                    \\\"allow_duplicate_url\\\": true\\n                }\\n            }\\n        },\\n        {\\n            \\\"data\\\": {\\n                \\\"text\\\": {\\n                    \\\"url\\\": \\\"https://samples.clarifai.com/negative_sentence_6.txt\\\",\\n                    \\\"allow_duplicate_url\\\": true\\n                }\\n            }\\n        },\\n        {\\n            \\\"data\\\": {\\n                \\\"text\\\": {\\n                    \\\"url\\\": \\\"https://samples.clarifai.com/negative_sentence_7.txt\\\",\\n                    \\\"allow_duplicate_url\\\": true\\n                }\\n            }\\n        },\\n        {\\n            \\\"data\\\": {\\n                \\\"text\\\": {\\n                    \\\"url\\\": \\\"https://samples.clarifai.com/negative_sentence_8.txt\\\",\\n                    \\\"allow_duplicate_url\\\": true\\n                }\\n            }\\n        },\\n        {\\n            \\\"data\\\": {\\n                \\\"text\\\": {\\n                    \\\"url\\\": \\\"https://samples.clarifai.com/negative_sentence_9.txt\\\",\\n                    \\\"allow_duplicate_url\\\": true\\n                }\\n            }\\n        },\\n        {\\n            \\\"data\\\": {\\n                \\\"text\\\": {\\n                    \\\"url\\\": \\\"https://samples.clarifai.com/negative_sentence_10.txt\\\",\\n                    \\\"allow_duplicate_url\\\": true\\n                }\\n            }\\n        }\\n    ]\\n}\""
              }
            }
          }
        },
        "parameters": [
          {
            "name": "Content-Type",
            "in": "header",
            "schema": {
              "type": "string"
            },
            "example": "application/json"
          },
          {
            "name": "Authorization",
            "in": "header",
            "schema": {
              "type": "string"
            },
            "example": "Key {{key}}"
          }
        ],
        "responses": {
          "200": {
            "description": "Successful response",
            "content": {
              "application/json": {}
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --request POST 'https://api.clarifai.com/v2/inputs' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Key {{key}}'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/models/{model_id}/versions/{version_id}/outputs": {
      "post": {
        "tags": [
          "Models > Create Visual Classifier Model Copy"
        ],
        "summary": "Predict On Image",
        "description": "## Parameters\n\n| Name | Type | Description |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| **`model_id`** | **string** | **Stores the Model ID** |\n| **`version_id`** | **string** | **Stores the Model Version ID** |\n| `inputs[0].data.image.url` | **string** | **Stores the URL of the Image Input** |",
        "requestBody": {
          "content": {
            "*/*": {
              "schema": {
                "type": "string",
                "example": "\"{\\n    \\\"inputs\\\":[\\n        {\\n            \\\"data\\\": {\\n                \\\"image\\\": {\\\"url\\\": \\\"https://s3.amazonaws.com/clarifai-img/a3/05/dc/b142653346b98ed0a4998c157f.jpg\\\"}\\n            }\\n        }\\n    ]\\n}\""
              }
            }
          }
        },
        "parameters": [
          {
            "name": "Authorization",
            "in": "header",
            "schema": {
              "type": "string"
            },
            "example": "Key {{key}}"
          },
          {
            "name": "Content-Type",
            "in": "header",
            "schema": {
              "type": "string"
            },
            "example": "application/json"
          },
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "model_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "version_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "200": {
            "description": "Successful response",
            "content": {
              "application/json": {}
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff --request POST 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/models/{model_id}/versions/{version_id}/outputs' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Authorization: Key {{key}}' \\\n--header 'Content-Type: application/json'"
          }
        ]
      }
    },
    "/v2/models/{model_id}/versions/{version_id}/outputs": {
      "post": {
        "tags": [
          "Models > Create Text to Text LLM Model"
        ],
        "summary": "Predict on Text",
        "requestBody": {
          "content": {
            "*/*": {
              "schema": {
                "type": "string",
                "example": "\"{\\n    \\\"inputs\\\":[\\n        {\\n            \\\"data\\\": {\\n                \\\"text\\\": {\\\"raw\\\": \\\"Butchart Gardens contains over 900 varieties of plants.\\\"}\\n            }\\n        },\\n        {\\n        \\t\\\"data\\\": {\\n                \\\"text\\\": {\\\"url\\\": \\\"https://samples.clarifai.com/negative_sentence_12.txt\\\"}\\n            }\\n        }\\n    ]\\n}\""
              }
            }
          }
        },
        "parameters": [
          {
            "name": "Authorization",
            "in": "header",
            "schema": {
              "type": "string"
            },
            "example": "Key {{key}}"
          },
          {
            "name": "Content-Type",
            "in": "header",
            "schema": {
              "type": "string"
            },
            "example": "application/json"
          },
          {
            "name": "model_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "version_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "200": {
            "description": "Successful response",
            "content": {
              "application/json": {}
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff --request POST 'https://api.clarifai.com/v2/models/{model_id}/versions/{version_id}/outputs' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Authorization: Key {{key}}' \\\n--header 'Content-Type: application/json'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/models/ids": {
      "patch": {
        "tags": [
          "Models"
        ],
        "summary": "Update Model ID",
        "description": "With this endpoint, you can change the old model ID with a new one.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application name** |\n| `ids[0].id` | **string** | **Stores the old Model ID** |\n| `ids[0].new_id` | **string** | **Stores the new Model ID** |\n| `action` | **string** | **Stores the action to take while patching** |",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "example": {
                  "ids": [
                    {
                      "id": "burgerz",
                      "new_id": "named-entity-recognition-diseases-english-text"
                    }
                  ],
                  "action": "overwrite"
                }
              }
            }
          }
        },
        "parameters": [
          {
            "name": "Authorization",
            "in": "header",
            "schema": {
              "type": "string"
            },
            "example": "Key {{key}}"
          },
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"fa3c14f038a9d9f2224c41ed2183f632\"\n    },\n    \"models\": [\n        {\n            \"id\": \"named-entity-recognition-diseases-english-text\",\n            \"name\": \"burgerz\",\n            \"created_at\": \"2023-11-23T11:15:32.431806Z\",\n            \"modified_at\": \"2023-11-23T11:16:02.900299Z\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"model_type_id\": \"embedding-classifier\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"metadata\": {},\n            \"presets\": {},\n            \"toolkits\": [],\n            \"use_cases\": [],\n            \"languages\": [],\n            \"languages_full\": [],\n            \"check_consents\": [],\n            \"workflow_recommended\": false\n        }\n    ]\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff --request PATCH 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/models/ids' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Authorization: Key {{key}}' \\\n--header 'Content-Type: application/json' \\\n--data '{\n  \"ids\": [\n    {\n      \"id\": \"burgerz\",\n      \"new_id\": \"named-entity-recognition-diseases-english-text\"\n    }\n  ],\n  \"action\": \"overwrite\"\n}'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/models/{model_id}/outputs": {
      "post": {
        "tags": [
          "Models"
        ],
        "summary": "Predict",
        "description": "This endpoint allows users to submit input data for prediction.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application name** |\n| **`model_id`** | **string** | **Stores the ID of the model to predict with** |\n| `inputs[0].data.image.url` | **string** | **Stores the URL to the data** |",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "example": {
                  "inputs": [
                    {
                      "data": {
                        "image": {
                          "url": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSnzXSi3jhSHbFdRu-ldBQ7B7d65QkQgHSoWA&usqp=CAU"
                        }
                      }
                    }
                  ]
                }
              }
            }
          }
        },
        "parameters": [
          {
            "name": "Authorization",
            "in": "header",
            "schema": {
              "type": "string"
            },
            "example": "Key {{key}}"
          },
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "model_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "examples": {
                  "example-0": {
                    "summary": "General Image Recognition",
                    "value": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"2c89487b3d6253a2df7d997209ab3c1d\"\n    },\n    \"outputs\": [\n        {\n            \"id\": \"4b999bc1f0b748b083af816b726f9577\",\n            \"status\": {\n                \"code\": 10000,\n                \"description\": \"Ok\"\n            },\n            \"created_at\": \"2023-11-23T11:20:25.638700893Z\",\n            \"model\": {\n                \"id\": \"apparel-recognition\",\n                \"name\": \"apparel\",\n                \"created_at\": \"2016-12-15T01:29:04.622209Z\",\n                \"modified_at\": \"2023-05-23T12:34:15.093542Z\",\n                \"app_id\": \"main\",\n                \"model_version\": {\n                    \"id\": \"dc2cd6d9bff5425a80bfe0c4105583c1\",\n                    \"created_at\": \"2016-12-15T01:29:04.622209Z\",\n                    \"status\": {\n                        \"code\": 21100,\n                        \"description\": \"Model is trained and ready\"\n                    },\n                    \"visibility\": {\n                        \"gettable\": 50\n                    },\n                    \"app_id\": \"main\",\n                    \"user_id\": \"clarifai\",\n                    \"metadata\": {}\n                },\n                \"display_name\": \"apparel-visual-classifier\",\n                \"user_id\": \"clarifai\",\n                \"model_type_id\": \"visual-classifier\",\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"toolkits\": [],\n                \"use_cases\": [],\n                \"languages\": [],\n                \"languages_full\": [],\n                \"check_consents\": [],\n                \"workflow_recommended\": false\n            },\n            \"input\": {\n                \"id\": \"c241fdade2284e7b84d6004948a3421f\",\n                \"data\": {\n                    \"image\": {\n                        \"url\": \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSnzXSi3jhSHbFdRu-ldBQ7B7d65QkQgHSoWA&usqp=CAU\"\n                    }\n                }\n            },\n            \"data\": {\n                \"concepts\": [\n                    {\n                        \"id\": \"ai_bXKvhw9n\",\n                        \"name\": \"Polos\",\n                        \"value\": 0.9338605,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_PxHDNZ7W\",\n                        \"name\": \"T-Shirt\",\n                        \"value\": 0.9111426,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_DB9fg9zx\",\n                        \"name\": \"Activewear T Shirt\",\n                        \"value\": 0.505948,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_fFlQDW0c\",\n                        \"name\": \"Blouse\",\n                        \"value\": 0.021622758,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_6wNWp7sV\",\n                        \"name\": \"Sleepwear\",\n                        \"value\": 0.017354352,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_ZdKP9568\",\n                        \"name\": \"Button-Down\",\n                        \"value\": 0.016567573,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_14DHBZb8\",\n                        \"name\": \"Sweatshirt\",\n                        \"value\": 0.015236078,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_zTGRW4d6\",\n                        \"name\": \"Blazer\",\n                        \"value\": 0.014492731,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_2fQ7dxwC\",\n                        \"name\": \"Tube Top\",\n                        \"value\": 0.013895297,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_sLXGd60v\",\n                        \"name\": \"Turtleneck\",\n                        \"value\": 0.012984616,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_fV17Rh8q\",\n                        \"name\": \"Sweater\",\n                        \"value\": 0.011040202,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_x1X1tNs5\",\n                        \"name\": \"One-Piece\",\n                        \"value\": 0.010136624,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_hN5FhfT7\",\n                        \"name\": \"Cardigan\",\n                        \"value\": 0.008903351,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_Q7xfVXVM\",\n                        \"name\": \"Tie\",\n                        \"value\": 0.00797598,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_BCpGJVsP\",\n                        \"name\": \"Halter Top\",\n                        \"value\": 0.007962421,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_zLQ06vpb\",\n                        \"name\": \"Men's Shorts\",\n                        \"value\": 0.0075161625,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_X9BxQ57Z\",\n                        \"name\": \"Kimono\",\n                        \"value\": 0.0073983357,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_Lvn6gxp9\",\n                        \"name\": \"Cocktail Dress\",\n                        \"value\": 0.006994879,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_BK6xHcfw\",\n                        \"name\": \"Hoodies\",\n                        \"value\": 0.006424995,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_FK40S8BP\",\n                        \"name\": \"Swimwear\",\n                        \"value\": 0.0057420935,\n                        \"app_id\": \"main\"\n                    }\n                ]\n            }\n        }\n    ]\n}"
                  },
                  "example-1": {
                    "summary": "Image Apparel Recognition",
                    "value": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"2c89487b3d6253a2df7d997209ab3c1d\"\n    },\n    \"outputs\": [\n        {\n            \"id\": \"4b999bc1f0b748b083af816b726f9577\",\n            \"status\": {\n                \"code\": 10000,\n                \"description\": \"Ok\"\n            },\n            \"created_at\": \"2023-11-23T11:20:25.638700893Z\",\n            \"model\": {\n                \"id\": \"apparel-recognition\",\n                \"name\": \"apparel\",\n                \"created_at\": \"2016-12-15T01:29:04.622209Z\",\n                \"modified_at\": \"2023-05-23T12:34:15.093542Z\",\n                \"app_id\": \"main\",\n                \"model_version\": {\n                    \"id\": \"dc2cd6d9bff5425a80bfe0c4105583c1\",\n                    \"created_at\": \"2016-12-15T01:29:04.622209Z\",\n                    \"status\": {\n                        \"code\": 21100,\n                        \"description\": \"Model is trained and ready\"\n                    },\n                    \"visibility\": {\n                        \"gettable\": 50\n                    },\n                    \"app_id\": \"main\",\n                    \"user_id\": \"clarifai\",\n                    \"metadata\": {}\n                },\n                \"display_name\": \"apparel-visual-classifier\",\n                \"user_id\": \"clarifai\",\n                \"model_type_id\": \"visual-classifier\",\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"toolkits\": [],\n                \"use_cases\": [],\n                \"languages\": [],\n                \"languages_full\": [],\n                \"check_consents\": [],\n                \"workflow_recommended\": false\n            },\n            \"input\": {\n                \"id\": \"c241fdade2284e7b84d6004948a3421f\",\n                \"data\": {\n                    \"image\": {\n                        \"url\": \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSnzXSi3jhSHbFdRu-ldBQ7B7d65QkQgHSoWA&usqp=CAU\"\n                    }\n                }\n            },\n            \"data\": {\n                \"concepts\": [\n                    {\n                        \"id\": \"ai_bXKvhw9n\",\n                        \"name\": \"Polos\",\n                        \"value\": 0.9338605,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_PxHDNZ7W\",\n                        \"name\": \"T-Shirt\",\n                        \"value\": 0.9111426,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_DB9fg9zx\",\n                        \"name\": \"Activewear T Shirt\",\n                        \"value\": 0.505948,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_fFlQDW0c\",\n                        \"name\": \"Blouse\",\n                        \"value\": 0.021622758,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_6wNWp7sV\",\n                        \"name\": \"Sleepwear\",\n                        \"value\": 0.017354352,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_ZdKP9568\",\n                        \"name\": \"Button-Down\",\n                        \"value\": 0.016567573,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_14DHBZb8\",\n                        \"name\": \"Sweatshirt\",\n                        \"value\": 0.015236078,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_zTGRW4d6\",\n                        \"name\": \"Blazer\",\n                        \"value\": 0.014492731,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_2fQ7dxwC\",\n                        \"name\": \"Tube Top\",\n                        \"value\": 0.013895297,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_sLXGd60v\",\n                        \"name\": \"Turtleneck\",\n                        \"value\": 0.012984616,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_fV17Rh8q\",\n                        \"name\": \"Sweater\",\n                        \"value\": 0.011040202,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_x1X1tNs5\",\n                        \"name\": \"One-Piece\",\n                        \"value\": 0.010136624,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_hN5FhfT7\",\n                        \"name\": \"Cardigan\",\n                        \"value\": 0.008903351,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_Q7xfVXVM\",\n                        \"name\": \"Tie\",\n                        \"value\": 0.00797598,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_BCpGJVsP\",\n                        \"name\": \"Halter Top\",\n                        \"value\": 0.007962421,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_zLQ06vpb\",\n                        \"name\": \"Men's Shorts\",\n                        \"value\": 0.0075161625,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_X9BxQ57Z\",\n                        \"name\": \"Kimono\",\n                        \"value\": 0.0073983357,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_Lvn6gxp9\",\n                        \"name\": \"Cocktail Dress\",\n                        \"value\": 0.006994879,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_BK6xHcfw\",\n                        \"name\": \"Hoodies\",\n                        \"value\": 0.006424995,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_FK40S8BP\",\n                        \"name\": \"Swimwear\",\n                        \"value\": 0.0057420935,\n                        \"app_id\": \"main\"\n                    }\n                ]\n            }\n        }\n    ]\n}"
                  },
                  "example-2": {
                    "summary": "Predict Video URL",
                    "value": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"b2064802534cb8f782eb97ac3954bf89\"\n    },\n    \"outputs\": [\n        {\n            \"id\": \"a6cc0e08506e473496121a22f84a2401\",\n            \"status\": {\n                \"code\": 10000,\n                \"description\": \"Ok\"\n            },\n            \"created_at\": \"2023-11-23T07:59:45.725443804Z\",\n            \"model\": {\n                \"id\": \"apparel-recognition\",\n                \"name\": \"apparel\",\n                \"created_at\": \"2016-12-15T01:29:04.622209Z\",\n                \"modified_at\": \"2023-05-23T12:34:15.093542Z\",\n                \"app_id\": \"main\",\n                \"model_version\": {\n                    \"id\": \"dc2cd6d9bff5425a80bfe0c4105583c1\",\n                    \"created_at\": \"2016-12-15T01:29:04.622209Z\",\n                    \"status\": {\n                        \"code\": 21100,\n                        \"description\": \"Model is trained and ready\"\n                    },\n                    \"visibility\": {\n                        \"gettable\": 50\n                    },\n                    \"app_id\": \"main\",\n                    \"user_id\": \"clarifai\",\n                    \"metadata\": {}\n                },\n                \"display_name\": \"apparel-visual-classifier\",\n                \"user_id\": \"clarifai\",\n                \"model_type_id\": \"visual-classifier\",\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"toolkits\": [],\n                \"use_cases\": [],\n                \"languages\": [],\n                \"languages_full\": [],\n                \"check_consents\": [],\n                \"workflow_recommended\": false\n            },\n            \"input\": {\n                \"id\": \"479a233a634d4e2e9c05098d6bfae6a2\",\n                \"data\": {\n                    \"video\": {\n                        \"url\": \"https://samples.clarifai.com/beer.mp4\"\n                    }\n                }\n            },\n            \"data\": {\n                \"frames\": [\n                    {\n                        \"frame_info\": {\n                            \"index\": 0,\n                            \"time\": 500\n                        },\n                        \"data\": {\n                            \"concepts\": [\n                                {\n                                    \"id\": \"ai_4NsJqZTB\",\n                                    \"name\": \"Ring\",\n                                    \"value\": 0.90268916,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_6nF00KsR\",\n                                    \"name\": \"Men's Dress Shoes\",\n                                    \"value\": 0.39901596,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_MZz07Pnw\",\n                                    \"name\": \"Bracelet\",\n                                    \"value\": 0.31126797,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_0PM9RnsJ\",\n                                    \"name\": \"Pumps\",\n                                    \"value\": 0.26319304,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_z5F7XFlb\",\n                                    \"name\": \"Platform Shoes\",\n                                    \"value\": 0.25784707,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_GWJG9K9S\",\n                                    \"name\": \"Wallet\",\n                                    \"value\": 0.25010008,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_dt6mFvGt\",\n                                    \"name\": \"Sunglasses\",\n                                    \"value\": 0.23802476,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_mtd6x6QM\",\n                                    \"name\": \"Flats\",\n                                    \"value\": 0.21309257,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_SGXgBwzJ\",\n                                    \"name\": \"Sneakers\",\n                                    \"value\": 0.19622034,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_P2cKDGMW\",\n                                    \"name\": \"Clogs\",\n                                    \"value\": 0.18752512,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_Xfs5V5DH\",\n                                    \"name\": \"Women's Boots\",\n                                    \"value\": 0.15672876,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_hlBWBcP8\",\n                                    \"name\": \"Men's Boots\",\n                                    \"value\": 0.14916864,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_fpjq5JLS\",\n                                    \"name\": \"Knee Length Skirt\",\n                                    \"value\": 0.13177338,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_cjhVr9Tf\",\n                                    \"name\": \"Necklace\",\n                                    \"value\": 0.118617296,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_T8DMpsgc\",\n                                    \"name\": \"Midi Skirt\",\n                                    \"value\": 0.09912549,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_Z79pShcW\",\n                                    \"name\": \"Oxfords\",\n                                    \"value\": 0.09695284,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_0zhMPWP5\",\n                                    \"name\": \"Wedding Dress\",\n                                    \"value\": 0.08764245,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_LLG9S3Fg\",\n                                    \"name\": \"Women's Sandals\",\n                                    \"value\": 0.07857549,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_kPdHSQBj\",\n                                    \"name\": \"Mini Skirt\",\n                                    \"value\": 0.0712471,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_x9xC4G2r\",\n                                    \"name\": \"Tote Bag\",\n                                    \"value\": 0.060722828,\n                                    \"app_id\": \"main\"\n                                }\n                            ]\n                        },\n                        \"id\": \"faa9f3d5c8569123d8bea365ea478031\"\n                    },\n                    {\n                        \"frame_info\": {\n                            \"index\": 1,\n                            \"time\": 1500\n                        },\n                        \"data\": {\n                            \"concepts\": [\n                                {\n                                    \"id\": \"ai_4NsJqZTB\",\n                                    \"name\": \"Ring\",\n                                    \"value\": 0.82924765,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_6nF00KsR\",\n                                    \"name\": \"Men's Dress Shoes\",\n                                    \"value\": 0.46936136,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_hlBWBcP8\",\n                                    \"name\": \"Men's Boots\",\n                                    \"value\": 0.2129777,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_0PM9RnsJ\",\n                                    \"name\": \"Pumps\",\n                                    \"value\": 0.18616039,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_P2cKDGMW\",\n                                    \"name\": \"Clogs\",\n                                    \"value\": 0.15437138,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_z5F7XFlb\",\n                                    \"name\": \"Platform Shoes\",\n                                    \"value\": 0.14095984,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_cjhVr9Tf\",\n                                    \"name\": \"Necklace\",\n                                    \"value\": 0.11307672,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_MZz07Pnw\",\n                                    \"name\": \"Bracelet\",\n                                    \"value\": 0.08222035,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_2KGqNjLM\",\n                                    \"name\": \"Earring\",\n                                    \"value\": 0.05832608,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_Z79pShcW\",\n                                    \"name\": \"Oxfords\",\n                                    \"value\": 0.055387706,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_Xfs5V5DH\",\n                                    \"name\": \"Women's Boots\",\n                                    \"value\": 0.05276431,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_jt5kSMNT\",\n                                    \"name\": \"Socks\",\n                                    \"value\": 0.04910677,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_0zhMPWP5\",\n                                    \"name\": \"Wedding Dress\",\n                                    \"value\": 0.04366389,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_GWJG9K9S\",\n                                    \"name\": \"Wallet\",\n                                    \"value\": 0.042876955,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_LLG9S3Fg\",\n                                    \"name\": \"Women's Sandals\",\n                                    \"value\": 0.04172343,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_dt6mFvGt\",\n                                    \"name\": \"Sunglasses\",\n                                    \"value\": 0.029591313,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_mtd6x6QM\",\n                                    \"name\": \"Flats\",\n                                    \"value\": 0.027568337,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_PpNXZGZl\",\n                                    \"name\": \"Loafers\",\n                                    \"value\": 0.025136162,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_QMTdVP1h\",\n                                    \"name\": \"Gloves\",\n                                    \"value\": 0.019275602,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_nJXcs1ns\",\n                                    \"name\": \"Maxi Skirt\",\n                                    \"value\": 0.018041234,\n                                    \"app_id\": \"main\"\n                                }\n                            ]\n                        },\n                        \"id\": \"8e5f8672bdda2f2682d59ccc019d48c0\"\n                    },\n                    {\n                        \"frame_info\": {\n                            \"index\": 2,\n                            \"time\": 2500\n                        },\n                        \"data\": {\n                            \"concepts\": [\n                                {\n                                    \"id\": \"ai_4NsJqZTB\",\n                                    \"name\": \"Ring\",\n                                    \"value\": 0.82376486,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_z5F7XFlb\",\n                                    \"name\": \"Platform Shoes\",\n                                    \"value\": 0.6011005,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_0PM9RnsJ\",\n                                    \"name\": \"Pumps\",\n                                    \"value\": 0.5200979,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_P2cKDGMW\",\n                                    \"name\": \"Clogs\",\n                                    \"value\": 0.37294456,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_6nF00KsR\",\n                                    \"name\": \"Men's Dress Shoes\",\n                                    \"value\": 0.354214,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_LLG9S3Fg\",\n                                    \"name\": \"Women's Sandals\",\n                                    \"value\": 0.21168526,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_Z79pShcW\",\n                                    \"name\": \"Oxfords\",\n                                    \"value\": 0.15548542,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_MZz07Pnw\",\n                                    \"name\": \"Bracelet\",\n                                    \"value\": 0.08805831,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_mtd6x6QM\",\n                                    \"name\": \"Flats\",\n                                    \"value\": 0.076873705,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_hlBWBcP8\",\n                                    \"name\": \"Men's Boots\",\n                                    \"value\": 0.054909162,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_cjhVr9Tf\",\n                                    \"name\": \"Necklace\",\n                                    \"value\": 0.046748944,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_Xfs5V5DH\",\n                                    \"name\": \"Women's Boots\",\n                                    \"value\": 0.030692348,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_PpNXZGZl\",\n                                    \"name\": \"Loafers\",\n                                    \"value\": 0.023155041,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_0zhMPWP5\",\n                                    \"name\": \"Wedding Dress\",\n                                    \"value\": 0.021140508,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_0tqh85Gt\",\n                                    \"name\": \"Boat Shoes\",\n                                    \"value\": 0.013938167,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_GWJG9K9S\",\n                                    \"name\": \"Wallet\",\n                                    \"value\": 0.013429959,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_dt6mFvGt\",\n                                    \"name\": \"Sunglasses\",\n                                    \"value\": 0.009777872,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_SGXgBwzJ\",\n                                    \"name\": \"Sneakers\",\n                                    \"value\": 0.009320901,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_jt5kSMNT\",\n                                    \"name\": \"Socks\",\n                                    \"value\": 0.008420322,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_5Zj0kZRQ\",\n                                    \"name\": \"Belt\",\n                                    \"value\": 0.00835875,\n                                    \"app_id\": \"main\"\n                                }\n                            ]\n                        },\n                        \"id\": \"3f4cd8b6cbe2361de2d3a3f84906723c\"\n                    },\n                    {\n                        \"frame_info\": {\n                            \"index\": 3,\n                            \"time\": 3500\n                        },\n                        \"data\": {\n                            \"concepts\": [\n                                {\n                                    \"id\": \"ai_4NsJqZTB\",\n                                    \"name\": \"Ring\",\n                                    \"value\": 0.65054405,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_cjhVr9Tf\",\n                                    \"name\": \"Necklace\",\n                                    \"value\": 0.6334438,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_0PM9RnsJ\",\n                                    \"name\": \"Pumps\",\n                                    \"value\": 0.5025431,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_z5F7XFlb\",\n                                    \"name\": \"Platform Shoes\",\n                                    \"value\": 0.47934076,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_0zhMPWP5\",\n                                    \"name\": \"Wedding Dress\",\n                                    \"value\": 0.37599844,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_mtd6x6QM\",\n                                    \"name\": \"Flats\",\n                                    \"value\": 0.24318114,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_Z79pShcW\",\n                                    \"name\": \"Oxfords\",\n                                    \"value\": 0.23650196,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_2KGqNjLM\",\n                                    \"name\": \"Earring\",\n                                    \"value\": 0.22577241,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_6nF00KsR\",\n                                    \"name\": \"Men's Dress Shoes\",\n                                    \"value\": 0.20225178,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_RnCjxbBF\",\n                                    \"name\": \"Bra\",\n                                    \"value\": 0.16639887,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_GWJG9K9S\",\n                                    \"name\": \"Wallet\",\n                                    \"value\": 0.14360128,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_MZz07Pnw\",\n                                    \"name\": \"Bracelet\",\n                                    \"value\": 0.13545996,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_hX5ZCZsC\",\n                                    \"name\": \"Formal Dress\",\n                                    \"value\": 0.11607494,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_RXF6zlWD\",\n                                    \"name\": \"Sports Bra\",\n                                    \"value\": 0.111197636,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_Lvn6gxp9\",\n                                    \"name\": \"Cocktail Dress\",\n                                    \"value\": 0.10976209,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_LLG9S3Fg\",\n                                    \"name\": \"Women's Sandals\",\n                                    \"value\": 0.106289394,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_P2cKDGMW\",\n                                    \"name\": \"Clogs\",\n                                    \"value\": 0.106163815,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_SGXgBwzJ\",\n                                    \"name\": \"Sneakers\",\n                                    \"value\": 0.092813924,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_hlBWBcP8\",\n                                    \"name\": \"Men's Boots\",\n                                    \"value\": 0.08313861,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_Xfs5V5DH\",\n                                    \"name\": \"Women's Boots\",\n                                    \"value\": 0.07134081,\n                                    \"app_id\": \"main\"\n                                }\n                            ]\n                        },\n                        \"id\": \"049f7331f17764126fa433ccc7eb27a6\"\n                    },\n                    {\n                        \"frame_info\": {\n                            \"index\": 4,\n                            \"time\": 4500\n                        },\n                        \"data\": {\n                            \"concepts\": [\n                                {\n                                    \"id\": \"ai_z5F7XFlb\",\n                                    \"name\": \"Platform Shoes\",\n                                    \"value\": 0.6462518,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_4NsJqZTB\",\n                                    \"name\": \"Ring\",\n                                    \"value\": 0.48306978,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_0PM9RnsJ\",\n                                    \"name\": \"Pumps\",\n                                    \"value\": 0.42331654,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_6nF00KsR\",\n                                    \"name\": \"Men's Dress Shoes\",\n                                    \"value\": 0.36030656,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_cjhVr9Tf\",\n                                    \"name\": \"Necklace\",\n                                    \"value\": 0.3336534,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_Lvn6gxp9\",\n                                    \"name\": \"Cocktail Dress\",\n                                    \"value\": 0.28118226,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_Z79pShcW\",\n                                    \"name\": \"Oxfords\",\n                                    \"value\": 0.28104377,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_0zhMPWP5\",\n                                    \"name\": \"Wedding Dress\",\n                                    \"value\": 0.22136128,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_mtd6x6QM\",\n                                    \"name\": \"Flats\",\n                                    \"value\": 0.22097284,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_nJXcs1ns\",\n                                    \"name\": \"Maxi Skirt\",\n                                    \"value\": 0.21044959,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_hX5ZCZsC\",\n                                    \"name\": \"Formal Dress\",\n                                    \"value\": 0.18968809,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_LLG9S3Fg\",\n                                    \"name\": \"Women's Sandals\",\n                                    \"value\": 0.18827343,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_SGXgBwzJ\",\n                                    \"name\": \"Sneakers\",\n                                    \"value\": 0.16092703,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_P2cKDGMW\",\n                                    \"name\": \"Clogs\",\n                                    \"value\": 0.14271584,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_Xfs5V5DH\",\n                                    \"name\": \"Women's Boots\",\n                                    \"value\": 0.13084769,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_MZz07Pnw\",\n                                    \"name\": \"Bracelet\",\n                                    \"value\": 0.129217,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_2KGqNjLM\",\n                                    \"name\": \"Earring\",\n                                    \"value\": 0.12058064,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_hlBWBcP8\",\n                                    \"name\": \"Men's Boots\",\n                                    \"value\": 0.118088774,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_fpjq5JLS\",\n                                    \"name\": \"Knee Length Skirt\",\n                                    \"value\": 0.11008041,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_GWJG9K9S\",\n                                    \"name\": \"Wallet\",\n                                    \"value\": 0.10578205,\n                                    \"app_id\": \"main\"\n                                }\n                            ]\n                        },\n                        \"id\": \"a815862119825cfb037834ec5dc24619\"\n                    },\n                    {\n                        \"frame_info\": {\n                            \"index\": 5,\n                            \"time\": 5500\n                        },\n                        \"data\": {\n                            \"concepts\": [\n                                {\n                                    \"id\": \"ai_z5F7XFlb\",\n                                    \"name\": \"Platform Shoes\",\n                                    \"value\": 0.4840905,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_4NsJqZTB\",\n                                    \"name\": \"Ring\",\n                                    \"value\": 0.46339297,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_cjhVr9Tf\",\n                                    \"name\": \"Necklace\",\n                                    \"value\": 0.37160143,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_6nF00KsR\",\n                                    \"name\": \"Men's Dress Shoes\",\n                                    \"value\": 0.36601314,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_Lvn6gxp9\",\n                                    \"name\": \"Cocktail Dress\",\n                                    \"value\": 0.27407753,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_nJXcs1ns\",\n                                    \"name\": \"Maxi Skirt\",\n                                    \"value\": 0.25172812,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_0PM9RnsJ\",\n                                    \"name\": \"Pumps\",\n                                    \"value\": 0.20354484,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_Z79pShcW\",\n                                    \"name\": \"Oxfords\",\n                                    \"value\": 0.20305806,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_0zhMPWP5\",\n                                    \"name\": \"Wedding Dress\",\n                                    \"value\": 0.19234876,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_hX5ZCZsC\",\n                                    \"name\": \"Formal Dress\",\n                                    \"value\": 0.17288539,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_Xfs5V5DH\",\n                                    \"name\": \"Women's Boots\",\n                                    \"value\": 0.16257846,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_SGXgBwzJ\",\n                                    \"name\": \"Sneakers\",\n                                    \"value\": 0.1478895,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_2KGqNjLM\",\n                                    \"name\": \"Earring\",\n                                    \"value\": 0.14602013,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_hlBWBcP8\",\n                                    \"name\": \"Men's Boots\",\n                                    \"value\": 0.14361155,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_fpjq5JLS\",\n                                    \"name\": \"Knee Length Skirt\",\n                                    \"value\": 0.13800688,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_RXF6zlWD\",\n                                    \"name\": \"Sports Bra\",\n                                    \"value\": 0.11743115,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_T8DMpsgc\",\n                                    \"name\": \"Midi Skirt\",\n                                    \"value\": 0.11440468,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_LLG9S3Fg\",\n                                    \"name\": \"Women's Sandals\",\n                                    \"value\": 0.106182925,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_GWJG9K9S\",\n                                    \"name\": \"Wallet\",\n                                    \"value\": 0.08954732,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_8Rp1jBcS\",\n                                    \"name\": \"Maxi Dress\",\n                                    \"value\": 0.085555926,\n                                    \"app_id\": \"main\"\n                                }\n                            ]\n                        },\n                        \"id\": \"14572f138018d23fcd39a87f0c51880e\"\n                    },\n                    {\n                        \"frame_info\": {\n                            \"index\": 6,\n                            \"time\": 6500\n                        },\n                        \"data\": {\n                            \"concepts\": [\n                                {\n                                    \"id\": \"ai_4NsJqZTB\",\n                                    \"name\": \"Ring\",\n                                    \"value\": 0.578912,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_z5F7XFlb\",\n                                    \"name\": \"Platform Shoes\",\n                                    \"value\": 0.4543976,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_cjhVr9Tf\",\n                                    \"name\": \"Necklace\",\n                                    \"value\": 0.34879935,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_6nF00KsR\",\n                                    \"name\": \"Men's Dress Shoes\",\n                                    \"value\": 0.322502,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_0PM9RnsJ\",\n                                    \"name\": \"Pumps\",\n                                    \"value\": 0.2644617,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_nJXcs1ns\",\n                                    \"name\": \"Maxi Skirt\",\n                                    \"value\": 0.22488362,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_Z79pShcW\",\n                                    \"name\": \"Oxfords\",\n                                    \"value\": 0.21371934,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_2KGqNjLM\",\n                                    \"name\": \"Earring\",\n                                    \"value\": 0.18831255,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_0zhMPWP5\",\n                                    \"name\": \"Wedding Dress\",\n                                    \"value\": 0.18543427,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_Xfs5V5DH\",\n                                    \"name\": \"Women's Boots\",\n                                    \"value\": 0.14219807,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_Lvn6gxp9\",\n                                    \"name\": \"Cocktail Dress\",\n                                    \"value\": 0.13228798,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_SGXgBwzJ\",\n                                    \"name\": \"Sneakers\",\n                                    \"value\": 0.11936176,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_MZz07Pnw\",\n                                    \"name\": \"Bracelet\",\n                                    \"value\": 0.11859587,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_hlBWBcP8\",\n                                    \"name\": \"Men's Boots\",\n                                    \"value\": 0.10120945,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_LLG9S3Fg\",\n                                    \"name\": \"Women's Sandals\",\n                                    \"value\": 0.100087315,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_GWJG9K9S\",\n                                    \"name\": \"Wallet\",\n                                    \"value\": 0.093706906,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_RXF6zlWD\",\n                                    \"name\": \"Sports Bra\",\n                                    \"value\": 0.089778036,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_mtd6x6QM\",\n                                    \"name\": \"Flats\",\n                                    \"value\": 0.08846618,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_hX5ZCZsC\",\n                                    \"name\": \"Formal Dress\",\n                                    \"value\": 0.08660594,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_T8DMpsgc\",\n                                    \"name\": \"Midi Skirt\",\n                                    \"value\": 0.08032408,\n                                    \"app_id\": \"main\"\n                                }\n                            ]\n                        },\n                        \"id\": \"7790d9923639183be7213e8330736ea5\"\n                    },\n                    {\n                        \"frame_info\": {\n                            \"index\": 7,\n                            \"time\": 7500\n                        },\n                        \"data\": {\n                            \"concepts\": [\n                                {\n                                    \"id\": \"ai_z5F7XFlb\",\n                                    \"name\": \"Platform Shoes\",\n                                    \"value\": 0.5954633,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_4NsJqZTB\",\n                                    \"name\": \"Ring\",\n                                    \"value\": 0.45100528,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_6nF00KsR\",\n                                    \"name\": \"Men's Dress Shoes\",\n                                    \"value\": 0.3607981,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_0PM9RnsJ\",\n                                    \"name\": \"Pumps\",\n                                    \"value\": 0.34550142,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_nJXcs1ns\",\n                                    \"name\": \"Maxi Skirt\",\n                                    \"value\": 0.25450236,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_cjhVr9Tf\",\n                                    \"name\": \"Necklace\",\n                                    \"value\": 0.22948706,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_Xfs5V5DH\",\n                                    \"name\": \"Women's Boots\",\n                                    \"value\": 0.22799394,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_Z79pShcW\",\n                                    \"name\": \"Oxfords\",\n                                    \"value\": 0.20895943,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_hlBWBcP8\",\n                                    \"name\": \"Men's Boots\",\n                                    \"value\": 0.16781448,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_Lvn6gxp9\",\n                                    \"name\": \"Cocktail Dress\",\n                                    \"value\": 0.16564906,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_0zhMPWP5\",\n                                    \"name\": \"Wedding Dress\",\n                                    \"value\": 0.16265132,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_SGXgBwzJ\",\n                                    \"name\": \"Sneakers\",\n                                    \"value\": 0.15834433,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_LLG9S3Fg\",\n                                    \"name\": \"Women's Sandals\",\n                                    \"value\": 0.14323366,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_fpjq5JLS\",\n                                    \"name\": \"Knee Length Skirt\",\n                                    \"value\": 0.11307957,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_hX5ZCZsC\",\n                                    \"name\": \"Formal Dress\",\n                                    \"value\": 0.11263673,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_RXF6zlWD\",\n                                    \"name\": \"Sports Bra\",\n                                    \"value\": 0.10770409,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_mtd6x6QM\",\n                                    \"name\": \"Flats\",\n                                    \"value\": 0.088091984,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_T8DMpsgc\",\n                                    \"name\": \"Midi Skirt\",\n                                    \"value\": 0.08515663,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_MZz07Pnw\",\n                                    \"name\": \"Bracelet\",\n                                    \"value\": 0.08285889,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_8Rp1jBcS\",\n                                    \"name\": \"Maxi Dress\",\n                                    \"value\": 0.0804608,\n                                    \"app_id\": \"main\"\n                                }\n                            ]\n                        },\n                        \"id\": \"32224efe9c43139c9f3070930bae4e6c\"\n                    },\n                    {\n                        \"frame_info\": {\n                            \"index\": 8,\n                            \"time\": 8500\n                        },\n                        \"data\": {\n                            \"concepts\": [\n                                {\n                                    \"id\": \"ai_z5F7XFlb\",\n                                    \"name\": \"Platform Shoes\",\n                                    \"value\": 0.5952685,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_4NsJqZTB\",\n                                    \"name\": \"Ring\",\n                                    \"value\": 0.45152456,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_6nF00KsR\",\n                                    \"name\": \"Men's Dress Shoes\",\n                                    \"value\": 0.3606025,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_0PM9RnsJ\",\n                                    \"name\": \"Pumps\",\n                                    \"value\": 0.34516576,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_nJXcs1ns\",\n                                    \"name\": \"Maxi Skirt\",\n                                    \"value\": 0.25459015,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_cjhVr9Tf\",\n                                    \"name\": \"Necklace\",\n                                    \"value\": 0.22953078,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_Xfs5V5DH\",\n                                    \"name\": \"Women's Boots\",\n                                    \"value\": 0.22791381,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_Z79pShcW\",\n                                    \"name\": \"Oxfords\",\n                                    \"value\": 0.20881763,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_hlBWBcP8\",\n                                    \"name\": \"Men's Boots\",\n                                    \"value\": 0.16780624,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_Lvn6gxp9\",\n                                    \"name\": \"Cocktail Dress\",\n                                    \"value\": 0.16563928,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_0zhMPWP5\",\n                                    \"name\": \"Wedding Dress\",\n                                    \"value\": 0.16275188,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_SGXgBwzJ\",\n                                    \"name\": \"Sneakers\",\n                                    \"value\": 0.15818237,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_LLG9S3Fg\",\n                                    \"name\": \"Women's Sandals\",\n                                    \"value\": 0.14315414,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_fpjq5JLS\",\n                                    \"name\": \"Knee Length Skirt\",\n                                    \"value\": 0.11310254,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_hX5ZCZsC\",\n                                    \"name\": \"Formal Dress\",\n                                    \"value\": 0.11259423,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_RXF6zlWD\",\n                                    \"name\": \"Sports Bra\",\n                                    \"value\": 0.10771175,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_mtd6x6QM\",\n                                    \"name\": \"Flats\",\n                                    \"value\": 0.08798519,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_T8DMpsgc\",\n                                    \"name\": \"Midi Skirt\",\n                                    \"value\": 0.085228026,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_MZz07Pnw\",\n                                    \"name\": \"Bracelet\",\n                                    \"value\": 0.0828697,\n                                    \"app_id\": \"main\"\n                                },\n                                {\n                                    \"id\": \"ai_8Rp1jBcS\",\n                                    \"name\": \"Maxi Dress\",\n                                    \"value\": 0.08047128,\n                                    \"app_id\": \"main\"\n                                }\n                            ]\n                        },\n                        \"id\": \"0c4dd5d602afa6754bcfd441998412af\"\n                    }\n                ]\n            }\n        }\n    ]\n}"
                  },
                  "example-3": {
                    "summary": "Predict base64 image - general model embeddings",
                    "value": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"c7b705f9f2c80d86acf4e5f14948e2f1\"\n    },\n    \"outputs\": [\n        {\n            \"id\": \"e1417061cec645e2a4bdb16119638063\",\n            \"status\": {\n                \"code\": 10000,\n                \"description\": \"Ok\"\n            },\n            \"created_at\": \"2023-11-23T11:21:14.416571380Z\",\n            \"model\": {\n                \"id\": \"apparel-recognition\",\n                \"name\": \"apparel\",\n                \"created_at\": \"2016-12-15T01:29:04.622209Z\",\n                \"modified_at\": \"2023-05-23T12:34:15.093542Z\",\n                \"app_id\": \"main\",\n                \"model_version\": {\n                    \"id\": \"dc2cd6d9bff5425a80bfe0c4105583c1\",\n                    \"created_at\": \"2016-12-15T01:29:04.622209Z\",\n                    \"status\": {\n                        \"code\": 21100,\n                        \"description\": \"Model is trained and ready\"\n                    },\n                    \"visibility\": {\n                        \"gettable\": 50\n                    },\n                    \"app_id\": \"main\",\n                    \"user_id\": \"clarifai\",\n                    \"metadata\": {}\n                },\n                \"display_name\": \"apparel-visual-classifier\",\n                \"user_id\": \"clarifai\",\n                \"model_type_id\": \"visual-classifier\",\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"toolkits\": [],\n                \"use_cases\": [],\n                \"languages\": [],\n                \"languages_full\": [],\n                \"check_consents\": [],\n                \"workflow_recommended\": false\n            },\n            \"input\": {\n                \"id\": \"8cb31c51e8654faf9f71d71bd6d4e245\",\n                \"data\": {\n                    \"image\": {\n                        \"url\": \"https://samples.clarifai.com/placeholder.gif\",\n                        \"base64\": \"dHJ1ZQ==\"\n                    }\n                }\n            },\n            \"data\": {\n                \"concepts\": [\n                    {\n                        \"id\": \"ai_fFlQDW0c\",\n                        \"name\": \"Blouse\",\n                        \"value\": 0.5755471,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_zLQ06vpb\",\n                        \"name\": \"Men's Shorts\",\n                        \"value\": 0.5166089,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_6wNWp7sV\",\n                        \"name\": \"Sleepwear\",\n                        \"value\": 0.46030346,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_xMKJC6lN\",\n                        \"name\": \"Panties\",\n                        \"value\": 0.4503619,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_x1X1tNs5\",\n                        \"name\": \"One-Piece\",\n                        \"value\": 0.4188718,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_9XBqh80b\",\n                        \"name\": \"Tank Top\",\n                        \"value\": 0.4135467,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_DB9fg9zx\",\n                        \"name\": \"Activewear T Shirt\",\n                        \"value\": 0.3532552,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_Wg8Xs8KN\",\n                        \"name\": \"Full Bikini\",\n                        \"value\": 0.33880612,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_Dq60HQ2h\",\n                        \"name\": \"Vest\",\n                        \"value\": 0.28446686,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_Rq009GMW\",\n                        \"name\": \"Women's Shorts\",\n                        \"value\": 0.28424734,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_bXKvhw9n\",\n                        \"name\": \"Polos\",\n                        \"value\": 0.28142613,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_zTGRW4d6\",\n                        \"name\": \"Blazer\",\n                        \"value\": 0.26786482,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_PxHDNZ7W\",\n                        \"name\": \"T-Shirt\",\n                        \"value\": 0.2649977,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_cjhVr9Tf\",\n                        \"name\": \"Necklace\",\n                        \"value\": 0.2638415,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_ZdKP9568\",\n                        \"name\": \"Button-Down\",\n                        \"value\": 0.24429752,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_14DHBZb8\",\n                        \"name\": \"Sweatshirt\",\n                        \"value\": 0.23941447,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_wtnFnM8F\",\n                        \"name\": \"Men's Underwear\",\n                        \"value\": 0.23446989,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_rq1T66Bh\",\n                        \"name\": \"Women's Jean Shorts\",\n                        \"value\": 0.21704626,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_FvtbNF9R\",\n                        \"name\": \"Men's Watch\",\n                        \"value\": 0.20862773,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_4RKVkQfZ\",\n                        \"name\": \"Bodysuit\",\n                        \"value\": 0.1812181,\n                        \"app_id\": \"main\"\n                    }\n                ]\n            }\n        }\n    ]\n}"
                  },
                  "example-4": {
                    "summary": "Predict By Input ID",
                    "value": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"74580428cf2e1bb4169cb1699f2dc321\"\n    },\n    \"outputs\": [\n        {\n            \"id\": \"61af16bb6cc8495fa61aa6bdb885367b\",\n            \"status\": {\n                \"code\": 10000,\n                \"description\": \"Ok\"\n            },\n            \"created_at\": \"2023-11-23T11:21:38.956835122Z\",\n            \"model\": {\n                \"id\": \"apparel-recognition\",\n                \"name\": \"apparel\",\n                \"created_at\": \"2016-12-15T01:29:04.622209Z\",\n                \"modified_at\": \"2023-05-23T12:34:15.093542Z\",\n                \"app_id\": \"main\",\n                \"model_version\": {\n                    \"id\": \"dc2cd6d9bff5425a80bfe0c4105583c1\",\n                    \"created_at\": \"2016-12-15T01:29:04.622209Z\",\n                    \"status\": {\n                        \"code\": 21100,\n                        \"description\": \"Model is trained and ready\"\n                    },\n                    \"visibility\": {\n                        \"gettable\": 50\n                    },\n                    \"app_id\": \"main\",\n                    \"user_id\": \"clarifai\",\n                    \"metadata\": {}\n                },\n                \"display_name\": \"apparel-visual-classifier\",\n                \"user_id\": \"clarifai\",\n                \"model_type_id\": \"visual-classifier\",\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"toolkits\": [],\n                \"use_cases\": [],\n                \"languages\": [],\n                \"languages_full\": [],\n                \"check_consents\": [],\n                \"workflow_recommended\": false\n            },\n            \"input\": {\n                \"id\": \"bda54f69edd94e7abe505079adfbfe7d\",\n                \"data\": {\n                    \"image\": {\n                        \"url\": \"https://s3.amazonaws.com/clarifai-api/img3/dev/orig/513b8946d6e24a1aa278db7c48fd1290/140c856dc82565d2c4d6ea720fceff78\"\n                    }\n                }\n            },\n            \"data\": {\n                \"concepts\": [\n                    {\n                        \"id\": \"ai_NNpl3TkD\",\n                        \"name\": \"Umbrella\",\n                        \"value\": 0.7486447,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_nVG9jntC\",\n                        \"name\": \"Capris\",\n                        \"value\": 0.6436113,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_FvtbNF9R\",\n                        \"name\": \"Men's Watch\",\n                        \"value\": 0.5759363,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_2fQ7dxwC\",\n                        \"name\": \"Tube Top\",\n                        \"value\": 0.3502354,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_jt5kSMNT\",\n                        \"name\": \"Socks\",\n                        \"value\": 0.33612183,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_wtnFnM8F\",\n                        \"name\": \"Men's Underwear\",\n                        \"value\": 0.27975702,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_6VnM6NTC\",\n                        \"name\": \"Prom Dress\",\n                        \"value\": 0.19003943,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_NPkNN4qJ\",\n                        \"name\": \"Leggings\",\n                        \"value\": 0.17621915,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_zLQ06vpb\",\n                        \"name\": \"Men's Shorts\",\n                        \"value\": 0.16405195,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_3gCkCSk4\",\n                        \"name\": \"Relaxed Pants\",\n                        \"value\": 0.14988193,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_cjhVr9Tf\",\n                        \"name\": \"Necklace\",\n                        \"value\": 0.1379794,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_X4lJmXLd\",\n                        \"name\": \"Men's Hat\",\n                        \"value\": 0.13780017,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_Vp0JwBbw\",\n                        \"name\": \"Women's Hat\",\n                        \"value\": 0.12682319,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_W5HG8nS7\",\n                        \"name\": \"Jeans\",\n                        \"value\": 0.11047637,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_PxHDNZ7W\",\n                        \"name\": \"T-Shirt\",\n                        \"value\": 0.10586095,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_Rq009GMW\",\n                        \"name\": \"Women's Shorts\",\n                        \"value\": 0.10512908,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_14DHBZb8\",\n                        \"name\": \"Sweatshirt\",\n                        \"value\": 0.09871419,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_hlBWBcP8\",\n                        \"name\": \"Men's Boots\",\n                        \"value\": 0.09317294,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_zTGRW4d6\",\n                        \"name\": \"Blazer\",\n                        \"value\": 0.09248342,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_fV17Rh8q\",\n                        \"name\": \"Sweater\",\n                        \"value\": 0.0920531,\n                        \"app_id\": \"main\"\n                    }\n                ]\n            }\n        }\n    ]\n}"
                  },
                  "example-5": {
                    "summary": "Predict with max_concepts/min_value",
                    "value": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"de47d1c83fc9503b2bdb9991feeb0260\"\n    },\n    \"outputs\": [\n        {\n            \"id\": \"5a50aef20c564600bdbd3ceaa96359c0\",\n            \"status\": {\n                \"code\": 10000,\n                \"description\": \"Ok\"\n            },\n            \"created_at\": \"2023-11-23T11:22:24.865190874Z\",\n            \"model\": {\n                \"id\": \"apparel-recognition\",\n                \"name\": \"apparel\",\n                \"created_at\": \"2016-12-15T01:29:04.622209Z\",\n                \"modified_at\": \"2023-05-23T12:34:15.093542Z\",\n                \"app_id\": \"main\",\n                \"model_version\": {\n                    \"id\": \"dc2cd6d9bff5425a80bfe0c4105583c1\",\n                    \"created_at\": \"2016-12-15T01:29:04.622209Z\",\n                    \"status\": {\n                        \"code\": 21100,\n                        \"description\": \"Model is trained and ready\"\n                    },\n                    \"visibility\": {\n                        \"gettable\": 50\n                    },\n                    \"app_id\": \"main\",\n                    \"user_id\": \"clarifai\",\n                    \"metadata\": {}\n                },\n                \"display_name\": \"apparel-visual-classifier\",\n                \"user_id\": \"clarifai\",\n                \"model_type_id\": \"visual-classifier\",\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"toolkits\": [],\n                \"use_cases\": [],\n                \"languages\": [],\n                \"languages_full\": [],\n                \"check_consents\": [],\n                \"workflow_recommended\": false\n            },\n            \"input\": {\n                \"id\": \"39bcb9a0adf141c999f23c02571718ae\",\n                \"data\": {\n                    \"image\": {\n                        \"url\": \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSnzXSi3jhSHbFdRu-ldBQ7B7d65QkQgHSoWA&usqp=CAU\"\n                    }\n                }\n            },\n            \"data\": {\n                \"concepts\": [\n                    {\n                        \"id\": \"ai_bXKvhw9n\",\n                        \"name\": \"Polos\",\n                        \"value\": 0.9338605,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_PxHDNZ7W\",\n                        \"name\": \"T-Shirt\",\n                        \"value\": 0.9111426,\n                        \"app_id\": \"main\"\n                    }\n                ]\n            }\n        }\n    ]\n}"
                  },
                  "example-6": {
                    "summary": "Predict With Model ID / Version ID",
                    "value": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"bf615d8e902a74d1ea061bdc401db1d0\"\n    },\n    \"outputs\": [\n        {\n            \"id\": \"a15bd42fde8e4324a44cfac0195bed01\",\n            \"status\": {\n                \"code\": 10000,\n                \"description\": \"Ok\"\n            },\n            \"created_at\": \"2023-11-23T11:22:43.276545155Z\",\n            \"model\": {\n                \"id\": \"apparel-recognition\",\n                \"name\": \"apparel\",\n                \"created_at\": \"2016-12-15T01:29:04.622209Z\",\n                \"modified_at\": \"2023-05-23T12:34:15.093542Z\",\n                \"app_id\": \"main\",\n                \"model_version\": {\n                    \"id\": \"dc2cd6d9bff5425a80bfe0c4105583c1\",\n                    \"created_at\": \"2016-12-15T01:29:04.622209Z\",\n                    \"status\": {\n                        \"code\": 21100,\n                        \"description\": \"Model is trained and ready\"\n                    },\n                    \"visibility\": {\n                        \"gettable\": 50\n                    },\n                    \"app_id\": \"main\",\n                    \"user_id\": \"clarifai\",\n                    \"metadata\": {}\n                },\n                \"display_name\": \"apparel-visual-classifier\",\n                \"user_id\": \"clarifai\",\n                \"model_type_id\": \"visual-classifier\",\n                \"visibility\": {\n                    \"gettable\": 50\n                },\n                \"toolkits\": [],\n                \"use_cases\": [],\n                \"languages\": [],\n                \"languages_full\": [],\n                \"check_consents\": [],\n                \"workflow_recommended\": false\n            },\n            \"input\": {\n                \"id\": \"0d1389b2f6c14891b6935ed324632806\",\n                \"data\": {\n                    \"image\": {\n                        \"url\": \"https://samples.clarifai.com/metro-north.jpg\"\n                    }\n                }\n            },\n            \"data\": {\n                \"concepts\": [\n                    {\n                        \"id\": \"ai_NNpl3TkD\",\n                        \"name\": \"Umbrella\",\n                        \"value\": 0.7486447,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_nVG9jntC\",\n                        \"name\": \"Capris\",\n                        \"value\": 0.6436113,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_FvtbNF9R\",\n                        \"name\": \"Men's Watch\",\n                        \"value\": 0.5759363,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_2fQ7dxwC\",\n                        \"name\": \"Tube Top\",\n                        \"value\": 0.3502354,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_jt5kSMNT\",\n                        \"name\": \"Socks\",\n                        \"value\": 0.33612183,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_wtnFnM8F\",\n                        \"name\": \"Men's Underwear\",\n                        \"value\": 0.27975702,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_6VnM6NTC\",\n                        \"name\": \"Prom Dress\",\n                        \"value\": 0.19003943,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_NPkNN4qJ\",\n                        \"name\": \"Leggings\",\n                        \"value\": 0.17621915,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_zLQ06vpb\",\n                        \"name\": \"Men's Shorts\",\n                        \"value\": 0.16405195,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_3gCkCSk4\",\n                        \"name\": \"Relaxed Pants\",\n                        \"value\": 0.14988193,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_cjhVr9Tf\",\n                        \"name\": \"Necklace\",\n                        \"value\": 0.1379794,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_X4lJmXLd\",\n                        \"name\": \"Men's Hat\",\n                        \"value\": 0.13780017,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_Vp0JwBbw\",\n                        \"name\": \"Women's Hat\",\n                        \"value\": 0.12682319,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_W5HG8nS7\",\n                        \"name\": \"Jeans\",\n                        \"value\": 0.11047637,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_PxHDNZ7W\",\n                        \"name\": \"T-Shirt\",\n                        \"value\": 0.10586095,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_Rq009GMW\",\n                        \"name\": \"Women's Shorts\",\n                        \"value\": 0.10512908,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_14DHBZb8\",\n                        \"name\": \"Sweatshirt\",\n                        \"value\": 0.09871419,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_hlBWBcP8\",\n                        \"name\": \"Men's Boots\",\n                        \"value\": 0.09317294,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_zTGRW4d6\",\n                        \"name\": \"Blazer\",\n                        \"value\": 0.09248342,\n                        \"app_id\": \"main\"\n                    },\n                    {\n                        \"id\": \"ai_fV17Rh8q\",\n                        \"name\": \"Sweater\",\n                        \"value\": 0.0920531,\n                        \"app_id\": \"main\"\n                    }\n                ]\n            }\n        }\n    ]\n}"
                  }
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/models/{model_id}/outputs' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Authorization: Key {{key}}' \\\n--header 'Content-Type: application/json' \\\n--data '{\n  \"inputs\": [\n    {\n      \"data\": {\n        \"image\": {\n          \"url\": \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSnzXSi3jhSHbFdRu-ldBQ7B7d65QkQgHSoWA&usqp=CAU\"\n        }\n      }\n    }\n  ]\n}'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/models/types": {
      "get": {
        "tags": [
          "Models"
        ],
        "summary": "List Training Templates",
        "description": "This endpoint lets the users list out all training templates in an app.",
        "parameters": [
          {
            "name": "Authorization",
            "in": "header",
            "schema": {
              "type": "string"
            },
            "example": "Key {{key}}"
          },
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"afd15c57c86ad17d539b579fa55a77cd\"\n    },\n    \"model_types\": [\n        {\n            \"id\": \"embedding-classifier\",\n            \"title\": \"Transfer Learning Classifier\",\n            \"description\": \"Classify images or texts based on the embedding model that has indexed them in your app. Transfer learning leverages feature representations from a pre-trained model based on massive amounts of data, so you don’t have to train a new model from scratch and can learn new things very quickly with minimal training data.\",\n            \"input_fields\": [\n                \"embeddings\"\n            ],\n            \"output_fields\": [\n                \"concepts\"\n            ],\n            \"trainable\": true,\n            \"creatable\": true,\n            \"model_type_fields\": [\n                {\n                    \"path\": \"output_info.data.concepts\",\n                    \"field_type\": 4,\n                    \"description\": \"Select the concepts that you want this model version to predict. These should be concepts that are in your training dataset with labels.\",\n                    \"placeholder\": \"List of concepts\",\n                    \"required\": true\n                },\n                {\n                    \"path\": \"output_info.output_config.concepts_mutually_exclusive\",\n                    \"field_type\": 1,\n                    \"default_value\": false,\n                    \"description\": \"Turn this on when there is no overlap between any of the model concepts, such as \\\"cat\\\" or \\\"dog\\\", \\\"car\\\" or \\\"bike\\\".\",\n                    \"placeholder\": \"Concepts Mutually Exclusive\"\n                },\n                {\n                    \"path\": \"input_info.base_embed_model\",\n                    \"field_type\": 12,\n                    \"description\": \"This is the base model version to use for embeddings. This has to be one of the embed models in the app workflow. This allows you to specify the specific model in case your default workflow of your app has multiple embedding models present in it.\",\n                    \"placeholder\": \"Base Model\"\n                },\n                {\n                    \"path\": \"output_info.output_config.training_timeout\",\n                    \"field_type\": 3,\n                    \"default_value\": 0,\n                    \"description\": \"The training timeout in seconds. Longer time allows for training to process more data before timing out.\",\n                    \"placeholder\": \"Training timeout\",\n                    \"internal_only\": true\n                },\n                {\n                    \"path\": \"output_info.output_config.hyper_params\",\n                    \"field_type\": 2,\n                    \"default_value\": null,\n                    \"description\": \"Additional hyperparameters to pass through to backend training service.\",\n                    \"placeholder\": \"Hyper params\",\n                    \"internal_only\": true\n                },\n                {\n                    \"path\": \"output_info.params.max_concepts\",\n                    \"field_type\": 3,\n                    \"default_value\": 20,\n                    \"description\": \"Maximum number of concepts in result.\",\n                    \"placeholder\": \"Maximum concepts\"\n                },\n                {\n                    \"path\": \"output_info.params.select_concepts\",\n                    \"field_type\": 18,\n                    \"default_value\": [],\n                    \"description\": \"Select concepts in result by name or by id.\",\n                    \"placeholder\": \"Select Concepts\"\n                },\n                {\n                    \"path\": \"output_info.params.min_value\",\n                    \"field_type\": 7,\n                    \"default_value\": 0,\n                    \"description\": \"Minimum value of concept's probability score in result. In other words, all concepts with a probability score less than this threshold will be filtered out.\",\n                    \"placeholder\": \"Min value\",\n                    \"model_type_range_info\": {\n                        \"max\": 1,\n                        \"step\": 0.01\n                    }\n                },\n                {\n                    \"path\": \"train_info.dataset\",\n                    \"field_type\": 19,\n                    \"description\": \"Dataset to use for training this model.\",\n                    \"placeholder\": \"Training Dataset\"\n                },\n                {\n                    \"path\": \"train_info.params.dataset_id\",\n                    \"field_type\": 16,\n                    \"default_value\": \"\",\n                    \"description\": \"Deprecated in favor of train_info.dataset.\",\n                    \"placeholder\": \"DEPRECATED: Training Dataset ID\"\n                },\n                {\n                    \"path\": \"train_info.dataset.version\",\n                    \"field_type\": 20,\n                    \"description\": \"Dataset version to use for training this model. If a dataset version is not specified but a dataset is, we will automatically generate a dataset version.\",\n                    \"placeholder\": \"Training Dataset Version\"\n                },\n                {\n                    \"path\": \"train_info.params.dataset_version_id\",\n                    \"field_type\": 17,\n                    \"default_value\": \"\",\n                    \"description\": \"Deprecated in favor of train_info.dataset.version\",\n                    \"placeholder\": \"DEPRECATED: Training Dataset Version ID\"\n                },\n                {\n                    \"path\": \"train_info.params.enrich_dataset\",\n                    \"field_type\": 8,\n                    \"default_value\": \"Automatic\",\n                    \"description\": \"Enrich with supplemental data from pre-built dataset of negative embeddings to improve model accuracy.\",\n                    \"placeholder\": \"Enrich Dataset\",\n                    \"model_type_enum_options\": [\n                        {\n                            \"id\": \"Automatic\",\n                            \"description\": \"Enrich dataset if additional data is available from the base embeddings model.\"\n                        },\n                        {\n                            \"id\": \"Disabled\",\n                            \"description\": \"Do not enrich dataset.\"\n                        }\n                    ]\n                },\n                {\n                    \"path\": \"eval_info.params.use_kfold\",\n                    \"field_type\": 1,\n                    \"default_value\": true,\n                    \"description\": \"If true (default value), we will perform a k-fold evaluation using 2 separate splits of the app data, each holding out 20%. If false, we will evaluate the trained model against the provided holdout dataset. If no holdout set is provided, we will use all the app inputs that contain concepts, from the trained model version, in their annotations.\",\n                    \"placeholder\": \"Use K-Fold Cross Validation\"\n                },\n                {\n                    \"path\": \"eval_info.params.dataset_id\",\n                    \"field_type\": 16,\n                    \"default_value\": \"\",\n                    \"description\": \"Dataset to use for evaluating this model. This is only used if use_kfold is set to false\",\n                    \"placeholder\": \"Eval Dataset ID\"\n                },\n                {\n                    \"path\": \"eval_info.params.dataset_version_id\",\n                    \"field_type\": 17,\n                    \"default_value\": \"\",\n                    \"description\": \"Dataset version to use for evaluating this model. If a dataset version is not specified but a dataset is, we will automatically generate a dataset version. This is only used if use_kfold is set to false\",\n                    \"placeholder\": \"Eval Dataset Version ID\"\n                }\n            ],\n            \"evaluation_type\": 1\n        },\n        {\n            \"id\": \"audio-embedder\",\n            \"title\": \"Audio Embedder\",\n            \"description\": \"Embed audio signal into a vector representing a high level understanding from our AI models. These embeddings enable similarity search and training on top of them.\",\n            \"input_fields\": [\n                \"audio\"\n            ],\n            \"output_fields\": [\n                \"embeddings\"\n            ]\n        },\n        {\n            \"id\": \"visual-detector-embedder\",\n            \"title\": \"Visual Detector + Embedder\",\n            \"description\": \"Detect bounding box regions in images or video frames where things occur and then embed them into a high level understanding from our AI models to enable visual search and training on top of them.\",\n            \"input_fields\": [\n                \"image\"\n            ],\n            \"output_fields\": [\n                \"regions[...].data.embeddings\"\n            ]\n        },\n        {\n            \"id\": \"optical-character-recognizer\",\n            \"title\": \"Optical Character Recognizer (OCR)\",\n            \"description\": \"Detect bounding box regions in images or video frames where text is present and then output the text read with the score.\",\n            \"input_fields\": [\n                \"image\"\n            ],\n            \"output_fields\": [\n                \"regions[...].region_info.bounding_box,regions[...].data.text,regions[...].value\"\n            ],\n            \"expected_input_layers\": [\n                {\n                    \"data_field_name\": \"image\",\n                    \"shapes\": [\n                        {\n                            \"dims\": [\n                                -1,\n                                -1,\n                                3\n                            ],\n                            \"max_dims\": [\n                                1024,\n                                1024,\n                                3\n                            ],\n                            \"data_type\": 2,\n                            \"description\": \"First two dimensions are the height and width, followed by the number of channels.\"\n                        }\n                    ],\n                    \"description\": \"Image urls or base64 are converted into numpy arrays of the specified size and forwarded to the model. If flexible dims are provided, inputs will be downsampled and padded to a default size.\"\n                }\n            ],\n            \"expected_output_layers\": [\n                {\n                    \"data_field_name\": \"regions[...].region_info.bounding_box\",\n                    \"shapes\": [\n                        {\n                            \"dims\": [\n                                -1,\n                                4\n                            ],\n                            \"data_type\": 5,\n                            \"description\": \"The normalized bounding box coordinates in the order: top_row, left_col, bottom_row, right_col.\"\n                        }\n                    ]\n                },\n                {\n                    \"data_field_name\": \"regions[...].data.text\",\n                    \"shapes\": [\n                        {\n                            \"dims\": [\n                                -1\n                            ],\n                            \"data_type\": 1,\n                            \"description\": \"Text that belongs to the respective bounding box.\"\n                        }\n                    ]\n                },\n                {\n                    \"data_field_name\": \"regions[...].value\",\n                    \"shapes\": [\n                        {\n                            \"dims\": [\n                                -1\n                            ],\n                            \"data_type\": 5,\n                            \"description\": \"Score that belongs to the respective bounding box.\"\n                        }\n                    ]\n                }\n            ]\n        },\n        {\n            \"id\": \"image-to-image\",\n            \"title\": \"Image to Image\",\n            \"description\": \"Given an image, apply a transformation on the input and return the post-processed image as output.\",\n            \"input_fields\": [\n                \"image\"\n            ],\n            \"output_fields\": [\n                \"image\"\n            ],\n            \"expected_input_layers\": [\n                {\n                    \"data_field_name\": \"image\",\n                    \"shapes\": [\n                        {\n                            \"dims\": [\n                                -1,\n                                -1,\n                                3\n                            ],\n                            \"max_dims\": [\n                                1024,\n                                1024,\n                                3\n                            ],\n                            \"data_type\": 2,\n                            \"description\": \"First two dimensions are the height and width, followed by the number of channels.\"\n                        }\n                    ],\n                    \"description\": \"Image urls or base64 are converted into numpy arrays of the specified size and forwarded to the model. If flexible dims are provided, inputs will be downsampled and padded to a default size.\"\n                }\n            ],\n            \"expected_output_layers\": [\n                {\n                    \"data_field_name\": \"image\",\n                    \"shapes\": [\n                        {\n                            \"dims\": [\n                                -1,\n                                -1,\n                                3\n                            ],\n                            \"data_type\": 2,\n                            \"description\": \"First two dimensions are the height and width, followed by the number of channels.\"\n                        }\n                    ],\n                    \"description\": \"Image output.\"\n                }\n            ]\n        },\n        {\n            \"id\": \"image-to-text\",\n            \"title\": \"Image To Text\",\n            \"description\": \"Takes in cropped regions with text in them and returns the text it sees.\",\n            \"input_fields\": [\n                \"image\"\n            ],\n            \"output_fields\": [\n                \"text\"\n            ],\n            \"expected_input_layers\": [\n                {\n                    \"data_field_name\": \"image\",\n                    \"shapes\": [\n                        {\n                            \"dims\": [\n                                -1,\n                                -1,\n                                3\n                            ],\n                            \"max_dims\": [\n                                1024,\n                                1024,\n                                3\n                            ],\n                            \"data_type\": 2,\n                            \"description\": \"First two dimensions are the height and width, followed by the number of channels.\"\n                        }\n                    ],\n                    \"description\": \"Image urls or base64 are converted into numpy arrays of the specified size and forwarded to the model. If flexible dims are provided, inputs will be downsampled and padded to a default size.\"\n                }\n            ],\n            \"expected_output_layers\": [\n                {\n                    \"data_field_name\": \"text\",\n                    \"shapes\": [\n                        {\n                            \"dims\": [\n                                1\n                            ],\n                            \"data_type\": 1,\n                            \"description\": \"Text output\"\n                        }\n                    ]\n                }\n            ]\n        },\n        {\n            \"id\": \"text-to-image\",\n            \"title\": \"Text To Image\",\n            \"description\": \"Takes in a prompt and generates an image.\",\n            \"input_fields\": [\n                \"text\"\n            ],\n            \"output_fields\": [\n                \"image\"\n            ],\n            \"expected_input_layers\": [\n                {\n                    \"data_field_name\": \"text\",\n                    \"shapes\": [\n                        {\n                            \"dims\": [\n                                -1\n                            ],\n                            \"data_type\": 1,\n                            \"description\": \"The text string sent to the model.\"\n                        }\n                    ],\n                    \"description\": \"Text urls content or raw text passed in through the API are directly sent to the model.\"\n                }\n            ],\n            \"expected_output_layers\": [\n                {\n                    \"data_field_name\": \"image\",\n                    \"shapes\": [\n                        {\n                            \"dims\": [\n                                -1,\n                                -1,\n                                3\n                            ],\n                            \"data_type\": 2,\n                            \"description\": \"First two dimensions are the height and width, followed by the number of channels.\"\n                        }\n                    ],\n                    \"description\": \"Image output.\"\n                }\n            ]\n        },\n        {\n            \"id\": \"clusterer\",\n            \"title\": \"Clusterer\",\n            \"description\": \"Cluster semantically similar images and video frames together in embedding space. This is the basis for good visual search within your app at scale or for grouping your data together without the need for annotated concepts.\",\n            \"input_fields\": [\n                \"embeddings\"\n            ],\n            \"output_fields\": [\n                \"clusters\"\n            ],\n            \"trainable\": true,\n            \"creatable\": true,\n            \"model_type_fields\": [\n                {\n                    \"path\": \"input_info.base_embed_model\",\n                    \"field_type\": 12,\n                    \"description\": \"This is the base model to use for embeddings. This has to be one of the embed models in the app workflow. This allows you to specify the specific model in case your default workflow of your app has multiple embedding models present in it.\",\n                    \"placeholder\": \"Base Model\"\n                },\n                {\n                    \"path\": \"train_info.params.coarse_clusters\",\n                    \"field_type\": 3,\n                    \"default_value\": 32,\n                    \"description\": \"Each embedding vector is first split into a fixed amount of subgroups. This is the integer value k, in k-means clustering, used to determine the numbers of centroids each subgroup is split and clustered into.\",\n                    \"placeholder\": \"Coarse Clusters\",\n                    \"model_type_range_info\": {\n                        \"min\": 2,\n                        \"max\": 512,\n                        \"step\": 1\n                    }\n                },\n                {\n                    \"path\": \"train_info.params.eval_holdout_fraction\",\n                    \"field_type\": 7,\n                    \"default_value\": 0.2,\n                    \"description\": \"Percentage of all examples to hold out for evaluation when training.\",\n                    \"placeholder\": \"Evaluation Holdout Fraction\",\n                    \"model_type_range_info\": {\n                        \"max\": 1,\n                        \"step\": 0.01\n                    }\n                },\n                {\n                    \"path\": \"train_info.params.query_holdout_fraction\",\n                    \"field_type\": 7,\n                    \"default_value\": 0.1,\n                    \"description\": \"Deprecated, please use eval_info.params.query_holdout_fraction. \",\n                    \"placeholder\": \"Query Holdout Fraction\",\n                    \"model_type_range_info\": {\n                        \"min\": 0.01,\n                        \"max\": 1,\n                        \"step\": 0.01\n                    }\n                },\n                {\n                    \"path\": \"train_info.params.to_be_indexed_queries_fraction\",\n                    \"field_type\": 7,\n                    \"default_value\": 0.25,\n                    \"description\": \"Deprecated, please use eval_info.params.to_be_indexed_queries_fraction. \",\n                    \"placeholder\": \"To Be Indexed Queries Fraction\",\n                    \"model_type_range_info\": {\n                        \"min\": 0.01,\n                        \"max\": 1,\n                        \"step\": 0.01\n                    }\n                },\n                {\n                    \"path\": \"train_info.params.max_num_query_embeddings\",\n                    \"field_type\": 3,\n                    \"default_value\": 100,\n                    \"description\": \"Deprecated, please use eval_info.params.max_num_query_embeddings. \",\n                    \"placeholder\": \"Max Number of Query Embeddings\"\n                },\n                {\n                    \"path\": \"train_info.params.num_results_per_query\",\n                    \"field_type\": 11,\n                    \"default_value\": [\n                        1,\n                        5,\n                        10,\n                        20\n                    ],\n                    \"description\": \"Deprecated, please use eval_info.params.num_results_per_query. \",\n                    \"placeholder\": \"Number of Results Per Query\"\n                },\n                {\n                    \"path\": \"train_info.params.max_visited\",\n                    \"field_type\": 3,\n                    \"default_value\": 32,\n                    \"description\": \"Deprecated, please use eval_info.params.max_visited. \",\n                    \"placeholder\": \"Max Visited\"\n                },\n                {\n                    \"path\": \"train_info.params.quota\",\n                    \"field_type\": 3,\n                    \"default_value\": 1000,\n                    \"description\": \"Deprecated, please use eval_info.params.quota. \",\n                    \"placeholder\": \"Quota\"\n                },\n                {\n                    \"path\": \"train_info.params.beta\",\n                    \"field_type\": 3,\n                    \"default_value\": 1,\n                    \"description\": \"Deprecated, please use eval_info.params.beta. \",\n                    \"placeholder\": \"Beta\"\n                },\n                {\n                    \"path\": \"train_info.params.training_timeout\",\n                    \"field_type\": 3,\n                    \"default_value\": 7200,\n                    \"description\": \"The training timeout in seconds. Longer time allows for training to process more data before timing out. default 2 hours.\",\n                    \"placeholder\": \"Training timeout\",\n                    \"internal_only\": true\n                },\n                {\n                    \"path\": \"train_info.params.dataset_id\",\n                    \"field_type\": 16,\n                    \"default_value\": \"\",\n                    \"description\": \"Dataset to use for training this model.\",\n                    \"placeholder\": \"Training Dataset ID\",\n                    \"internal_only\": true\n                },\n                {\n                    \"path\": \"train_info.params.train_iters\",\n                    \"field_type\": 3,\n                    \"default_value\": 10,\n                    \"description\": \"The number of training iterations.\",\n                    \"placeholder\": \"Training iterations\",\n                    \"internal_only\": true\n                },\n                {\n                    \"path\": \"train_info.dataset.version\",\n                    \"field_type\": 20,\n                    \"description\": \"Dataset version to use for training this model. If a dataset version is not specified but a dataset is, we will automatically generate a dataset version.\",\n                    \"placeholder\": \"Training Dataset Version\"\n                },\n                {\n                    \"path\": \"train_info.params.dataset_version_id\",\n                    \"field_type\": 17,\n                    \"default_value\": \"\",\n                    \"description\": \"Deprecated in favor of train_info.dataset.version\",\n                    \"placeholder\": \"DEPRECATED: Training Dataset Version ID\"\n                },\n                {\n                    \"path\": \"eval_info.params.dataset_id\",\n                    \"field_type\": 16,\n                    \"default_value\": \"\",\n                    \"description\": \"Dataset to use for evaluating this model. \",\n                    \"placeholder\": \"Eval Dataset ID\"\n                },\n                {\n                    \"path\": \"eval_info.params.dataset_version_id\",\n                    \"field_type\": 17,\n                    \"default_value\": \"\",\n                    \"description\": \"Dataset version to use for evaluating this model. If a dataset version is not specified but a dataset is, we will automatically generate a dataset version. \",\n                    \"placeholder\": \"Eval Dataset Version ID\"\n                },\n                {\n                    \"path\": \"eval_info.params.query_holdout_fraction\",\n                    \"field_type\": 7,\n                    \"default_value\": 0.1,\n                    \"description\": \"When evaluating, the examples held out for evaluation are split into two, potentially overlapping subsets: indexed and query examples. The indexed subset is indexed in-memory as the original and new projected position are used to compare their distance from the query subset to produce evaluations. query_holdout_fraction is the data percentage used from the evaluation subset for querying.\",\n                    \"placeholder\": \"Query Holdout Fraction\",\n                    \"internal_only\": true,\n                    \"model_type_range_info\": {\n                        \"min\": 0.01,\n                        \"max\": 1,\n                        \"step\": 0.01\n                    }\n                },\n                {\n                    \"path\": \"eval_info.params.to_be_indexed_queries_fraction\",\n                    \"field_type\": 7,\n                    \"default_value\": 0.25,\n                    \"description\": \"When evaluating, the examples held out for evaluation are split into two, potentially overlapping subsets: indexed and query examples. The indexed subset is indexed in-memory as their original and new projected position are used to compare their distance from the query subset to produce evaluations. to_be_indexed_queries_fraction is the data percentage used from the evaluation subset for indexing.\",\n                    \"placeholder\": \"To Be Indexed Queries Fraction\",\n                    \"internal_only\": true,\n                    \"model_type_range_info\": {\n                        \"min\": 0.01,\n                        \"max\": 1,\n                        \"step\": 0.01\n                    }\n                },\n                {\n                    \"path\": \"eval_info.params.max_num_query_embeddings\",\n                    \"field_type\": 3,\n                    \"default_value\": 100,\n                    \"description\": \"Max number of queries examples used when evaluating. The lesser value between max_num_query_embeddings or [query_holdout_fraction * hold out set size] will be used to decide the number of query embeddings used. Larger number of query embeddings will result in slower evaluations.\",\n                    \"placeholder\": \"Max Number of Query Embeddings\",\n                    \"internal_only\": true\n                },\n                {\n                    \"path\": \"eval_info.params.num_results_per_query\",\n                    \"field_type\": 11,\n                    \"default_value\": [\n                        1,\n                        5,\n                        10,\n                        20\n                    ],\n                    \"description\": \"A list of numbers, each representing the number of nearest examples to consider per query when evaluating recall. Max num_results_per_query should be less than or equal to quota.\",\n                    \"placeholder\": \"Number of Results Per Query\",\n                    \"internal_only\": true\n                },\n                {\n                    \"path\": \"eval_info.params.max_visited\",\n                    \"field_type\": 3,\n                    \"default_value\": 32,\n                    \"description\": \"A integer will be used for both evaluation and search. During both search and evaluation, it cuts off the number of centroids we are going to search against. We compare the distance to every example for each centroid searched against, up until the quota number of examples. Larger numbers will result in slower evaluations and search.\",\n                    \"placeholder\": \"Max Visited\",\n                    \"internal_only\": true\n                },\n                {\n                    \"path\": \"eval_info.params.quota\",\n                    \"field_type\": 3,\n                    \"default_value\": 1000,\n                    \"description\": \"During evaluations it cuts off the max number of examples we are going to search against. The max number of examples searched against is also limited by max_visited and the number of indexed examples. Larger numbers will result in slower evaluations.\",\n                    \"placeholder\": \"Quota\",\n                    \"internal_only\": true\n                },\n                {\n                    \"path\": \"eval_info.params.beta\",\n                    \"field_type\": 3,\n                    \"default_value\": 1,\n                    \"description\": \"Beta is a positive number which scales the importance of recall over precision. Beta < 1 lends more weight to precision, while beta > 1 favors recall. Beta = 1 results in standard f1 calculations.\",\n                    \"placeholder\": \"Beta\",\n                    \"internal_only\": true\n                }\n            ],\n            \"evaluation_type\": 4\n        },\n        {\n            \"id\": \"image-color-recognizer\",\n            \"title\": \"Image Color Recognizer\",\n            \"description\": \"Recognize standard color formats and the proportion each color that covers an image.\",\n            \"input_fields\": [\n                \"image\"\n            ],\n            \"output_fields\": [\n                \"colors\"\n            ]\n        },\n        {\n            \"id\": \"concept-thresholder\",\n            \"title\": \"Concept Thresholder\",\n            \"description\": \"Threshold input concepts according to both a threshold and an operator (>, >=, =, <=, or <). For example, assume the \\\" > \\\" threshold type is set for the model, then if the input concept.value is greater than the threshold for that concept, the input concept will be output from this model, otherwise it will not be output by the model.\",\n            \"input_fields\": [\n                \"concepts\"\n            ],\n            \"output_fields\": [\n                \"concepts\"\n            ],\n            \"creatable\": true,\n            \"model_type_fields\": [\n                {\n                    \"path\": \"output_info.data.concepts\",\n                    \"field_type\": 5,\n                    \"default_value\": [],\n                    \"description\": \"List of concepts and each concept has concept.value set to the threshold. If a concept is not specified here then that concept will be allowed through to the output always.\",\n                    \"placeholder\": \"List of concepts and each concept has concept.value set to the threshold.\",\n                    \"required\": true\n                },\n                {\n                    \"path\": \"output_info.params.concept_threshold_type\",\n                    \"field_type\": 8,\n                    \"default_value\": \"GREATER_THAN\",\n                    \"description\": \"This is the operation used to to compare such as input value {concept_threshold_type} concept.value where concept.value is defined in this model's config and represents the threshold for each concept. For example if this concept_threshold_type is GREATER_THAN_OR_EQUAL and the concept.value for the 'dog' concept is 0.75 then any data coming into this model with the concept of dog greater than or equal to 0.75 will be output from this model.\",\n                    \"placeholder\": \"Concept Threshold Type\",\n                    \"model_type_enum_options\": [\n                        {\n                            \"id\": \"GREATER_THAN\"\n                        },\n                        {\n                            \"id\": \"GREATER_THAN_OR_EQUAL\"\n                        },\n                        {\n                            \"id\": \"LESS_THAN\"\n                        },\n                        {\n                            \"id\": \"LESS_THAN_OR_EQUAL\"\n                        },\n                        {\n                            \"id\": \"EQUAL\"\n                        }\n                    ],\n                    \"required\": true\n                },\n                {\n                    \"path\": \"output_info.params.filter_other_concepts\",\n                    \"field_type\": 1,\n                    \"default_value\": false,\n                    \"description\": \"The default setting of False for this parameter means that the concepts found in the input data but that are NOT defined in output_info.data.concepts will be let through. Setting filter_other_concepts = True will filter out these additional concepts found in the input that are not defined in output_info.data.concepts.\",\n                    \"placeholder\": \"Keep other concepts found in input (default) or set to True to filter them out when not in the list of concepts for this model.\"\n                }\n            ]\n        },\n        {\n            \"id\": \"region-thresholder\",\n            \"title\": \"Region Thresholder\",\n            \"description\": \"Threshold regions based on the concepts that they contain using a threshold per concept and an overall operator (>, >=, =, <=, or <). For example, assume the \\\" > \\\" threshold type is set for the model, then if the input regions[...].data.concepts.value is greater than the threshold for that concept, the input concept will be output from this model, otherwise it will not be output by the model. If the entire list of concepts at regions[...].data.concepts is filtered out then the overall region will also be removed.\",\n            \"input_fields\": [\n                \"regions[...].data.concepts\"\n            ],\n            \"output_fields\": [\n                \"regions[...].data.concepts\"\n            ],\n            \"creatable\": true,\n            \"model_type_fields\": [\n                {\n                    \"path\": \"output_info.data.concepts\",\n                    \"field_type\": 5,\n                    \"default_value\": [],\n                    \"description\": \"List of concepts and each concept has concept.value set to the threshold. If a concept is not specified here then that concept will be allowed through to the output always.\",\n                    \"placeholder\": \"List of concepts and each concept has concept.value set to the threshold.\",\n                    \"required\": true\n                },\n                {\n                    \"path\": \"output_info.params.concept_threshold_type\",\n                    \"field_type\": 8,\n                    \"default_value\": \"GREATER_THAN\",\n                    \"description\": \"This is the operation used to to compare such as input value {concept_threshold_type} concept.value where concept.value is defined in this model's config and represents the threshold for each concept. For example if this concept_threshold_type is GREATER_THAN_OR_EQUAL and the concept.value for the 'dog' concept is 0.75 then any data coming into this model with the concept of dog greater than or equal to 0.75 will be output from this model.\",\n                    \"placeholder\": \"Concept Threshold Type\",\n                    \"model_type_enum_options\": [\n                        {\n                            \"id\": \"GREATER_THAN\"\n                        },\n                        {\n                            \"id\": \"GREATER_THAN_OR_EQUAL\"\n                        },\n                        {\n                            \"id\": \"LESS_THAN\"\n                        },\n                        {\n                            \"id\": \"LESS_THAN_OR_EQUAL\"\n                        },\n                        {\n                            \"id\": \"EQUAL\"\n                        }\n                    ],\n                    \"required\": true\n                },\n                {\n                    \"path\": \"output_info.params.filter_other_concepts\",\n                    \"field_type\": 1,\n                    \"default_value\": false,\n                    \"description\": \"The default setting of False for this parameter means that the concepts found in the input data but that are NOT defined in output_info.data.concepts will be let through. Setting filter_other_concepts = True will filter out these additional concepts found in the input that are not defined in output_info.data.concepts.\",\n                    \"placeholder\": \"Keep other concepts found in input (default) or set to True to filter them out when not in the list of concepts for this model.\"\n                },\n                {\n                    \"path\": \"output_info.params.filter_empty_input_regions\",\n                    \"field_type\": 1,\n                    \"default_value\": false,\n                    \"description\": \"This controls regions that originally had no concepts in them. If filter_empty_input_regions is True then we will remove those regions. If False (default) we will let those regions through.\",\n                    \"placeholder\": \"Filter out empty regions in input.\"\n                }\n            ]\n        },\n        {\n            \"id\": \"concept-synonym-mapper\",\n            \"title\": \"Concept Synonym Mapper\",\n            \"description\": \"Map the input concepts to output concepts by following synonym concept relations in the knowledge graph of your app. \",\n            \"input_fields\": [\n                \"concepts\"\n            ],\n            \"output_fields\": [\n                \"concepts\"\n            ],\n            \"creatable\": true,\n            \"model_type_fields\": [\n                {\n                    \"path\": \"output_info.params.knowledge_graph_id\",\n                    \"field_type\": 2,\n                    \"default_value\": \"\",\n                    \"description\": \"An optional knowledge graph id that is present in your app's concept relations. This allows you to carve out a subset of all the concept relations in your app and use a subset for mapping with this model.\",\n                    \"placeholder\": \"Knowledge graph ID\"\n                }\n            ]\n        },\n        {\n            \"id\": \"annotation-writer\",\n            \"title\": \"Annotation Writer\",\n            \"description\": \"Write the input data to the database in the form of an annotation with a specified status as if a specific user created the annotation.\",\n            \"input_fields\": [\n                \"any\"\n            ],\n            \"output_fields\": [\n                \"any\"\n            ],\n            \"creatable\": true,\n            \"model_type_fields\": [\n                {\n                    \"path\": \"output_info.params.annotation_status\",\n                    \"field_type\": 8,\n                    \"default_value\": \"ANNOTATION_SUCCESS\",\n                    \"description\": \"This is the status for the annotations created by annotation-writer model.\",\n                    \"placeholder\": \"Model metadata annotation status\",\n                    \"model_type_enum_options\": [\n                        {\n                            \"id\": \"ANNOTATION_SUCCESS\",\n                            \"aliases\": [\n                                {\n                                    \"id_int\": \"24150\"\n                                }\n                            ]\n                        },\n                        {\n                            \"id\": \"ANNOTATION_PENDING\",\n                            \"aliases\": [\n                                {\n                                    \"id_int\": \"24151\"\n                                }\n                            ]\n                        },\n                        {\n                            \"id\": \"ANNOTATION_AWAITING_REVIEW\",\n                            \"aliases\": [\n                                {\n                                    \"id_int\": \"24157\"\n                                }\n                            ]\n                        },\n                        {\n                            \"id\": \"ANNOTATION_AWAITING_CONSENSUS_REVIEW\",\n                            \"aliases\": [\n                                {\n                                    \"id_int\": \"24159\"\n                                }\n                            ]\n                        }\n                    ],\n                    \"required\": true\n                },\n                {\n                    \"path\": \"output_info.params.annotation_user_id\",\n                    \"field_type\": 9,\n                    \"default_value\": \"\",\n                    \"description\": \"This is the user_id for which to write the annotation on their behalf as if they manually did the work themselves.\",\n                    \"placeholder\": \"user_id to write that annotation as\",\n                    \"required\": true\n                },\n                {\n                    \"path\": \"output_info.params.annotation_info\",\n                    \"field_type\": 10,\n                    \"default_value\": {},\n                    \"description\": \"Additional JSON annotation information to attach to each annotation written by this model. For example, if you use {\\\"task_id\\\": \\\"my-task-id\\\"} and make annotation_status PENDING with annotation_user_id set to a labeler worker, you can have a never ending set of annotations for that user to work on.\",\n                    \"placeholder\": \"Annotation Info\"\n                },\n                {\n                    \"path\": \"output_info.params.task_id\",\n                    \"field_type\": 2,\n                    \"default_value\": \"\",\n                    \"description\": \"The id of the task annotation belongs to\",\n                    \"placeholder\": \"Task id\"\n                }\n            ]\n        },\n        {\n            \"id\": \"image-crop\",\n            \"title\": \"Image Cropper\",\n            \"description\": \"Crop the input image according to each input region that is present in the input. When used in a workflow this model can look back along the graph of the workflow to find the input image if the preceding model does not output an image itself so that you can do image -> detector -> cropper type of workflow easily.\",\n            \"input_fields\": [\n                \"image\",\n                \"regions\"\n            ],\n            \"output_fields\": [\n                \"regions[...].data.image\"\n            ],\n            \"creatable\": true,\n            \"model_type_fields\": [\n                {\n                    \"path\": \"output_info.params.margin\",\n                    \"field_type\": 7,\n                    \"default_value\": 1,\n                    \"description\": \"A margin to increase/decrease around the bounding boxes before doing the crop. A 2.0 margin would mean making a bounding box 2x larger with the same center location and conducting crop using that box.\",\n                    \"placeholder\": \"Margin around the image\",\n                    \"model_type_range_info\": {\n                        \"max\": 10\n                    }\n                }\n            ]\n        },\n        {\n            \"id\": \"random-sample\",\n            \"title\": \"Random Sampler\",\n            \"description\": \"Randomly sample allowing the input to pass to the output. This is done with the conditional keep_fraction > rand() where keep_fraction is the fraction to allow through on average.\",\n            \"input_fields\": [\n                \"any\"\n            ],\n            \"output_fields\": [\n                \"any\"\n            ],\n            \"creatable\": true,\n            \"model_type_fields\": [\n                {\n                    \"path\": \"output_info.params.keep_fraction\",\n                    \"field_type\": 7,\n                    \"default_value\": 0.5,\n                    \"description\": \"This is the fraction of input to randomly keep. This is implemented as simply: if keep_fraction > rand() { then output this input from the model }. This is applied independently for each input sent in a batch to the model.\",\n                    \"placeholder\": \"Sampling fraction\",\n                    \"required\": true,\n                    \"model_type_range_info\": {\n                        \"max\": 1,\n                        \"step\": 0.01\n                    }\n                }\n            ]\n        },\n        {\n            \"id\": \"knn-concept\",\n            \"title\": \"KNN Classifier\",\n            \"description\": \"Use k nearest neighbor search and plurality voting amongst the nearest neighbors to classify new instances. Recommended when you only have a small dataset like one image per concept.\",\n            \"input_fields\": [\n                \"embeddings\"\n            ],\n            \"output_fields\": [\n                \"concepts\"\n            ],\n            \"creatable\": true,\n            \"model_type_fields\": [\n                {\n                    \"path\": \"output_info.params.min_value\",\n                    \"field_type\": 7,\n                    \"default_value\": 0,\n                    \"description\": \"Minimum value of concept's probability score in result. In other words, all concepts with a probability score less than this threshold will be filtered out.\",\n                    \"placeholder\": \"Min value\",\n                    \"model_type_range_info\": {\n                        \"max\": 1,\n                        \"step\": 0.01\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.max_concepts\",\n                    \"field_type\": 3,\n                    \"default_value\": 20,\n                    \"description\": \"Maximum number of concepts in result.\",\n                    \"placeholder\": \"Maximum concepts\"\n                }\n            ]\n        },\n        {\n            \"id\": \"visual-keypointer\",\n            \"title\": \"Visual Keypoint\",\n            \"description\": \"This model detects keypoints in images or video frames.\",\n            \"input_fields\": [\n                \"image\"\n            ],\n            \"output_fields\": [\n                \"regions[...].data.concepts,regions[...].region_info.keypoint_locations\"\n            ],\n            \"expected_input_layers\": [\n                {\n                    \"data_field_name\": \"image\",\n                    \"shapes\": [\n                        {\n                            \"dims\": [\n                                -1,\n                                -1,\n                                3\n                            ],\n                            \"max_dims\": [\n                                1024,\n                                1024,\n                                3\n                            ],\n                            \"data_type\": 2,\n                            \"description\": \"First two dimensions are the height and width, followed by the number of channels.\"\n                        }\n                    ],\n                    \"description\": \"Image urls or base64 are converted into numpy arrays of the specified size and forwarded to the model. If flexible dims are provided, inputs will be downsampled and padded to a default size.\"\n                }\n            ],\n            \"expected_output_layers\": [\n                {\n                    \"data_field_name\": \"regions[...].data.concepts,regions[...].region_info.keypoint_locations\",\n                    \"shapes\": [\n                        {\n                            \"dims\": [\n                                -1,\n                                3\n                            ],\n                            \"data_type\": 5,\n                            \"description\": \"First dimension corresponds to each models ordered keypoint as specified in the keypoint names of the concept, and the second dimensions corresponds to the x, y, and z location of that keypoint in the image.\"\n                        },\n                        {\n                            \"dims\": [\n                                -1,\n                                2\n                            ],\n                            \"data_type\": 5,\n                            \"description\": \"First dimension corresponds to each models ordered keypoint as specified in the keypoint names of the concept, and the second dimensions corresponds to the x and y location of that keypoint in the image.\"\n                        }\n                    ],\n                    \"requires_label_filename\": true\n                }\n            ]\n        },\n        {\n            \"id\": \"status-push\",\n            \"title\": \"Status Push\",\n            \"description\": \"This model pushes processing status of a batch of inputs ingested through vendor/inputs endpoint in one request.\",\n            \"input_fields\": [\n                \"any\"\n            ],\n            \"output_fields\": [\n                \"any\"\n            ],\n            \"creatable\": true,\n            \"internal_only\": true\n        },\n        {\n            \"id\": \"results-push\",\n            \"title\": \"Results Push\",\n            \"description\": \"This model pushes clarifai prediction results in an external format.\",\n            \"input_fields\": [\n                \"any\"\n            ],\n            \"output_fields\": [\n                \"any\"\n            ],\n            \"creatable\": true,\n            \"internal_only\": true\n        },\n        {\n            \"id\": \"email\",\n            \"title\": \"Email Alert\",\n            \"description\": \"Email alert model will send an email if there are any data fields input to this model.\",\n            \"input_fields\": [\n                \"any\"\n            ],\n            \"output_fields\": [\n                \"any\"\n            ],\n            \"creatable\": true,\n            \"internal_only\": true,\n            \"model_type_fields\": [\n                {\n                    \"path\": \"output_info.params.to\",\n                    \"field_type\": 2,\n                    \"default_value\": \"\",\n                    \"description\": \"A comma separated list of up to 3 different emails to send to. For example \\\"Bob <bob@example.com>, Stacy <stacy@example.com>\\\"\",\n                    \"placeholder\": \"Bob <bob@example.com>, Stacy <stacy@example.com>\",\n                    \"required\": true\n                },\n                {\n                    \"path\": \"output_info.params.subject\",\n                    \"field_type\": 2,\n                    \"default_value\": \"\",\n                    \"description\": \"Subject of your email.\",\n                    \"placeholder\": \"Subject of your email here...\",\n                    \"required\": true\n                },\n                {\n                    \"path\": \"output_info.params.html\",\n                    \"field_type\": 2,\n                    \"default_value\": \"<html><body>Wrapped html body of your email.</body></html>\",\n                    \"description\": \"Formatted html body. This must be provided as valid HTML including the <html></html> tags.\",\n                    \"placeholder\": \"<html><body>Wrapped html body of your email.</body></html>\",\n                    \"required\": true\n                },\n                {\n                    \"path\": \"output_info.params.text\",\n                    \"field_type\": 2,\n                    \"default_value\": \"\",\n                    \"description\": \"Text body as fallback in case the email client of recipient can't read HTML.\",\n                    \"placeholder\": \"Fallback text body for older email clients goes here...\",\n                    \"required\": true\n                }\n            ]\n        },\n        {\n            \"id\": \"sms\",\n            \"title\": \"SMS Alert\",\n            \"description\": \"SMS alert model will send a SMS if there are any data fields input to this model.\",\n            \"input_fields\": [\n                \"any\"\n            ],\n            \"output_fields\": [\n                \"any\"\n            ],\n            \"creatable\": true,\n            \"internal_only\": true,\n            \"model_type_fields\": [\n                {\n                    \"path\": \"output_info.params.to\",\n                    \"field_type\": 2,\n                    \"default_value\": \"\",\n                    \"description\": \"A comma separated list of up to 3 different phone numbers to send to.\",\n                    \"placeholder\": \"123-456-7890, 1-987-654-3210\",\n                    \"required\": true\n                },\n                {\n                    \"path\": \"output_info.params.body\",\n                    \"field_type\": 2,\n                    \"default_value\": \"\",\n                    \"description\": \"The text body of your SMS message.\",\n                    \"placeholder\": \"Body of your SMS message here...\",\n                    \"required\": true\n                }\n            ]\n        },\n        {\n            \"id\": \"aws-lambda\",\n            \"title\": \"AWS Lambda\",\n            \"description\": \"This model sends data to an AWS lambda function so you can implement any arbitrary logic to be handled within a model predict or workflow. The request our API sends is a PostModelOutputsRequest in the 'request' field and the response we expect is a MultiOutputResponse response in the 'response' field.\",\n            \"input_fields\": [\n                \"any\"\n            ],\n            \"output_fields\": [\n                \"any\"\n            ],\n            \"creatable\": true,\n            \"internal_only\": true,\n            \"model_type_fields\": [\n                {\n                    \"path\": \"output_info.params.arn\",\n                    \"field_type\": 2,\n                    \"default_value\": \"\",\n                    \"description\": \"The ARN for the lambda function.\",\n                    \"placeholder\": \"arn:aws:lambda:us-east-1:{AWS_ACCOUNT_ID}:function:{FUNC_NAME}\",\n                    \"required\": true\n                }\n            ]\n        },\n        {\n            \"id\": \"custom-code-operator\",\n            \"title\": \"Custom Code Operator\",\n            \"description\": \"This model expects a Python 3.9 driver function with the following signature: \\\"def main(req):\\\". Here, \\\"req\\\" is a dictionary with a single key \\\"inputs\\\" that holds a list of \\\"Input\\\" objects from \\\"clarifai_grpc.grpc.api.service_pb2\\\"; these inputs are normally sent in API prediction requests.\\nThe available libraries for importing are: numpy, scipy, PIL and clarifai_grpc.\\nThe response should either be a python dictionary whose nested structure mirrors that of MultiOutputResponse in clarifai_grpc.grpc.api.service_pb2.\\nIDs in inputs should be forwared to outputs 1-to-1. You can also provide helpers to reference in your main implementation.\\nAll the code must be passed in via output_info.params.operator_code.\\nEach Execution can last up to 50 seconds and consume 256 MBs of memory.\",\n            \"input_fields\": [\n                \"any\"\n            ],\n            \"output_fields\": [\n                \"any\"\n            ],\n            \"creatable\": true,\n            \"model_type_fields\": [\n                {\n                    \"path\": \"output_info.params.operator_code\",\n                    \"field_type\": 15,\n                    \"default_value\": \"# Example code to geotag image inputs\\n# If you're sending this using JSON, you must replace newlines with '\\\\n'\\nfrom clarifai_grpc.grpc.api import resources_pb2, service_pb2\\nfrom clarifai_grpc.grpc.api.status import status_code_pb2, status_pb2\\nfrom google.protobuf.json_format import MessageToDict, ParseDict\\n\\n# Define commonly used constants outside functions to increase performance.\\nNYC_LAT, NYC_LON = 40.7128, 74.0060\\nNYC_GEO_POINT = resources_pb2.GeoPoint(latitude=NYC_LAT, longitude=NYC_LON)\\n\\n# checks if the input contained an image\\ndef validate_image_is_present(image_pbf, input_id):\\n    if image_pbf.ByteSize() == 0: # image is not set\\n        err_status = status_pb2.Status(code=status_code_pb2.INPUT_INVALID_ARGUMENT,\\n            description=f'No Image Received for Input with ID {input_id}')\\n        err_resp = service_pb2.MultiOutputResponse(status=err_status)\\n        return err_resp\\n    return\\n\\n\\n# extract inputs to operator from request, and report error if none are present.\\ndef get_inputs_from_req(req):\\n    req_inputs = req.get('inputs', None)\\n    if not req_inputs:\\n      err_status = status_pb2.Status(code=status_code_pb2.INPUT_INVALID_ARGUMENT,\\n        description='No Inputs Received')\\n      err_resp = service_pb2.MultiOutputResponse(status=err_status)\\n      return None, err_resp\\n    return req_inputs, None\\n\\n# add a geo-tag to the data if it contains an image, if no image is present return error.\\ndef build_geotagged_image_from_input_image(input_pbf):\\n    data_pbf = input_pbf.data\\n    image_pbf = data_pbf.image\\n    input_id = input_pbf.id # id is just a string\\n    # verify there is an image to geo-tag\\n    err_resp = validate_image_is_present(image_pbf, input_id)\\n    if err_resp != None:\\n      return None, err_resp\\n    req_output = resources_pb2.Output(id=input_id) # we must forward the ID, otherwise errors will occur.\\n    # Here, we copy the original data, which includes the input image to tag, into the output.\\n    req_output.data.CopyFrom(data_pbf)\\n    # Now, we add a geo-tag to the input.\\n    req_output.data.geo.geo_point.CopyFrom(NYC_GEO_POINT)\\n\\n    return req_output, None\\n\\ndef main(req):\\n  inputs, err_resp = get_inputs_from_req(req)\\n  if err_resp != None:\\n      return MessageToDict(err_resp, preserving_proto_field_name=True)\\n  resp_outputs = []\\n  for inp in inputs:\\n      input_pbf = ParseDict(inp, resources_pb2.Input())\\n      output, err_resp = build_geotagged_image_from_input_image(input_pbf)\\n      if err_resp != None:\\n          return MessageToDict(err_resp, preserving_proto_field_name=True)\\n      resp_outputs.append(output)\\n  resp = service_pb2.MultiOutputResponse(outputs=resp_outputs,\\n    status=status_pb2.Status(code=status_code_pb2.SUCCESS)) # expected format of the response\\n  return MessageToDict(resp, preserving_proto_field_name=True)\\n\",\n                    \"description\": \"Custom Python 3.9 code to be executed\",\n                    \"placeholder\": \"Custom Python 3.9 code to be executed\",\n                    \"required\": true\n                }\n            ]\n        },\n        {\n            \"id\": \"object-counter\",\n            \"title\": \"Object Counter\",\n            \"description\": \"count number of regions that match this model's active concepts frame by frame.\",\n            \"input_fields\": [\n                \"regions[...].data.concepts\"\n            ],\n            \"output_fields\": [\n                \"metadata\"\n            ],\n            \"creatable\": true,\n            \"model_type_fields\": [\n                {\n                    \"path\": \"output_info.data.concepts\",\n                    \"field_type\": 4,\n                    \"description\": \"List of concepts you want this model to use to count regions with matching concepts from each frame.  if none are specified, all regions with any concepts will be counted\",\n                    \"placeholder\": \"List of concepts\"\n                }\n            ]\n        },\n        {\n            \"id\": \"image-align\",\n            \"title\": \"Image Align\",\n            \"description\": \"Aligns images using keypoints\",\n            \"input_fields\": [\n                \"image\",\n                \"regions[...].data.concepts,regions[...].region_info.keypoint_locations\"\n            ],\n            \"output_fields\": [\n                \"image\"\n            ],\n            \"creatable\": true,\n            \"model_type_fields\": [\n                {\n                    \"path\": \"output_info.params.alignment_type\",\n                    \"field_type\": 2,\n                    \"default_value\": \"SIMILARITY\",\n                    \"description\": \"Image Alignment transform type\",\n                    \"placeholder\": \"Image Alignment transform type\",\n                    \"model_type_enum_options\": [\n                        {\n                            \"id\": \"SIMILARITY\",\n                            \"description\": \"Deprecated, please use THREE_POINT_SIMILARITY.\"\n                        },\n                        {\n                            \"id\": \"THREE_POINT_SIMILARITY\",\n                            \"description\": \"3 point alignment with similarity transform.\"\n                        },\n                        {\n                            \"id\": \"FIVE_POINT_SIMILARITY\",\n                            \"description\": \"5 point alignment with similarity transform.\"\n                        }\n                    ],\n                    \"required\": true\n                },\n                {\n                    \"path\": \"output_info.params.output_size\",\n                    \"field_type\": 3,\n                    \"default_value\": 112,\n                    \"description\": \"Image Alignment output size\",\n                    \"placeholder\": \"Image Alignment output size\",\n                    \"required\": true,\n                    \"model_type_range_info\": {\n                        \"min\": 32,\n                        \"max\": 1080\n                    }\n                }\n            ]\n        },\n        {\n            \"id\": \"input-searcher\",\n            \"title\": \"Cross-App Input Searcher\",\n            \"description\": \"Triggers a visual search in another app based on the model configs if concept(s) are found in images and returns the matched search hits as regions.\",\n            \"input_fields\": [\n                \"concepts\",\n                \"image\",\n                \"text\"\n            ],\n            \"output_fields\": [\n                \"hits\"\n            ],\n            \"creatable\": true,\n            \"internal_only\": true,\n            \"model_type_fields\": [\n                {\n                    \"path\": \"output_info.params.key\",\n                    \"field_type\": 2,\n                    \"default_value\": \"\",\n                    \"description\": \"A personal access token (PAT) or API Key to authenticate search requests.\",\n                    \"placeholder\": \"4bc27...\",\n                    \"required\": true\n                },\n                {\n                    \"path\": \"output_info.params.app_id\",\n                    \"field_type\": 2,\n                    \"default_value\": \"\",\n                    \"description\": \"A unique ID indicating which application should be searched.\",\n                    \"placeholder\": \"bc45a3...\",\n                    \"required\": true\n                },\n                {\n                    \"path\": \"output_info.params.min_score\",\n                    \"field_type\": 7,\n                    \"default_value\": 0,\n                    \"description\": \"Minimum search score to forward search hit in results.\",\n                    \"placeholder\": \"Minimum search score.\",\n                    \"model_type_range_info\": {\n                        \"max\": 1,\n                        \"step\": 0.01\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.max_results\",\n                    \"field_type\": 3,\n                    \"default_value\": 20,\n                    \"description\": \"Maximum number of search results to present in results.\",\n                    \"placeholder\": \"Max number of search results.\"\n                },\n                {\n                    \"path\": \"output_info.params.input_type\",\n                    \"field_type\": 2,\n                    \"default_value\": \"IMAGE\",\n                    \"description\": \"Whether to perform search on image or text.\",\n                    \"placeholder\": \"Input type to search on.\",\n                    \"model_type_enum_options\": [\n                        {\n                            \"id\": \"IMAGE\",\n                            \"description\": \"Input search on concepts and images.\"\n                        },\n                        {\n                            \"id\": \"TEXT\",\n                            \"description\": \"Input search on text.\"\n                        }\n                    ],\n                    \"required\": true\n                }\n            ]\n        },\n        {\n            \"id\": \"input-filter\",\n            \"title\": \"Input Filter\",\n            \"description\": \"If the input going through this model does not match those we are filtering for, it will not be passed on in the workflow branch.\",\n            \"input_fields\": [\n                \"any\"\n            ],\n            \"output_fields\": [\n                \"any\"\n            ],\n            \"creatable\": true,\n            \"internal_only\": true,\n            \"model_type_fields\": [\n                {\n                    \"path\": \"output_info.params.filter_for_image\",\n                    \"field_type\": 1,\n                    \"default_value\": false,\n                    \"description\": \"Whether we should allow image inputs to pass through\",\n                    \"placeholder\": \"Filter For Image\"\n                },\n                {\n                    \"path\": \"output_info.params.filter_for_text\",\n                    \"field_type\": 1,\n                    \"default_value\": false,\n                    \"description\": \"Whether we should allow text inputs to pass through\",\n                    \"placeholder\": \"Filter For Text\"\n                },\n                {\n                    \"path\": \"output_info.params.filter_for_audio\",\n                    \"field_type\": 1,\n                    \"default_value\": false,\n                    \"description\": \"Whether we should allow audio inputs to pass through\",\n                    \"placeholder\": \"Filter For Audio\"\n                }\n            ]\n        },\n        {\n            \"id\": \"text-to-audio\",\n            \"title\": \"Text to Audio\",\n            \"description\": \"Given text input, this model produces an audio file containing the spoken version of the input.\",\n            \"input_fields\": [\n                \"text\"\n            ],\n            \"output_fields\": [\n                \"audio\"\n            ],\n            \"expected_input_layers\": [\n                {\n                    \"data_field_name\": \"text\",\n                    \"shapes\": [\n                        {\n                            \"dims\": [\n                                -1\n                            ],\n                            \"data_type\": 1,\n                            \"description\": \"The text string sent to the model.\"\n                        }\n                    ],\n                    \"description\": \"Text urls content or raw text passed in through the API are directly sent to the model.\"\n                }\n            ],\n            \"expected_output_layers\": [\n                {\n                    \"data_field_name\": \"audio\",\n                    \"shapes\": [\n                        {\n                            \"dims\": [\n                                -1\n                            ],\n                            \"data_type\": 5,\n                            \"description\": \"Audio file with spoken verison of input. Audio samples returned should represent a wav file.\"\n                        }\n                    ]\n                },\n                {\n                    \"data_field_name\": \"audio.audio_info.sample_rate\",\n                    \"shapes\": [\n                        {\n                            \"dims\": [\n                                1\n                            ],\n                            \"data_type\": 3,\n                            \"description\": \"The sample rate of the audio.\"\n                        }\n                    ]\n                }\n            ]\n        },\n        {\n            \"id\": \"regex-based-classifier\",\n            \"title\": \"Regex Based Classifier\",\n            \"description\": \"Classifies text using regex. If the regex matches, the text is classified as the provided concepts.\",\n            \"input_fields\": [\n                \"text\"\n            ],\n            \"output_fields\": [\n                \"concepts\"\n            ],\n            \"creatable\": true,\n            \"model_type_fields\": [\n                {\n                    \"path\": \"output_info.data.concepts\",\n                    \"field_type\": 4,\n                    \"default_value\": [],\n                    \"description\": \"Select the concepts that you want this model version to predict.\",\n                    \"placeholder\": \"List of concepts\",\n                    \"required\": true\n                },\n                {\n                    \"path\": \"output_info.params.regex\",\n                    \"field_type\": 2,\n                    \"default_value\": \"\",\n                    \"description\": \"This is the regex that will be used to classify the text. If it matches, the text will be classified as the concepts selected defined for this model version.\",\n                    \"placeholder\": \"Regex\",\n                    \"required\": true\n                }\n            ]\n        },\n        {\n            \"id\": \"prompter\",\n            \"title\": \"Prompter\",\n            \"description\": \"Prompt template where inputted text will be inserted into placeholders marked with '{data.text.raw}'.\",\n            \"input_fields\": [\n                \"text\"\n            ],\n            \"output_fields\": [\n                \"text\"\n            ],\n            \"creatable\": true,\n            \"model_type_fields\": [\n                {\n                    \"path\": \"output_info.params.prompt_template\",\n                    \"field_type\": 2,\n                    \"default_value\": \"{data.text.raw}\",\n                    \"description\": \"Template used as a template for creating prompts with dynamic values. The prompt template must contain atleast one instance of '{data.text.raw}'. At inference time, all instances of '{data.text.raw}' in the prompt template will be replaced with the inputted text data.\",\n                    \"placeholder\": \"{data.text.raw}\",\n                    \"required\": true\n                }\n            ]\n        },\n        {\n            \"id\": \"remote-operator\",\n            \"title\": \"Remote Operator\",\n            \"description\": \"This model executes any code using a remote runner.\",\n            \"input_fields\": [\n                \"any\"\n            ],\n            \"output_fields\": [\n                \"any\"\n            ],\n            \"creatable\": true,\n            \"internal_only\": true,\n            \"model_type_fields\": [\n                {\n                    \"path\": \"output_info.params.runner_labels\",\n                    \"field_type\": 13,\n                    \"default_value\": [],\n                    \"description\": \"A list of runner labels to match on for this task. Ex: laptop, model-abc123\",\n                    \"required\": true\n                }\n            ]\n        },\n        {\n            \"id\": \"rag-prompter\",\n            \"title\": \"RAG Prompter\",\n            \"description\": \"A prompt template where we will perform a semantic search in the app with the incoming text. The inputted text will be inserted into placeholders marked with '{data.text.raw}' and search results will be inserted into placeholders with '{data.hits}', which will be new line separated.\",\n            \"input_fields\": [\n                \"text\"\n            ],\n            \"output_fields\": [\n                \"text\"\n            ],\n            \"creatable\": true,\n            \"internal_only\": true,\n            \"model_type_fields\": [\n                {\n                    \"path\": \"output_info.params.prompt_template\",\n                    \"field_type\": 2,\n                    \"default_value\": \"Answer the following question: {data.text.raw}\\nGiven the following context:\\n{data.hits}\",\n                    \"description\": \"Template used as a template for creating prompts with dynamic values. The prompt template must contain atleast one instance of '{data.text.raw}' and one instance of {data.hits}. At inference time, all instances of '{data.text.raw}' in the prompt template will be replaced with the inputted text data and '{data.hits}' will be replaced with new line separated hits.\",\n                    \"placeholder\": \"Answer the following question: {data.text.raw}\\nGiven the following context: {data.hits}\"\n                },\n                {\n                    \"path\": \"output_info.params.min_score\",\n                    \"field_type\": 7,\n                    \"default_value\": 0,\n                    \"description\": \"Minimum search score to forward search hit in results.\",\n                    \"placeholder\": \"Minimum search score.\",\n                    \"model_type_range_info\": {\n                        \"max\": 1,\n                        \"step\": 0.01\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.max_results\",\n                    \"field_type\": 7,\n                    \"default_value\": 5,\n                    \"description\": \"Maximum number of search results to present in results.\",\n                    \"placeholder\": \"Max number of search results.\",\n                    \"model_type_range_info\": {\n                        \"min\": 1,\n                        \"max\": 128,\n                        \"step\": 1\n                    }\n                }\n            ]\n        },\n        {\n            \"id\": \"isolation-operator\",\n            \"title\": \"Isolation Operator\",\n            \"description\": \"Operator that computes distance between detections and assigns isolation label.\",\n            \"input_fields\": [\n                \"regions[...].data.concepts,regions[...].region_info.bounding_box\"\n            ],\n            \"output_fields\": [\n                \"regions[...].data.concepts,regions[...].region_info.bounding_box\"\n            ],\n            \"creatable\": true,\n            \"model_type_fields\": [\n                {\n                    \"path\": \"output_info.params.size_diff_threshold\",\n                    \"field_type\": 7,\n                    \"default_value\": 0.2,\n                    \"description\": \"This is the relative size difference threshold to consider detections to be of similar size.\",\n                    \"placeholder\": \"size_diff_threshold\",\n                    \"model_type_range_info\": {\n                        \"max\": 1000000\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.isolation_threshold\",\n                    \"field_type\": 7,\n                    \"default_value\": 3,\n                    \"description\": \"Minimum distance relative to detection size to consider the detection isolated.\",\n                    \"placeholder\": \"isolation_threshold\",\n                    \"model_type_range_info\": {\n                        \"max\": 1000000\n                    }\n                }\n            ]\n        },\n        {\n            \"id\": \"barcode-operator\",\n            \"title\": \"Barcode Operator\",\n            \"description\": \"Operator that detects and recognizes barcodes from the image. It assigns regions with barcode text for each detected barcode. Supports EAN/UPC, Code 128, Code 39, Interleaved 2 of 5 and QR Code.\",\n            \"input_fields\": [\n                \"image\"\n            ],\n            \"output_fields\": [\n                \"regions[...].data.text\"\n            ],\n            \"creatable\": true\n        },\n        {\n            \"id\": \"image-tiling-operator\",\n            \"title\": \"Image Tiling Operator\",\n            \"description\": \"Operator for tiling images into a fixed number of equal sized images.\",\n            \"input_fields\": [\n                \"image\"\n            ],\n            \"output_fields\": [\n                \"regions[...].data.image,regions[...].region_info.bounding_box\"\n            ],\n            \"creatable\": true,\n            \"internal_only\": true,\n            \"model_type_fields\": [\n                {\n                    \"path\": \"output_info.params.tile_size\",\n                    \"field_type\": 7,\n                    \"default_value\": 512,\n                    \"description\": \"Determines the number of pixels in each dimension of each square tile.\",\n                    \"placeholder\": \"tile_size\",\n                    \"model_type_range_info\": {\n                        \"min\": 32,\n                        \"max\": 1024,\n                        \"step\": 1\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.max_obj_size\",\n                    \"field_type\": 7,\n                    \"default_value\": 120,\n                    \"description\": \"Number of pixels you estimate the largest objects will be in either length or width. This number is used to calculate tile overlap (1.5 * max_obj_size) to ensure all objects are fully contained within a tile with some surrounding context. til_size must be grater than 1.5 * max_obj_size.\",\n                    \"placeholder\": \"max_obj_size\",\n                    \"model_type_range_info\": {\n                        \"min\": 32,\n                        \"max\": 1024,\n                        \"step\": 1\n                    }\n                }\n            ]\n        },\n        {\n            \"id\": \"language-id-operator\",\n            \"title\": \"Language Identification Operator\",\n            \"description\": \"Operator for language identification using the langdetect library.\",\n            \"input_fields\": [\n                \"text\"\n            ],\n            \"output_fields\": [\n                \"concepts\"\n            ],\n            \"creatable\": true,\n            \"internal_only\": true,\n            \"model_type_fields\": [\n                {\n                    \"path\": \"output_info.params.library\",\n                    \"field_type\": 8,\n                    \"default_value\": \"fasttext\",\n                    \"description\": \"The library to use for language identification. The available libraries are:\",\n                    \"placeholder\": \"library\",\n                    \"model_type_enum_options\": [\n                        {\n                            \"id\": \"fasttext\"\n                        },\n                        {\n                            \"id\": \"langdetect\"\n                        }\n                    ]\n                },\n                {\n                    \"path\": \"output_info.params.topk\",\n                    \"field_type\": 3,\n                    \"default_value\": 1,\n                    \"description\": \"Maximum number of predicted languages.\",\n                    \"placeholder\": \"topk\"\n                },\n                {\n                    \"path\": \"output_info.params.threshold\",\n                    \"field_type\": 7,\n                    \"default_value\": 0.1,\n                    \"description\": \"Languages with confidence level above this value will be returned.\",\n                    \"placeholder\": \"threshold\",\n                    \"model_type_range_info\": {\n                        \"max\": 1\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.lowercase\",\n                    \"field_type\": 1,\n                    \"default_value\": true,\n                    \"description\": \"Converts the text to lowercase letters if set True\",\n                    \"placeholder\": \"lowercase\"\n                }\n            ]\n        },\n        {\n            \"id\": \"tesseract-operator\",\n            \"title\": \"Tesseract Operator\",\n            \"description\": \"Operator for Optical Character Recognition using the Tesseract libraries\",\n            \"input_fields\": [\n                \"image\"\n            ],\n            \"output_fields\": [\n                \"text\"\n            ],\n            \"creatable\": true,\n            \"model_type_fields\": [\n                {\n                    \"path\": \"output_info.params.language\",\n                    \"field_type\": 8,\n                    \"default_value\": \"eng\",\n                    \"description\": \"The language model(s) to use for Optical Character Recognition (OCR). Multiple language models can be listed, separated by '+'.  The available languages are:\",\n                    \"placeholder\": \"language\",\n                    \"model_type_enum_options\": [\n                        {\n                            \"id\": \"afr\"\n                        },\n                        {\n                            \"id\": \"amh\"\n                        },\n                        {\n                            \"id\": \"ara\"\n                        },\n                        {\n                            \"id\": \"asm\"\n                        },\n                        {\n                            \"id\": \"aze\"\n                        },\n                        {\n                            \"id\": \"aze_cyrl\"\n                        },\n                        {\n                            \"id\": \"bel\"\n                        },\n                        {\n                            \"id\": \"ben\"\n                        },\n                        {\n                            \"id\": \"bod\"\n                        },\n                        {\n                            \"id\": \"bos\"\n                        },\n                        {\n                            \"id\": \"bre\"\n                        },\n                        {\n                            \"id\": \"bul\"\n                        },\n                        {\n                            \"id\": \"cat\"\n                        },\n                        {\n                            \"id\": \"ceb\"\n                        },\n                        {\n                            \"id\": \"ces\"\n                        },\n                        {\n                            \"id\": \"chi_sim\"\n                        },\n                        {\n                            \"id\": \"chi_sim_vert\"\n                        },\n                        {\n                            \"id\": \"chi_tra\"\n                        },\n                        {\n                            \"id\": \"chi_tra_vert\"\n                        },\n                        {\n                            \"id\": \"chr\"\n                        },\n                        {\n                            \"id\": \"cos\"\n                        },\n                        {\n                            \"id\": \"cym\"\n                        },\n                        {\n                            \"id\": \"dan\"\n                        },\n                        {\n                            \"id\": \"deu\"\n                        },\n                        {\n                            \"id\": \"div\"\n                        },\n                        {\n                            \"id\": \"dzo\"\n                        },\n                        {\n                            \"id\": \"ell\"\n                        },\n                        {\n                            \"id\": \"eng\"\n                        },\n                        {\n                            \"id\": \"enm\"\n                        },\n                        {\n                            \"id\": \"epo\"\n                        },\n                        {\n                            \"id\": \"est\"\n                        },\n                        {\n                            \"id\": \"eus\"\n                        },\n                        {\n                            \"id\": \"fao\"\n                        },\n                        {\n                            \"id\": \"fas\"\n                        },\n                        {\n                            \"id\": \"fil\"\n                        },\n                        {\n                            \"id\": \"fin\"\n                        },\n                        {\n                            \"id\": \"fra\"\n                        },\n                        {\n                            \"id\": \"frk\"\n                        },\n                        {\n                            \"id\": \"frm\"\n                        },\n                        {\n                            \"id\": \"fry\"\n                        },\n                        {\n                            \"id\": \"gla\"\n                        },\n                        {\n                            \"id\": \"gle\"\n                        },\n                        {\n                            \"id\": \"glg\"\n                        },\n                        {\n                            \"id\": \"grc\"\n                        },\n                        {\n                            \"id\": \"guj\"\n                        },\n                        {\n                            \"id\": \"hat\"\n                        },\n                        {\n                            \"id\": \"heb\"\n                        },\n                        {\n                            \"id\": \"hin\"\n                        },\n                        {\n                            \"id\": \"hrv\"\n                        },\n                        {\n                            \"id\": \"hun\"\n                        },\n                        {\n                            \"id\": \"hye\"\n                        },\n                        {\n                            \"id\": \"iku\"\n                        },\n                        {\n                            \"id\": \"ind\"\n                        },\n                        {\n                            \"id\": \"isl\"\n                        },\n                        {\n                            \"id\": \"ita\"\n                        },\n                        {\n                            \"id\": \"ita_old\"\n                        },\n                        {\n                            \"id\": \"jav\"\n                        },\n                        {\n                            \"id\": \"jpn\"\n                        },\n                        {\n                            \"id\": \"jpn_vert\"\n                        },\n                        {\n                            \"id\": \"kan\"\n                        },\n                        {\n                            \"id\": \"kat\"\n                        },\n                        {\n                            \"id\": \"kat_old\"\n                        },\n                        {\n                            \"id\": \"kaz\"\n                        },\n                        {\n                            \"id\": \"khm\"\n                        },\n                        {\n                            \"id\": \"kir\"\n                        },\n                        {\n                            \"id\": \"kmr\"\n                        },\n                        {\n                            \"id\": \"kor\"\n                        },\n                        {\n                            \"id\": \"kor_vert\"\n                        },\n                        {\n                            \"id\": \"lao\"\n                        },\n                        {\n                            \"id\": \"lat\"\n                        },\n                        {\n                            \"id\": \"lav\"\n                        },\n                        {\n                            \"id\": \"lit\"\n                        },\n                        {\n                            \"id\": \"ltz\"\n                        },\n                        {\n                            \"id\": \"mal\"\n                        },\n                        {\n                            \"id\": \"mar\"\n                        },\n                        {\n                            \"id\": \"mkd\"\n                        },\n                        {\n                            \"id\": \"mlt\"\n                        },\n                        {\n                            \"id\": \"mon\"\n                        },\n                        {\n                            \"id\": \"mri\"\n                        },\n                        {\n                            \"id\": \"msa\"\n                        },\n                        {\n                            \"id\": \"mya\"\n                        },\n                        {\n                            \"id\": \"nep\"\n                        },\n                        {\n                            \"id\": \"nld\"\n                        },\n                        {\n                            \"id\": \"nor\"\n                        },\n                        {\n                            \"id\": \"oci\"\n                        },\n                        {\n                            \"id\": \"ori\"\n                        },\n                        {\n                            \"id\": \"osd\"\n                        },\n                        {\n                            \"id\": \"pan\"\n                        },\n                        {\n                            \"id\": \"pol\"\n                        },\n                        {\n                            \"id\": \"por\"\n                        },\n                        {\n                            \"id\": \"pus\"\n                        },\n                        {\n                            \"id\": \"que\"\n                        },\n                        {\n                            \"id\": \"ron\"\n                        },\n                        {\n                            \"id\": \"rus\"\n                        },\n                        {\n                            \"id\": \"san\"\n                        },\n                        {\n                            \"id\": \"script/Arabic\"\n                        },\n                        {\n                            \"id\": \"script/Armenian\"\n                        },\n                        {\n                            \"id\": \"script/Bengali\"\n                        },\n                        {\n                            \"id\": \"script/Canadian_Aboriginal\"\n                        },\n                        {\n                            \"id\": \"script/Cherokee\"\n                        },\n                        {\n                            \"id\": \"script/Cyrillic\"\n                        },\n                        {\n                            \"id\": \"script/Devanagari\"\n                        },\n                        {\n                            \"id\": \"script/Ethiopic\"\n                        },\n                        {\n                            \"id\": \"script/Fraktur\"\n                        },\n                        {\n                            \"id\": \"script/Georgian\"\n                        },\n                        {\n                            \"id\": \"script/Greek\"\n                        },\n                        {\n                            \"id\": \"script/Gujarati\"\n                        },\n                        {\n                            \"id\": \"script/Gurmukhi\"\n                        },\n                        {\n                            \"id\": \"script/HanS\"\n                        },\n                        {\n                            \"id\": \"script/HanS_vert\"\n                        },\n                        {\n                            \"id\": \"script/HanT\"\n                        },\n                        {\n                            \"id\": \"script/HanT_vert\"\n                        },\n                        {\n                            \"id\": \"script/Hangul\"\n                        },\n                        {\n                            \"id\": \"script/Hangul_vert\"\n                        },\n                        {\n                            \"id\": \"script/Hebrew\"\n                        },\n                        {\n                            \"id\": \"script/Japanese\"\n                        },\n                        {\n                            \"id\": \"script/Japanese_vert\"\n                        },\n                        {\n                            \"id\": \"script/Kannada\"\n                        },\n                        {\n                            \"id\": \"script/Khmer\"\n                        },\n                        {\n                            \"id\": \"script/Lao\"\n                        },\n                        {\n                            \"id\": \"script/Latin\"\n                        },\n                        {\n                            \"id\": \"script/Malayalam\"\n                        },\n                        {\n                            \"id\": \"script/Myanmar\"\n                        },\n                        {\n                            \"id\": \"script/Oriya\"\n                        },\n                        {\n                            \"id\": \"script/Sinhala\"\n                        },\n                        {\n                            \"id\": \"script/Syriac\"\n                        },\n                        {\n                            \"id\": \"script/Tamil\"\n                        },\n                        {\n                            \"id\": \"script/Telugu\"\n                        },\n                        {\n                            \"id\": \"script/Thaana\"\n                        },\n                        {\n                            \"id\": \"script/Thai\"\n                        },\n                        {\n                            \"id\": \"script/Tibetan\"\n                        },\n                        {\n                            \"id\": \"script/Vietnamese\"\n                        },\n                        {\n                            \"id\": \"sin\"\n                        },\n                        {\n                            \"id\": \"slk\"\n                        },\n                        {\n                            \"id\": \"slv\"\n                        },\n                        {\n                            \"id\": \"snd\"\n                        },\n                        {\n                            \"id\": \"snum\"\n                        },\n                        {\n                            \"id\": \"spa\"\n                        },\n                        {\n                            \"id\": \"spa_old\"\n                        },\n                        {\n                            \"id\": \"sqi\"\n                        },\n                        {\n                            \"id\": \"srp\"\n                        },\n                        {\n                            \"id\": \"srp_latn\"\n                        },\n                        {\n                            \"id\": \"sun\"\n                        },\n                        {\n                            \"id\": \"swa\"\n                        },\n                        {\n                            \"id\": \"swe\"\n                        },\n                        {\n                            \"id\": \"syr\"\n                        },\n                        {\n                            \"id\": \"tam\"\n                        },\n                        {\n                            \"id\": \"tat\"\n                        },\n                        {\n                            \"id\": \"tel\"\n                        },\n                        {\n                            \"id\": \"tgk\"\n                        },\n                        {\n                            \"id\": \"tha\"\n                        },\n                        {\n                            \"id\": \"tir\"\n                        },\n                        {\n                            \"id\": \"ton\"\n                        },\n                        {\n                            \"id\": \"tur\"\n                        },\n                        {\n                            \"id\": \"uig\"\n                        },\n                        {\n                            \"id\": \"ukr\"\n                        },\n                        {\n                            \"id\": \"urd\"\n                        },\n                        {\n                            \"id\": \"uzb\"\n                        },\n                        {\n                            \"id\": \"uzb_cyrl\"\n                        },\n                        {\n                            \"id\": \"vie\"\n                        },\n                        {\n                            \"id\": \"yid\"\n                        },\n                        {\n                            \"id\": \"yor\"\n                        }\n                    ]\n                }\n            ]\n        },\n        {\n            \"id\": \"text-aggregation-operator\",\n            \"title\": \"Text Aggregation Operator\",\n            \"description\": \"Operator that combines text detections into text body for the whole image. Detections are sorted from left to right first and then top to bottom, using the top-left corner of the bounding box as reference.\",\n            \"input_fields\": [\n                \"regions[...].data.text\"\n            ],\n            \"output_fields\": [\n                \"text\"\n            ],\n            \"creatable\": true,\n            \"model_type_fields\": [\n                {\n                    \"path\": \"output_info.params.avg_word_width_window_factor\",\n                    \"field_type\": 7,\n                    \"default_value\": 2,\n                    \"description\": \"Width of the window within which words are considered part of the same line, relative to the average word width\",\n                    \"placeholder\": \"avg_word_width_window_factor\",\n                    \"model_type_range_info\": {\n                        \"max\": 1000000\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.avg_word_height_window_factor\",\n                    \"field_type\": 7,\n                    \"default_value\": 1,\n                    \"description\": \"Height of the window within which words are considered part of the same line, relative to the average word height.\",\n                    \"placeholder\": \"avg_word_height_window_factor\",\n                    \"model_type_range_info\": {\n                        \"max\": 1000000\n                    }\n                }\n            ]\n        },\n        {\n            \"id\": \"tiling-region-aggregator-operator\",\n            \"title\": \"Tiling Region Aggregator Operator\",\n            \"description\": \"Operator to be used as a follow up to the image-tiling-operator and visual detector. This operator will transform the detections on each of tiles back to the original image and perform non-maximum suppression. Only the top class prediction for each box is considered.\",\n            \"input_fields\": [\n                \"regions[...].region_info.bounding_box,regions[...].data.regions[...].region_info.bounding_box,regions[...].data.regions[...].data.concepts\",\n                \"image\"\n            ],\n            \"output_fields\": [\n                \"regions[...].data.concepts,regions[...].region_info.bounding_box\"\n            ],\n            \"creatable\": true,\n            \"internal_only\": true,\n            \"model_type_fields\": [\n                {\n                    \"path\": \"output_info.params.iou_threshold\",\n                    \"field_type\": 7,\n                    \"default_value\": 0.5,\n                    \"description\": \"Determines the iou threshold in the nms step used after aggregating resulting detections from each tile since some tiles may overlap.\",\n                    \"placeholder\": \"iou_threshold\",\n                    \"model_type_range_info\": {\n                        \"min\": 0.01,\n                        \"max\": 1\n                    }\n                }\n            ]\n        },\n        {\n            \"id\": \"tokens-to-entity-operator\",\n            \"title\": \"Tokens to Entity Operator\",\n            \"description\": \"Operator that combines text tokens into entities, e.g. `New` + `York` -> `New York`.\",\n            \"input_fields\": [\n                \"regions[...].data.text,regions[...].data.concepts\"\n            ],\n            \"output_fields\": [\n                \"regions[...].data.text,regions[...].data.concepts\"\n            ],\n            \"creatable\": true,\n            \"internal_only\": true,\n            \"model_type_fields\": [\n                {\n                    \"path\": \"output_info.params.aggregation_mode\",\n                    \"field_type\": 8,\n                    \"default_value\": \"MEAN\",\n                    \"description\": \"Token aggregation methods\",\n                    \"placeholder\": \"aggregation_mode\",\n                    \"model_type_enum_options\": [\n                        {\n                            \"id\": \"MEAN\"\n                        },\n                        {\n                            \"id\": \"MAX\"\n                        },\n                        {\n                            \"id\": \"FIRST\"\n                        }\n                    ]\n                },\n                {\n                    \"path\": \"output_info.params.annotation_type\",\n                    \"field_type\": 8,\n                    \"default_value\": \"BIO\",\n                    \"description\": \"Token annotation types\",\n                    \"placeholder\": \"annotation_type\",\n                    \"model_type_enum_options\": [\n                        {\n                            \"id\": \"IO\"\n                        },\n                        {\n                            \"id\": \"BIO\"\n                        },\n                        {\n                            \"id\": \"BMEWO\"\n                        },\n                        {\n                            \"id\": \"BMEWO+\"\n                        },\n                        {\n                            \"id\": \"OTHER\"\n                        }\n                    ]\n                },\n                {\n                    \"path\": \"output_info.params.subword_prefix\",\n                    \"field_type\": 2,\n                    \"default_value\": \"##\",\n                    \"description\": \"Prefix string for subword e.g. ##ing. Letters, numbers, and punctuations are not allowed to be subword prefix\",\n                    \"placeholder\": \"subword_prefix\"\n                }\n            ]\n        },\n        {\n            \"id\": \"track-representation-operator\",\n            \"title\": \"Track Representation Operator\",\n            \"description\": \"The operator takes embedding of each track frame and aggregate them to form a track embedding.\",\n            \"input_fields\": [\n                \"frames[...].data.regions[...].track_id\",\n                \"frames[...].data.regions[...].data.embeddings\"\n            ],\n            \"output_fields\": [\n                \"tracks[...].data.embeddings\"\n            ],\n            \"creatable\": true,\n            \"internal_only\": true,\n            \"model_type_fields\": [\n                {\n                    \"path\": \"output_info.params.embedding_index\",\n                    \"field_type\": 7,\n                    \"default_value\": 0,\n                    \"description\": \"the i-th embedding of the embeddings\",\n                    \"placeholder\": \"embedding_index\",\n                    \"model_type_range_info\": {\n                        \"max\": 1000000,\n                        \"step\": 1\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.normalize\",\n                    \"field_type\": 1,\n                    \"default_value\": true,\n                    \"description\": \"if true, normalize the embedding\",\n                    \"placeholder\": \"normalize\"\n                }\n            ]\n        },\n        {\n            \"id\": \"keyword-filter-operator\",\n            \"title\": \"Keyword Filter Operator\",\n            \"description\": \"This operator is initialized with a set of words, and then determines which are found in the input text.\",\n            \"input_fields\": [\n                \"text\"\n            ],\n            \"output_fields\": [\n                \"concepts\"\n            ],\n            \"creatable\": true,\n            \"internal_only\": true,\n            \"model_type_fields\": [\n                {\n                    \"path\": \"output_info.params.keywords\",\n                    \"field_type\": 13,\n                    \"default_value\": [\n                        \"\"\n                    ],\n                    \"description\": \"A list of keywords to search for in the text.\",\n                    \"placeholder\": \"keywords\"\n                },\n                {\n                    \"path\": \"output_info.params.case_sensitive\",\n                    \"field_type\": 1,\n                    \"default_value\": false,\n                    \"description\": \"Match keywords only when the cases match.\",\n                    \"placeholder\": \"case_sensitive\"\n                }\n            ]\n        },\n        {\n            \"id\": \"neural-lite-tracker\",\n            \"title\": \"Neural Lite Tracker\",\n            \"description\": \"Neural Lite Tracker uses light-weight trainable graphical models to infer states of tracks and perform associations using hybrid similairty of IoU and centroid distance\",\n            \"input_fields\": [\n                \"frames[...].data.regions[...].data.concepts,frames[...].data.regions[...].region_info.bounding_box\"\n            ],\n            \"output_fields\": [\n                \"frames[...].data.regions[...].track_id\"\n            ],\n            \"creatable\": true,\n            \"internal_only\": true,\n            \"model_type_fields\": [\n                {\n                    \"path\": \"output_info.params.iou_dist_ratio\",\n                    \"field_type\": 7,\n                    \"default_value\": 0,\n                    \"description\": \"if 1.0 purely IoU similarity, if 0.0 purely centroid distance similarity\",\n                    \"placeholder\": \"iou_dist_ratio\",\n                    \"model_type_range_info\": {\n                        \"max\": 1\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.mortal_th\",\n                    \"field_type\": 7,\n                    \"default_value\": 0.9,\n                    \"description\": \"mortality threshold\",\n                    \"placeholder\": \"mortal_th\",\n                    \"model_type_range_info\": {\n                        \"max\": 1\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.min_box_area\",\n                    \"field_type\": 7,\n                    \"default_value\": 0.00001,\n                    \"description\": \"minimum area of a valid box\",\n                    \"placeholder\": \"min_box_area\",\n                    \"model_type_range_info\": {\n                        \"max\": 1\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.min_activity\",\n                    \"field_type\": 7,\n                    \"default_value\": 0,\n                    \"description\": \"return only tracks with activities above min_activity\",\n                    \"placeholder\": \"min_activity\",\n                    \"model_type_range_info\": {\n                        \"max\": 1\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.nms_iou_th\",\n                    \"field_type\": 7,\n                    \"default_value\": 0.25,\n                    \"description\": \"NMS IoU threshold\",\n                    \"placeholder\": \"nms_iou_th\",\n                    \"model_type_range_info\": {\n                        \"max\": 1\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.shrink_factor\",\n                    \"field_type\": 7,\n                    \"default_value\": 1,\n                    \"description\": \"change box size by `shrink_factor`\",\n                    \"placeholder\": \"shrink_factor\",\n                    \"model_type_range_info\": {\n                        \"max\": \"Infinity\"\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.min_confidence\",\n                    \"field_type\": 7,\n                    \"default_value\": 0,\n                    \"description\": \"This is the minimum confidence score for detections to be considered for tracking.\",\n                    \"placeholder\": \"min_confidence\",\n                    \"model_type_range_info\": {\n                        \"max\": 1\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.max_disappeared\",\n                    \"field_type\": 7,\n                    \"default_value\": 15,\n                    \"description\": \"This is the number of maximum consecutive frames a given object is allowed to be marked as \\\"disappeared\\\" until we need to deregister the object from tracking.\",\n                    \"placeholder\": \"max_disappeared\",\n                    \"model_type_range_info\": {\n                        \"max\": 1000,\n                        \"step\": 1\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.min_visible_frames\",\n                    \"field_type\": 7,\n                    \"default_value\": 0,\n                    \"description\": \"only return tracks with minimum visible frames > min_visible_frames.\",\n                    \"placeholder\": \"min_visible_frames\",\n                    \"model_type_range_info\": {\n                        \"max\": 1000,\n                        \"step\": 1\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.max_distance\",\n                    \"field_type\": 7,\n                    \"default_value\": 0.4,\n                    \"description\": \"associate tracks with detections only when their distance is below max_distance.\",\n                    \"placeholder\": \"max_distance\",\n                    \"model_type_range_info\": {\n                        \"max\": 1.41\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.track_id_prefix\",\n                    \"field_type\": 2,\n                    \"default_value\": \"\",\n                    \"description\": \"Prefix to add on to track to eliminate conflicts\",\n                    \"placeholder\": \"track_id_prefix\"\n                }\n            ],\n            \"evaluation_type\": 5\n        },\n        {\n            \"id\": \"neural-tracker\",\n            \"title\": \"Neural Tracker\",\n            \"description\": \"Neural Tracker uses neural probabilistic models to perform filtering and association.\",\n            \"input_fields\": [\n                \"frames[...].data.regions[...].data.concepts,frames[...].data.regions[...].region_info.bounding_box\"\n            ],\n            \"output_fields\": [\n                \"frames[...].data.regions[...].track_id\"\n            ],\n            \"creatable\": true,\n            \"internal_only\": true,\n            \"model_type_fields\": [\n                {\n                    \"path\": \"output_info.params.filtered_probability\",\n                    \"field_type\": 1,\n                    \"default_value\": false,\n                    \"description\": \"if false, return original detection probability; if true return processed probability from the tracker\",\n                    \"placeholder\": \"filtered_probability\"\n                },\n                {\n                    \"path\": \"output_info.params.min_visible_frames\",\n                    \"field_type\": 3,\n                    \"default_value\": 0,\n                    \"description\": \"only return tracks with minimum visible frames > min_visible_frame\",\n                    \"placeholder\": \"min_visible_frames\"\n                },\n                {\n                    \"path\": \"output_info.params.min_confidence\",\n                    \"field_type\": 3,\n                    \"default_value\": 0.6,\n                    \"description\": \"only track detections with confidence > min_confidence; confidence is specified by the detector\",\n                    \"placeholder\": \"min_confidence\"\n                },\n                {\n                    \"path\": \"output_info.params.max_disappeared\",\n                    \"field_type\": 3,\n                    \"default_value\": 30,\n                    \"description\": \"max number of missed framed before deregistering the track\",\n                    \"placeholder\": \"max_disappeared\"\n                },\n                {\n                    \"path\": \"output_info.params.max_detection\",\n                    \"field_type\": 3,\n                    \"default_value\": 50,\n                    \"description\": \"max detection per frame\",\n                    \"placeholder\": \"max_detection\"\n                },\n                {\n                    \"path\": \"output_info.params.has_probability\",\n                    \"field_type\": 1,\n                    \"default_value\": true,\n                    \"placeholder\": \"has_probability\"\n                },\n                {\n                    \"path\": \"output_info.params.has_embedding\",\n                    \"field_type\": 1,\n                    \"default_value\": true,\n                    \"placeholder\": \"has_embedding\"\n                }\n            ],\n            \"evaluation_type\": 5\n        },\n        {\n            \"id\": \"byte-tracker\",\n            \"title\": \"BYTE Tracker\",\n            \"description\": \"BYTE Track\",\n            \"input_fields\": [\n                \"frames[...].data.regions[...].data.concepts,frames[...].data.regions[...].region_info.bounding_box\"\n            ],\n            \"output_fields\": [\n                \"frames[...].data.regions[...].track_id\"\n            ],\n            \"creatable\": true,\n            \"model_type_fields\": [\n                {\n                    \"path\": \"output_info.params.min_confidence\",\n                    \"field_type\": 7,\n                    \"default_value\": 0,\n                    \"description\": \"This is the minimum confidence score for detections to be considered for tracking.\",\n                    \"placeholder\": \"min_confidence\",\n                    \"model_type_range_info\": {\n                        \"max\": 1\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.min_visible_frames\",\n                    \"field_type\": 7,\n                    \"default_value\": 0,\n                    \"description\": \"only return tracks with minimum visible frames > min_visible_frames.\",\n                    \"placeholder\": \"min_visible_frames\",\n                    \"model_type_range_info\": {\n                        \"max\": 1000,\n                        \"step\": 1\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.track_id_prefix\",\n                    \"field_type\": 2,\n                    \"default_value\": \"\",\n                    \"description\": \"Prefix to add on to track to eliminate conflicts\",\n                    \"placeholder\": \"track_id_prefix\"\n                },\n                {\n                    \"path\": \"output_info.params.max_disappeared\",\n                    \"field_type\": 7,\n                    \"default_value\": 15,\n                    \"description\": \"This is the number of maximum consecutive frames a given object is allowed to be marked as \\\"disappeared\\\" until we need to deregister the object from tracking.\",\n                    \"placeholder\": \"max_disappeared\",\n                    \"model_type_range_info\": {\n                        \"max\": 1000,\n                        \"step\": 1\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.new_track_confidence_thresh\",\n                    \"field_type\": 7,\n                    \"default_value\": 0,\n                    \"description\": \"Initilize new track if confidence score of new detection is greater than the setting.\",\n                    \"placeholder\": \"new_track_confidence_thresh\",\n                    \"model_type_range_info\": {\n                        \"max\": 1\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.confidence_thresh\",\n                    \"field_type\": 7,\n                    \"default_value\": 0,\n                    \"description\": \"This is used to categorize high score detections for the first association if their scores are greater, and the second association if not.\",\n                    \"placeholder\": \"confidence_thresh\",\n                    \"model_type_range_info\": {\n                        \"max\": 1\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.high_confidence_match_thresh\",\n                    \"field_type\": 7,\n                    \"default_value\": 0.8,\n                    \"description\": \"The distance threshold for high score detection.\",\n                    \"placeholder\": \"high_confidence_match_thresh\",\n                    \"model_type_range_info\": {\n                        \"max\": 1\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.low_confidence_match_thresh\",\n                    \"field_type\": 7,\n                    \"default_value\": 0.7,\n                    \"description\": \"The distance threshold for low score detection.\",\n                    \"placeholder\": \"low_confidence_match_thresh\",\n                    \"model_type_range_info\": {\n                        \"max\": 1\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.unconfirmed_match_thresh\",\n                    \"field_type\": 3,\n                    \"default_value\": 0.5,\n                    \"description\": \"The distance threshold for unconfirmed tracks, usually tracks with only one beginning frame. {\\\"min\\\": 0, \\\"max\\\": 1}     \",\n                    \"placeholder\": \"unconfirmed_match_thresh\"\n                },\n                {\n                    \"path\": \"output_info.params.min_confidence\",\n                    \"field_type\": 7,\n                    \"default_value\": 0,\n                    \"description\": \"This is the minimum confidence score for detections to be considered for tracking.\",\n                    \"placeholder\": \"min_confidence\",\n                    \"model_type_range_info\": {\n                        \"max\": 1\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.max_disappeared\",\n                    \"field_type\": 7,\n                    \"default_value\": 15,\n                    \"description\": \"This is the number of maximum consecutive frames a given object is allowed to be marked as \\\"disappeared\\\" until we need to deregister the object from tracking.\",\n                    \"placeholder\": \"max_disappeared\",\n                    \"model_type_range_info\": {\n                        \"max\": 1000,\n                        \"step\": 1\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.min_visible_frames\",\n                    \"field_type\": 7,\n                    \"default_value\": 0,\n                    \"description\": \"only return tracks with minimum visible frames > min_visible_frames.\",\n                    \"placeholder\": \"min_visible_frames\",\n                    \"model_type_range_info\": {\n                        \"max\": 1000,\n                        \"step\": 1\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.max_distance\",\n                    \"field_type\": 7,\n                    \"default_value\": 0.4,\n                    \"description\": \"associate tracks with detections only when their distance is below max_distance.\",\n                    \"placeholder\": \"max_distance\",\n                    \"model_type_range_info\": {\n                        \"max\": 1.41\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.track_id_prefix\",\n                    \"field_type\": 2,\n                    \"default_value\": \"\",\n                    \"description\": \"Prefix to add on to track to eliminate conflicts\",\n                    \"placeholder\": \"track_id_prefix\"\n                }\n            ],\n            \"evaluation_type\": 5\n        },\n        {\n            \"id\": \"centroid-tracker\",\n            \"title\": \"Centroid Tracker\",\n            \"description\": \"Centroid trackers rely on the Euclidean distance between centroids of regions in different video frames to assign the same track ID to detections of the same object.\",\n            \"input_fields\": [\n                \"frames[...].data.regions[...].data.concepts,frames[...].data.regions[...].region_info.bounding_box\"\n            ],\n            \"output_fields\": [\n                \"frames[...].data.regions[...].track_id\"\n            ],\n            \"creatable\": true,\n            \"internal_only\": true,\n            \"model_type_fields\": [\n                {\n                    \"path\": \"output_info.params.min_confidence\",\n                    \"field_type\": 7,\n                    \"default_value\": 0,\n                    \"description\": \"This is the minimum confidence score for detections to be considered for tracking.\",\n                    \"placeholder\": \"min_confidence\",\n                    \"model_type_range_info\": {\n                        \"max\": 1\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.max_disappeared\",\n                    \"field_type\": 7,\n                    \"default_value\": 15,\n                    \"description\": \"This is the number of maximum consecutive frames a given object is allowed to be marked as \\\"disappeared\\\" until we need to deregister the object from tracking.\",\n                    \"placeholder\": \"max_disappeared\",\n                    \"model_type_range_info\": {\n                        \"max\": 1000,\n                        \"step\": 1\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.min_visible_frames\",\n                    \"field_type\": 7,\n                    \"default_value\": 0,\n                    \"description\": \"only return tracks with minimum visible frames > min_visible_frames.\",\n                    \"placeholder\": \"min_visible_frames\",\n                    \"model_type_range_info\": {\n                        \"max\": 1000,\n                        \"step\": 1\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.max_distance\",\n                    \"field_type\": 7,\n                    \"default_value\": 0.4,\n                    \"description\": \"associate tracks with detections only when their distance is below max_distance.\",\n                    \"placeholder\": \"max_distance\",\n                    \"model_type_range_info\": {\n                        \"max\": 1.41\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.track_id_prefix\",\n                    \"field_type\": 2,\n                    \"default_value\": \"\",\n                    \"description\": \"Prefix to add on to track to eliminate conflicts\",\n                    \"placeholder\": \"track_id_prefix\"\n                }\n            ],\n            \"evaluation_type\": 5\n        },\n        {\n            \"id\": \"kalman-filter-tracker\",\n            \"title\": \"Kalman Filter Hungarian Tracker\",\n            \"description\": \"Kalman Filter trackers rely on the Kalman Filter algorithm to estimate the next position of an object based on its position and velocity in previous frames. Then detections are matched to predictions by using the Hungarian algorithm.\",\n            \"input_fields\": [\n                \"frames[...].data.regions[...].data.concepts,frames[...].data.regions[...].region_info.bounding_box\"\n            ],\n            \"output_fields\": [\n                \"frames[...].data.regions[...].track_id\"\n            ],\n            \"creatable\": true,\n            \"internal_only\": true,\n            \"model_type_fields\": [\n                {\n                    \"path\": \"output_info.params.min_confidence\",\n                    \"field_type\": 7,\n                    \"default_value\": 0,\n                    \"description\": \"This is the minimum confidence score for detections to be considered for tracking.\",\n                    \"placeholder\": \"min_confidence\",\n                    \"model_type_range_info\": {\n                        \"max\": 1\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.association_confidence\",\n                    \"field_type\": 11,\n                    \"default_value\": [\n                        0\n                    ],\n                    \"description\": \"The list of association confidences to perform for each round.\",\n                    \"placeholder\": \"association_confidence\"\n                },\n                {\n                    \"path\": \"output_info.params.max_disappeared\",\n                    \"field_type\": 7,\n                    \"default_value\": 15,\n                    \"description\": \"This is the number of maximum consecutive frames a given object is allowed to be marked as \\\"disappeared\\\" until we need to deregister the object from tracking.\",\n                    \"placeholder\": \"max_disappeared\",\n                    \"model_type_range_info\": {\n                        \"max\": 1000,\n                        \"step\": 1\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.min_visible_frames\",\n                    \"field_type\": 7,\n                    \"default_value\": 0,\n                    \"description\": \"only return tracks with minimum visible frames > min_visible_frames.\",\n                    \"placeholder\": \"min_visible_frames\",\n                    \"model_type_range_info\": {\n                        \"max\": 1000,\n                        \"step\": 1\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.max_distance\",\n                    \"field_type\": 7,\n                    \"default_value\": 0.4,\n                    \"description\": \"associate tracks with detections only when their distance is below max_distance.\",\n                    \"placeholder\": \"max_distance\",\n                    \"model_type_range_info\": {\n                        \"max\": 1.41\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.track_id_prefix\",\n                    \"field_type\": 2,\n                    \"default_value\": \"\",\n                    \"description\": \"Prefix to add on to track to eliminate conflicts\",\n                    \"placeholder\": \"track_id_prefix\"\n                },\n                {\n                    \"path\": \"output_info.params.covariance_error\",\n                    \"field_type\": 7,\n                    \"default_value\": 1,\n                    \"description\": \"Magnitude of the uncertainty on the initial state.\",\n                    \"placeholder\": \"covariance_error\",\n                    \"model_type_range_info\": {\n                        \"max\": 1000000\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.observation_error\",\n                    \"field_type\": 7,\n                    \"default_value\": 0.1,\n                    \"description\": \"Magnitude of the uncertainty on detection coordinates.\",\n                    \"placeholder\": \"observation_error\",\n                    \"model_type_range_info\": {\n                        \"max\": 1000000\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.distance_metric\",\n                    \"field_type\": 8,\n                    \"default_value\": \"centroid_distance\",\n                    \"description\": \"Distance metric for Hungarian matching\",\n                    \"placeholder\": \"distance_metric\",\n                    \"model_type_enum_options\": [\n                        {\n                            \"id\": \"centroid_distance\"\n                        },\n                        {\n                            \"id\": \"iou\"\n                        },\n                        {\n                            \"id\": \"visual_and_iou\"\n                        }\n                    ]\n                },\n                {\n                    \"path\": \"output_info.params.initialization_confidence\",\n                    \"field_type\": 7,\n                    \"default_value\": 0,\n                    \"description\": \"Confidence for starting a new track. must be > min_confidence to have an effect.\",\n                    \"placeholder\": \"initialization_confidence\",\n                    \"model_type_range_info\": {\n                        \"max\": 1\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.project_track\",\n                    \"field_type\": 7,\n                    \"default_value\": 0,\n                    \"description\": \"How many frames in total to project box when detection isn't recorded for track.\",\n                    \"placeholder\": \"project_track\",\n                    \"model_type_range_info\": {\n                        \"max\": 1000,\n                        \"step\": 1\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.use_detect_box\",\n                    \"field_type\": 7,\n                    \"default_value\": 1,\n                    \"description\": \"How many frames to project the last detection box, should be less than project_track_frames (1 is current frame).\",\n                    \"placeholder\": \"use_detect_box\",\n                    \"model_type_range_info\": {\n                        \"max\": 1000,\n                        \"step\": 1\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.project_without_detect\",\n                    \"field_type\": 7,\n                    \"default_value\": 1,\n                    \"description\": \"Whether to keep projecting the box forward if no detect is matched.\",\n                    \"placeholder\": \"project_without_detect\",\n                    \"model_type_range_info\": {\n                        \"max\": 1,\n                        \"step\": 1\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.project_fix_box_size\",\n                    \"field_type\": 7,\n                    \"default_value\": 0,\n                    \"description\": \"Whether to fix the box size when the track is in a project state\",\n                    \"placeholder\": \"project_fix_box_size\",\n                    \"model_type_range_info\": {\n                        \"max\": 1,\n                        \"step\": 1\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.detect_box_fall_back\",\n                    \"field_type\": 7,\n                    \"default_value\": 2,\n                    \"description\": \"Rely on detect box if association error is above this value\",\n                    \"placeholder\": \"detect_box_fall_back\",\n                    \"model_type_range_info\": {\n                        \"max\": 2\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.keep_track_in_image\",\n                    \"field_type\": 7,\n                    \"default_value\": 0,\n                    \"description\": \"if this is 1, then push the tracker predict to stay inside image boundaries\",\n                    \"placeholder\": \"keep_track_in_image\",\n                    \"model_type_range_info\": {\n                        \"max\": 1,\n                        \"step\": 1\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.match_limit_ratio\",\n                    \"field_type\": 7,\n                    \"default_value\": -1,\n                    \"description\": \"Multiplier to constrain association (< 1 is ignored) based on other associations\",\n                    \"placeholder\": \"match_limit_ratio\",\n                    \"model_type_range_info\": {\n                        \"min\": -1,\n                        \"max\": 10\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.match_limit_min_matches\",\n                    \"field_type\": 7,\n                    \"default_value\": 3,\n                    \"description\": \"Min Number of matched tracks needed to invoke match limit\",\n                    \"placeholder\": \"match_limit_min_matches\",\n                    \"model_type_range_info\": {\n                        \"min\": 1,\n                        \"max\": 10,\n                        \"step\": 1\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.optimal_assignment\",\n                    \"field_type\": 7,\n                    \"default_value\": 0,\n                    \"description\": \"If True, rule out pairs with distance > max_distance before assignment\",\n                    \"placeholder\": \"optimal_assignment\",\n                    \"model_type_range_info\": {\n                        \"max\": 1,\n                        \"step\": 1\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.min_confidence\",\n                    \"field_type\": 7,\n                    \"default_value\": 0,\n                    \"description\": \"This is the minimum confidence score for detections to be considered for tracking.\",\n                    \"placeholder\": \"min_confidence\",\n                    \"model_type_range_info\": {\n                        \"max\": 1\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.max_disappeared\",\n                    \"field_type\": 7,\n                    \"default_value\": 15,\n                    \"description\": \"This is the number of maximum consecutive frames a given object is allowed to be marked as \\\"disappeared\\\" until we need to deregister the object from tracking.\",\n                    \"placeholder\": \"max_disappeared\",\n                    \"model_type_range_info\": {\n                        \"max\": 1000,\n                        \"step\": 1\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.min_visible_frames\",\n                    \"field_type\": 7,\n                    \"default_value\": 0,\n                    \"description\": \"only return tracks with minimum visible frames > min_visible_frames.\",\n                    \"placeholder\": \"min_visible_frames\",\n                    \"model_type_range_info\": {\n                        \"max\": 1000,\n                        \"step\": 1\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.max_distance\",\n                    \"field_type\": 7,\n                    \"default_value\": 0.4,\n                    \"description\": \"associate tracks with detections only when their distance is below max_distance.\",\n                    \"placeholder\": \"max_distance\",\n                    \"model_type_range_info\": {\n                        \"max\": 1.41\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.track_id_prefix\",\n                    \"field_type\": 2,\n                    \"default_value\": \"\",\n                    \"description\": \"Prefix to add on to track to eliminate conflicts\",\n                    \"placeholder\": \"track_id_prefix\"\n                }\n            ],\n            \"evaluation_type\": 5\n        },\n        {\n            \"id\": \"kalman-reid-tracker\",\n            \"title\": \"Kalman Tracker w/ re-ID\",\n            \"description\": \"Kalman reid tracker is a kalman filter tracker that expects the Embedding proto field to be populated for detections, and reassigns track IDs based off of embedding distance\",\n            \"input_fields\": [\n                \"frames[...].data.regions[...].data.concepts\"\n            ],\n            \"output_fields\": [\n                \"frames[...].data.regions[...].track_id\"\n            ],\n            \"creatable\": true,\n            \"internal_only\": true,\n            \"model_type_fields\": [\n                {\n                    \"path\": \"output_info.params.max_emb_distance\",\n                    \"field_type\": 7,\n                    \"default_value\": 1,\n                    \"description\": \"Max embedding distance to be considered a re-identification\",\n                    \"placeholder\": \"max_emb_distance\",\n                    \"model_type_range_info\": {\n                        \"max\": 1000000\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.max_dead\",\n                    \"field_type\": 7,\n                    \"default_value\": 100,\n                    \"description\": \"Max number of frames for track to be dead before we re-assign the ID\",\n                    \"placeholder\": \"max_dead\",\n                    \"model_type_range_info\": {\n                        \"min\": 1,\n                        \"max\": 1000000\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.var_tracker\",\n                    \"field_type\": 8,\n                    \"default_value\": \"na\",\n                    \"description\": \"String that determines how embeddings from multiple timesteps are aggregated, defaults to \\\"na\\\" (most recent embedding overwrites past embeddings)\",\n                    \"placeholder\": \"var_tracker\",\n                    \"model_type_enum_options\": [\n                        {\n                            \"id\": \"med\"\n                        },\n                        {\n                            \"id\": \"ma\"\n                        },\n                        {\n                            \"id\": \"ema\"\n                        },\n                        {\n                            \"id\": \"na\"\n                        }\n                    ]\n                },\n                {\n                    \"path\": \"output_info.params.reid_model_path\",\n                    \"field_type\": 2,\n                    \"default_value\": \"\",\n                    \"description\": \"The path to the linker\",\n                    \"placeholder\": \"reid_model_path\"\n                },\n                {\n                    \"path\": \"output_info.params.min_confidence\",\n                    \"field_type\": 7,\n                    \"default_value\": 0,\n                    \"description\": \"This is the minimum confidence score for detections to be considered for tracking.\",\n                    \"placeholder\": \"min_confidence\",\n                    \"model_type_range_info\": {\n                        \"max\": 1\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.association_confidence\",\n                    \"field_type\": 11,\n                    \"default_value\": [\n                        0\n                    ],\n                    \"description\": \"The list of association confidences to perform for each round.\",\n                    \"placeholder\": \"association_confidence\"\n                },\n                {\n                    \"path\": \"output_info.params.max_disappeared\",\n                    \"field_type\": 7,\n                    \"default_value\": 15,\n                    \"description\": \"This is the number of maximum consecutive frames a given object is allowed to be marked as \\\"disappeared\\\" until we need to deregister the object from tracking.\",\n                    \"placeholder\": \"max_disappeared\",\n                    \"model_type_range_info\": {\n                        \"max\": 1000,\n                        \"step\": 1\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.min_visible_frames\",\n                    \"field_type\": 7,\n                    \"default_value\": 0,\n                    \"description\": \"only return tracks with minimum visible frames > min_visible_frames.\",\n                    \"placeholder\": \"min_visible_frames\",\n                    \"model_type_range_info\": {\n                        \"max\": 1000,\n                        \"step\": 1\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.max_distance\",\n                    \"field_type\": 3,\n                    \"default_value\": 0.4,\n                    \"description\": \"associate tracks with detections only when their distance is below max_distance (per round if a List)\",\n                    \"placeholder\": \"max_distance\"\n                },\n                {\n                    \"path\": \"output_info.params.track_id_prefix\",\n                    \"field_type\": 2,\n                    \"default_value\": \"\",\n                    \"description\": \"Prefix to add on to track to eliminate conflict\",\n                    \"placeholder\": \"track_id_prefix\"\n                },\n                {\n                    \"path\": \"output_info.params.covariance_error\",\n                    \"field_type\": 7,\n                    \"default_value\": 1,\n                    \"description\": \"Magnitude of the uncertainty on the initial state.\",\n                    \"placeholder\": \"covariance_error\",\n                    \"model_type_range_info\": {\n                        \"max\": 1000000\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.observation_error\",\n                    \"field_type\": 7,\n                    \"default_value\": 0.1,\n                    \"description\": \"Magnitude of the uncertainty on detection coordinates.\",\n                    \"placeholder\": \"observation_error\",\n                    \"model_type_range_info\": {\n                        \"max\": 1000000\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.distance_metric\",\n                    \"field_type\": 8,\n                    \"default_value\": \"centroid_distance\",\n                    \"description\": \"Distance metric for Hungarian matching\",\n                    \"placeholder\": \"distance_metric\",\n                    \"model_type_enum_options\": [\n                        {\n                            \"id\": \"centroid_distance\"\n                        },\n                        {\n                            \"id\": \"iou\"\n                        },\n                        {\n                            \"id\": \"visual_and_iou\"\n                        }\n                    ]\n                },\n                {\n                    \"path\": \"output_info.params.initialization_confidence\",\n                    \"field_type\": 7,\n                    \"default_value\": 0,\n                    \"description\": \"Confidence for starting a new track. must be > min_confidence to have an effect.\",\n                    \"placeholder\": \"initialization_confidence\",\n                    \"model_type_range_info\": {\n                        \"max\": 1\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.project_track\",\n                    \"field_type\": 7,\n                    \"default_value\": 0,\n                    \"description\": \"How many frames in total to project box when detection isn't recorded for track.\",\n                    \"placeholder\": \"project_track\",\n                    \"model_type_range_info\": {\n                        \"max\": 1000,\n                        \"step\": 1\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.use_detect_box\",\n                    \"field_type\": 7,\n                    \"default_value\": 1,\n                    \"description\": \"How many frames to project the last detection box, should be less than project_track_frames (1 is current frame).\",\n                    \"placeholder\": \"use_detect_box\",\n                    \"model_type_range_info\": {\n                        \"max\": 1000,\n                        \"step\": 1\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.project_without_detect\",\n                    \"field_type\": 7,\n                    \"default_value\": 1,\n                    \"description\": \"Whether to keep projecting the box forward if no detect is matched.\",\n                    \"placeholder\": \"project_without_detect\",\n                    \"model_type_range_info\": {\n                        \"max\": 1,\n                        \"step\": 1\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.project_fix_box_size\",\n                    \"field_type\": 7,\n                    \"default_value\": 0,\n                    \"description\": \"Whether to fix the box size when the track is in a project state\",\n                    \"placeholder\": \"project_fix_box_size\",\n                    \"model_type_range_info\": {\n                        \"max\": 1,\n                        \"step\": 1\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.detect_box_fall_back\",\n                    \"field_type\": 7,\n                    \"default_value\": 2,\n                    \"description\": \"Rely on detect box if association error is above this value\",\n                    \"placeholder\": \"detect_box_fall_back\",\n                    \"model_type_range_info\": {\n                        \"max\": 2\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.keep_track_in_image\",\n                    \"field_type\": 7,\n                    \"default_value\": 0,\n                    \"description\": \"if this is 1, then push the tracker predict to stay inside image boundaries\",\n                    \"placeholder\": \"keep_track_in_image\",\n                    \"model_type_range_info\": {\n                        \"max\": 1,\n                        \"step\": 1\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.match_limit_ratio\",\n                    \"field_type\": 7,\n                    \"default_value\": -1,\n                    \"description\": \"Multiplier to constrain association (< 1 is ignored) based on other associations\",\n                    \"placeholder\": \"match_limit_ratio\",\n                    \"model_type_range_info\": {\n                        \"min\": -1,\n                        \"max\": 10\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.match_limit_min_matches\",\n                    \"field_type\": 7,\n                    \"default_value\": 3,\n                    \"description\": \"Min Number of matched tracks needed to invoke match limit\",\n                    \"placeholder\": \"match_limit_min_matches\",\n                    \"model_type_range_info\": {\n                        \"min\": 1,\n                        \"max\": 10,\n                        \"step\": 1\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.optimal_assignment\",\n                    \"field_type\": 7,\n                    \"default_value\": 0,\n                    \"description\": \"If True, rule out pairs with distance > max_distance before assignment\",\n                    \"placeholder\": \"optimal_assignment\",\n                    \"model_type_range_info\": {\n                        \"max\": 1,\n                        \"step\": 1\n                    }\n                }\n            ],\n            \"evaluation_type\": 5\n        },\n        {\n            \"id\": \"text-embedder\",\n            \"title\": \"Text Embedder\",\n            \"description\": \"Embed text into a vector representing a high level understanding from our AI models. These embeddings enable similarity search and training on top of them.\",\n            \"input_fields\": [\n                \"text\"\n            ],\n            \"output_fields\": [\n                \"embeddings\"\n            ],\n            \"expected_input_layers\": [\n                {\n                    \"data_field_name\": \"text\",\n                    \"shapes\": [\n                        {\n                            \"dims\": [\n                                1\n                            ],\n                            \"data_type\": 1,\n                            \"description\": \"The text string sent to the model\"\n                        }\n                    ],\n                    \"description\": \"Text urls content or raw text passed in through the API are directly sent to the model\"\n                }\n            ],\n            \"expected_output_layers\": [\n                {\n                    \"data_field_name\": \"embeddings\",\n                    \"shapes\": [\n                        {\n                            \"dims\": [\n                                -1\n                            ],\n                            \"data_type\": 5\n                        }\n                    ],\n                    \"description\": \"The embedding vector returned by the model\"\n                }\n            ]\n        },\n        {\n            \"id\": \"text-token-classifier\",\n            \"title\": \"Text Token Classifier\",\n            \"description\": \"Classify tokens from a set of entity classes.\",\n            \"input_fields\": [\n                \"text\"\n            ],\n            \"output_fields\": [\n                \"regions[...].region_info.span,regions[...].data.concepts\"\n            ],\n            \"model_type_fields\": [\n                {\n                    \"path\": \"output_info.params.max_concepts\",\n                    \"field_type\": 3,\n                    \"default_value\": 20,\n                    \"description\": \"Maximum number of concepts in result\",\n                    \"placeholder\": \"Maximum concepts\"\n                },\n                {\n                    \"path\": \"output_info.params.select_concepts\",\n                    \"field_type\": 18,\n                    \"default_value\": [],\n                    \"description\": \"Select concepts in result by name or by id\",\n                    \"placeholder\": \"Select Concepts\"\n                },\n                {\n                    \"path\": \"output_info.params.min_value\",\n                    \"field_type\": 7,\n                    \"default_value\": 0,\n                    \"description\": \"Minimum value of concept's probability score in result. In other words, all concepts with a probability score less than this threshold will be filtered out.\",\n                    \"placeholder\": \"Min value\",\n                    \"model_type_range_info\": {\n                        \"max\": 1,\n                        \"step\": 0.01\n                    }\n                }\n            ],\n            \"expected_input_layers\": [\n                {\n                    \"data_field_name\": \"text\",\n                    \"shapes\": [\n                        {\n                            \"dims\": [\n                                1\n                            ],\n                            \"data_type\": 1,\n                            \"description\": \"The text string sent to the model\"\n                        }\n                    ],\n                    \"description\": \"Text urls content or raw text passed in through the API are directly sent to the model\"\n                }\n            ],\n            \"expected_output_layers\": [\n                {\n                    \"data_field_name\": \"regions[...].region_info.span.char_start\",\n                    \"shapes\": [\n                        {\n                            \"dims\": [\n                                -1,\n                                1\n                            ],\n                            \"data_type\": 3,\n                            \"description\": \"The starting character number for each entity.\"\n                        }\n                    ]\n                },\n                {\n                    \"data_field_name\": \"regions[...].region_info.span.char_end\",\n                    \"shapes\": [\n                        {\n                            \"dims\": [\n                                -1,\n                                1\n                            ],\n                            \"data_type\": 3,\n                            \"description\": \"The ending character number for each entity.\"\n                        }\n                    ]\n                },\n                {\n                    \"data_field_name\": \"regions[...].data.concepts[...].id\",\n                    \"shapes\": [\n                        {\n                            \"dims\": [\n                                -1,\n                                1\n                            ],\n                            \"data_type\": 3,\n                            \"description\": \"The concept number for each entity.\"\n                        }\n                    ],\n                    \"requires_label_filename\": true\n                },\n                {\n                    \"data_field_name\": \"regions[...].data.concepts[...].value\",\n                    \"shapes\": [\n                        {\n                            \"dims\": [\n                                -1,\n                                1\n                            ],\n                            \"data_type\": 5,\n                            \"description\": \"The confidence value for the predicted concept for each entity.\"\n                        }\n                    ]\n                }\n            ]\n        },\n        {\n            \"id\": \"visual-anomaly-heatmap\",\n            \"title\": \"Visual Anomaly\",\n            \"description\": \"Visual anomaly detection with image-level score and anomaly heatmap\",\n            \"input_fields\": [\n                \"image\"\n            ],\n            \"output_fields\": [\n                \"concepts,heatmaps\"\n            ],\n            \"trainable\": true,\n            \"creatable\": true,\n            \"internal_only\": true,\n            \"model_type_fields\": [\n                {\n                    \"path\": \"output_info.data.concepts\",\n                    \"field_type\": 4,\n                    \"description\": \"Single-element list containing the anomaly concept\",\n                    \"placeholder\": \"Anomaly concept\",\n                    \"required\": true\n                },\n                {\n                    \"path\": \"train_info.params.invalid_data_tolerance_percent\",\n                    \"field_type\": 7,\n                    \"default_value\": 5,\n                    \"description\": \"Percentage value (0 to 100) of user's tolerance level to invalid inputs among all training inputs. Training will be stopped with error thrown if actual percent of invalid inputs is higher than this\",\n                    \"placeholder\": \"Invalid Data Tolerance Percentage\",\n                    \"model_type_range_info\": {\n                        \"max\": 100,\n                        \"step\": 0.1\n                    }\n                },\n                {\n                    \"path\": \"train_info.params.dataset_id\",\n                    \"field_type\": 16,\n                    \"default_value\": \"\",\n                    \"description\": \"Deprecated in favor of train_info.dataset\",\n                    \"placeholder\": \"DEPRECATED: Training Dataset ID\"\n                },\n                {\n                    \"path\": \"train_info.dataset\",\n                    \"field_type\": 19,\n                    \"default_value\": \"\",\n                    \"description\": \"Dataset to use for training this model\",\n                    \"placeholder\": \"Training Dataset\"\n                },\n                {\n                    \"path\": \"train_info.params.dataset_version_id\",\n                    \"field_type\": 17,\n                    \"default_value\": \"\",\n                    \"description\": \"Deprecated in favor of train_info.dataset.version\",\n                    \"placeholder\": \"DEPRECATED: Training Dataset Version ID\"\n                },\n                {\n                    \"path\": \"train_info.dataset.version\",\n                    \"field_type\": 20,\n                    \"default_value\": \"\",\n                    \"description\": \"Dataset version to use for training this model. If a dataset version is not specified but a dataset is, we will automatically generate a dataset version.\",\n                    \"placeholder\": \"Training Dataset Version\"\n                },\n                {\n                    \"path\": \"train_info.params.template\",\n                    \"field_type\": 14,\n                    \"default_value\": \"Anomalib_PatchCore\",\n                    \"description\": \"The template name is a pre-configured model template to train with on your data. Depending on your data you might want to try a few templates to see which yields optimal results.\",\n                    \"placeholder\": \"Training Template\",\n                    \"model_type_enum_options\": [\n                        {\n                            \"id\": \"Anomalib_PatchCore\",\n                            \"description\": \"A training template that uses the Anomalib toolkit and PatchCore configuration \",\n                            \"model_type_fields\": [\n                                {\n                                    \"path\": \"train_info.params.anomalib_config_json\",\n                                    \"field_type\": 2,\n                                    \"default_value\": \"\",\n                                    \"description\": \"json with anomalib config to use over defaults\",\n                                    \"placeholder\": \"anomalib_config_json\",\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.gpu_enabled\",\n                                    \"field_type\": 1,\n                                    \"default_value\": true,\n                                    \"description\": \"whether to train using gpu\",\n                                    \"placeholder\": \"gpu_enabled\",\n                                    \"internal_only\": true\n                                }\n                            ],\n                            \"internal_only\": true,\n                            \"recommended\": true\n                        }\n                    ],\n                    \"required\": true\n                }\n            ]\n        },\n        {\n            \"id\": \"zero-shot-image-segmenter\",\n            \"title\": \"Zero Shot Image Segmenter\",\n            \"description\": \"Dynamically segment a per-pixel mask in images where things are and then classify objects, descriptive words or topics within the masks.\",\n            \"input_fields\": [\n                \"image\",\n                \"concepts\"\n            ],\n            \"output_fields\": [\n                \"regions[...].region_info.mask,regions[...].data.concepts\"\n            ],\n            \"model_type_fields\": [\n                {\n                    \"path\": \"output_info.data.concepts\",\n                    \"field_type\": 4,\n                    \"description\": \"List of concepts you want this model to predict from. The concept name will be sent to the model.\",\n                    \"placeholder\": \"List of concepts\"\n                }\n            ],\n            \"expected_input_layers\": [\n                {\n                    \"data_field_name\": \"image\",\n                    \"shapes\": [\n                        {\n                            \"dims\": [\n                                -1,\n                                -1,\n                                3\n                            ],\n                            \"max_dims\": [\n                                1024,\n                                1024,\n                                3\n                            ],\n                            \"data_type\": 2,\n                            \"description\": \"First two dimensions are the height and width, followed by the number of channels.\"\n                        }\n                    ],\n                    \"description\": \"Image urls or base64 are converted into numpy arrays of the specified size and forwarded to the model. If flexible dims are provided, inputs will be downsampled and padded to a default size.\"\n                },\n                {\n                    \"data_field_name\": \"concepts[...].name\",\n                    \"shapes\": [\n                        {\n                            \"dims\": [\n                                -1\n                            ],\n                            \"max_dims\": [\n                                1000\n                            ],\n                            \"data_type\": 1\n                        }\n                    ],\n                    \"description\": \"A list of the concept names to forward to the model. Pixel values should use the concepts index values in the list.\"\n                }\n            ],\n            \"expected_output_layers\": [\n                {\n                    \"data_field_name\": \"regions[...].region_info.mask,regions[...].data.concepts\",\n                    \"shapes\": [\n                        {\n                            \"dims\": [\n                                -1,\n                                -1\n                            ],\n                            \"data_type\": 4,\n                            \"description\": \"The pixel class numbers of each image pixel. Pixel values should use the concepts index values in the list.\"\n                        }\n                    ],\n                    \"description\": \"The image mask returned by the model\",\n                    \"requires_label_filename\": true\n                }\n            ]\n        },\n        {\n            \"id\": \"zero-shot-text-classifier\",\n            \"title\": \"Zero Shot Text Classifier\",\n            \"description\": \"Classify text into a set of concepts provided by user using a pretrained model.\",\n            \"input_fields\": [\n                \"text\",\n                \"concepts\"\n            ],\n            \"output_fields\": [\n                \"concepts\"\n            ],\n            \"model_type_fields\": [\n                {\n                    \"path\": \"eval_info.params.dataset_id\",\n                    \"field_type\": 16,\n                    \"default_value\": \"\",\n                    \"description\": \"Dataset to use for evaluating this model.\",\n                    \"placeholder\": \"Eval Dataset ID\"\n                },\n                {\n                    \"path\": \"eval_info.params.dataset_version_id\",\n                    \"field_type\": 17,\n                    \"default_value\": \"\",\n                    \"description\": \"Dataset version to use for evaluating this model. If a dataset version is not specified but a dataset is, we will automatically generate a dataset version.\",\n                    \"placeholder\": \"Eval Dataset Version ID\"\n                },\n                {\n                    \"path\": \"output_info.data.concepts\",\n                    \"field_type\": 4,\n                    \"description\": \"List of concepts you want this model to predict from. The concept name will be sent to the model.\",\n                    \"placeholder\": \"List of concepts\"\n                },\n                {\n                    \"path\": \"output_info.params.max_concepts\",\n                    \"field_type\": 3,\n                    \"default_value\": 20,\n                    \"description\": \"Maximum number of concepts in result\",\n                    \"placeholder\": \"Maximum concepts\"\n                },\n                {\n                    \"path\": \"output_info.params.min_value\",\n                    \"field_type\": 7,\n                    \"default_value\": 0,\n                    \"description\": \"Minimum value of concept's probability score in result. In other words, all concepts with a probability score less than this threshold will be filtered out.\",\n                    \"placeholder\": \"Min value\",\n                    \"model_type_range_info\": {\n                        \"max\": 1,\n                        \"step\": 0.01\n                    }\n                }\n            ],\n            \"expected_input_layers\": [\n                {\n                    \"data_field_name\": \"text\",\n                    \"shapes\": [\n                        {\n                            \"dims\": [\n                                1\n                            ],\n                            \"data_type\": 1,\n                            \"description\": \"The text string sent to the model\"\n                        }\n                    ],\n                    \"description\": \"Text urls content or raw text passed in through the API are directly sent to the model\"\n                },\n                {\n                    \"data_field_name\": \"concepts[...].name\",\n                    \"shapes\": [\n                        {\n                            \"dims\": [\n                                -1\n                            ],\n                            \"max_dims\": [\n                                1000\n                            ],\n                            \"data_type\": 1\n                        }\n                    ],\n                    \"description\": \"A list of the concept names to forward to the model\"\n                }\n            ],\n            \"expected_output_layers\": [\n                {\n                    \"data_field_name\": \"concepts[...].name\",\n                    \"shapes\": [\n                        {\n                            \"dims\": [\n                                -1\n                            ],\n                            \"data_type\": 1\n                        }\n                    ],\n                    \"description\": \"A list of the concept names returned by the model\"\n                },\n                {\n                    \"data_field_name\": \"concepts[...].value\",\n                    \"shapes\": [\n                        {\n                            \"dims\": [\n                                -1\n                            ],\n                            \"data_type\": 5,\n                            \"description\": \"The confidence value for the respective predicted concept.\"\n                        }\n                    ]\n                }\n            ],\n            \"evaluation_type\": 1\n        },\n        {\n            \"id\": \"audio-classifier\",\n            \"title\": \"Audio Classifier\",\n            \"description\": \"Classify audio into a set of concepts.\",\n            \"input_fields\": [\n                \"audio\"\n            ],\n            \"output_fields\": [\n                \"concepts\"\n            ],\n            \"model_type_fields\": [\n                {\n                    \"path\": \"eval_info.params.dataset_id\",\n                    \"field_type\": 16,\n                    \"default_value\": \"\",\n                    \"description\": \"Dataset to use for evaluating this model.\",\n                    \"placeholder\": \"Eval Dataset ID\"\n                },\n                {\n                    \"path\": \"eval_info.params.dataset_version_id\",\n                    \"field_type\": 17,\n                    \"default_value\": \"\",\n                    \"description\": \"Dataset version to use for evaluating this model. If a dataset version is not specified but a dataset is, we will automatically generate a dataset version.\",\n                    \"placeholder\": \"Eval Dataset Version ID\"\n                },\n                {\n                    \"path\": \"output_info.data.concepts\",\n                    \"field_type\": 4,\n                    \"description\": \"List of concepts you want this model to predict from any existing concepts in your app.\",\n                    \"placeholder\": \"List of concepts\",\n                    \"required\": true\n                },\n                {\n                    \"path\": \"output_info.params.max_concepts\",\n                    \"field_type\": 3,\n                    \"default_value\": 20,\n                    \"description\": \"Maximum number of concepts in result\",\n                    \"placeholder\": \"Maximum concepts\"\n                },\n                {\n                    \"path\": \"output_info.params.min_value\",\n                    \"field_type\": 7,\n                    \"default_value\": 0,\n                    \"description\": \"Minimum value of concept's probability score in result. In other words, all concepts with a probability score less than this threshold will be filtered out.\",\n                    \"placeholder\": \"Min value\",\n                    \"model_type_range_info\": {\n                        \"max\": 1,\n                        \"step\": 0.01\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.select_concepts\",\n                    \"field_type\": 18,\n                    \"default_value\": [],\n                    \"description\": \"Select concepts in result by name or by id\",\n                    \"placeholder\": \"Select Concepts\"\n                }\n            ],\n            \"expected_input_layers\": [\n                {\n                    \"data_field_name\": \"audio\",\n                    \"shapes\": [\n                        {\n                            \"dims\": [\n                                -1\n                            ],\n                            \"max_dims\": [\n                                320000\n                            ],\n                            \"data_type\": 5,\n                            \"description\": \"The sampled audio\"\n                        }\n                    ],\n                    \"description\": \"Audio urls content or base64 passed in through the API are directly sent to the model\"\n                }\n            ],\n            \"expected_output_layers\": [\n                {\n                    \"data_field_name\": \"concepts\",\n                    \"shapes\": [\n                        {\n                            \"dims\": [\n                                -1\n                            ],\n                            \"data_type\": 5,\n                            \"description\": \"Length of the list is expected to be the number of concepts returned by this model, with each value being the confidence for the respective model output.\"\n                        }\n                    ],\n                    \"description\": \"Concepts defined in the model should be the same order as specified in the label file.\",\n                    \"requires_label_filename\": true\n                }\n            ],\n            \"evaluation_type\": 1\n        },\n        {\n            \"id\": \"audio-to-text\",\n            \"title\": \"Audio To Text\",\n            \"description\": \"Classify audio signal into string of text.\",\n            \"input_fields\": [\n                \"audio\"\n            ],\n            \"output_fields\": [\n                \"text\"\n            ],\n            \"expected_input_layers\": [\n                {\n                    \"data_field_name\": \"audio\",\n                    \"shapes\": [\n                        {\n                            \"dims\": [\n                                -1\n                            ],\n                            \"max_dims\": [\n                                320000\n                            ],\n                            \"data_type\": 5,\n                            \"description\": \"The sampled audio\"\n                        }\n                    ],\n                    \"description\": \"Audio urls content or base64 passed in through the API are directly sent to the model\"\n                }\n            ],\n            \"expected_output_layers\": [\n                {\n                    \"data_field_name\": \"text\",\n                    \"shapes\": [\n                        {\n                            \"dims\": [\n                                1\n                            ],\n                            \"data_type\": 1\n                        }\n                    ],\n                    \"description\": \"The text inferenced by the model.\"\n                }\n            ]\n        },\n        {\n            \"id\": \"text-classifier\",\n            \"title\": \"Text Classifier\",\n            \"description\": \"Classify text into a set of concepts.\",\n            \"input_fields\": [\n                \"text\"\n            ],\n            \"output_fields\": [\n                \"concepts\"\n            ],\n            \"trainable\": true,\n            \"creatable\": true,\n            \"model_type_fields\": [\n                {\n                    \"path\": \"eval_info.params.dataset_id\",\n                    \"field_type\": 16,\n                    \"default_value\": \"\",\n                    \"description\": \"Dataset to use for evaluating this model.\",\n                    \"placeholder\": \"Eval Dataset ID\"\n                },\n                {\n                    \"path\": \"eval_info.params.dataset_version_id\",\n                    \"field_type\": 17,\n                    \"default_value\": \"\",\n                    \"description\": \"Dataset version to use for evaluating this model. If a dataset version is not specified but a dataset is, we will automatically generate a dataset version.\",\n                    \"placeholder\": \"Eval Dataset Version ID\"\n                },\n                {\n                    \"path\": \"output_info.data.concepts\",\n                    \"field_type\": 4,\n                    \"description\": \"List of concepts you want this model to predict from any existing concepts in your app.\",\n                    \"placeholder\": \"List of concepts\",\n                    \"required\": true\n                },\n                {\n                    \"path\": \"output_info.params.max_concepts\",\n                    \"field_type\": 3,\n                    \"default_value\": 20,\n                    \"description\": \"Maximum number of concepts in result\",\n                    \"placeholder\": \"Maximum concepts\"\n                },\n                {\n                    \"path\": \"output_info.params.min_value\",\n                    \"field_type\": 7,\n                    \"default_value\": 0,\n                    \"description\": \"Minimum value of concept's probability score in result. In other words, all concepts with a probability score less than this threshold will be filtered out.\",\n                    \"placeholder\": \"Min value\",\n                    \"model_type_range_info\": {\n                        \"max\": 1,\n                        \"step\": 0.01\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.select_concepts\",\n                    \"field_type\": 18,\n                    \"default_value\": [],\n                    \"description\": \"Select concepts in result by name or by id\",\n                    \"placeholder\": \"Select Concepts\"\n                },\n                {\n                    \"path\": \"train_info.params.invalid_data_tolerance_percent\",\n                    \"field_type\": 7,\n                    \"default_value\": 5,\n                    \"description\": \"Percentage value (0 to 100) of user's tolerance level to invalid inputs among all training inputs. Training will be stopped with error thrown if actual percent of invalid inputs is higher than this\",\n                    \"placeholder\": \"Invalid Data Tolerance Percentage\",\n                    \"model_type_range_info\": {\n                        \"max\": 100,\n                        \"step\": 0.1\n                    }\n                },\n                {\n                    \"path\": \"train_info.params.dataset_id\",\n                    \"field_type\": 16,\n                    \"default_value\": \"\",\n                    \"description\": \"Deprecated in favor of train_info.dataset\",\n                    \"placeholder\": \"DEPRECATED: Training Dataset ID\"\n                },\n                {\n                    \"path\": \"train_info.dataset\",\n                    \"field_type\": 19,\n                    \"default_value\": \"\",\n                    \"description\": \"Dataset to use for training this model\",\n                    \"placeholder\": \"Training Dataset\"\n                },\n                {\n                    \"path\": \"train_info.params.dataset_version_id\",\n                    \"field_type\": 17,\n                    \"default_value\": \"\",\n                    \"description\": \"Deprecated in favor of train_info.dataset.version\",\n                    \"placeholder\": \"DEPRECATED: Training Dataset Version ID\"\n                },\n                {\n                    \"path\": \"train_info.dataset.version\",\n                    \"field_type\": 20,\n                    \"default_value\": \"\",\n                    \"description\": \"Dataset version to use for training this model. If a dataset version is not specified but a dataset is, we will automatically generate a dataset version.\",\n                    \"placeholder\": \"Training Dataset Version\"\n                },\n                {\n                    \"path\": \"train_info.resume_from_model\",\n                    \"field_type\": 22,\n                    \"default_value\": \"\",\n                    \"description\": \"Model specifying the checkpoint to resume training from.\",\n                    \"placeholder\": \"This is the model and model version to resume training from. Model must be the same type.\",\n                    \"internal_only\": true\n                },\n                {\n                    \"path\": \"train_info.params.template\",\n                    \"field_type\": 14,\n                    \"default_value\": \"HF_GPTNeo_125m_lora\",\n                    \"description\": \"The template name is a pre-configured model template to train with on your data. Depending on your data you might want to try a few templates to see which yields optimal results.\",\n                    \"placeholder\": \"Training Template\",\n                    \"model_type_enum_options\": [\n                        {\n                            \"id\": \"HuggingFace\",\n                            \"description\": \"A text classification training template that uses the Huggingface toolkit\",\n                            \"model_type_fields\": [\n                                {\n                                    \"path\": \"train_info.params.num_gpus\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 1,\n                                    \"description\": \"[internal_only] the number of gpus to train with.\",\n                                    \"placeholder\": \"num_gpus\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"max\": 1,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.model_config\",\n                                    \"field_type\": 10,\n                                    \"default_value\": {\n                                        \"pretrained_model_name_or_path\": \"bert-base-cased\",\n                                        \"problem_type\": \"multi_label_classification\"\n                                    },\n                                    \"description\": \"keys and values are passed to transformers.AutoModelForSequenceClassification.from_pretrained(). Specifying a resume_from_model in the train_info of the PostModelVersions request overrides the pretrained_model_name_or_path.\",\n                                    \"placeholder\": \"model_config\"\n                                },\n                                {\n                                    \"path\": \"train_info.params.tokenizer_config\",\n                                    \"field_type\": 10,\n                                    \"default_value\": {},\n                                    \"description\": \"keys and values are passed to transformers.AutoTokenizer.from_pretrained().  If not specified, uses the model name from the model config.\",\n                                    \"placeholder\": \"tokenizer_config\"\n                                },\n                                {\n                                    \"path\": \"train_info.params.trainer_config\",\n                                    \"field_type\": 10,\n                                    \"default_value\": {\n                                        \"auto_find_batch_size\": true,\n                                        \"num_train_epochs\": 1,\n                                        \"output_dir\": \"checkpoint\"\n                                    },\n                                    \"description\": \"keys and values are passed to transformers.TrainingArguments()\",\n                                    \"placeholder\": \"trainer_config\"\n                                }\n                            ]\n                        },\n                        {\n                            \"id\": \"HF_GPTNeo_125m_lora\",\n                            \"description\": \"A text classification training template that uses the Huggingface toolkit\",\n                            \"model_type_fields\": [\n                                {\n                                    \"path\": \"train_info.params.num_gpus\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 1,\n                                    \"description\": \"[internal_only] the number of gpus to train with.\",\n                                    \"placeholder\": \"num_gpus\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"max\": 1,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.model_config\",\n                                    \"field_type\": 10,\n                                    \"default_value\": {\n                                        \"pretrained_model_name\": \"EleutherAI/gpt-neo-125m\",\n                                        \"problem_type\": \"multi_label_classification\"\n                                    },\n                                    \"description\": \"keys and values are passed to transformers.AutoModelForSequenceClassification.from_pretrained(). Specifying a resume_from_model in the train_info of the PostModelVersions request overrides the pretrained_model_name_or_path.\",\n                                    \"placeholder\": \"model_config\"\n                                },\n                                {\n                                    \"path\": \"train_info.params.peft_config\",\n                                    \"field_type\": 10,\n                                    \"default_value\": {\n                                        \"peft_type\": \"LORA\"\n                                    },\n                                    \"description\": \"keys and values are passed to peft.get_peft_model(base_model, peft_config)\",\n                                    \"placeholder\": \"peft_config\"\n                                },\n                                {\n                                    \"path\": \"train_info.params.tokenizer_config\",\n                                    \"field_type\": 10,\n                                    \"default_value\": {},\n                                    \"description\": \"keys and values are passed to transformers.AutoTokenizer.from_pretrained().  If not specified, uses the model name from the model config.\",\n                                    \"placeholder\": \"tokenizer_config\"\n                                },\n                                {\n                                    \"path\": \"train_info.params.trainer_config\",\n                                    \"field_type\": 10,\n                                    \"default_value\": {\n                                        \"auto_find_batch_size\": true,\n                                        \"num_train_epochs\": 1\n                                    },\n                                    \"description\": \"keys and values are passed to transformers.TrainingArguments()\",\n                                    \"placeholder\": \"trainer_config\"\n                                }\n                            ],\n                            \"recommended\": true\n                        },\n                        {\n                            \"id\": \"HF_GPTNeo_2p7b_lora\",\n                            \"description\": \"A text classification training template that uses the Huggingface toolkit\",\n                            \"model_type_fields\": [\n                                {\n                                    \"path\": \"train_info.params.num_gpus\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 1,\n                                    \"description\": \"[internal_only] the number of gpus to train with.\",\n                                    \"placeholder\": \"num_gpus\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"max\": 1,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.model_config\",\n                                    \"field_type\": 10,\n                                    \"default_value\": {\n                                        \"pretrained_model_name\": \"EleutherAI/gpt-neo-2.7B\",\n                                        \"problem_type\": \"multi_label_classification\"\n                                    },\n                                    \"description\": \"keys and values are passed to transformers.AutoModelForSequenceClassification.from_pretrained(). Specifying a resume_from_model in the train_info of the PostModelVersions request overrides the pretrained_model_name_or_path.\",\n                                    \"placeholder\": \"model_config\"\n                                },\n                                {\n                                    \"path\": \"train_info.params.peft_config\",\n                                    \"field_type\": 10,\n                                    \"default_value\": {\n                                        \"peft_type\": \"LORA\"\n                                    },\n                                    \"description\": \"keys and values are passed to peft.get_peft_model(base_model, peft_config)\",\n                                    \"placeholder\": \"peft_config\"\n                                },\n                                {\n                                    \"path\": \"train_info.params.tokenizer_config\",\n                                    \"field_type\": 10,\n                                    \"default_value\": {},\n                                    \"description\": \"keys and values are passed to transformers.AutoTokenizer.from_pretrained().  If not specified, uses the model name from the model config.\",\n                                    \"placeholder\": \"tokenizer_config\"\n                                },\n                                {\n                                    \"path\": \"train_info.params.trainer_config\",\n                                    \"field_type\": 10,\n                                    \"default_value\": {\n                                        \"num_train_epochs\": 1,\n                                        \"per_device_train_batch_size\": 2\n                                    },\n                                    \"description\": \"keys and values are passed to transformers.TrainingArguments()\",\n                                    \"placeholder\": \"trainer_config\"\n                                }\n                            ]\n                        }\n                    ],\n                    \"required\": true\n                }\n            ],\n            \"expected_input_layers\": [\n                {\n                    \"data_field_name\": \"text\",\n                    \"shapes\": [\n                        {\n                            \"dims\": [\n                                1\n                            ],\n                            \"data_type\": 1,\n                            \"description\": \"The text string sent to the model\"\n                        }\n                    ],\n                    \"description\": \"Text urls content or raw text passed in through the API are directly sent to the model\"\n                }\n            ],\n            \"expected_output_layers\": [\n                {\n                    \"data_field_name\": \"concepts\",\n                    \"shapes\": [\n                        {\n                            \"dims\": [\n                                -1\n                            ],\n                            \"data_type\": 5,\n                            \"description\": \"Length of the list is expected to be the number of concepts returned by this model, with each value being the confidence for the respective model output.\"\n                        }\n                    ],\n                    \"description\": \"Concepts defined in the model should be the same order as specified in the label file.\",\n                    \"requires_label_filename\": true\n                }\n            ],\n            \"evaluation_type\": 1\n        },\n        {\n            \"id\": \"zero-shot-image-classifier\",\n            \"title\": \"Zero Shot Image Classifier\",\n            \"description\": \"Classify image into a set of concepts provided by user using a pretrained model.\",\n            \"input_fields\": [\n                \"image\",\n                \"concepts\"\n            ],\n            \"output_fields\": [\n                \"concepts\"\n            ],\n            \"model_type_fields\": [\n                {\n                    \"path\": \"eval_info.params.dataset_id\",\n                    \"field_type\": 16,\n                    \"default_value\": \"\",\n                    \"description\": \"Dataset to use for evaluating this model.\",\n                    \"placeholder\": \"Eval Dataset ID\"\n                },\n                {\n                    \"path\": \"eval_info.params.dataset_version_id\",\n                    \"field_type\": 17,\n                    \"default_value\": \"\",\n                    \"description\": \"Dataset version to use for evaluating this model. If a dataset version is not specified but a dataset is, we will automatically generate a dataset version.\",\n                    \"placeholder\": \"Eval Dataset Version ID\"\n                },\n                {\n                    \"path\": \"output_info.data.concepts\",\n                    \"field_type\": 4,\n                    \"description\": \"List of concepts you want this model to predict from. The concept name will be sent to the model.\",\n                    \"placeholder\": \"List of concepts\"\n                },\n                {\n                    \"path\": \"output_info.params.max_concepts\",\n                    \"field_type\": 3,\n                    \"default_value\": 20,\n                    \"description\": \"Maximum number of concepts in result\",\n                    \"placeholder\": \"Maximum concepts\"\n                },\n                {\n                    \"path\": \"output_info.params.min_value\",\n                    \"field_type\": 7,\n                    \"default_value\": 0,\n                    \"description\": \"Minimum value of concept's probability score in result. In other words, all concepts with a probability score less than this threshold will be filtered out.\",\n                    \"placeholder\": \"Min value\",\n                    \"model_type_range_info\": {\n                        \"max\": 1,\n                        \"step\": 0.01\n                    }\n                }\n            ],\n            \"expected_input_layers\": [\n                {\n                    \"data_field_name\": \"image\",\n                    \"shapes\": [\n                        {\n                            \"dims\": [\n                                -1,\n                                -1,\n                                3\n                            ],\n                            \"max_dims\": [\n                                1024,\n                                1024,\n                                3\n                            ],\n                            \"data_type\": 2,\n                            \"description\": \"First two dimensions are the height and width, followed by the number of channels.\"\n                        }\n                    ],\n                    \"description\": \"Image urls or base64 are converted into numpy arrays of the specified size and forwarded to the model. If flexible dims are provided, inputs will be downsampled and padded to a default size.\"\n                },\n                {\n                    \"data_field_name\": \"concepts[...].name\",\n                    \"shapes\": [\n                        {\n                            \"dims\": [\n                                -1\n                            ],\n                            \"max_dims\": [\n                                1000\n                            ],\n                            \"data_type\": 1\n                        }\n                    ],\n                    \"description\": \"A list of the concept names to forward to the model\"\n                }\n            ],\n            \"expected_output_layers\": [\n                {\n                    \"data_field_name\": \"concepts[...].name\",\n                    \"shapes\": [\n                        {\n                            \"dims\": [\n                                -1\n                            ],\n                            \"data_type\": 1\n                        }\n                    ],\n                    \"description\": \"A list of the concept names returned by the model\"\n                },\n                {\n                    \"data_field_name\": \"concepts[...].value\",\n                    \"shapes\": [\n                        {\n                            \"dims\": [\n                                -1\n                            ],\n                            \"data_type\": 5,\n                            \"description\": \"The confidence value for the respective predicted concept.\"\n                        }\n                    ]\n                }\n            ],\n            \"evaluation_type\": 1\n        },\n        {\n            \"id\": \"multimodal-to-text\",\n            \"title\": \"Multimodal To Text\",\n            \"description\": \"Generate text from either text or images or both as input, allowing it to understand and respond to questions about those images\",\n            \"input_fields\": [\n                \"any\"\n            ],\n            \"output_fields\": [\n                \"text\"\n            ],\n            \"model_type_fields\": [\n                {\n                    \"path\": \"input_info.params.text_token_max_count_warning\",\n                    \"field_type\": 7,\n                    \"default_value\": 4096,\n                    \"description\": \"A warning to reflect model behaviour that text tokens beyond this limit will be truncated. Note that changing this field value will not result in any actual changes to how the model processes the text inputs, since this field value does only have informational purpose. Set this value to simply reflect the model behaviour. If you are not sure if the model has such a limitation, you may leave it empty.\",\n                    \"placeholder\": \"Text token max count warning\",\n                    \"model_type_range_info\": {\n                        \"max\": 5000,\n                        \"step\": 1\n                    }\n                }\n            ],\n            \"expected_input_layers\": [\n                {\n                    \"data_field_name\": \"image\",\n                    \"shapes\": [\n                        {\n                            \"dims\": [\n                                -1,\n                                -1,\n                                3\n                            ],\n                            \"max_dims\": [\n                                1024,\n                                1024,\n                                3\n                            ],\n                            \"data_type\": 2,\n                            \"description\": \"First two dimensions are the height and width, followed by the number of channels.\"\n                        }\n                    ],\n                    \"description\": \"Image urls or base64 are converted into numpy arrays of the specified size and forwarded to the model. If flexible dims are provided, inputs will be downsampled and padded to a default size.\"\n                },\n                {\n                    \"data_field_name\": \"text\",\n                    \"shapes\": [\n                        {\n                            \"dims\": [\n                                1\n                            ],\n                            \"data_type\": 1,\n                            \"description\": \"The text string sent to the model\"\n                        }\n                    ],\n                    \"description\": \"Text urls content or raw text passed in through the API are directly sent to the model\"\n                }\n            ],\n            \"expected_output_layers\": [\n                {\n                    \"data_field_name\": \"text\",\n                    \"shapes\": [\n                        {\n                            \"dims\": [\n                                1\n                            ],\n                            \"data_type\": 1\n                        }\n                    ],\n                    \"description\": \"The text output returned by the model\"\n                }\n            ]\n        },\n        {\n            \"id\": \"visual-classifier\",\n            \"title\": \"Visual Classifier\",\n            \"description\": \"Classify images and videos frames into set of concepts.\",\n            \"input_fields\": [\n                \"image\"\n            ],\n            \"output_fields\": [\n                \"concepts\"\n            ],\n            \"trainable\": true,\n            \"creatable\": true,\n            \"model_type_fields\": [\n                {\n                    \"path\": \"eval_info.params.dataset_id\",\n                    \"field_type\": 16,\n                    \"default_value\": \"\",\n                    \"description\": \"Dataset to use for evaluating this model.\",\n                    \"placeholder\": \"Eval Dataset ID\"\n                },\n                {\n                    \"path\": \"eval_info.params.dataset_version_id\",\n                    \"field_type\": 17,\n                    \"default_value\": \"\",\n                    \"description\": \"Dataset version to use for evaluating this model. If a dataset version is not specified but a dataset is, we will automatically generate a dataset version.\",\n                    \"placeholder\": \"Eval Dataset Version ID\"\n                },\n                {\n                    \"path\": \"output_info.data.concepts\",\n                    \"field_type\": 4,\n                    \"description\": \"List of concepts you want this model to predict from any existing concepts in your app.\",\n                    \"placeholder\": \"List of concepts\",\n                    \"required\": true\n                },\n                {\n                    \"path\": \"output_info.params.max_concepts\",\n                    \"field_type\": 3,\n                    \"default_value\": 20,\n                    \"description\": \"Maximum number of concepts in result\",\n                    \"placeholder\": \"Maximum concepts\"\n                },\n                {\n                    \"path\": \"output_info.params.min_value\",\n                    \"field_type\": 7,\n                    \"default_value\": 0,\n                    \"description\": \"Minimum value of concept's probability score in result. In other words, all concepts with a probability score less than this threshold will be filtered out.\",\n                    \"placeholder\": \"Min value\",\n                    \"model_type_range_info\": {\n                        \"max\": 1,\n                        \"step\": 0.01\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.select_concepts\",\n                    \"field_type\": 18,\n                    \"default_value\": [],\n                    \"description\": \"Select concepts in result by name or by id\",\n                    \"placeholder\": \"Select Concepts\"\n                },\n                {\n                    \"path\": \"train_info.params.invalid_data_tolerance_percent\",\n                    \"field_type\": 7,\n                    \"default_value\": 5,\n                    \"description\": \"Percentage value (0 to 100) of user's tolerance level to invalid inputs among all training inputs. Training will be stopped with error thrown if actual percent of invalid inputs is higher than this\",\n                    \"placeholder\": \"Invalid Data Tolerance Percentage\",\n                    \"model_type_range_info\": {\n                        \"max\": 100,\n                        \"step\": 0.1\n                    }\n                },\n                {\n                    \"path\": \"train_info.params.dataset_id\",\n                    \"field_type\": 16,\n                    \"default_value\": \"\",\n                    \"description\": \"Deprecated in favor of train_info.dataset\",\n                    \"placeholder\": \"DEPRECATED: Training Dataset ID\"\n                },\n                {\n                    \"path\": \"train_info.dataset\",\n                    \"field_type\": 19,\n                    \"default_value\": \"\",\n                    \"description\": \"Dataset to use for training this model\",\n                    \"placeholder\": \"Training Dataset\"\n                },\n                {\n                    \"path\": \"train_info.params.dataset_version_id\",\n                    \"field_type\": 17,\n                    \"default_value\": \"\",\n                    \"description\": \"Deprecated in favor of train_info.dataset.version\",\n                    \"placeholder\": \"DEPRECATED: Training Dataset Version ID\"\n                },\n                {\n                    \"path\": \"train_info.dataset.version\",\n                    \"field_type\": 20,\n                    \"default_value\": \"\",\n                    \"description\": \"Dataset version to use for training this model. If a dataset version is not specified but a dataset is, we will automatically generate a dataset version.\",\n                    \"placeholder\": \"Training Dataset Version\"\n                },\n                {\n                    \"path\": \"train_info.params.template\",\n                    \"field_type\": 14,\n                    \"default_value\": \"MMClassification_ResNet_50_RSB_A1\",\n                    \"description\": \"The template name is a pre-configured model template to train with on your data. Depending on your data you might want to try a few templates to see which yields optimal results.\",\n                    \"placeholder\": \"Training Template\",\n                    \"model_type_enum_options\": [\n                        {\n                            \"id\": \"classification_inception_general_v1_3_transfer_embednorm\",\n                            \"description\": \"This is a private base class for our visual classifier models with optimizations for transfer\\nlearning on top of the embedding vectors. \",\n                            \"model_type_fields\": [\n                                {\n                                    \"path\": \"train_info.params.num_gpus\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 1,\n                                    \"description\": \"[internal_only] the number of gpus to train with.\",\n                                    \"placeholder\": \"num_gpus\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"max\": 1,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.logreg\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 1,\n                                    \"description\": \"Whether to use sigmoid units (logreg=1) or softmax (logreg=0).\",\n                                    \"placeholder\": \"logreg\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"max\": 1,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.image_size\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 256,\n                                    \"description\": \"Input image size (minimum side dimension).\",\n                                    \"placeholder\": \"image_size\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"min\": 32,\n                                        \"max\": 1024,\n                                        \"step\": 16\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.batch_size\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 128,\n                                    \"description\": \"the batch size to use during training.\",\n                                    \"placeholder\": \"batch_size\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 128,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.lrate\",\n                                    \"field_type\": 3,\n                                    \"default_value\": 0.1,\n                                    \"description\": \"the learning rate (per minibatch)\",\n                                    \"placeholder\": \"lrate\",\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.base_gradient_multiplier\",\n                                    \"field_type\": 3,\n                                    \"default_value\": 0.001,\n                                    \"description\": \"learning rate multipler applied to the pre-initialized backbone model weights\",\n                                    \"placeholder\": \"base_gradient_multiplier\",\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.num_epochs\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 20,\n                                    \"description\": \"the total number of epochs to train for.\",\n                                    \"placeholder\": \"num_epochs\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 200,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.num_items_per_epoch\",\n                                    \"field_type\": 3,\n                                    \"default_value\": 0,\n                                    \"description\": \"number of input images that constitute an \\\"epoch\\\".  Default is the number of images in the dataset.\",\n                                    \"placeholder\": \"num_items_per_epoch\",\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.embeddings_layer\",\n                                    \"field_type\": 2,\n                                    \"default_value\": \"mod5B.concat\",\n                                    \"description\": \"the embedding layer to use as output from this model.\",\n                                    \"placeholder\": \"embeddings_layer\",\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.average_horizontal_flips\",\n                                    \"field_type\": 1,\n                                    \"default_value\": true,\n                                    \"description\": \"if true then average the embeddings from the image and a horizontal flip of the image to get the final embedding vectors to output.\",\n                                    \"placeholder\": \"average_horizontal_flips\",\n                                    \"internal_only\": true\n                                }\n                            ],\n                            \"internal_only\": true\n                        },\n                        {\n                            \"id\": \"classification_basemodel_v1\",\n                            \"description\": \"A training template that uses Clarifais training implementation. \",\n                            \"model_type_fields\": [\n                                {\n                                    \"path\": \"train_info.params.num_gpus\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 1,\n                                    \"description\": \"[internal_only] the number of gpus to train with.\",\n                                    \"placeholder\": \"num_gpus\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"max\": 1,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.model_cfg\",\n                                    \"field_type\": 2,\n                                    \"default_value\": \"resnext\",\n                                    \"description\": \"the underlying model configuration to use.\",\n                                    \"placeholder\": \"model_cfg\",\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.preinit\",\n                                    \"field_type\": 2,\n                                    \"default_value\": \"general-v1.5\",\n                                    \"description\": \"specifies pre-initialized net to use.\",\n                                    \"placeholder\": \"preinit\",\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.logreg\",\n                                    \"field_type\": 3,\n                                    \"default_value\": 1,\n                                    \"description\": \"Whether to use sigmoid units (logreg=1) or softmax (logreg=0).\",\n                                    \"placeholder\": \"logreg\",\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.image_size\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 256,\n                                    \"description\": \"Input image size (minimum side dimension).\",\n                                    \"placeholder\": \"image_size\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"min\": 32,\n                                        \"max\": 1024,\n                                        \"step\": 16\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.batch_size\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 64,\n                                    \"description\": \"the batch size to use during training.\",\n                                    \"placeholder\": \"batch_size\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 128,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.init_epochs\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 25,\n                                    \"description\": \"number of epochs to run at the initial learning rate.\",\n                                    \"placeholder\": \"init_epochs\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 200,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.step_epochs\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 7,\n                                    \"description\": \"the number of epochs between learning rate decreases.\",\n                                    \"placeholder\": \"step_epochs\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 200,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.num_epochs\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 65,\n                                    \"description\": \"the total number of epochs to train for.\",\n                                    \"placeholder\": \"num_epochs\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 200,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.per_item_lrate\",\n                                    \"field_type\": 3,\n                                    \"default_value\": 0.000078125,\n                                    \"description\": \"the initial learning rate per item. The overall learning rate (per step) is set to lrate = batch_size * per_item_lrate\",\n                                    \"placeholder\": \"per_item_lrate\",\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.num_items_per_epoch\",\n                                    \"field_type\": 3,\n                                    \"default_value\": 0,\n                                    \"description\": \"number of input images that constitute an \\\"epoch\\\".  Default is the number of images in the dataset.\",\n                                    \"placeholder\": \"num_items_per_epoch\",\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.inference_crop_type\",\n                                    \"field_type\": 2,\n                                    \"default_value\": \"sorta2\",\n                                    \"description\": \"the crop type to use for inference (used when evaluating the model).\",\n                                    \"placeholder\": \"inference_crop_type\",\n                                    \"internal_only\": true\n                                }\n                            ],\n                            \"internal_only\": true\n                        },\n                        {\n                            \"id\": \"classification_cifar10_v1\",\n                            \"description\": \"A runner optimized for cifar10 training. Not to be used in real use cases. \",\n                            \"model_type_fields\": [\n                                {\n                                    \"path\": \"train_info.params.num_gpus\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 1,\n                                    \"description\": \"the number of gpus to train with.\",\n                                    \"placeholder\": \"num_gpus\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"max\": 1,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.image_size\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 32,\n                                    \"description\": \"the image size to train on. This is for the minimum dimension.\",\n                                    \"placeholder\": \"image_size\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"min\": 32,\n                                        \"max\": 1024,\n                                        \"step\": 16\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.batch_size\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 128,\n                                    \"description\": \"the batch size to use during training.\",\n                                    \"placeholder\": \"batch_size\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 128,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.num_epochs\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 65,\n                                    \"description\": \"the total number of epochs to train for.\",\n                                    \"placeholder\": \"num_epochs\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 200,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.inference_crop_type\",\n                                    \"field_type\": 2,\n                                    \"default_value\": \"sorta2\",\n                                    \"description\": \"the crop type to use for inference (used when evaluating the model).\",\n                                    \"placeholder\": \"inference_crop_type\",\n                                    \"internal_only\": true\n                                }\n                            ],\n                            \"internal_only\": true\n                        },\n                        {\n                            \"id\": \"Clarifai_InceptionTransferEmbedNorm\",\n                            \"description\": \"A custom visual classifier template inspired by Inception networks and tuned for speed with\\nother optimizations for transfer learning. \",\n                            \"model_type_fields\": [\n                                {\n                                    \"path\": \"train_info.params.logreg\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 1,\n                                    \"description\": \"Whether to use sigmoid units (logreg=1) or softmax (logreg=0).\",\n                                    \"placeholder\": \"logreg\",\n                                    \"model_type_range_info\": {\n                                        \"max\": 1,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.image_size\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 256,\n                                    \"description\": \"Input image size (minimum side dimension).\",\n                                    \"placeholder\": \"image_size\",\n                                    \"model_type_range_info\": {\n                                        \"min\": 32,\n                                        \"max\": 1024,\n                                        \"step\": 16\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.batch_size\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 128,\n                                    \"description\": \"the batch size to use during training.\",\n                                    \"placeholder\": \"batch_size\",\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 128,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.lrate\",\n                                    \"field_type\": 3,\n                                    \"default_value\": 0.1,\n                                    \"description\": \"the learning rate (per minibatch)\",\n                                    \"placeholder\": \"lrate\"\n                                },\n                                {\n                                    \"path\": \"train_info.params.base_gradient_multiplier\",\n                                    \"field_type\": 3,\n                                    \"default_value\": 0.001,\n                                    \"description\": \"learning rate multipler applied to the pre-initialized backbone model weights\",\n                                    \"placeholder\": \"base_gradient_multiplier\"\n                                },\n                                {\n                                    \"path\": \"train_info.params.num_epochs\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 20,\n                                    \"description\": \"the total number of epochs to train for.\",\n                                    \"placeholder\": \"num_epochs\",\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 200,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.num_items_per_epoch\",\n                                    \"field_type\": 3,\n                                    \"default_value\": 0,\n                                    \"description\": \"number of input images that constitute an \\\"epoch\\\".  Default is the number of images in the dataset.\",\n                                    \"placeholder\": \"num_items_per_epoch\"\n                                },\n                                {\n                                    \"path\": \"train_info.params.average_horizontal_flips\",\n                                    \"field_type\": 1,\n                                    \"default_value\": true,\n                                    \"description\": \"if true then average the embeddings from the image and a horizontal flip of the image to get the final embedding vectors to output.\",\n                                    \"placeholder\": \"average_horizontal_flips\"\n                                }\n                            ]\n                        },\n                        {\n                            \"id\": \"Clarifai_ResNext\",\n                            \"description\": \"A custom visual classifier template inspired by ResNext networks. \",\n                            \"model_type_fields\": [\n                                {\n                                    \"path\": \"train_info.params.logreg\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 1,\n                                    \"description\": \"Whether to use sigmoid units (logreg=1) or softmax (logreg=0).\",\n                                    \"placeholder\": \"logreg\",\n                                    \"model_type_range_info\": {\n                                        \"max\": 1,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.image_size\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 256,\n                                    \"description\": \"Input image size (minimum side dimension).\",\n                                    \"placeholder\": \"image_size\",\n                                    \"model_type_range_info\": {\n                                        \"min\": 32,\n                                        \"max\": 1024,\n                                        \"step\": 16\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.batch_size\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 64,\n                                    \"description\": \"the batch size to use during training.\",\n                                    \"placeholder\": \"batch_size\",\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 128,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.init_epochs\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 25,\n                                    \"description\": \"number of epochs to run at the initial learning rate.\",\n                                    \"placeholder\": \"init_epochs\",\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 200,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.step_epochs\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 7,\n                                    \"description\": \"the number of epochs between learning rate decreases.\",\n                                    \"placeholder\": \"step_epochs\",\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 200,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.num_epochs\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 65,\n                                    \"description\": \"the total number of epochs to train for.\",\n                                    \"placeholder\": \"num_epochs\",\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 200,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.per_item_lrate\",\n                                    \"field_type\": 3,\n                                    \"default_value\": 0.000078125,\n                                    \"description\": \"the initial learning rate per item. The overall learning rate (per step) is set to lrate = batch_size * per_item_lrate\",\n                                    \"placeholder\": \"per_item_lrate\"\n                                },\n                                {\n                                    \"path\": \"train_info.params.num_items_per_epoch\",\n                                    \"field_type\": 3,\n                                    \"default_value\": 0,\n                                    \"description\": \"number of input images that constitute an \\\"epoch\\\".  Default is the number of images in the dataset.\",\n                                    \"placeholder\": \"num_items_per_epoch\"\n                                }\n                            ]\n                        },\n                        {\n                            \"id\": \"Clarifai_InceptionV2\",\n                            \"description\": \"A custom visual classifier template inspired by Inception-V2 networks. \",\n                            \"model_type_fields\": [\n                                {\n                                    \"path\": \"train_info.params.logreg\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 1,\n                                    \"description\": \"Whether to use sigmoid units (logreg=1) or softmax (logreg=0).\",\n                                    \"placeholder\": \"logreg\",\n                                    \"model_type_range_info\": {\n                                        \"max\": 1,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.image_size\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 256,\n                                    \"description\": \"Input image size (minimum side dimension).\",\n                                    \"placeholder\": \"image_size\",\n                                    \"model_type_range_info\": {\n                                        \"min\": 32,\n                                        \"max\": 1024,\n                                        \"step\": 16\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.batch_size\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 64,\n                                    \"description\": \"the batch size to use during training.\",\n                                    \"placeholder\": \"batch_size\",\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 128,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.init_epochs\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 25,\n                                    \"description\": \"number of epochs to run at the initial learning rate.\",\n                                    \"placeholder\": \"init_epochs\",\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 200,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.step_epochs\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 7,\n                                    \"description\": \"the number of epochs between learning rate decreases.\",\n                                    \"placeholder\": \"step_epochs\",\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 200,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.num_epochs\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 65,\n                                    \"description\": \"the total number of epochs to train for.\",\n                                    \"placeholder\": \"num_epochs\",\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 200,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.per_item_lrate\",\n                                    \"field_type\": 3,\n                                    \"default_value\": 0.000078125,\n                                    \"description\": \"the initial learning rate per item. The overall learning rate (per step) is set to lrate = batch_size * per_item_lrate\",\n                                    \"placeholder\": \"per_item_lrate\"\n                                },\n                                {\n                                    \"path\": \"train_info.params.num_items_per_epoch\",\n                                    \"field_type\": 3,\n                                    \"default_value\": 0,\n                                    \"description\": \"number of input images that constitute an \\\"epoch\\\".  Default is the number of images in the dataset.\",\n                                    \"placeholder\": \"num_items_per_epoch\"\n                                }\n                            ]\n                        },\n                        {\n                            \"id\": \"Clarifai_InceptionBatchNorm\",\n                            \"description\": \"A custom visual classifier template inspired by Inception networks tuned for speed. \",\n                            \"model_type_fields\": [\n                                {\n                                    \"path\": \"train_info.params.logreg\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 1,\n                                    \"description\": \"Whether to use sigmoid units (logreg=1) or softmax (logreg=0).\",\n                                    \"placeholder\": \"logreg\",\n                                    \"model_type_range_info\": {\n                                        \"max\": 1,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.image_size\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 256,\n                                    \"description\": \"Input image size (minimum side dimension).\",\n                                    \"placeholder\": \"image_size\",\n                                    \"model_type_range_info\": {\n                                        \"min\": 32,\n                                        \"max\": 1024,\n                                        \"step\": 16\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.batch_size\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 64,\n                                    \"description\": \"the batch size to use during training.\",\n                                    \"placeholder\": \"batch_size\",\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 128,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.init_epochs\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 25,\n                                    \"description\": \"number of epochs to run at the initial learning rate.\",\n                                    \"placeholder\": \"init_epochs\",\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 200,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.step_epochs\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 7,\n                                    \"description\": \"the number of epochs between learning rate decreases.\",\n                                    \"placeholder\": \"step_epochs\",\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 200,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.num_epochs\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 65,\n                                    \"description\": \"the total number of epochs to train for.\",\n                                    \"placeholder\": \"num_epochs\",\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 200,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.per_item_lrate\",\n                                    \"field_type\": 3,\n                                    \"default_value\": 0.000078125,\n                                    \"description\": \"the initial learning rate per item. The overall learning rate (per step) is set to lrate = batch_size * per_item_lrate\",\n                                    \"placeholder\": \"per_item_lrate\"\n                                },\n                                {\n                                    \"path\": \"train_info.params.num_items_per_epoch\",\n                                    \"field_type\": 3,\n                                    \"default_value\": 0,\n                                    \"description\": \"number of input images that constitute an \\\"epoch\\\".  Default is the number of images in the dataset.\",\n                                    \"placeholder\": \"num_items_per_epoch\"\n                                }\n                            ]\n                        },\n                        {\n                            \"id\": \"MMClassification\",\n                            \"description\": \"A training template that uses the MMClassification toolkit and a custom configuration \",\n                            \"model_type_fields\": [\n                                {\n                                    \"path\": \"train_info.params.seed\",\n                                    \"field_type\": 3,\n                                    \"default_value\": -1,\n                                    \"description\": \"[internal_only] the random seed to init training. If seed < 0, it is not set\",\n                                    \"placeholder\": \"seed\",\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.custom_config\",\n                                    \"field_type\": 15,\n                                    \"default_value\": \"\\n_base_ = '/mmclassification/configs/resnext/resnext101_32x4d_b32x8_imagenet.py'\\nrunner = dict(type='EpochBasedRunner', max_epochs=60)\\ndata = dict(\\n    train=dict(\\n        data_prefix='',\\n        ann_file='',\\n        classes=''),\\n    val=dict(\\n        data_prefix='',\\n        ann_file='',\\n        classes=''))\\n\",\n                                    \"description\": \"custom mmclassification config, in python config file format. Note that the '_base_' field, if used, should be a config file relative to the parent directory '/mmclassification/', e.g. \\\"_base_ = '/mmclassification/configs/efficientnet/efficientnet-b8_8xb32-01norm_in1k.py'\\\". The 'num_classes' field must be included somewhere in the config. The 'data' section should include 'train' and 'val' sections, each with 'ann_file', 'data_prefix', and 'classes' fields with empty strings as values. These values will be overwritten to be compatible with Clarifai's system, but must be included in the imported config.\",\n                                    \"placeholder\": \"custom_config\"\n                                },\n                                {\n                                    \"path\": \"train_info.params.concepts_mutually_exclusive\",\n                                    \"field_type\": 1,\n                                    \"default_value\": false,\n                                    \"description\": \"whether the concepts are mutually exclusive. If true then each input is expected to only be tagged with a single concept.\",\n                                    \"placeholder\": \"concepts_mutually_exclusive\"\n                                },\n                                {\n                                    \"path\": \"train_info.params.num_gpus\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 1,\n                                    \"description\": \"[internal_only] the number of gpus to train with.\",\n                                    \"placeholder\": \"num_gpus\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"max\": 1,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.image_size\",\n                                    \"field_type\": 11,\n                                    \"default_value\": [\n                                        320\n                                    ],\n                                    \"description\": \"the image size for inference (the training image size is defined in the mmcv config). If a single value, specifies the size of the min side.\",\n                                    \"placeholder\": \"image_size\"\n                                }\n                            ]\n                        },\n                        {\n                            \"id\": \"MMClassification_EfficientNet\",\n                            \"description\": \"A training template that uses the MMClassification toolkit and EfficientNet-B8 configuration \",\n                            \"model_type_fields\": [\n                                {\n                                    \"path\": \"train_info.params.seed\",\n                                    \"field_type\": 3,\n                                    \"default_value\": -1,\n                                    \"description\": \"[internal_only] the random seed to init training. If seed < 0, we will not set it.\",\n                                    \"placeholder\": \"seed\",\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.num_gpus\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 1,\n                                    \"description\": \"[internal_only] the number of gpus to train with.\",\n                                    \"placeholder\": \"num_gpus\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"max\": 1,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.image_size\",\n                                    \"field_type\": 3,\n                                    \"default_value\": 336,\n                                    \"description\": \"the image size for training and inference. EfficientNet works on square images.\",\n                                    \"placeholder\": \"image_size\",\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.batch_size\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 4,\n                                    \"description\": \"the batch size to use during training.\",\n                                    \"placeholder\": \"batch_size\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 256,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.num_epochs\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 30,\n                                    \"description\": \"the total number of epochs to train for.\",\n                                    \"placeholder\": \"num_epochs\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 200,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.per_item_lrate\",\n                                    \"field_type\": 3,\n                                    \"default_value\": 0.000390625,\n                                    \"description\": \"the initial learning rate per item. The overall learning rate (per step) is set to lrate = batch_size * per_item_lrate\",\n                                    \"placeholder\": \"per_item_lrate\",\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.weight_decay\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 0.0001,\n                                    \"description\": \"the weight decay value\",\n                                    \"placeholder\": \"weight_decay\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"max\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.momentum\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 0.9,\n                                    \"description\": \"the momentum value for the SGD optimizer\",\n                                    \"placeholder\": \"momentum\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"max\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.pretrained_weights\",\n                                    \"field_type\": 8,\n                                    \"default_value\": \"ImageNet-1k\",\n                                    \"description\": \"whether to use pretrained weights.\",\n                                    \"placeholder\": \"pretrained_weights\",\n                                    \"model_type_enum_options\": [\n                                        {\n                                            \"id\": \"None\"\n                                        },\n                                        {\n                                            \"id\": \"ImageNet-1k\"\n                                        }\n                                    ],\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.flip_probability\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 0.5,\n                                    \"description\": \"the probability an image will be flipped during training\",\n                                    \"placeholder\": \"flip_probability\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"max\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.flip_direction\",\n                                    \"field_type\": 8,\n                                    \"default_value\": \"horizontal\",\n                                    \"description\": \"the direction to randomly flip during training.\",\n                                    \"placeholder\": \"flip_direction\",\n                                    \"model_type_enum_options\": [\n                                        {\n                                            \"id\": \"horizontal\"\n                                        },\n                                        {\n                                            \"id\": \"vertical\"\n                                        }\n                                    ],\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.concepts_mutually_exclusive\",\n                                    \"field_type\": 1,\n                                    \"default_value\": false,\n                                    \"description\": \"whether the concepts are mutually exclusive. If true then each input is expected to only be tagged with a single concept.\",\n                                    \"placeholder\": \"concepts_mutually_exclusive\",\n                                    \"internal_only\": true\n                                }\n                            ],\n                            \"internal_only\": true\n                        },\n                        {\n                            \"id\": \"MMClassification_ResNet_50_RSB_A1\",\n                            \"description\": \"A training template that uses the MMClassification toolkit and ResNet-50 (rsb-a1) configuration \",\n                            \"model_type_fields\": [\n                                {\n                                    \"path\": \"train_info.params.seed\",\n                                    \"field_type\": 3,\n                                    \"default_value\": -1,\n                                    \"description\": \"[internal_only] the random seed to init training. If seed < 0, we will not set it.\",\n                                    \"placeholder\": \"seed\",\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.num_gpus\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 1,\n                                    \"description\": \"[internal_only] the number of gpus to train with.\",\n                                    \"placeholder\": \"num_gpus\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"max\": 1,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.image_size\",\n                                    \"field_type\": 3,\n                                    \"default_value\": 224,\n                                    \"description\": \"the image size for training and inference. ResNet uses square images.\",\n                                    \"placeholder\": \"image_size\"\n                                },\n                                {\n                                    \"path\": \"train_info.params.batch_size\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 64,\n                                    \"description\": \"the batch size to use during training.\",\n                                    \"placeholder\": \"batch_size\",\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 256,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.num_epochs\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 60,\n                                    \"description\": \"the total number of epochs to train for.\",\n                                    \"placeholder\": \"num_epochs\",\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 600,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.per_item_lrate\",\n                                    \"field_type\": 3,\n                                    \"default_value\": 0.00001953125,\n                                    \"description\": \"the initial learning rate per item. The overall learning rate (per step) is set to lrate = batch_size * per_item_lrate\",\n                                    \"placeholder\": \"per_item_lrate\"\n                                },\n                                {\n                                    \"path\": \"train_info.params.weight_decay\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 0.01,\n                                    \"description\": \"the weight decay value\",\n                                    \"placeholder\": \"weight_decay\",\n                                    \"model_type_range_info\": {\n                                        \"max\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.per_item_min_lrate\",\n                                    \"field_type\": 3,\n                                    \"default_value\": 1.5625e-08,\n                                    \"description\": \"The minimum learning (per item) at end of training using cosine schedule.\",\n                                    \"placeholder\": \"per_item_min_lrate\"\n                                },\n                                {\n                                    \"path\": \"train_info.params.warmup_iters\",\n                                    \"field_type\": 3,\n                                    \"default_value\": 100,\n                                    \"description\": \"The number of steps in the warmup phase\",\n                                    \"placeholder\": \"warmup_iters\"\n                                },\n                                {\n                                    \"path\": \"train_info.params.warmup_ratio\",\n                                    \"field_type\": 3,\n                                    \"default_value\": 0.0001,\n                                    \"description\": \" Warmup phase learning rate multiplier\",\n                                    \"placeholder\": \"warmup_ratio\"\n                                },\n                                {\n                                    \"path\": \"train_info.params.pretrained_weights\",\n                                    \"field_type\": 8,\n                                    \"default_value\": \"ImageNet-1k\",\n                                    \"description\": \"whether to use pretrained weights.\",\n                                    \"placeholder\": \"pretrained_weights\",\n                                    \"model_type_enum_options\": [\n                                        {\n                                            \"id\": \"None\"\n                                        },\n                                        {\n                                            \"id\": \"ImageNet-1k\"\n                                        }\n                                    ]\n                                },\n                                {\n                                    \"path\": \"train_info.params.flip_probability\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 0.5,\n                                    \"description\": \"the probability an image will be flipped during training\",\n                                    \"placeholder\": \"flip_probability\",\n                                    \"model_type_range_info\": {\n                                        \"max\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.flip_direction\",\n                                    \"field_type\": 8,\n                                    \"default_value\": \"horizontal\",\n                                    \"description\": \"the direction to randomly flip during training.\",\n                                    \"placeholder\": \"flip_direction\",\n                                    \"model_type_enum_options\": [\n                                        {\n                                            \"id\": \"horizontal\"\n                                        },\n                                        {\n                                            \"id\": \"vertical\"\n                                        }\n                                    ]\n                                },\n                                {\n                                    \"path\": \"train_info.params.concepts_mutually_exclusive\",\n                                    \"field_type\": 1,\n                                    \"default_value\": false,\n                                    \"description\": \"whether the concepts are mutually exclusive. If true then each input is expected to only be tagged with a single concept.\",\n                                    \"placeholder\": \"concepts_mutually_exclusive\"\n                                }\n                            ],\n                            \"recommended\": true\n                        },\n                        {\n                            \"id\": \"MMClassification_ResNet_50\",\n                            \"description\": \"A training template that uses the MMClassification toolkit and ResNet-50 configuration \",\n                            \"model_type_fields\": [\n                                {\n                                    \"path\": \"train_info.params.seed\",\n                                    \"field_type\": 3,\n                                    \"default_value\": -1,\n                                    \"description\": \"[internal_only] the random seed to init training. If seed < 0, we will not set it.\",\n                                    \"placeholder\": \"seed\",\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.num_gpus\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 1,\n                                    \"description\": \"[internal_only] the number of gpus to train with.\",\n                                    \"placeholder\": \"num_gpus\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"max\": 1,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.image_size\",\n                                    \"field_type\": 3,\n                                    \"default_value\": 224,\n                                    \"description\": \"the image size for training and inference. ResNet works on square images.\",\n                                    \"placeholder\": \"image_size\",\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.batch_size\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 64,\n                                    \"description\": \"the batch size to use per gpu during training.\",\n                                    \"placeholder\": \"batch_size\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 256,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.num_epochs\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 60,\n                                    \"description\": \"the total number of epochs to train for.\",\n                                    \"placeholder\": \"num_epochs\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 600,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.per_item_lrate\",\n                                    \"field_type\": 3,\n                                    \"default_value\": 0.000390625,\n                                    \"description\": \"the initial learning rate per item. The overall learning rate (per step) is set to lrate = batch_size * per_item_lrate\",\n                                    \"placeholder\": \"per_item_lrate\",\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.learning_rate_steps\",\n                                    \"field_type\": 11,\n                                    \"default_value\": [\n                                        30,\n                                        40,\n                                        50\n                                    ],\n                                    \"description\": \"epoch schedule for stepping down learning rate\",\n                                    \"placeholder\": \"learning_rate_steps\",\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.weight_decay\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 0.0001,\n                                    \"description\": \"the weight decay value\",\n                                    \"placeholder\": \"weight_decay\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"max\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.momentum\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 0.9,\n                                    \"description\": \"the momentum value for the SGD optimizer\",\n                                    \"placeholder\": \"momentum\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"max\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.pretrained_weights\",\n                                    \"field_type\": 8,\n                                    \"default_value\": \"ImageNet-1k\",\n                                    \"description\": \"whether to use pretrained weights.\",\n                                    \"placeholder\": \"pretrained_weights\",\n                                    \"model_type_enum_options\": [\n                                        {\n                                            \"id\": \"None\"\n                                        },\n                                        {\n                                            \"id\": \"ImageNet-1k\"\n                                        }\n                                    ],\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.flip_probability\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 0.5,\n                                    \"description\": \"the probability an image will be flipped during training\",\n                                    \"placeholder\": \"flip_probability\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"max\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.flip_direction\",\n                                    \"field_type\": 8,\n                                    \"default_value\": \"horizontal\",\n                                    \"description\": \"the direction to randomly flip during training.\",\n                                    \"placeholder\": \"flip_direction\",\n                                    \"model_type_enum_options\": [\n                                        {\n                                            \"id\": \"horizontal\"\n                                        },\n                                        {\n                                            \"id\": \"vertical\"\n                                        }\n                                    ],\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.concepts_mutually_exclusive\",\n                                    \"field_type\": 1,\n                                    \"default_value\": false,\n                                    \"description\": \"whether the concepts are mutually exclusive. If true then each input is expected to only be tagged with a single concept.\",\n                                    \"placeholder\": \"concepts_mutually_exclusive\",\n                                    \"internal_only\": true\n                                }\n                            ],\n                            \"internal_only\": true\n                        }\n                    ],\n                    \"required\": true\n                }\n            ],\n            \"expected_input_layers\": [\n                {\n                    \"data_field_name\": \"image\",\n                    \"shapes\": [\n                        {\n                            \"dims\": [\n                                -1,\n                                -1,\n                                3\n                            ],\n                            \"max_dims\": [\n                                1024,\n                                1024,\n                                3\n                            ],\n                            \"data_type\": 2,\n                            \"description\": \"First two dimensions are the height and width, followed by the number of channels.\"\n                        }\n                    ],\n                    \"description\": \"Image urls or base64 are converted into numpy arrays of the specified size and forwarded to the model. If flexible dims are provided, inputs will be downsampled and padded to a default size.\"\n                }\n            ],\n            \"expected_output_layers\": [\n                {\n                    \"data_field_name\": \"concepts\",\n                    \"shapes\": [\n                        {\n                            \"dims\": [\n                                -1\n                            ],\n                            \"data_type\": 5,\n                            \"description\": \"Length of the list is expected to be the number of concepts returned by this model, with each value being the confidence for the respective model output.\"\n                        }\n                    ],\n                    \"description\": \"Concepts defined in the model should be the same order as specified in the label file.\",\n                    \"requires_label_filename\": true\n                }\n            ],\n            \"evaluation_type\": 1\n        },\n        {\n            \"id\": \"visual-embedder\",\n            \"title\": \"Visual Embedder\",\n            \"description\": \"Embed images and videos frames into a vector representing a high level understanding from our AI models. These embeddings enable visual search and training on top of them.\",\n            \"input_fields\": [\n                \"image\"\n            ],\n            \"output_fields\": [\n                \"embeddings\"\n            ],\n            \"trainable\": true,\n            \"creatable\": true,\n            \"model_type_fields\": [\n                {\n                    \"path\": \"output_info.data.concepts\",\n                    \"field_type\": 4,\n                    \"description\": \"List of concepts you want this models embeddings to be learned on.\",\n                    \"placeholder\": \"List of concepts\",\n                    \"required\": true\n                },\n                {\n                    \"path\": \"train_info.params.invalid_data_tolerance_percent\",\n                    \"field_type\": 7,\n                    \"default_value\": 5,\n                    \"description\": \"Percentage value (0 to 100) of user's tolerance level to invalid inputs among all training inputs. Training will be stopped with error thrown if actual percent of invalid inputs is higher than this\",\n                    \"placeholder\": \"Invalid Data Tolerance Percentage\",\n                    \"model_type_range_info\": {\n                        \"max\": 100,\n                        \"step\": 0.1\n                    }\n                },\n                {\n                    \"path\": \"train_info.params.dataset_id\",\n                    \"field_type\": 16,\n                    \"default_value\": \"\",\n                    \"description\": \"Deprecated in favor of train_info.dataset\",\n                    \"placeholder\": \"DEPRECATED: Training Dataset ID\"\n                },\n                {\n                    \"path\": \"train_info.dataset\",\n                    \"field_type\": 19,\n                    \"default_value\": \"\",\n                    \"description\": \"Dataset to use for training this model\",\n                    \"placeholder\": \"Training Dataset\"\n                },\n                {\n                    \"path\": \"train_info.params.dataset_version_id\",\n                    \"field_type\": 17,\n                    \"default_value\": \"\",\n                    \"description\": \"Deprecated in favor of train_info.dataset.version\",\n                    \"placeholder\": \"DEPRECATED: Training Dataset Version ID\"\n                },\n                {\n                    \"path\": \"train_info.dataset.version\",\n                    \"field_type\": 20,\n                    \"default_value\": \"\",\n                    \"description\": \"Dataset version to use for training this model. If a dataset version is not specified but a dataset is, we will automatically generate a dataset version.\",\n                    \"placeholder\": \"Training Dataset Version\"\n                },\n                {\n                    \"path\": \"train_info.params.template\",\n                    \"field_type\": 14,\n                    \"default_value\": \"Clarifai_ResNext\",\n                    \"description\": \"The template name is a pre-configured model template to train with on your data. Depending on your data you might want to try a few templates to see which yields optimal results.\",\n                    \"placeholder\": \"Training Template\",\n                    \"model_type_enum_options\": [\n                        {\n                            \"id\": \"classification_basemodel_v1_embed\",\n                            \"description\": \"This is a private base class for our visual embedder models. \",\n                            \"model_type_fields\": [\n                                {\n                                    \"path\": \"train_info.params.num_gpus\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 1,\n                                    \"description\": \"[internal_only] the number of gpus to train with.\",\n                                    \"placeholder\": \"num_gpus\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"max\": 1,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.model_cfg\",\n                                    \"field_type\": 2,\n                                    \"default_value\": \"resnext\",\n                                    \"description\": \"the underlying model configuration to use.\",\n                                    \"placeholder\": \"model_cfg\",\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.preinit\",\n                                    \"field_type\": 2,\n                                    \"default_value\": \"general-v1.5\",\n                                    \"description\": \"model to start from to initialize weights\",\n                                    \"placeholder\": \"preinit\",\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.logreg\",\n                                    \"field_type\": 3,\n                                    \"default_value\": 1,\n                                    \"description\": \"whether to use sigmoid units or softmax\",\n                                    \"placeholder\": \"logreg\",\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.image_size\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 256,\n                                    \"description\": \"Input image size (minimum side dimension).\",\n                                    \"placeholder\": \"image_size\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"min\": 32,\n                                        \"max\": 1024,\n                                        \"step\": 16\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.batch_size\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 64,\n                                    \"description\": \"the batch size to use during training.\",\n                                    \"placeholder\": \"batch_size\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 128,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.init_epochs\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 25,\n                                    \"description\": \"number of epochs to run at the initial learning rate.\",\n                                    \"placeholder\": \"init_epochs\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 200,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.step_epochs\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 7,\n                                    \"description\": \"the number of epochs between learning rate decreases.\",\n                                    \"placeholder\": \"step_epochs\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 200,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.num_epochs\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 65,\n                                    \"description\": \"the total number of epochs to train for.\",\n                                    \"placeholder\": \"num_epochs\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 200,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.per_item_lrate\",\n                                    \"field_type\": 3,\n                                    \"default_value\": 0.000078125,\n                                    \"description\": \"the initial learning rate per item. The overall learning rate (per step) is set to lrate = batch_size * per_item_lrate\",\n                                    \"placeholder\": \"per_item_lrate\",\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.num_items_per_epoch\",\n                                    \"field_type\": 3,\n                                    \"default_value\": 0,\n                                    \"description\": \"number of input images that constitute an \\\"epoch\\\".  Default is the number of images in the dataset.\",\n                                    \"placeholder\": \"num_items_per_epoch\",\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.inference_crop_type\",\n                                    \"field_type\": 2,\n                                    \"default_value\": \"sorta2\",\n                                    \"description\": \"[internal_only] the crop type to use for inference (used when evaluating the model).\",\n                                    \"placeholder\": \"inference_crop_type\",\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.embeddings_layer\",\n                                    \"field_type\": 2,\n                                    \"default_value\": \"fc_layers/Mean\",\n                                    \"description\": \"the embedding layer to use as output from this model.\",\n                                    \"placeholder\": \"embeddings_layer\",\n                                    \"internal_only\": true\n                                }\n                            ],\n                            \"internal_only\": true\n                        },\n                        {\n                            \"id\": \"Clarifai_ResNext\",\n                            \"description\": \"A custom visual embedder template inspired by Resnext. \",\n                            \"model_type_fields\": [\n                                {\n                                    \"path\": \"train_info.params.logreg\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 1,\n                                    \"description\": \"Whether to use sigmoid units (logreg=1) or softmax (logreg=0) when comparing against training target labels.\",\n                                    \"placeholder\": \"logreg\",\n                                    \"model_type_range_info\": {\n                                        \"max\": 1,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.image_size\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 256,\n                                    \"description\": \"Input image size (minimum side dimension).\",\n                                    \"placeholder\": \"image_size\",\n                                    \"model_type_range_info\": {\n                                        \"min\": 32,\n                                        \"max\": 1024,\n                                        \"step\": 16\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.batch_size\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 64,\n                                    \"description\": \"the batch size to use during training.\",\n                                    \"placeholder\": \"batch_size\",\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 128,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.init_epochs\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 25,\n                                    \"description\": \"number of epochs to run at the initial learning rate.\",\n                                    \"placeholder\": \"init_epochs\",\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 200,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.step_epochs\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 7,\n                                    \"description\": \"the number of epochs between learning rate decreases.\",\n                                    \"placeholder\": \"step_epochs\",\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 200,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.num_epochs\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 65,\n                                    \"description\": \"the total number of epochs to train for.\",\n                                    \"placeholder\": \"num_epochs\",\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 200,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.per_item_lrate\",\n                                    \"field_type\": 3,\n                                    \"default_value\": 0.000078125,\n                                    \"description\": \"the initial learning rate per item. The overall learning rate (per step) is set to lrate = batch_size * per_item_lrate\",\n                                    \"placeholder\": \"per_item_lrate\"\n                                },\n                                {\n                                    \"path\": \"train_info.params.num_items_per_epoch\",\n                                    \"field_type\": 3,\n                                    \"default_value\": 0,\n                                    \"description\": \"number of input images that constitute an \\\"epoch\\\".  Default is the number of images in the dataset.\",\n                                    \"placeholder\": \"num_items_per_epoch\"\n                                }\n                            ],\n                            \"recommended\": true\n                        },\n                        {\n                            \"id\": \"Clarifai_InceptionBatchNorm\",\n                            \"description\": \"A custom visual embedder template inspired by Inception networks tuned for speed. \",\n                            \"model_type_fields\": [\n                                {\n                                    \"path\": \"train_info.params.logreg\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 1,\n                                    \"description\": \"Whether to use sigmoid units (logreg=1) or softmax (logreg=0) when comparing against training target labels.\",\n                                    \"placeholder\": \"logreg\",\n                                    \"model_type_range_info\": {\n                                        \"max\": 1,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.image_size\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 256,\n                                    \"description\": \"Input image size (minimum side dimension).\",\n                                    \"placeholder\": \"image_size\",\n                                    \"model_type_range_info\": {\n                                        \"min\": 32,\n                                        \"max\": 1024,\n                                        \"step\": 16\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.batch_size\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 64,\n                                    \"description\": \"the batch size to use during training.\",\n                                    \"placeholder\": \"batch_size\",\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 128,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.init_epochs\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 25,\n                                    \"description\": \"number of epochs to run at the initial learning rate.\",\n                                    \"placeholder\": \"init_epochs\",\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 200,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.step_epochs\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 7,\n                                    \"description\": \"the number of epochs between learning rate decreases.\",\n                                    \"placeholder\": \"step_epochs\",\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 200,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.num_epochs\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 65,\n                                    \"description\": \"the total number of epochs to train for.\",\n                                    \"placeholder\": \"num_epochs\",\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 200,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.per_item_lrate\",\n                                    \"field_type\": 3,\n                                    \"default_value\": 0.000078125,\n                                    \"description\": \"the initial learning rate per item. The overall learning rate (per step) is set to lrate = batch_size * per_item_lrate\",\n                                    \"placeholder\": \"per_item_lrate\"\n                                },\n                                {\n                                    \"path\": \"train_info.params.num_items_per_epoch\",\n                                    \"field_type\": 3,\n                                    \"default_value\": 0,\n                                    \"description\": \"number of input images that constitute an \\\"epoch\\\".  Default is the number of images in the dataset.\",\n                                    \"placeholder\": \"num_items_per_epoch\"\n                                }\n                            ]\n                        },\n                        {\n                            \"id\": \"classification_angular_margin_embed\",\n                            \"description\": \"This is a private base class for our visual embedder models with additive angular margin loss. \",\n                            \"model_type_fields\": [\n                                {\n                                    \"path\": \"train_info.params.num_gpus\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 1,\n                                    \"description\": \"[internal_only] the number of gpus to train with.\",\n                                    \"placeholder\": \"num_gpus\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"max\": 1,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.image_size\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 112,\n                                    \"description\": \"Input image size (minimum side dimension).\",\n                                    \"placeholder\": \"image_size\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"min\": 32,\n                                        \"max\": 1024,\n                                        \"step\": 16\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.batch_size\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 64,\n                                    \"description\": \"the batch size to use during training.\",\n                                    \"placeholder\": \"batch_size\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 128,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.init_epochs\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 20,\n                                    \"description\": \"number of epochs to run at the initial learning rate.\",\n                                    \"placeholder\": \"init_epochs\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 200,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.step_epochs\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 40,\n                                    \"description\": \"the number of epochs between learning rate decreases.\",\n                                    \"placeholder\": \"step_epochs\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 200,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.num_epochs\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 65,\n                                    \"description\": \"the total number of epochs to train for.\",\n                                    \"placeholder\": \"num_epochs\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 200,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.per_item_lrate\",\n                                    \"field_type\": 3,\n                                    \"default_value\": 0.0000390625,\n                                    \"description\": \"the initial learning rate per item. The overall learning rate (per step) is set to lrate = batch_size * per_item_lrate\",\n                                    \"placeholder\": \"per_item_lrate\",\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.num_items_per_epoch\",\n                                    \"field_type\": 3,\n                                    \"default_value\": 0,\n                                    \"description\": \"number of input images that constitute an \\\"epoch\\\".  Default is the number of images in the dataset.\",\n                                    \"placeholder\": \"num_items_per_epoch\",\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.inference_crop_type\",\n                                    \"field_type\": 2,\n                                    \"default_value\": \"center1\",\n                                    \"description\": \"[internal_only] the crop type to use for inference (used when evaluating the model).\",\n                                    \"placeholder\": \"inference_crop_type\",\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.embeddings_size\",\n                                    \"field_type\": 3,\n                                    \"default_value\": 512,\n                                    \"description\": \"the embedding dimension to use as output from this model.\",\n                                    \"placeholder\": \"embeddings_size\",\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.angular_scale\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 64,\n                                    \"description\": \"radius hyperparam used in angular margin loss\",\n                                    \"placeholder\": \"angular_scale\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"min\": 16,\n                                        \"max\": 128,\n                                        \"step\": 16\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.angular_margin\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 0.2,\n                                    \"description\": \"margin hyperparam used in angular margin loss\",\n                                    \"placeholder\": \"angular_margin\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"min\": 0.1,\n                                        \"max\": 0.9\n                                    }\n                                }\n                            ],\n                            \"internal_only\": true\n                        },\n                        {\n                            \"id\": \"Clarifai_ResNet_AngularMargin\",\n                            \"description\": \"A custom visual embedder template inspired by ResNet101 with Additive Angular Margin loss. \",\n                            \"model_type_fields\": [\n                                {\n                                    \"path\": \"train_info.params.image_size\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 112,\n                                    \"description\": \"Input image size (minimum side dimension).\",\n                                    \"placeholder\": \"image_size\",\n                                    \"model_type_range_info\": {\n                                        \"min\": 32,\n                                        \"max\": 1024,\n                                        \"step\": 16\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.batch_size\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 64,\n                                    \"description\": \"the batch size to use during training.\",\n                                    \"placeholder\": \"batch_size\",\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 128,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.init_epochs\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 20,\n                                    \"description\": \"number of epochs to run at the initial learning rate.\",\n                                    \"placeholder\": \"init_epochs\",\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 200,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.step_epochs\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 40,\n                                    \"description\": \"the number of epochs between learning rate decreases.\",\n                                    \"placeholder\": \"step_epochs\",\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 200,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.num_epochs\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 65,\n                                    \"description\": \"the total number of epochs to train for.\",\n                                    \"placeholder\": \"num_epochs\",\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 200,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.per_item_lrate\",\n                                    \"field_type\": 3,\n                                    \"default_value\": 0.0000390625,\n                                    \"description\": \"the initial learning rate per item. The overall learning rate (per step) is set to lrate = batch_size * per_item_lrate\",\n                                    \"placeholder\": \"per_item_lrate\"\n                                },\n                                {\n                                    \"path\": \"train_info.params.num_items_per_epoch\",\n                                    \"field_type\": 3,\n                                    \"default_value\": 0,\n                                    \"description\": \"number of input images that constitute an \\\"epoch\\\".  Default is the number of images in the dataset.\",\n                                    \"placeholder\": \"num_items_per_epoch\"\n                                },\n                                {\n                                    \"path\": \"train_info.params.angular_scale\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 64,\n                                    \"description\": \"radius hyperparam used in angular margin loss\",\n                                    \"placeholder\": \"angular_scale\",\n                                    \"model_type_range_info\": {\n                                        \"min\": 16,\n                                        \"max\": 128,\n                                        \"step\": 16\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.angular_margin\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 0.2,\n                                    \"description\": \"margin hyperparam used in angular margin loss\",\n                                    \"placeholder\": \"angular_margin\",\n                                    \"model_type_range_info\": {\n                                        \"min\": 0.1,\n                                        \"max\": 0.9\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.embeddings_size\",\n                                    \"field_type\": 3,\n                                    \"default_value\": 512,\n                                    \"description\": \"the embedding dimension to use as output from this model.\",\n                                    \"placeholder\": \"embeddings_size\"\n                                },\n                                {\n                                    \"path\": \"train_info.params.inference_crop_type\",\n                                    \"field_type\": 2,\n                                    \"default_value\": \"center1\",\n                                    \"description\": \"[internal_only] the crop type to use for inference (used when evaluating the model).\",\n                                    \"placeholder\": \"inference_crop_type\",\n                                    \"internal_only\": true\n                                }\n                            ]\n                        }\n                    ],\n                    \"required\": true\n                }\n            ],\n            \"expected_input_layers\": [\n                {\n                    \"data_field_name\": \"image\",\n                    \"shapes\": [\n                        {\n                            \"dims\": [\n                                -1,\n                                -1,\n                                3\n                            ],\n                            \"max_dims\": [\n                                1024,\n                                1024,\n                                3\n                            ],\n                            \"data_type\": 2,\n                            \"description\": \"First two dimensions are the height and width, followed by the number of channels.\"\n                        }\n                    ],\n                    \"description\": \"Image urls or base64 are converted into numpy arrays of the specified size and forwarded to the model. If flexible dims are provided, inputs will be downsampled and padded to a default size.\"\n                }\n            ],\n            \"expected_output_layers\": [\n                {\n                    \"data_field_name\": \"embeddings\",\n                    \"shapes\": [\n                        {\n                            \"dims\": [\n                                -1\n                            ],\n                            \"data_type\": 5\n                        }\n                    ],\n                    \"description\": \"The embedding vector returned by the model\"\n                }\n            ]\n        },\n        {\n            \"id\": \"visual-segmenter\",\n            \"title\": \"Visual Segmenter\",\n            \"description\": \"Segment a per-pixel mask in images where things are and then classify objects, descriptive words or topics within the masks.\",\n            \"input_fields\": [\n                \"image\"\n            ],\n            \"output_fields\": [\n                \"regions[...].region_info.mask,regions[...].data.concepts\"\n            ],\n            \"trainable\": true,\n            \"creatable\": true,\n            \"model_type_fields\": [\n                {\n                    \"path\": \"output_info.data.concepts\",\n                    \"field_type\": 4,\n                    \"description\": \"List of concepts you want this models embeddings to be learned on.\",\n                    \"placeholder\": \"List of concepts\",\n                    \"required\": true\n                },\n                {\n                    \"path\": \"train_info.params.invalid_data_tolerance_percent\",\n                    \"field_type\": 7,\n                    \"default_value\": 5,\n                    \"description\": \"Percentage value (0 to 100) of user's tolerance level to invalid inputs among all training inputs. Training will be stopped with error thrown if actual percent of invalid inputs is higher than this\",\n                    \"placeholder\": \"Invalid Data Tolerance Percentage\",\n                    \"model_type_range_info\": {\n                        \"max\": 100,\n                        \"step\": 0.1\n                    }\n                },\n                {\n                    \"path\": \"train_info.params.dataset_id\",\n                    \"field_type\": 16,\n                    \"default_value\": \"\",\n                    \"description\": \"Deprecated in favor of train_info.dataset\",\n                    \"placeholder\": \"DEPRECATED: Training Dataset ID\"\n                },\n                {\n                    \"path\": \"train_info.dataset\",\n                    \"field_type\": 19,\n                    \"default_value\": \"\",\n                    \"description\": \"Dataset to use for training this model\",\n                    \"placeholder\": \"Training Dataset\"\n                },\n                {\n                    \"path\": \"train_info.params.dataset_version_id\",\n                    \"field_type\": 17,\n                    \"default_value\": \"\",\n                    \"description\": \"Deprecated in favor of train_info.dataset.version\",\n                    \"placeholder\": \"DEPRECATED: Training Dataset Version ID\"\n                },\n                {\n                    \"path\": \"train_info.dataset.version\",\n                    \"field_type\": 20,\n                    \"default_value\": \"\",\n                    \"description\": \"Dataset version to use for training this model. If a dataset version is not specified but a dataset is, we will automatically generate a dataset version.\",\n                    \"placeholder\": \"Training Dataset Version\"\n                },\n                {\n                    \"path\": \"train_info.params.template\",\n                    \"field_type\": 14,\n                    \"default_value\": \"MMSegmentation_SegFormer\",\n                    \"description\": \"The template name is a pre-configured model template to train with on your data. Depending on your data you might want to try a few templates to see which yields optimal results.\",\n                    \"placeholder\": \"Training Template\",\n                    \"model_type_enum_options\": [\n                        {\n                            \"id\": \"MMSegmentation\",\n                            \"description\": \"A training template that uses the MMSegmentation toolkit and custom configuration \",\n                            \"model_type_fields\": [\n                                {\n                                    \"path\": \"train_info.params.seed\",\n                                    \"field_type\": 3,\n                                    \"default_value\": -1,\n                                    \"description\": \"[internal_only] the random seed to init training. If seed < 0, we will not set it.\",\n                                    \"placeholder\": \"seed\",\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.num_gpus\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 1,\n                                    \"description\": \"[internal_only] the number of gpus to train with.\",\n                                    \"placeholder\": \"num_gpus\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"max\": 1,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.custom_config\",\n                                    \"field_type\": 15,\n                                    \"default_value\": \"\\n_base_ = '/mmsegmentation/configs/segformer/segformer_mit-b2_512x512_160k_ade20k.py'\\nmodel = dict(\\n    pretrained=None,\\n    decode_head=dict(num_classes=0))\\noptimizer = dict(\\n    lr=1.5e-05)\\nrunner = dict(type='EpochBasedRunner', max_epochs=10, max_iters=None)\\ncrop_size = (520, 520)\\nimg_norm_cfg={'mean': [123.675, 116.28, 103.53],'std': [58.395, 57.12, 57.375],'to_rgb': True}\\ntrain_pipeline = [\\n    dict(type='LoadImageFromFile'),\\n    dict(type='LoadAnnotations', reduce_zero_label=False),\\n    dict(type='Resize', img_scale=(520, 520), ratio_range=(0.5, 2.0)),\\n    dict(type='RandomCrop', crop_size=crop_size, cat_max_ratio=0.75),\\n    dict(type='RandomFlip', prob=0.5),\\n    dict(type='PhotoMetricDistortion'),\\n    dict(type='Normalize', **img_norm_cfg),\\n    dict(type='Pad', size=crop_size, pad_val=0, seg_pad_val=255),\\n    dict(type='DefaultFormatBundle'),\\n    dict(type='Collect', keys=['img', 'gt_semantic_seg']),\\n]\\ntest_pipeline = [\\n    dict(type='LoadImageFromFile'),\\n    dict(\\n        type='MultiScaleFlipAug',\\n        img_scale=(520, 520),\\n        flip=False,\\n        transforms=[\\n            dict(type='Resize', keep_ratio=True),\\n            dict(type='RandomFlip'),\\n            dict(type='Normalize', **img_norm_cfg),\\n            dict(type='ImageToTensor', keys=['img']),\\n            dict(type='Collect', keys=['img']),\\n        ])\\n]\\ndata_root=None\\ndataset_type = 'CustomDataset'\\ndata = dict(\\n    samples_per_gpu=2,\\n    workers_per_gpu=2,\\n    train=dict(\\n        type=dataset_type,\\n        pipeline=train_pipeline,\\n        data_root=data_root,\\n        img_dir='',\\n        ann_dir='',\\n        classes=''),\\n    val=dict(\\n        type=dataset_type,\\n        pipeline=test_pipeline,\\n        data_root=data_root,\\n        img_dir='',\\n        ann_dir='',\\n        classes=''))\\nload_from='https://download.openmmlab.com/mmsegmentation/v0.5/segformer/segformer_mit-b2_512x512_160k_ade20k/segformer_mit-b2_512x512_160k_ade20k_20210726_112103-cbd414ac.pth'\\n\",\n                                    \"description\": \"custom mmsegmentation config, in python config file format. Note that the '_base_' field, if used, should be a config file relative to the parent directory '/mmsegmentation/', e.g. \\\"_base_ = '/mmsegmentation/configs/segformer/segformer_mit-b2_512x512_160k_ade20k.py'\\\". The 'num_classes' field must be included somewhere in the config. The 'data' section should include 'train' and 'val' sections, each with 'ann_dir', 'img_dir', and 'classes' fields with empty strings as values. These values will be overwritten to be compatible with Clarifai's system, but must be included in the imported config. 'reduce_zero_label' should be set to False. A background class will be automatically added to the training vocab.\",\n                                    \"placeholder\": \"custom_config\"\n                                },\n                                {\n                                    \"path\": \"train_info.params.image_size\",\n                                    \"field_type\": 11,\n                                    \"default_value\": [\n                                        520\n                                    ],\n                                    \"description\": \"the image size for inference. can be 1 or 2 elements. when a single value, specifies min side\",\n                                    \"placeholder\": \"image_size\"\n                                }\n                            ]\n                        },\n                        {\n                            \"id\": \"MMSegmentation_SegFormer\",\n                            \"description\": \"A training template that uses the MMSegmentation toolkit and SegFormer configuration \",\n                            \"model_type_fields\": [\n                                {\n                                    \"path\": \"train_info.params.seed\",\n                                    \"field_type\": 3,\n                                    \"default_value\": -1,\n                                    \"description\": \"[internal_only] the random seed to init training. If seed < 0, we will not set it.\",\n                                    \"placeholder\": \"seed\",\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.num_gpus\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 1,\n                                    \"description\": \"[internal_only] the number of gpus to train with.\",\n                                    \"placeholder\": \"num_gpus\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"max\": 1,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.image_size\",\n                                    \"field_type\": 11,\n                                    \"default_value\": [\n                                        520\n                                    ],\n                                    \"description\": \"the image size for training and inference. can be 1 or 2 elements. when a single value, specifies min side\",\n                                    \"placeholder\": \"image_size\"\n                                },\n                                {\n                                    \"path\": \"train_info.params.batch_size\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 2,\n                                    \"description\": \"the batch size to use during training.\",\n                                    \"placeholder\": \"batch_size\",\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 16,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.num_epochs\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 1,\n                                    \"description\": \"the total number of epochs to train for.\",\n                                    \"placeholder\": \"num_epochs\",\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 200,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.per_item_lrate\",\n                                    \"field_type\": 3,\n                                    \"default_value\": 0.0000075,\n                                    \"description\": \"the initial learning rate per item. The overall learning rate (per step) is set to lrate = batch_size * per_item_lrate\",\n                                    \"placeholder\": \"per_item_lrate\"\n                                },\n                                {\n                                    \"path\": \"train_info.params.pretrained_weights\",\n                                    \"field_type\": 8,\n                                    \"default_value\": \"ade20k\",\n                                    \"description\": \"whether to init with pretrained weights.\",\n                                    \"placeholder\": \"pretrained_weights\",\n                                    \"model_type_enum_options\": [\n                                        {\n                                            \"id\": \"None\"\n                                        },\n                                        {\n                                            \"id\": \"ade20k\"\n                                        }\n                                    ]\n                                }\n                            ],\n                            \"recommended\": true\n                        }\n                    ],\n                    \"required\": true\n                }\n            ],\n            \"expected_input_layers\": [\n                {\n                    \"data_field_name\": \"image\",\n                    \"shapes\": [\n                        {\n                            \"dims\": [\n                                -1,\n                                -1,\n                                3\n                            ],\n                            \"max_dims\": [\n                                1024,\n                                1024,\n                                3\n                            ],\n                            \"data_type\": 2,\n                            \"description\": \"First two dimensions are the height and width, followed by the number of channels.\"\n                        }\n                    ],\n                    \"description\": \"Image urls or base64 are converted into numpy arrays of the specified size and forwarded to the model. If flexible dims are provided, inputs will be downsampled and padded to a default size.\"\n                }\n            ],\n            \"expected_output_layers\": [\n                {\n                    \"data_field_name\": \"regions[...].region_info.mask,regions[...].data.concepts\",\n                    \"shapes\": [\n                        {\n                            \"dims\": [\n                                -1,\n                                -1\n                            ],\n                            \"data_type\": 4,\n                            \"description\": \"The pixel class numbers of each image pixel\"\n                        }\n                    ],\n                    \"description\": \"The image mask returned by the model\",\n                    \"requires_label_filename\": true\n                }\n            ]\n        },\n        {\n            \"id\": \"multimodal-embedder\",\n            \"title\": \"Multimodal Embedder\",\n            \"description\": \"Embed text or image into a vector representing a high level understanding from our AI models, e.g. CLIP. These embeddings enable similarity search and training on top of them.\",\n            \"input_fields\": [\n                \"any\"\n            ],\n            \"output_fields\": [\n                \"embeddings\"\n            ],\n            \"internal_only\": true,\n            \"model_type_fields\": [\n                {\n                    \"path\": \"input_info.params.text_token_max_count_warning\",\n                    \"field_type\": 7,\n                    \"default_value\": 0,\n                    \"description\": \"A warning to reflect model behaviour that text tokens beyond this limit will be truncated. For example, CLIP model is internally hardcoded to use up to 77 tokens. Note that changing this field value will not result in any actual changes to how the model processes the text inputs, since this field value does only have informational purpose, as we can not change the hardcoded behaviour in imported models like CLIP. Set this value to simply reflect the model behaviour. If you are not sure if the model has such a limitation, you may leave it empty.\",\n                    \"placeholder\": \"Text token max count warning\",\n                    \"model_type_range_info\": {\n                        \"max\": 5000,\n                        \"step\": 1\n                    }\n                }\n            ],\n            \"expected_input_layers\": [\n                {\n                    \"data_field_name\": \"image\",\n                    \"shapes\": [\n                        {\n                            \"dims\": [\n                                -1,\n                                -1,\n                                3\n                            ],\n                            \"max_dims\": [\n                                1024,\n                                1024,\n                                3\n                            ],\n                            \"data_type\": 2,\n                            \"description\": \"First two dimensions are the height and width, followed by the number of channels.\"\n                        }\n                    ],\n                    \"description\": \"Image urls or base64 are converted into numpy arrays of the specified size and forwarded to the model. If flexible dims are provided, inputs will be downsampled and padded to a default size.\"\n                },\n                {\n                    \"data_field_name\": \"text\",\n                    \"shapes\": [\n                        {\n                            \"dims\": [\n                                1\n                            ],\n                            \"data_type\": 1,\n                            \"description\": \"The text string sent to the model\"\n                        }\n                    ],\n                    \"description\": \"Text urls content or raw text passed in through the API are directly sent to the model\"\n                }\n            ],\n            \"expected_output_layers\": [\n                {\n                    \"data_field_name\": \"embeddings\",\n                    \"shapes\": [\n                        {\n                            \"dims\": [\n                                -1\n                            ],\n                            \"data_type\": 5\n                        }\n                    ],\n                    \"description\": \"The embedding vector returned by the model\"\n                }\n            ]\n        },\n        {\n            \"id\": \"text-to-text\",\n            \"title\": \"Text To Text\",\n            \"description\": \"Generate or convert text based on text input, e.g. prompt completion, translation or summarization\",\n            \"input_fields\": [\n                \"text\"\n            ],\n            \"output_fields\": [\n                \"text\"\n            ],\n            \"trainable\": true,\n            \"creatable\": true,\n            \"model_type_fields\": [\n                {\n                    \"path\": \"train_info.params.invalid_data_tolerance_percent\",\n                    \"field_type\": 7,\n                    \"default_value\": 5,\n                    \"description\": \"Percentage value (0 to 100) of user's tolerance level to invalid inputs among all training inputs. Training will be stopped with error thrown if actual percent of invalid inputs is higher than this\",\n                    \"placeholder\": \"Invalid Data Tolerance Percentage\",\n                    \"model_type_range_info\": {\n                        \"max\": 100,\n                        \"step\": 0.1\n                    }\n                },\n                {\n                    \"path\": \"train_info.params.dataset_id\",\n                    \"field_type\": 16,\n                    \"default_value\": \"\",\n                    \"description\": \"Deprecated in favor of train_info.dataset\",\n                    \"placeholder\": \"DEPRECATED: Training Dataset ID\"\n                },\n                {\n                    \"path\": \"train_info.dataset\",\n                    \"field_type\": 19,\n                    \"default_value\": \"\",\n                    \"description\": \"Dataset to use for training this model\",\n                    \"placeholder\": \"Training Dataset\"\n                },\n                {\n                    \"path\": \"train_info.params.dataset_version_id\",\n                    \"field_type\": 17,\n                    \"default_value\": \"\",\n                    \"description\": \"Deprecated in favor of train_info.dataset.version\",\n                    \"placeholder\": \"DEPRECATED: Training Dataset Version ID\"\n                },\n                {\n                    \"path\": \"train_info.dataset.version\",\n                    \"field_type\": 20,\n                    \"default_value\": \"\",\n                    \"description\": \"Dataset version to use for training this model. If a dataset version is not specified but a dataset is, we will automatically generate a dataset version.\",\n                    \"placeholder\": \"Training Dataset Version\"\n                },\n                {\n                    \"path\": \"train_info.resume_from_model\",\n                    \"field_type\": 22,\n                    \"default_value\": \"\",\n                    \"description\": \"Model specifying the checkpoint to resume training from.\",\n                    \"placeholder\": \"This is the model and model version to resume training from. Model must be the same type.\",\n                    \"internal_only\": true\n                },\n                {\n                    \"path\": \"train_info.params.template\",\n                    \"field_type\": 14,\n                    \"default_value\": \"HF_GPTNeo_2p7b_lora\",\n                    \"description\": \"The template name is a pre-configured model template to train with on your data. Depending on your data you might want to try a few templates to see which yields optimal results.\",\n                    \"placeholder\": \"Training Template\",\n                    \"model_type_enum_options\": [\n                        {\n                            \"id\": \"HuggingFace\",\n                            \"description\": \"A text classification training template that uses the Huggingface toolkit\",\n                            \"model_type_fields\": [\n                                {\n                                    \"path\": \"train_info.params.num_gpus\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 1,\n                                    \"description\": \"[internal_only] the number of gpus to train with.\",\n                                    \"placeholder\": \"num_gpus\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"max\": 1,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.model_config\",\n                                    \"field_type\": 10,\n                                    \"default_value\": {\n                                        \"pretrained_model_name_or_path\": \"facebook/opt-125m\"\n                                    },\n                                    \"description\": \"keys and values are passed to transformers.AutoModelForSequenceClassification.from_pretrained(). Specifying a resume_from_model in the train_info of the PostModelVersions request overrides the pretrained_model_name_or_path.\",\n                                    \"placeholder\": \"model_config\"\n                                },\n                                {\n                                    \"path\": \"train_info.params.tokenizer_config\",\n                                    \"field_type\": 10,\n                                    \"default_value\": {\n                                        \"model_max_length\": 512\n                                    },\n                                    \"description\": \"keys and values are passed to transformers.AutoTokenizer.from_pretrained().  If not specified, uses the model name from the model config. Specifying a resume_from_model in the train_info of the PostModelVersions request overrides the pretrained_model_name_or_path.\",\n                                    \"placeholder\": \"tokenizer_config\"\n                                },\n                                {\n                                    \"path\": \"train_info.params.trainer_config\",\n                                    \"field_type\": 10,\n                                    \"default_value\": {\n                                        \"auto_find_batch_size\": true,\n                                        \"num_train_epochs\": 1,\n                                        \"output_dir\": \"checkpoint\"\n                                    },\n                                    \"description\": \"keys and values are passed to transformers.TrainingArguments()\",\n                                    \"placeholder\": \"trainer_config\"\n                                }\n                            ]\n                        },\n                        {\n                            \"id\": \"HF_GPTNeo_125m_lora\",\n                            \"description\": \"A text classification training template that uses the Huggingface toolkit\",\n                            \"model_type_fields\": [\n                                {\n                                    \"path\": \"train_info.params.num_gpus\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 1,\n                                    \"description\": \"[internal_only] the number of gpus to train with.\",\n                                    \"placeholder\": \"num_gpus\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"max\": 1,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.model_config\",\n                                    \"field_type\": 10,\n                                    \"default_value\": {\n                                        \"pretrained_model_name\": \"EleutherAI/gpt-neo-125m\"\n                                    },\n                                    \"description\": \"keys and values are passed to transformers.AutoModelForSequenceClassification.from_pretrained(). Specifying a resume_from_model in the train_info of the PostModelVersions request overrides the pretrained_model_name_or_path.\",\n                                    \"placeholder\": \"model_config\"\n                                },\n                                {\n                                    \"path\": \"train_info.params.peft_config\",\n                                    \"field_type\": 10,\n                                    \"default_value\": {\n                                        \"peft_type\": \"LORA\"\n                                    },\n                                    \"description\": \"keys and values are passed to peft.get_peft_model(base_model, peft_config)\",\n                                    \"placeholder\": \"peft_config\"\n                                },\n                                {\n                                    \"path\": \"train_info.params.tokenizer_config\",\n                                    \"field_type\": 10,\n                                    \"default_value\": {},\n                                    \"description\": \"keys and values are passed to transformers.AutoTokenizer.from_pretrained().  If not specified, uses the model name from the model config.\",\n                                    \"placeholder\": \"tokenizer_config\"\n                                },\n                                {\n                                    \"path\": \"train_info.params.trainer_config\",\n                                    \"field_type\": 10,\n                                    \"default_value\": {\n                                        \"auto_find_batch_size\": true,\n                                        \"num_train_epochs\": 1\n                                    },\n                                    \"description\": \"keys and values are passed to transformers.TrainingArguments()\",\n                                    \"placeholder\": \"trainer_config\"\n                                }\n                            ]\n                        },\n                        {\n                            \"id\": \"HF_GPTNeo_2p7b_lora\",\n                            \"description\": \"A text classification training template that uses the Huggingface toolkit\",\n                            \"model_type_fields\": [\n                                {\n                                    \"path\": \"train_info.params.num_gpus\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 1,\n                                    \"description\": \"[internal_only] the number of gpus to train with.\",\n                                    \"placeholder\": \"num_gpus\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"max\": 1,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.model_config\",\n                                    \"field_type\": 10,\n                                    \"default_value\": {\n                                        \"pretrained_model_name\": \"EleutherAI/gpt-neo-2.7B\"\n                                    },\n                                    \"description\": \"keys and values are passed to transformers.AutoModelForSequenceClassification.from_pretrained(). Specifying a resume_from_model in the train_info of the PostModelVersions request overrides the pretrained_model_name_or_path. \",\n                                    \"placeholder\": \"model_config\"\n                                },\n                                {\n                                    \"path\": \"train_info.params.peft_config\",\n                                    \"field_type\": 10,\n                                    \"default_value\": {\n                                        \"peft_type\": \"LORA\"\n                                    },\n                                    \"description\": \"keys and values are passed to peft.get_peft_model(base_model, peft_config)\",\n                                    \"placeholder\": \"peft_config\"\n                                },\n                                {\n                                    \"path\": \"train_info.params.tokenizer_config\",\n                                    \"field_type\": 10,\n                                    \"default_value\": {},\n                                    \"description\": \"keys and values are passed to transformers.AutoTokenizer.from_pretrained().  If not specified, uses the model name from the model config.\",\n                                    \"placeholder\": \"tokenizer_config\"\n                                },\n                                {\n                                    \"path\": \"train_info.params.trainer_config\",\n                                    \"field_type\": 10,\n                                    \"default_value\": {\n                                        \"num_train_epochs\": 1,\n                                        \"per_device_train_batch_size\": 2\n                                    },\n                                    \"description\": \"keys and values are passed to transformers.TrainingArguments()\",\n                                    \"placeholder\": \"trainer_config\"\n                                }\n                            ],\n                            \"recommended\": true\n                        }\n                    ],\n                    \"required\": true\n                }\n            ],\n            \"expected_input_layers\": [\n                {\n                    \"data_field_name\": \"text\",\n                    \"shapes\": [\n                        {\n                            \"dims\": [\n                                1\n                            ],\n                            \"data_type\": 1,\n                            \"description\": \"The text string sent to the model\"\n                        }\n                    ],\n                    \"description\": \"Text urls content or raw text passed in through the API are directly sent to the model\"\n                }\n            ],\n            \"expected_output_layers\": [\n                {\n                    \"data_field_name\": \"text\",\n                    \"shapes\": [\n                        {\n                            \"dims\": [\n                                1\n                            ],\n                            \"data_type\": 1\n                        }\n                    ],\n                    \"description\": \"The text inferred by the model.\"\n                }\n            ]\n        },\n        {\n            \"id\": \"visual-detector\",\n            \"title\": \"Visual Detector\",\n            \"description\": \"Detect bounding box regions in images or video frames where things and then classify objects, descriptive words or topics within the boxes.\",\n            \"input_fields\": [\n                \"image\"\n            ],\n            \"output_fields\": [\n                \"regions[...].data.concepts,regions[...].region_info.bounding_box\"\n            ],\n            \"trainable\": true,\n            \"creatable\": true,\n            \"model_type_fields\": [\n                {\n                    \"path\": \"eval_info.params.dataset_id\",\n                    \"field_type\": 16,\n                    \"default_value\": \"\",\n                    \"description\": \"Dataset to use for evaluating this model.\",\n                    \"placeholder\": \"Eval Dataset ID\"\n                },\n                {\n                    \"path\": \"eval_info.params.dataset_version_id\",\n                    \"field_type\": 17,\n                    \"default_value\": \"\",\n                    \"description\": \"Dataset version to use for evaluating this model. If a dataset version is not specified but a dataset is, we will automatically generate a dataset version.\",\n                    \"placeholder\": \"Eval Dataset Version ID\"\n                },\n                {\n                    \"path\": \"output_info.data.concepts\",\n                    \"field_type\": 4,\n                    \"description\": \"List of concepts you want this model to predict from any existing concepts in your app.\",\n                    \"placeholder\": \"List of concepts\",\n                    \"required\": true\n                },\n                {\n                    \"path\": \"output_info.params.max_concepts\",\n                    \"field_type\": 3,\n                    \"default_value\": 20,\n                    \"description\": \"Maximum number of concepts in result\",\n                    \"placeholder\": \"Maximum concepts\"\n                },\n                {\n                    \"path\": \"output_info.params.select_concepts\",\n                    \"field_type\": 18,\n                    \"default_value\": [],\n                    \"description\": \"Select concepts in result by name or by id\",\n                    \"placeholder\": \"Select Concepts\"\n                },\n                {\n                    \"path\": \"train_info.params.invalid_data_tolerance_percent\",\n                    \"field_type\": 7,\n                    \"default_value\": 5,\n                    \"description\": \"Percentage value (0 to 100) of user's tolerance level to invalid inputs among all training inputs. Training will be stopped with error thrown if actual percent of invalid inputs is higher than this\",\n                    \"placeholder\": \"Invalid Data Tolerance Percentage\",\n                    \"model_type_range_info\": {\n                        \"max\": 100,\n                        \"step\": 0.1\n                    }\n                },\n                {\n                    \"path\": \"output_info.params.detection_threshold\",\n                    \"field_type\": 7,\n                    \"default_value\": 0,\n                    \"description\": \"Percentage value (0 to 1.0) for the detection threshold. Detections with scores equal to or below this value will be filtered out.\",\n                    \"placeholder\": \"Detection Threshold\",\n                    \"model_type_range_info\": {\n                        \"max\": 1\n                    }\n                },\n                {\n                    \"path\": \"train_info.params.dataset_id\",\n                    \"field_type\": 16,\n                    \"default_value\": \"\",\n                    \"description\": \"Deprecated in favor of train_info.dataset\",\n                    \"placeholder\": \"DEPRECATED: Training Dataset ID\"\n                },\n                {\n                    \"path\": \"train_info.dataset\",\n                    \"field_type\": 19,\n                    \"default_value\": \"\",\n                    \"description\": \"Dataset to use for training this model\",\n                    \"placeholder\": \"Training Dataset\"\n                },\n                {\n                    \"path\": \"train_info.params.dataset_version_id\",\n                    \"field_type\": 17,\n                    \"default_value\": \"\",\n                    \"description\": \"Deprecated in favor of train_info.dataset.version\",\n                    \"placeholder\": \"DEPRECATED: Training Dataset Version ID\"\n                },\n                {\n                    \"path\": \"train_info.dataset.version\",\n                    \"field_type\": 20,\n                    \"default_value\": \"\",\n                    \"description\": \"Dataset version to use for training this model. If a dataset version is not specified but a dataset is, we will automatically generate a dataset version.\",\n                    \"placeholder\": \"Training Dataset Version\"\n                },\n                {\n                    \"path\": \"train_info.params.template\",\n                    \"field_type\": 14,\n                    \"default_value\": \"MMDetection_YoloF\",\n                    \"description\": \"The template name is a pre-configured model template to train with on your data. Depending on your data you might want to try a few templates to see which yields optimal results.\",\n                    \"placeholder\": \"Training Template\",\n                    \"model_type_enum_options\": [\n                        {\n                            \"id\": \"detection_msc10\",\n                            \"description\": \"A training template that uses Clarifais training implementation. \",\n                            \"model_type_fields\": [\n                                {\n                                    \"path\": \"train_info.params.num_gpus\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 1,\n                                    \"description\": \"[internal_only] the number of gpus to train with.\",\n                                    \"placeholder\": \"num_gpus\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"max\": 1,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.image_size\",\n                                    \"field_type\": 3,\n                                    \"default_value\": 512,\n                                    \"description\": \"Input image size (minimum side dimension). Valid choices are: 320, 512, or 800.\",\n                                    \"placeholder\": \"image_size\",\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.batch_size\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 4,\n                                    \"description\": \"the batch size to use during training.\",\n                                    \"placeholder\": \"batch_size\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 128,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.num_epochs\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 9,\n                                    \"description\": \"the total number of epochs to train for.\",\n                                    \"placeholder\": \"num_epochs\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 200,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.pretrain_base_data\",\n                                    \"field_type\": 2,\n                                    \"default_value\": \"mscoco\",\n                                    \"description\": \"pre-initialization weights\",\n                                    \"placeholder\": \"pretrain_base_data\",\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.use_perclass_regression\",\n                                    \"field_type\": 1,\n                                    \"default_value\": true,\n                                    \"description\": \"whether to separate use box coorindate regressors for each class, or one set for all classes.\",\n                                    \"placeholder\": \"use_perclass_regression\",\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.base_model\",\n                                    \"field_type\": 2,\n                                    \"default_value\": \"InceptionV4\",\n                                    \"description\": \"the base model architecture to use for the detector.\",\n                                    \"placeholder\": \"base_model\",\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.anchor_ratios\",\n                                    \"field_type\": 11,\n                                    \"default_value\": [\n                                        1,\n                                        2,\n                                        0.5\n                                    ],\n                                    \"description\": \"the ratios w / h to use in anchor boxes of the detector.\",\n                                    \"placeholder\": \"anchor_ratios\",\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.use_focal_loss\",\n                                    \"field_type\": 1,\n                                    \"default_value\": true,\n                                    \"description\": \"whether use focal loss during training or online hard example mining\",\n                                    \"placeholder\": \"use_focal_loss\",\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.per_item_lrate\",\n                                    \"field_type\": 3,\n                                    \"default_value\": 0.0004125,\n                                    \"description\": \"the initial learning rate per item. The overall learning rate (per step) is set to lrate = batch_size * per_item_lrate\",\n                                    \"placeholder\": \"per_item_lrate\",\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.continue_from_eid\",\n                                    \"field_type\": 3,\n                                    \"default_value\": -1,\n                                    \"description\": \" if set, initialize with weights from this eid\",\n                                    \"placeholder\": \"continue_from_eid\",\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.trainer_type\",\n                                    \"field_type\": 2,\n                                    \"default_value\": \"tf_striate\",\n                                    \"description\": \"[internal_only] the trainer type to use. If set to mini_batch trainer then will only train for 10 minibatches\",\n                                    \"placeholder\": \"trainer_type\",\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.inference_crop_type\",\n                                    \"field_type\": 2,\n                                    \"default_value\": \"sortapad1\",\n                                    \"description\": \"[internal_only] the crop type to use for inference (used when evaluating the model).\",\n                                    \"placeholder\": \"inference_crop_type\",\n                                    \"internal_only\": true\n                                }\n                            ],\n                            \"internal_only\": true\n                        },\n                        {\n                            \"id\": \"Clarifai_InceptionV2\",\n                            \"description\": \"A custom visual detector template that uses a Inception-V2-like base. \",\n                            \"model_type_fields\": [\n                                {\n                                    \"path\": \"train_info.params.image_size\",\n                                    \"field_type\": 3,\n                                    \"default_value\": 512,\n                                    \"description\": \"Input image size (minimum side dimension). Valid choices are: 320, 512, or 800.\",\n                                    \"placeholder\": \"image_size\"\n                                },\n                                {\n                                    \"path\": \"train_info.params.batch_size\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 4,\n                                    \"description\": \"the batch size to use during training.\",\n                                    \"placeholder\": \"batch_size\",\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 16,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.num_epochs\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 9,\n                                    \"description\": \"the total number of epochs to train for.\",\n                                    \"placeholder\": \"num_epochs\",\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 200,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.use_perclass_regression\",\n                                    \"field_type\": 1,\n                                    \"default_value\": true,\n                                    \"description\": \"whether to separate use box coorindate regressors for each class, or one set for all classes.\",\n                                    \"placeholder\": \"use_perclass_regression\"\n                                },\n                                {\n                                    \"path\": \"train_info.params.anchor_ratios\",\n                                    \"field_type\": 11,\n                                    \"default_value\": [\n                                        1,\n                                        2,\n                                        0.5\n                                    ],\n                                    \"description\": \"the ratios w / h to use in anchor boxes of the detector.\",\n                                    \"placeholder\": \"anchor_ratios\"\n                                },\n                                {\n                                    \"path\": \"train_info.params.use_focal_loss\",\n                                    \"field_type\": 1,\n                                    \"default_value\": true,\n                                    \"description\": \"whether use focal loss during training or online hard example mining\",\n                                    \"placeholder\": \"use_focal_loss\"\n                                },\n                                {\n                                    \"path\": \"train_info.params.per_item_lrate\",\n                                    \"field_type\": 3,\n                                    \"default_value\": 0.0004125,\n                                    \"description\": \"the initial learning rate per item. The overall learning rate (per step) is set to lrate = batch_size * per_item_lrate\",\n                                    \"placeholder\": \"per_item_lrate\"\n                                }\n                            ]\n                        },\n                        {\n                            \"id\": \"Clarifai_InceptionV4\",\n                            \"description\": \"A custom visual detector template that uses a Inception-V4-like base. \",\n                            \"model_type_fields\": [\n                                {\n                                    \"path\": \"train_info.params.image_size\",\n                                    \"field_type\": 3,\n                                    \"default_value\": 512,\n                                    \"description\": \"Input image size (minimum side dimension). Valid choices are: 320, 512, or 800.\",\n                                    \"placeholder\": \"image_size\"\n                                },\n                                {\n                                    \"path\": \"train_info.params.batch_size\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 4,\n                                    \"description\": \"the batch size to use during training.\",\n                                    \"placeholder\": \"batch_size\",\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 16,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.num_epochs\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 9,\n                                    \"description\": \"the total number of epochs to train for.\",\n                                    \"placeholder\": \"num_epochs\",\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 200,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.use_perclass_regression\",\n                                    \"field_type\": 1,\n                                    \"default_value\": true,\n                                    \"description\": \"whether to separate use box coorindate regressors for each class, or one set for all classes.\",\n                                    \"placeholder\": \"use_perclass_regression\"\n                                },\n                                {\n                                    \"path\": \"train_info.params.anchor_ratios\",\n                                    \"field_type\": 11,\n                                    \"default_value\": [\n                                        1,\n                                        2,\n                                        0.5\n                                    ],\n                                    \"description\": \"the ratios w / h to use in anchor boxes of the detector.\",\n                                    \"placeholder\": \"anchor_ratios\"\n                                },\n                                {\n                                    \"path\": \"train_info.params.use_focal_loss\",\n                                    \"field_type\": 1,\n                                    \"default_value\": true,\n                                    \"description\": \"whether use focal loss during training or online hard example mining\",\n                                    \"placeholder\": \"use_focal_loss\"\n                                },\n                                {\n                                    \"path\": \"train_info.params.per_item_lrate\",\n                                    \"field_type\": 3,\n                                    \"default_value\": 0.0004125,\n                                    \"description\": \"the initial learning rate per item. The overall learning rate (per step) is set to lrate = batch_size * per_item_lrate\",\n                                    \"placeholder\": \"per_item_lrate\"\n                                }\n                            ]\n                        },\n                        {\n                            \"id\": \"MMDetection_FasterRCNN\",\n                            \"description\": \"A training template that uses the MMDetection toolkit and Faster R-CNN configuration \",\n                            \"model_type_fields\": [\n                                {\n                                    \"path\": \"train_info.params.seed\",\n                                    \"field_type\": 3,\n                                    \"default_value\": -1,\n                                    \"description\": \"[internal_only] the random seed to init training. If seed < 0, we will not set it.\",\n                                    \"placeholder\": \"seed\",\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.num_gpus\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 1,\n                                    \"description\": \"[internal_only] the number of gpus to train with.\",\n                                    \"placeholder\": \"num_gpus\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"max\": 1,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.image_size\",\n                                    \"field_type\": 11,\n                                    \"default_value\": [\n                                        800\n                                    ],\n                                    \"description\": \"the image size for training and inference. can be 1 or 2 elements. when a single value, specifies min side\",\n                                    \"placeholder\": \"image_size\"\n                                },\n                                {\n                                    \"path\": \"train_info.params.random_resize_lower\",\n                                    \"field_type\": 11,\n                                    \"default_value\": [\n                                        640\n                                    ],\n                                    \"description\": \"lower limit of random resizes during training. same 1 or 2 element format as image_size (uses image_size if empty). \",\n                                    \"placeholder\": \"random_resize_lower\"\n                                },\n                                {\n                                    \"path\": \"train_info.params.random_resize_upper\",\n                                    \"field_type\": 11,\n                                    \"default_value\": [],\n                                    \"description\": \"upper limit of random resizes during training. same 1 or 2 element format as image_size (uses image_size if empty)\",\n                                    \"placeholder\": \"random_resize_upper\"\n                                },\n                                {\n                                    \"path\": \"train_info.params.batch_size\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 2,\n                                    \"description\": \"the batch size to use during training.\",\n                                    \"placeholder\": \"batch_size\",\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 32,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.num_epochs\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 12,\n                                    \"description\": \"the total number of epochs to train for.\",\n                                    \"placeholder\": \"num_epochs\",\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 200,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.per_item_lrate\",\n                                    \"field_type\": 3,\n                                    \"default_value\": 0.00125,\n                                    \"description\": \"the initial learning rate per item. The overall learning rate (per step) is set to lrate = batch_size * per_item_lrate\",\n                                    \"placeholder\": \"per_item_lrate\"\n                                },\n                                {\n                                    \"path\": \"train_info.params.pretrained_weights\",\n                                    \"field_type\": 8,\n                                    \"default_value\": \"coco\",\n                                    \"description\": \"whether to init with pretrained weights.\",\n                                    \"placeholder\": \"pretrained_weights\",\n                                    \"model_type_enum_options\": [\n                                        {\n                                            \"id\": \"None\"\n                                        },\n                                        {\n                                            \"id\": \"coco\"\n                                        }\n                                    ]\n                                }\n                            ]\n                        },\n                        {\n                            \"id\": \"MMDetection_SSD\",\n                            \"description\": \"A training template that uses the MMDetection toolkit and SSD configuration \",\n                            \"model_type_fields\": [\n                                {\n                                    \"path\": \"train_info.params.seed\",\n                                    \"field_type\": 3,\n                                    \"default_value\": -1,\n                                    \"description\": \"[internal_only] the random seed to init training. If seed < 0, we will not set it.\",\n                                    \"placeholder\": \"seed\",\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.num_gpus\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 1,\n                                    \"description\": \"[internal_only] the number of gpus to train with.\",\n                                    \"placeholder\": \"num_gpus\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"max\": 1,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.image_size\",\n                                    \"field_type\": 11,\n                                    \"default_value\": [\n                                        320\n                                    ],\n                                    \"description\": \"the image size to train on.\",\n                                    \"placeholder\": \"image_size\"\n                                },\n                                {\n                                    \"path\": \"train_info.params.batch_size\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 24,\n                                    \"description\": \"the batch size to use during training.\",\n                                    \"placeholder\": \"batch_size\",\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 32,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.num_epochs\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 120,\n                                    \"description\": \"the total number of epochs to train for.\",\n                                    \"placeholder\": \"num_epochs\",\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 200,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.per_item_lrate\",\n                                    \"field_type\": 3,\n                                    \"default_value\": 0.000078125,\n                                    \"description\": \"the initial learning rate per item. The overall learning rate (per step) is set to lrate = batch_size * per_item_lrate\",\n                                    \"placeholder\": \"per_item_lrate\"\n                                },\n                                {\n                                    \"path\": \"train_info.params.pretrained_weights\",\n                                    \"field_type\": 8,\n                                    \"default_value\": \"coco\",\n                                    \"description\": \"whether to init with pretrained weights.\",\n                                    \"placeholder\": \"pretrained_weights\",\n                                    \"model_type_enum_options\": [\n                                        {\n                                            \"id\": \"None\"\n                                        },\n                                        {\n                                            \"id\": \"coco\"\n                                        }\n                                    ]\n                                }\n                            ]\n                        },\n                        {\n                            \"id\": \"MMDetection\",\n                            \"description\": \"A training template that uses the MMDetection toolkit and a custom configuration \",\n                            \"model_type_fields\": [\n                                {\n                                    \"path\": \"train_info.params.seed\",\n                                    \"field_type\": 3,\n                                    \"default_value\": -1,\n                                    \"description\": \"[internal_only] the random seed to init training. If seed < 0, we will not set it.\",\n                                    \"placeholder\": \"seed\",\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.custom_config\",\n                                    \"field_type\": 15,\n                                    \"default_value\": \"\\n_base_ = '/mmdetection/configs/yolof/yolof_r50_c5_8x8_1x_coco.py'\\nrunner = dict(type='EpochBasedRunner', max_epochs=10)\\nmodel=dict(\\n  bbox_head=dict(num_classes=0),\\n  )\\ndata=dict(\\n  train=dict(\\n    ann_file='',\\n    img_prefix='',\\n    classes=''\\n    ),\\n  val=dict(\\n    ann_file='',\\n    img_prefix='',\\n    classes=''))\\n\",\n                                    \"description\": \"custom mmdetection config, in python config file format. Note that the '_base_' field, if used, should be a config file relative to the parent directory '/mmdetection/', e.g. \\\"_base_ = '/mmdetection/configs/yolof/yolof_r50_c5_8x8_1x_coco.py'\\\". The 'num_classes' field must be included somewhere in the config. The 'data' section should include 'train' and 'val' sections, each with 'ann_file', 'img_prefix', and 'classes' fields with empty strings as values. These values will be overwritten to be compatible with Clarifai's system, but must be included in the imported config.\",\n                                    \"placeholder\": \"custom_config\"\n                                },\n                                {\n                                    \"path\": \"train_info.params.num_gpus\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 1,\n                                    \"description\": \"[internal_only] the number of gpus to train with.\",\n                                    \"placeholder\": \"num_gpus\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"max\": 1,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.image_size\",\n                                    \"field_type\": 11,\n                                    \"default_value\": [\n                                        320\n                                    ],\n                                    \"description\": \"the image size for inference. can be 1 or 2 elements. when a single value, specifies min side\",\n                                    \"placeholder\": \"image_size\"\n                                }\n                            ]\n                        },\n                        {\n                            \"id\": \"MMDetection_YoloF\",\n                            \"description\": \"A training template that uses the MMDetection toolkit and Yolof configuration \",\n                            \"model_type_fields\": [\n                                {\n                                    \"path\": \"train_info.params.seed\",\n                                    \"field_type\": 3,\n                                    \"default_value\": -1,\n                                    \"description\": \"[internal_only] the random seed to init training. If seed < 0, we will not set it.\",\n                                    \"placeholder\": \"seed\",\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.num_gpus\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 1,\n                                    \"description\": \"[internal_only] the number of gpus to train with.\",\n                                    \"placeholder\": \"num_gpus\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"max\": 1,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.image_size\",\n                                    \"field_type\": 11,\n                                    \"default_value\": [\n                                        512\n                                    ],\n                                    \"description\": \"the input image size. when a single value, specifies the minimum side. if more than one value, specifies exact (width, height) when combined with keep_aspect_ratio=False\",\n                                    \"placeholder\": \"image_size\"\n                                },\n                                {\n                                    \"path\": \"train_info.params.max_aspect_ratio\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 1.5,\n                                    \"description\": \"for keep_aspect_ratio=True, maximum length of longer side relative to shorter side\",\n                                    \"placeholder\": \"max_aspect_ratio\",\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 5\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.keep_aspect_ratio\",\n                                    \"field_type\": 1,\n                                    \"default_value\": true,\n                                    \"description\": \"whether to keep the original aspect ratio of the image (True, default), or use non-aspect-preserving resizes (False)\",\n                                    \"placeholder\": \"keep_aspect_ratio\"\n                                },\n                                {\n                                    \"path\": \"train_info.params.batch_size\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 16,\n                                    \"description\": \"the batch size to use during training.\",\n                                    \"placeholder\": \"batch_size\",\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 32,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.num_epochs\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 10,\n                                    \"description\": \"the total number of epochs to train for.\",\n                                    \"placeholder\": \"num_epochs\",\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 200,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.min_samples_per_epoch\",\n                                    \"field_type\": 3,\n                                    \"default_value\": 300,\n                                    \"description\": \"for very small datasets, minimum number of samples in one epoch (the dataset is repeated)\",\n                                    \"placeholder\": \"min_samples_per_epoch\"\n                                },\n                                {\n                                    \"path\": \"train_info.params.per_item_lrate\",\n                                    \"field_type\": 3,\n                                    \"default_value\": 0.001875,\n                                    \"description\": \"the initial learning rate per item. The overall learning rate (per step) is set to lrate = batch_size * per_item_lrate\",\n                                    \"placeholder\": \"per_item_lrate\"\n                                },\n                                {\n                                    \"path\": \"train_info.params.pretrained_weights\",\n                                    \"field_type\": 8,\n                                    \"default_value\": \"coco\",\n                                    \"description\": \"whether to init with pretrained weights.\",\n                                    \"placeholder\": \"pretrained_weights\",\n                                    \"model_type_enum_options\": [\n                                        {\n                                            \"id\": \"None\"\n                                        },\n                                        {\n                                            \"id\": \"coco\"\n                                        }\n                                    ]\n                                },\n                                {\n                                    \"path\": \"train_info.params.frozen_stages\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 1,\n                                    \"description\": \"backbone network stages to keep frozen\",\n                                    \"placeholder\": \"frozen_stages\",\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 4,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.inference_max_batch_size\",\n                                    \"field_type\": 3,\n                                    \"default_value\": 2,\n                                    \"description\": \"[internal_only] max batch size to use during inference\",\n                                    \"placeholder\": \"inference_max_batch_size\",\n                                    \"internal_only\": true\n                                }\n                            ],\n                            \"recommended\": true\n                        },\n                        {\n                            \"id\": \"_Ultralytics_YoloV5\",\n                            \"description\": \"A training template that uses the Ultrylatics YoloV5 implementation. \",\n                            \"model_type_fields\": [\n                                {\n                                    \"path\": \"train_info.params.num_gpus\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 1,\n                                    \"description\": \"[internal_only] the number of gpus to train with.\",\n                                    \"placeholder\": \"num_gpus\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"max\": 1,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.model\",\n                                    \"field_type\": 8,\n                                    \"default_value\": \"yolov5s\",\n                                    \"description\": \"which specific model architecture to use.\",\n                                    \"placeholder\": \"model\",\n                                    \"model_type_enum_options\": [\n                                        {\n                                            \"id\": \"yolov5n\"\n                                        },\n                                        {\n                                            \"id\": \"yolov5s\"\n                                        },\n                                        {\n                                            \"id\": \"yolov5m\"\n                                        },\n                                        {\n                                            \"id\": \"yolov5l\"\n                                        },\n                                        {\n                                            \"id\": \"yolov5x\"\n                                        }\n                                    ],\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.num_epochs\",\n                                    \"field_type\": 7,\n                                    \"default_value\": 30,\n                                    \"description\": \"the total number of epochs to train for.\",\n                                    \"placeholder\": \"num_epochs\",\n                                    \"internal_only\": true,\n                                    \"model_type_range_info\": {\n                                        \"min\": 1,\n                                        \"max\": 500,\n                                        \"step\": 1\n                                    }\n                                },\n                                {\n                                    \"path\": \"train_info.params.pretrained_weights\",\n                                    \"field_type\": 8,\n                                    \"default_value\": \"coco\",\n                                    \"description\": \"whether to init with pretrained weights.\",\n                                    \"placeholder\": \"pretrained_weights\",\n                                    \"model_type_enum_options\": [\n                                        {\n                                            \"id\": \"None\"\n                                        },\n                                        {\n                                            \"id\": \"coco\"\n                                        }\n                                    ],\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.image_size\",\n                                    \"field_type\": 3,\n                                    \"default_value\": 640,\n                                    \"description\": \"the input image size. If rectangular training is true, this is the larger side.\",\n                                    \"placeholder\": \"image_size\",\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.rectangular_training\",\n                                    \"field_type\": 1,\n                                    \"default_value\": false,\n                                    \"description\": \"whether to train on rectangular images. Preserves image ratio.\",\n                                    \"placeholder\": \"rectangular_training\",\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.multi_scale\",\n                                    \"field_type\": 1,\n                                    \"default_value\": false,\n                                    \"description\": \"whether to vary the image size +/- 50%\",\n                                    \"placeholder\": \"multi_scale\",\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.single_cls\",\n                                    \"field_type\": 1,\n                                    \"default_value\": false,\n                                    \"description\": \"whether to train multi-class data as single-class\",\n                                    \"placeholder\": \"single_cls\",\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.optimizer\",\n                                    \"field_type\": 8,\n                                    \"default_value\": \"SGD\",\n                                    \"description\": \"which optimizer to use.\",\n                                    \"placeholder\": \"optimizer\",\n                                    \"model_type_enum_options\": [\n                                        {\n                                            \"id\": \"SGD\"\n                                        },\n                                        {\n                                            \"id\": \"Adam\"\n                                        },\n                                        {\n                                            \"id\": \"AdamW\"\n                                        }\n                                    ],\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.cosine_lr\",\n                                    \"field_type\": 1,\n                                    \"default_value\": false,\n                                    \"description\": \"whether to use a cosine LR scheduler\",\n                                    \"placeholder\": \"cosine_lr\",\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.label_smoothing\",\n                                    \"field_type\": 3,\n                                    \"default_value\": 0,\n                                    \"description\": \"If > `0` then smooth the labels.\",\n                                    \"placeholder\": \"label_smoothing\",\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.patience\",\n                                    \"field_type\": 3,\n                                    \"default_value\": 100,\n                                    \"description\": \"EarlyStopping patience. After how many epochs to stop training when there is no improvement since the best epoch.\",\n                                    \"placeholder\": \"patience\",\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.seed\",\n                                    \"field_type\": 3,\n                                    \"default_value\": 0,\n                                    \"description\": \"the random seed to init training.\",\n                                    \"placeholder\": \"seed\",\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.use_best_checkpoint\",\n                                    \"field_type\": 1,\n                                    \"default_value\": false,\n                                    \"description\": \"whether you want to use the checkpoint that did the best on the validation set for inferencing. False means you will use the latest checkpoint.\",\n                                    \"placeholder\": \"use_best_checkpoint\",\n                                    \"internal_only\": true\n                                },\n                                {\n                                    \"path\": \"train_info.params.hyperparameters\",\n                                    \"field_type\": 10,\n                                    \"default_value\": {\n                                        \"anchor_t\": 4,\n                                        \"box\": 0.05,\n                                        \"cls\": 0.5,\n                                        \"cls_pw\": 1,\n                                        \"copy_paste\": 0,\n                                        \"degrees\": 0,\n                                        \"fl_gamma\": 0,\n                                        \"fliplr\": 0.5,\n                                        \"flipud\": 0,\n                                        \"hsv_h\": 0.015,\n                                        \"hsv_s\": 0.7,\n                                        \"hsv_v\": 0.4,\n                                        \"iou_t\": 0.2,\n                                        \"lr0\": 0.01,\n                                        \"lrf\": 0.01,\n                                        \"mixup\": 0,\n                                        \"momentum\": 0.937,\n                                        \"mosaic\": 1,\n                                        \"obj\": 1,\n                                        \"obj_pw\": 1,\n                                        \"perspective\": 0,\n                                        \"scale\": 0.5,\n                                        \"shear\": 0,\n                                        \"translate\": 0.1,\n                                        \"warmup_bias_lr\": 0.1,\n                                        \"warmup_epochs\": 3,\n                                        \"warmup_momentum\": 0.8,\n                                        \"weight_decay\": 0.0005\n                                    },\n                                    \"description\": \"dict of hyperparameters to pass to training. Defaults to values from https://github.com/ultralytics/yolov5/blob/ed887b5976d94dc61fa3f7e8e07170623dc7d6ee/data/hyps/hyp.scratch-low.yaml.\",\n                                    \"placeholder\": \"hyperparameters\",\n                                    \"internal_only\": true\n                                }\n                            ],\n                            \"internal_only\": true\n                        }\n                    ],\n                    \"required\": true\n                }\n            ],\n            \"expected_input_layers\": [\n                {\n                    \"data_field_name\": \"image\",\n                    \"shapes\": [\n                        {\n                            \"dims\": [\n                                -1,\n                                -1,\n                                3\n                            ],\n                            \"max_dims\": [\n                                1024,\n                                1024,\n                                3\n                            ],\n                            \"data_type\": 2,\n                            \"description\": \"First two dimensions are the height and width, followed by the number of channels.\"\n                        }\n                    ],\n                    \"description\": \"Image urls or base64 are converted into numpy arrays of the specified size and forwarded to the model. If flexible dims are provided, inputs will be downsampled and padded to a default size.\"\n                }\n            ],\n            \"expected_output_layers\": [\n                {\n                    \"data_field_name\": \"regions[...].region_info.bounding_box\",\n                    \"shapes\": [\n                        {\n                            \"dims\": [\n                                -1,\n                                4\n                            ],\n                            \"data_type\": 5,\n                            \"description\": \"The normalized bounding box coordinates in the order: top_row, left_col, bottom_row, right_col.\"\n                        }\n                    ]\n                },\n                {\n                    \"data_field_name\": \"regions[...].data.concepts[...].id\",\n                    \"shapes\": [\n                        {\n                            \"dims\": [\n                                -1,\n                                1\n                            ],\n                            \"data_type\": 3,\n                            \"description\": \"The concept number that belongs to the respective bounding box. Concept numbers should be in the same order of the concepts defined in the label file.\"\n                        }\n                    ],\n                    \"requires_label_filename\": true\n                },\n                {\n                    \"data_field_name\": \"regions[...].data.concepts[...].value\",\n                    \"shapes\": [\n                        {\n                            \"dims\": [\n                                -1,\n                                1\n                            ],\n                            \"data_type\": 5,\n                            \"description\": \"The confidence value for the predicted concept\"\n                        }\n                    ]\n                }\n            ],\n            \"evaluation_type\": 2\n        }\n    ],\n    \"model_importers\": {\n        \"path\": \"import_info.params.toolkit\",\n        \"field_type\": 14,\n        \"description\": \"Third party toolkits to import models from.\",\n        \"placeholder\": \"Toolkit\",\n        \"model_type_enum_options\": [\n            {\n                \"id\": \"HuggingFace\",\n                \"description\": \"Importer for HuggingFace pipelines.\",\n                \"model_type_fields\": [\n                    {\n                        \"path\": \"import_info.params.use_gpu\",\n                        \"field_type\": 1,\n                        \"default_value\": true,\n                        \"description\": \"whether to import the model for usage on cpu or gpu.\",\n                        \"placeholder\": \"use_gpu\"\n                    },\n                    {\n                        \"path\": \"import_info.params.model_name\",\n                        \"field_type\": 2,\n                        \"default_value\": \"\",\n                        \"description\": \"[internal_only] This is the name of the model we want to import, e.g. 'bert-base-uncased'.\",\n                        \"placeholder\": \"model_name\",\n                        \"internal_only\": true\n                    },\n                    {\n                        \"path\": \"import_info.params.pipeline_name\",\n                        \"field_type\": 8,\n                        \"default_value\": \"\",\n                        \"description\": \"This is the name of the pipeline to deploy. The available pipelines are:\",\n                        \"placeholder\": \"pipeline_name\",\n                        \"model_type_enum_options\": [\n                            {\n                                \"id\": \"text2text-generation\",\n                                \"description\": \"If this model supports prompts, each text input should contain the prompt when inferencing.\"\n                            },\n                            {\n                                \"id\": \"summarization\"\n                            },\n                            {\n                                \"id\": \"text-generation\"\n                            },\n                            {\n                                \"id\": \"text-classification\"\n                            },\n                            {\n                                \"id\": \"feature-extraction\",\n                                \"description\": \"Extract feature embeddings from text.\"\n                            },\n                            {\n                                \"id\": \"ner\",\n                                \"description\": \"Token classification with entity aggregation (aggregation_strategy=`simple`).\"\n                            },\n                            {\n                                \"id\": \"sentiment-analysis\"\n                            },\n                            {\n                                \"id\": \"translation_xx_to_yy\",\n                                \"aliases\": [\n                                    {\n                                        \"wildcard_string\": \"^translation_.._to_..$\"\n                                    }\n                                ],\n                                \"description\": \"xx and yy should be replaced by language codes if this model is capable of translating between multiple pairs of languages.\"\n                            },\n                            {\n                                \"id\": \"automatic-speech-recognition\"\n                            },\n                            {\n                                \"id\": \"audio-classification\",\n                                \"description\": \"Tokenizers are not supported.\"\n                            },\n                            {\n                                \"id\": \"question-answering\",\n                                \"description\": \"Prompt must be in format 'question: QUESTION context: CONTEXT'\"\n                            },\n                            {\n                                \"id\": \"zero-shot-classification\",\n                                \"description\": \"Classify texts using custom labels without retraining.\"\n                            },\n                            {\n                                \"id\": \"zero-shot-image-classification\",\n                                \"description\": \"Classify images using custom labels without retraining.\"\n                            },\n                            {\n                                \"id\": \"object-detection\"\n                            },\n                            {\n                                \"id\": \"image-segmentation\"\n                            },\n                            {\n                                \"id\": \"image-classification\"\n                            }\n                        ]\n                    },\n                    {\n                        \"path\": \"import_info.params.tokenizer_config\",\n                        \"field_type\": 10,\n                        \"default_value\": {\n                            \"model_max_length\": 512\n                        },\n                        \"description\": \"Tokenizer configuration fields; by default, the tokenizer will use values saved with the model\",\n                        \"placeholder\": \"tokenizer_config\"\n                    }\n                ]\n            },\n            {\n                \"id\": \"MMDetection\",\n                \"description\": \"Importer for MMDetection models.\",\n                \"model_type_fields\": [\n                    {\n                        \"path\": \"import_info.params.checkpoint_file_url\",\n                        \"field_type\": 2,\n                        \"default_value\": \"\",\n                        \"description\": \"The url to the checkpoint file to be downloaded.\",\n                        \"placeholder\": \"checkpoint_file_url\"\n                    },\n                    {\n                        \"path\": \"import_info.params.mmdet_config_path\",\n                        \"field_type\": 2,\n                        \"default_value\": \"\",\n                        \"description\": \"The absolute path to the mmdet config inside the mmdet repo.\",\n                        \"placeholder\": \"mmdet_config_path\"\n                    },\n                    {\n                        \"path\": \"import_info.params.inference_image_size\",\n                        \"field_type\": 11,\n                        \"default_value\": [\n                            320\n                        ],\n                        \"description\": \"the image size for inference. can be 1 or 2 elements. when a single value, specifies min side\",\n                        \"placeholder\": \"inference_image_size\"\n                    },\n                    {\n                        \"path\": \"import_info.params.use_gpu\",\n                        \"field_type\": 1,\n                        \"default_value\": true,\n                        \"description\": \"[internal_only] Whether to deploy for inference on GPU.\",\n                        \"placeholder\": \"use_gpu\",\n                        \"internal_only\": true\n                    }\n                ]\n            }\n        ]\n    },\n    \"triton_conda_envs_info\": [\n        {\n            \"conda_pack_url\": \"s3://clarifai-api/triton-conda-envs/dev/triton_conda-cp3.8-torch1.13.1-19f97078.tar.gz\",\n            \"conda_yaml_url\": \"s3://clarifai-api/triton-conda-envs/dev/triton_conda-cp3.8-torch1.13.1-19f97078.yaml\"\n        },\n        {\n            \"conda_pack_url\": \"s3://clarifai-api/triton-conda-envs/dev/default.tar.gz\",\n            \"conda_yaml_url\": \"s3://clarifai-api/triton-conda-envs/dev/default.yaml\"\n        },\n        {\n            \"conda_pack_url\": \"s3://clarifai-api/triton-conda-envs/dev/triton_conda-cp3.8-torch2.0.0-ce980f28.tar.gz\",\n            \"conda_yaml_url\": \"s3://clarifai-api/triton-conda-envs/dev/triton_conda-cp3.8-torch2.0.0-ce980f28.yaml\"\n        }\n    ]\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/models/types' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Authorization: Key {{key}}'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/models/{model_id}": {
      "get": {
        "tags": [
          "Models"
        ],
        "summary": "Get Model By Model ID",
        "description": "With this endpoint, you can retrieve information about a specific model by providing the model ID.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application name** |\n| `model_id` | **string** | **Stores the Model ID** |",
        "parameters": [
          {
            "name": "Authorization",
            "in": "header",
            "schema": {
              "type": "string"
            },
            "example": "Key {{key}}"
          },
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "model_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"27496a6fb2275dffcbcc6cf0d88ecca6\"\n    },\n    \"model\": {\n        \"id\": \"custom-config\",\n        \"name\": \"custom-config\",\n        \"created_at\": \"2023-11-23T09:41:08.002419Z\",\n        \"modified_at\": \"2023-11-23T09:41:08.002419Z\",\n        \"app_id\": \"test-app-1700638575-empty\",\n        \"model_version\": {\n            \"id\": \"1e4c121974f849209abb658cdf682585\",\n            \"created_at\": \"2023-11-23T09:41:18.470087Z\",\n            \"status\": {\n                \"code\": 21110,\n                \"description\": \"datasets.dataset.DataBatchEmpty: No databatch found in train set's file directory\\nFailed to create a training dataset, because there are no appropriately annotated inputs. Expected annotations with concepts for model type id text-classifier. \"\n            },\n            \"active_concept_count\": 6,\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"metadata\": {},\n            \"output_info\": {\n                \"output_config\": {\n                    \"max_concepts\": 0,\n                    \"min_value\": 0\n                },\n                \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                \"params\": {\n                    \"max_concepts\": 20,\n                    \"min_value\": 0,\n                    \"select_concepts\": []\n                }\n            },\n            \"input_info\": {},\n            \"train_info\": {\n                \"params\": {\n                    \"dataset_id\": \"\",\n                    \"dataset_version_id\": \"\",\n                    \"invalid_data_tolerance_percent\": 5,\n                    \"model_config\": {\n                        \"pretrained_model_name\": \"EleutherAI/gpt-neo-125m\"\n                    },\n                    \"num_gpus\": 1,\n                    \"peft_config\": {\n                        \"peft_type\": \"LORA\"\n                    },\n                    \"template\": \"HF_GPTNeo_125m_lora\",\n                    \"tokenizer_config\": {},\n                    \"trainer_config\": {\n                        \"auto_find_batch_size\": true,\n                        \"num_train_epochs\": 20,\n                        \"output_dir\": \"checkpoint\"\n                    }\n                }\n            },\n            \"import_info\": {}\n        },\n        \"user_id\": \"a0btrubbaefn\",\n        \"model_type_id\": \"text-classifier\",\n        \"visibility\": {\n            \"gettable\": 10\n        },\n        \"metadata\": {},\n        \"toolkits\": [],\n        \"use_cases\": [],\n        \"languages\": [],\n        \"languages_full\": [],\n        \"check_consents\": [],\n        \"workflow_recommended\": false\n    }\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/models/{model_id}' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Authorization: Key {{key}}'"
          }
        ]
      },
      "delete": {
        "tags": [
          "Models"
        ],
        "summary": "Delete Model",
        "description": "This endpoint enables users to delete a model by providing model ID.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application name** |\n| `model_id` | **string** | **Stores the Model ID** |",
        "parameters": [
          {
            "name": "Authorization",
            "in": "header",
            "schema": {
              "type": "string"
            },
            "example": "Key {{key}}"
          },
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "model_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "examples": {
                  "example-0": {
                    "summary": "Delete Model By modelID",
                    "value": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"4d6112a93a9fe7c8ab1e5914ceadfefa\"\n    }\n}"
                  },
                  "example-1": {
                    "summary": "Delete batch by ids (async)",
                    "value": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"335e601fdec0dfddfc7fdc088dbdbd21\"\n    }\n}"
                  }
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff --request DELETE 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/models/{model_id}' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Authorization: Key {{key}}'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/models/{model_id}/output_info": {
      "get": {
        "tags": [
          "Models"
        ],
        "summary": "Get Model Output Info By Model ID",
        "description": "This request lets the user to retrieve the output configuration of a model by providing the model ID.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application name** |\n| `model_id` | **string** | **Stores the Model ID** |",
        "parameters": [
          {
            "name": "Authorization",
            "in": "header",
            "schema": {
              "type": "string"
            },
            "example": "Key {{key}}"
          },
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "model_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"39bd7343fe3a1fcc8f0a905d4b487932\"\n    },\n    \"model\": {\n        \"id\": \"custom-config\",\n        \"name\": \"custom-config\",\n        \"created_at\": \"2023-11-23T09:41:08.002419Z\",\n        \"modified_at\": \"2023-11-23T09:41:08.002419Z\",\n        \"app_id\": \"test-app-1700638575-empty\",\n        \"model_version\": {\n            \"id\": \"1e4c121974f849209abb658cdf682585\",\n            \"created_at\": \"2023-11-23T09:41:18.470087Z\",\n            \"status\": {\n                \"code\": 21110,\n                \"description\": \"datasets.dataset.DataBatchEmpty: No databatch found in train set's file directory\\nFailed to create a training dataset, because there are no appropriately annotated inputs. Expected annotations with concepts for model type id text-classifier. \"\n            },\n            \"active_concept_count\": 6,\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"user_id\": \"a0btrubbaefn\",\n            \"metadata\": {},\n            \"output_info\": {\n                \"data\": {\n                    \"concepts\": [\n                        {\n                            \"id\": \"id-identity_hate\",\n                            \"name\": \"id-identity_hate\",\n                            \"value\": 1,\n                            \"created_at\": \"2023-11-23T09:41:18.499731Z\",\n                            \"language\": \"en\",\n                            \"app_id\": \"test-app-1700638575-empty\",\n                            \"visibility\": {\n                                \"gettable\": 10\n                            },\n                            \"user_id\": \"a0btrubbaefn\"\n                        },\n                        {\n                            \"id\": \"id-insult\",\n                            \"name\": \"id-insult\",\n                            \"value\": 1,\n                            \"created_at\": \"2023-11-23T09:41:18.499734Z\",\n                            \"language\": \"en\",\n                            \"app_id\": \"test-app-1700638575-empty\",\n                            \"visibility\": {\n                                \"gettable\": 10\n                            },\n                            \"user_id\": \"a0btrubbaefn\"\n                        },\n                        {\n                            \"id\": \"id-obscene\",\n                            \"name\": \"id-obscene\",\n                            \"value\": 1,\n                            \"created_at\": \"2023-11-23T09:41:18.499741Z\",\n                            \"language\": \"en\",\n                            \"app_id\": \"test-app-1700638575-empty\",\n                            \"visibility\": {\n                                \"gettable\": 10\n                            },\n                            \"user_id\": \"a0btrubbaefn\"\n                        },\n                        {\n                            \"id\": \"id-severe_toxic\",\n                            \"name\": \"id-severe_toxic\",\n                            \"value\": 1,\n                            \"created_at\": \"2023-11-23T09:41:18.499743Z\",\n                            \"language\": \"en\",\n                            \"app_id\": \"test-app-1700638575-empty\",\n                            \"visibility\": {\n                                \"gettable\": 10\n                            },\n                            \"user_id\": \"a0btrubbaefn\"\n                        },\n                        {\n                            \"id\": \"id-threat\",\n                            \"name\": \"id-threat\",\n                            \"value\": 1,\n                            \"created_at\": \"2023-11-23T09:41:18.499738Z\",\n                            \"language\": \"en\",\n                            \"app_id\": \"test-app-1700638575-empty\",\n                            \"visibility\": {\n                                \"gettable\": 10\n                            },\n                            \"user_id\": \"a0btrubbaefn\"\n                        },\n                        {\n                            \"id\": \"id-toxic\",\n                            \"name\": \"id-toxic\",\n                            \"value\": 1,\n                            \"created_at\": \"2023-11-23T09:41:18.499746Z\",\n                            \"language\": \"en\",\n                            \"app_id\": \"test-app-1700638575-empty\",\n                            \"visibility\": {\n                                \"gettable\": 10\n                            },\n                            \"user_id\": \"a0btrubbaefn\"\n                        }\n                    ]\n                },\n                \"output_config\": {\n                    \"max_concepts\": 0,\n                    \"min_value\": 0\n                },\n                \"params\": {\n                    \"max_concepts\": 20,\n                    \"min_value\": 0,\n                    \"select_concepts\": []\n                }\n            },\n            \"input_info\": {},\n            \"train_info\": {\n                \"params\": {\n                    \"dataset_id\": \"\",\n                    \"dataset_version_id\": \"\",\n                    \"invalid_data_tolerance_percent\": 5,\n                    \"model_config\": {\n                        \"pretrained_model_name\": \"EleutherAI/gpt-neo-125m\"\n                    },\n                    \"num_gpus\": 1,\n                    \"peft_config\": {\n                        \"peft_type\": \"LORA\"\n                    },\n                    \"template\": \"HF_GPTNeo_125m_lora\",\n                    \"tokenizer_config\": {},\n                    \"trainer_config\": {\n                        \"auto_find_batch_size\": true,\n                        \"num_train_epochs\": 20,\n                        \"output_dir\": \"checkpoint\"\n                    }\n                }\n            },\n            \"import_info\": {}\n        },\n        \"user_id\": \"a0btrubbaefn\",\n        \"model_type_id\": \"text-classifier\",\n        \"visibility\": {\n            \"gettable\": 10\n        },\n        \"metadata\": {},\n        \"presets\": {},\n        \"toolkits\": [],\n        \"use_cases\": [],\n        \"languages\": [],\n        \"languages_full\": [],\n        \"check_consents\": [],\n        \"workflow_recommended\": false\n    }\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/models/{model_id}/output_info' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Authorization: Key {{key}}'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/models/{model_id}/inputs": {
      "get": {
        "tags": [
          "Models"
        ],
        "summary": "List Training Inputs By Model ID",
        "description": "This endpoint allows you to list the inputs given to a specific model by providing the model ID.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application name** |\n| `model_id` | **string** | **Stores the Model ID** |",
        "parameters": [
          {
            "name": "Authorization",
            "in": "header",
            "schema": {
              "type": "string"
            },
            "example": "Key {{key}}"
          },
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "model_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"c9eb2c214ffd6e97eb7fb2999dff3502\"\n    },\n    \"inputs\": []\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/models/{model_id}/inputs' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Authorization: Key {{key}}'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/models/{model_id}/versions/{version_id}/metrics": {
      "get": {
        "tags": [
          "Models"
        ],
        "summary": "Eval Metrics By Model Version",
        "description": "By sending a request to this endpoint, you can list out the metrics associated with your model version.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application name** |\n| `model_id` | **string** | **Stores the Model ID** |\n| `version_id` | **string** | **Stores the model version number** |",
        "parameters": [
          {
            "name": "Authorization",
            "in": "header",
            "schema": {
              "type": "string"
            },
            "example": "Key {{key}}"
          },
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "model_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "version_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"0242178db771f4797f4febb3153a81fc\"\n    },\n    \"model_version\": {\n        \"id\": \"00668896e0a64cf5b37302c000e96f23\",\n        \"created_at\": \"2023-11-23T08:06:12.577745Z\",\n        \"status\": {\n            \"code\": 21100,\n            \"description\": \"Model is trained and ready\"\n        },\n        \"active_concept_count\": 16,\n        \"metrics\": {\n            \"status\": {\n                \"code\": 21300,\n                \"description\": \"Model was successfully evaluated.\"\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"id\": \"356e26563e8d4da5b1fe5a8ff8710ab5\",\n            \"model\": {\n                \"id\": \"deep_cls_bg1\",\n                \"app_id\": \"test-app-1700638575-empty\",\n                \"model_version\": {\n                    \"id\": \"00668896e0a64cf5b37302c000e96f23\",\n                    \"created_at\": \"2023-11-23T08:06:12.577745Z\",\n                    \"status\": {\n                        \"code\": 21100,\n                        \"description\": \"Model is trained and ready\"\n                    },\n                    \"active_concept_count\": 16,\n                    \"metrics\": {\n                        \"status\": {\n                            \"code\": 21300,\n                            \"description\": \"Model was successfully evaluated.\"\n                        },\n                        \"summary\": {\n                            \"macro_avg_roc_auc\": 0.52151275,\n                            \"macro_std_roc_auc\": 0.34620512,\n                            \"macro_avg_f1_score\": 0.40380955,\n                            \"macro_std_f1_score\": 0.17236254,\n                            \"macro_avg_precision\": 0.09772728,\n                            \"macro_avg_recall\": 0.52380955\n                        }\n                    },\n                    \"completed_at\": \"2023-11-23T08:21:08.303040Z\",\n                    \"visibility\": {\n                        \"gettable\": 10\n                    },\n                    \"app_id\": \"test-app-1700638575-empty\",\n                    \"user_id\": \"a0btrubbaefn\",\n                    \"metadata\": {},\n                    \"output_info\": {\n                        \"output_config\": {\n                            \"max_concepts\": 0,\n                            \"min_value\": 0\n                        },\n                        \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                        \"fields_map\": {\n                            \"concepts\": \"probs\"\n                        },\n                        \"params\": {\n                            \"max_concepts\": 20,\n                            \"min_value\": 0,\n                            \"select_concepts\": []\n                        }\n                    },\n                    \"input_info\": {\n                        \"fields_map\": {\n                            \"image\": \"input\"\n                        }\n                    },\n                    \"train_info\": {\n                        \"params\": {\n                            \"batch_size\": 64,\n                            \"concepts_mutually_exclusive\": false,\n                            \"dataset_id\": \"\",\n                            \"dataset_version_id\": \"\",\n                            \"flip_direction\": \"horizontal\",\n                            \"flip_probability\": 0.5,\n                            \"image_size\": 224,\n                            \"invalid_data_tolerance_percent\": 5,\n                            \"num_epochs\": 60,\n                            \"num_gpus\": 1,\n                            \"per_item_lrate\": 0.00001953125,\n                            \"per_item_min_lrate\": 1.5625e-08,\n                            \"pretrained_weights\": \"ImageNet-1k\",\n                            \"seed\": -1,\n                            \"template\": \"MMClassification_ResNet_50_RSB_A1\",\n                            \"warmup_iters\": 100,\n                            \"warmup_ratio\": 0.0001,\n                            \"weight_decay\": 0.01\n                        }\n                    },\n                    \"import_info\": {}\n                },\n                \"user_id\": \"a0btrubbaefn\",\n                \"model_type_id\": \"visual-classifier\",\n                \"toolkits\": [],\n                \"use_cases\": [],\n                \"languages\": [],\n                \"languages_full\": [],\n                \"check_consents\": []\n            },\n            \"summary\": {\n                \"macro_avg_roc_auc\": 0.52151275,\n                \"macro_std_roc_auc\": 0.34620512,\n                \"macro_avg_f1_score\": 0.40380955,\n                \"macro_std_f1_score\": 0.17236254,\n                \"macro_avg_precision\": 0.09772728,\n                \"macro_avg_recall\": 0.52380955\n            },\n            \"eval_info\": {\n                \"params\": {\n                    \"dataset_id\": \"\",\n                    \"dataset_version_id\": \"\"\n                }\n            }\n        },\n        \"completed_at\": \"2023-11-23T08:21:08.303040Z\",\n        \"visibility\": {\n            \"gettable\": 10\n        },\n        \"app_id\": \"test-app-1700638575-empty\",\n        \"user_id\": \"a0btrubbaefn\",\n        \"metadata\": {},\n        \"output_info\": {\n            \"output_config\": {\n                \"max_concepts\": 0,\n                \"min_value\": 0\n            },\n            \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n            \"fields_map\": {\n                \"concepts\": \"probs\"\n            },\n            \"params\": {\n                \"max_concepts\": 20,\n                \"min_value\": 0,\n                \"select_concepts\": []\n            }\n        },\n        \"input_info\": {\n            \"fields_map\": {\n                \"image\": \"input\"\n            }\n        },\n        \"train_info\": {\n            \"params\": {\n                \"batch_size\": 64,\n                \"concepts_mutually_exclusive\": false,\n                \"dataset_id\": \"\",\n                \"dataset_version_id\": \"\",\n                \"flip_direction\": \"horizontal\",\n                \"flip_probability\": 0.5,\n                \"image_size\": 224,\n                \"invalid_data_tolerance_percent\": 5,\n                \"num_epochs\": 60,\n                \"num_gpus\": 1,\n                \"per_item_lrate\": 0.00001953125,\n                \"per_item_min_lrate\": 1.5625e-08,\n                \"pretrained_weights\": \"ImageNet-1k\",\n                \"seed\": -1,\n                \"template\": \"MMClassification_ResNet_50_RSB_A1\",\n                \"warmup_iters\": 100,\n                \"warmup_ratio\": 0.0001,\n                \"weight_decay\": 0.01\n            }\n        },\n        \"import_info\": {},\n        \"train_log\": \"\"\n    }\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/models/{model_id}/versions/{version_id}/metrics' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Authorization: Key {{key}}'"
          }
        ]
      },
      "post": {
        "tags": [
          "Models"
        ],
        "summary": "Eval Metrics By Model Version",
        "description": "This endpoint allows users to evaluate the performance metrics of a specific model version.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application name** |\n| `model_id` | **string** | **Stores the Model ID** |\n| `version_id` | **string** | **Stores the model version** |",
        "requestBody": {
          "content": {}
        },
        "parameters": [
          {
            "name": "Authorization",
            "in": "header",
            "schema": {
              "type": "string"
            },
            "example": "Key {{key}}"
          },
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "model_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "version_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"5b02a1d821b84630bec1811917d415e1\"\n    },\n    \"model_version\": {\n        \"id\": \"00668896e0a64cf5b37302c000e96f23\",\n        \"created_at\": \"2023-11-23T08:06:12.577745Z\",\n        \"status\": {\n            \"code\": 21100,\n            \"description\": \"Model is trained and ready\"\n        },\n        \"active_concept_count\": 16,\n        \"metrics\": {\n            \"status\": {\n                \"code\": 21303,\n                \"description\": \"Model is queued for evaluation.\"\n            }\n        },\n        \"completed_at\": \"2023-11-23T08:21:08.303040Z\",\n        \"visibility\": {\n            \"gettable\": 10\n        },\n        \"app_id\": \"test-app-1700638575-empty\",\n        \"user_id\": \"a0btrubbaefn\",\n        \"metadata\": {},\n        \"output_info\": {\n            \"output_config\": {\n                \"max_concepts\": 0,\n                \"min_value\": 0\n            },\n            \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n            \"fields_map\": {\n                \"concepts\": \"probs\"\n            },\n            \"params\": {\n                \"max_concepts\": 20,\n                \"min_value\": 0,\n                \"select_concepts\": []\n            }\n        },\n        \"input_info\": {\n            \"fields_map\": {\n                \"image\": \"input\"\n            }\n        },\n        \"train_info\": {\n            \"params\": {\n                \"batch_size\": 64,\n                \"concepts_mutually_exclusive\": false,\n                \"dataset_id\": \"\",\n                \"dataset_version_id\": \"\",\n                \"flip_direction\": \"horizontal\",\n                \"flip_probability\": 0.5,\n                \"image_size\": 224,\n                \"invalid_data_tolerance_percent\": 5,\n                \"num_epochs\": 60,\n                \"num_gpus\": 1,\n                \"per_item_lrate\": 0.00001953125,\n                \"per_item_min_lrate\": 1.5625e-08,\n                \"pretrained_weights\": \"ImageNet-1k\",\n                \"seed\": -1,\n                \"template\": \"MMClassification_ResNet_50_RSB_A1\",\n                \"warmup_iters\": 100,\n                \"warmup_ratio\": 0.0001,\n                \"weight_decay\": 0.01\n            }\n        },\n        \"import_info\": {}\n    }\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff --request POST 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/models/{model_id}/versions/{version_id}/metrics' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Authorization: Key {{key}}'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/models/{model_id}/versions/{version_id}": {
      "delete": {
        "tags": [
          "Models"
        ],
        "summary": "Delete Model Version",
        "description": "This endpoint enables users to delete a model version by providing a model ID along with a version ID.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application name** |\n| `model_id` | **string** | **Stores the Model ID** |\n| `version_id` | **string** | **Stores the model version** |",
        "parameters": [
          {
            "name": "Authorization",
            "in": "header",
            "schema": {
              "type": "string"
            },
            "example": "Key {{key}}"
          },
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "model_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "version_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"8aa9e4631e731e0b5d4245e9804aa549\"\n    }\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff --request DELETE 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/models/{model_id}/versions/{version_id}' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Authorization: Key {{key}}'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/evaluations": {
      "post": {
        "tags": [
          "Models"
        ],
        "summary": "Post Model Evaluations",
        "description": "This endpoint allows users to evaluate the performance metrics of a specific model seamlessly.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application name** |\n| `model_id` | **string** | **Stores the Model ID** |\n| `version_id` | **string** | **Stores the model version** |\n| `ground_truth_dataset` | **string** | **Stores the ID of ground truth dataset** |",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "example": {
                  "user_app_id": {
                    "user_id": "{{user_id}}",
                    "app_id": "test-app-1700638575-empty"
                  },
                  "eval_metrics": [
                    {
                      "app_id": "test-app-1700638575-empty",
                      "model": {
                        "id": "deep_cls_bg1",
                        "model_version": {
                          "id": "00668896e0a64cf5b37302c000e96f23"
                        },
                        "user_id": "{{user_id}}",
                        "app_id": "test-app-1700638575-empty"
                      },
                      "ground_truth_dataset": {
                        "id": "image-data",
                        "user_id": "{{user_id}}",
                        "app_id": "test-app-1700638575-empty"
                      }
                    }
                  ]
                }
              }
            }
          }
        },
        "parameters": [
          {
            "name": "Authorization",
            "in": "header",
            "schema": {
              "type": "string"
            },
            "example": "Key {{key}}"
          },
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"e40e62394f269812f019da87f77dbb57\"\n    },\n    \"eval_metrics\": [\n        {\n            \"status\": {\n                \"code\": 21303,\n                \"description\": \"Model is queued for evaluation.\"\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"id\": \"a9751e189c8c466b8273e7a45197728b\"\n        }\n    ]\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/evaluations' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Authorization: Key {{key}}' \\\n--header 'Content-Type: application/json' \\\n--data '{\n  \"user_app_id\": {\n    \"user_id\": \"{{user_id}}\",\n    \"app_id\": \"test-app-1700638575-empty\"\n  },\n  \"eval_metrics\": [\n    {\n      \"app_id\": \"test-app-1700638575-empty\",\n      \"model\": {\n        \"id\": \"deep_cls_bg1\",\n        \"model_version\": {\n          \"id\": \"00668896e0a64cf5b37302c000e96f23\"\n        },\n        \"user_id\": \"{{user_id}}\",\n        \"app_id\": \"test-app-1700638575-empty\"\n      },\n      \"ground_truth_dataset\": {\n        \"id\": \"image-data\",\n        \"user_id\": \"{{user_id}}\",\n        \"app_id\": \"test-app-1700638575-empty\"\n      }\n    }\n  ]\n}'"
          }
        ]
      },
      "get": {
        "tags": [
          "Models"
        ],
        "summary": "List Evaluation",
        "description": "This endpoint allows users to list all evaluations performed.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application name** |",
        "parameters": [
          {
            "name": "Authorization",
            "in": "header",
            "schema": {
              "type": "string"
            },
            "example": "Key {{key}}"
          },
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"4f9601520b81a13a9fb4c3918c5dc9e5\"\n    },\n    \"eval_metrics\": [\n        {\n            \"status\": {\n                \"code\": 21300,\n                \"description\": \"Model was successfully evaluated.\"\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"id\": \"356e26563e8d4da5b1fe5a8ff8710ab5\",\n            \"model\": {\n                \"id\": \"deep_cls_bg1\",\n                \"app_id\": \"test-app-1700638575-empty\",\n                \"model_version\": {\n                    \"id\": \"00668896e0a64cf5b37302c000e96f23\",\n                    \"created_at\": \"2023-11-23T08:06:12.577745Z\",\n                    \"status\": {\n                        \"code\": 21100,\n                        \"description\": \"Model is trained and ready\"\n                    },\n                    \"active_concept_count\": 16,\n                    \"metrics\": {\n                        \"status\": {\n                            \"code\": 21317,\n                            \"description\": \"Model evaluation failed.\"\n                        },\n                        \"summary\": {\n                            \"macro_avg_roc_auc\": 0,\n                            \"macro_std_roc_auc\": 0,\n                            \"macro_avg_f1_score\": 0,\n                            \"macro_std_f1_score\": 0,\n                            \"macro_avg_precision\": 0,\n                            \"macro_avg_recall\": 0\n                        }\n                    },\n                    \"completed_at\": \"2023-11-23T08:21:08.303040Z\",\n                    \"visibility\": {\n                        \"gettable\": 10\n                    },\n                    \"app_id\": \"test-app-1700638575-empty\",\n                    \"user_id\": \"a0btrubbaefn\",\n                    \"metadata\": {},\n                    \"output_info\": {\n                        \"output_config\": {\n                            \"max_concepts\": 0,\n                            \"min_value\": 0\n                        },\n                        \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                        \"fields_map\": {\n                            \"concepts\": \"probs\"\n                        },\n                        \"params\": {\n                            \"max_concepts\": 20,\n                            \"min_value\": 0,\n                            \"select_concepts\": []\n                        }\n                    },\n                    \"input_info\": {\n                        \"fields_map\": {\n                            \"image\": \"input\"\n                        }\n                    },\n                    \"train_info\": {\n                        \"params\": {\n                            \"batch_size\": 64,\n                            \"concepts_mutually_exclusive\": false,\n                            \"dataset_id\": \"\",\n                            \"dataset_version_id\": \"\",\n                            \"flip_direction\": \"horizontal\",\n                            \"flip_probability\": 0.5,\n                            \"image_size\": 224,\n                            \"invalid_data_tolerance_percent\": 5,\n                            \"num_epochs\": 60,\n                            \"num_gpus\": 1,\n                            \"per_item_lrate\": 0.00001953125,\n                            \"per_item_min_lrate\": 1.5625e-08,\n                            \"pretrained_weights\": \"ImageNet-1k\",\n                            \"seed\": -1,\n                            \"template\": \"MMClassification_ResNet_50_RSB_A1\",\n                            \"warmup_iters\": 100,\n                            \"warmup_ratio\": 0.0001,\n                            \"weight_decay\": 0.01\n                        }\n                    },\n                    \"import_info\": {}\n                },\n                \"user_id\": \"a0btrubbaefn\",\n                \"model_type_id\": \"visual-classifier\",\n                \"toolkits\": [],\n                \"use_cases\": [],\n                \"languages\": [],\n                \"languages_full\": [],\n                \"check_consents\": []\n            },\n            \"summary\": {\n                \"macro_avg_roc_auc\": 0.52151275,\n                \"macro_std_roc_auc\": 0.34620512,\n                \"macro_avg_f1_score\": 0.40380955,\n                \"macro_std_f1_score\": 0.17236254,\n                \"macro_avg_precision\": 0.09772728,\n                \"macro_avg_recall\": 0.52380955\n            },\n            \"eval_info\": {\n                \"params\": {\n                    \"dataset_id\": \"\",\n                    \"dataset_version_id\": \"\"\n                }\n            }\n        },\n        {\n            \"status\": {\n                \"code\": 21300,\n                \"description\": \"Model was successfully evaluated.\"\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"id\": \"4c78090f47fe4d6688db7128263b7dd1\",\n            \"model\": {\n                \"id\": \"deep_cls_bg1\",\n                \"app_id\": \"test-app-1700638575-empty\",\n                \"model_version\": {\n                    \"id\": \"00668896e0a64cf5b37302c000e96f23\",\n                    \"created_at\": \"2023-11-23T08:06:12.577745Z\",\n                    \"status\": {\n                        \"code\": 21100,\n                        \"description\": \"Model is trained and ready\"\n                    },\n                    \"active_concept_count\": 16,\n                    \"metrics\": {\n                        \"status\": {\n                            \"code\": 21317,\n                            \"description\": \"Model evaluation failed.\"\n                        },\n                        \"summary\": {\n                            \"macro_avg_roc_auc\": 0,\n                            \"macro_std_roc_auc\": 0,\n                            \"macro_avg_f1_score\": 0,\n                            \"macro_std_f1_score\": 0,\n                            \"macro_avg_precision\": 0,\n                            \"macro_avg_recall\": 0\n                        }\n                    },\n                    \"completed_at\": \"2023-11-23T08:21:08.303040Z\",\n                    \"visibility\": {\n                        \"gettable\": 10\n                    },\n                    \"app_id\": \"test-app-1700638575-empty\",\n                    \"user_id\": \"a0btrubbaefn\",\n                    \"metadata\": {},\n                    \"output_info\": {\n                        \"output_config\": {\n                            \"max_concepts\": 0,\n                            \"min_value\": 0\n                        },\n                        \"message\": \"Show output_info with: GET /models/{model_id}/output_info\",\n                        \"fields_map\": {\n                            \"concepts\": \"probs\"\n                        },\n                        \"params\": {\n                            \"max_concepts\": 20,\n                            \"min_value\": 0,\n                            \"select_concepts\": []\n                        }\n                    },\n                    \"input_info\": {\n                        \"fields_map\": {\n                            \"image\": \"input\"\n                        }\n                    },\n                    \"train_info\": {\n                        \"params\": {\n                            \"batch_size\": 64,\n                            \"concepts_mutually_exclusive\": false,\n                            \"dataset_id\": \"\",\n                            \"dataset_version_id\": \"\",\n                            \"flip_direction\": \"horizontal\",\n                            \"flip_probability\": 0.5,\n                            \"image_size\": 224,\n                            \"invalid_data_tolerance_percent\": 5,\n                            \"num_epochs\": 60,\n                            \"num_gpus\": 1,\n                            \"per_item_lrate\": 0.00001953125,\n                            \"per_item_min_lrate\": 1.5625e-08,\n                            \"pretrained_weights\": \"ImageNet-1k\",\n                            \"seed\": -1,\n                            \"template\": \"MMClassification_ResNet_50_RSB_A1\",\n                            \"warmup_iters\": 100,\n                            \"warmup_ratio\": 0.0001,\n                            \"weight_decay\": 0.01\n                        }\n                    },\n                    \"import_info\": {}\n                },\n                \"user_id\": \"a0btrubbaefn\",\n                \"model_type_id\": \"visual-classifier\",\n                \"toolkits\": [],\n                \"use_cases\": [],\n                \"languages\": [],\n                \"languages_full\": [],\n                \"check_consents\": []\n            },\n            \"ground_truth_dataset\": {\n                \"id\": \"image-data\",\n                \"app_id\": \"test-app-1700638575-empty\",\n                \"user_id\": \"a0btrubbaefn\",\n                \"version\": {\n                    \"id\": \"auto-generated-khqjmznboouydyrb\"\n                }\n            },\n            \"summary\": {\n                \"macro_avg_roc_auc\": 0,\n                \"macro_std_roc_auc\": 0,\n                \"macro_avg_f1_score\": 0,\n                \"macro_std_f1_score\": 0,\n                \"macro_avg_precision\": 0,\n                \"macro_avg_recall\": 0\n            },\n            \"eval_info\": {\n                \"params\": {\n                    \"dataset_id\": \"image-data\",\n                    \"dataset_version_id\": \"auto-generated-khqjmznboouydyrb\"\n                }\n            }\n        }\n    ]\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/evaluations' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Authorization: Key {{key}}'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/evaluations/a9751e189c8c466b8273e7a45197728b": {
      "get": {
        "tags": [
          "Models"
        ],
        "summary": "Get Evaluation",
        "description": "This endpoint allows users to display the evaluation information of a model.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application name** |\n| `evaluation_id` | **string** | **Stores the evaluation ID** |",
        "parameters": [
          {
            "name": "Authorization",
            "in": "header",
            "schema": {
              "type": "string"
            },
            "example": "Key {{key}}"
          },
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"d361b8b0dcf89adb7c19c312219c6c47\"\n    },\n    \"eval_metrics\": {\n        \"status\": {\n            \"code\": 21317,\n            \"description\": \"No data found for evaluation.\"\n        },\n        \"user_id\": \"a0btrubbaefn\",\n        \"app_id\": \"test-app-1700638575-empty\",\n        \"id\": \"a9751e189c8c466b8273e7a45197728b\"\n    }\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/evaluations/a9751e189c8c466b8273e7a45197728b' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Authorization: Key {{key}}'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/annotations/searches": {
      "post": {
        "tags": [
          "Search > Filter Annotations"
        ],
        "summary": "Filter By Input ID And Task ID",
        "description": "This request filters images based on task ID and input ID. It allows you to retrieve images that match specific task ID for a particular input.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `task_id` | **string** | **Stores the task ID** |\n| `input_id` | **string** | **Stores the input ID** |",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "example": {
                  "searches": [
                    {
                      "query": {
                        "filters": [
                          {
                            "annotation": {
                              "input_id": "{{input_id}}"
                            }
                          },
                          {
                            "annotation": {
                              "task_id": "123"
                            }
                          }
                        ]
                      }
                    }
                  ]
                }
              }
            }
          }
        },
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"223307e6294046960e0cc8eef1aaa070\"\n    },\n    \"hits\": [],\n    \"searches\": [\n        {\n            \"query\": {\n                \"filters\": [\n                    {\n                        \"annotation\": {\n                            \"input_id\": \"input1\"\n                        }\n                    },\n                    {\n                        \"annotation\": {\n                            \"task_id\": \"123\"\n                        }\n                    }\n                ]\n            }\n        }\n    ]\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/annotations/searches' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Content-Type: application/json' \\\n--data '{\n  \"searches\": [\n    {\n      \"query\": {\n        \"filters\": [\n          {\n            \"annotation\": {\n              \"input_id\": \"{{input_id}}\"\n            }\n          },\n          {\n            \"annotation\": {\n              \"task_id\": \"123\"\n            }\n          }\n        ]\n      }\n    }\n  ]\n}'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/inputs/searches": {
      "post": {
        "tags": [
          "Search > Filter Annotations"
        ],
        "summary": "Count Filter By Custom Concept",
        "description": "The custom concept acts as a filter criterion, and the request returns the count of items associated with that concept within the specified dataset.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `name` | **string** | **Stores the concept name** |\n| `value` | **string** | **Stores the concept value** |\n| `only_count` | **bool** | **Stores the option for count filter** |",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "example": {
                  "searches": [
                    {
                      "query": {
                        "filters": [
                          {
                            "annotation": {
                              "data": {
                                "concepts": [
                                  {
                                    "name": "train",
                                    "value": 1
                                  }
                                ]
                              }
                            }
                          }
                        ]
                      }
                    }
                  ],
                  "only_count": true
                }
              }
            }
          }
        },
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"c348a80f4d7314ef2d4fa81d8353f645\"\n    },\n    \"hits\": [],\n    \"searches\": [\n        {\n            \"query\": {\n                \"filters\": [\n                    {\n                        \"annotation\": {\n                            \"data\": {\n                                \"concepts\": [\n                                    {\n                                        \"name\": \"train\",\n                                        \"value\": 1\n                                    }\n                                ]\n                            }\n                        }\n                    }\n                ]\n            }\n        }\n    ],\n    \"hit_counts\": [\n        {}\n    ]\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/inputs/searches' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Content-Type: application/json' \\\n--data '{\n  \"searches\": [\n    {\n      \"query\": {\n        \"filters\": [\n          {\n            \"annotation\": {\n              \"data\": {\n                \"concepts\": [\n                  {\n                    \"name\": \"train\",\n                    \"value\": 1\n                  }\n                ]\n              }\n            }\n          }\n        ]\n      }\n    }\n  ],\n  \"only_count\": true\n}'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/annotations/searches/metrics": {
      "post": {
        "tags": [
          "Search > Metrics"
        ],
        "summary": "Annotation Search Metrics",
        "description": "This request is used to add metrics to annotations.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `set` | **string** | **Stores the annotation info** |\n| `concept_id` | **string** | **Stores the concept ID** |\n| `evaluation_type` | **string** | **Stores the type of evaluation to perform** |",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "example": {
                  "ground_truth": {
                    "query": {
                      "ands": [
                        {
                          "annotation": {
                            "annotation_info": {
                              "set": "ground_truth"
                            }
                          }
                        }
                      ]
                    }
                  },
                  "search_to_eval": {
                    "query": {
                      "ands": [
                        {
                          "annotation": {
                            "annotation_info": {
                              "set": "set_to_eval"
                            }
                          }
                        }
                      ]
                    }
                  },
                  "data": {
                    "concepts": [
                      {
                        "id": "test-concept"
                      },
                      {
                        "id": "test-concept-1"
                      }
                    ]
                  },
                  "evaluation_type": "Detection"
                }
              }
            }
          }
        },
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"16df336918b07a8afdbcfaefb2881bc5\"\n    },\n    \"annotation_search_metrics\": [\n        {\n            \"ground_truth\": {\n                \"query\": {\n                    \"ands\": [\n                        {\n                            \"annotation\": {\n                                \"annotation_info\": {\n                                    \"set\": \"ground_truth\"\n                                }\n                            }\n                        }\n                    ]\n                }\n            },\n            \"search_to_eval\": {\n                \"query\": {\n                    \"ands\": [\n                        {\n                            \"annotation\": {\n                                \"annotation_info\": {\n                                    \"set\": \"set_to_eval\"\n                                }\n                            }\n                        }\n                    ]\n                }\n            },\n            \"metrics\": {\n                \"status\": {\n                    \"code\": 43100,\n                    \"description\": \"Evaluation queued\"\n                },\n                \"id\": \"b9262de40e5f469486716d46f03f4908\"\n            },\n            \"data\": {\n                \"concepts\": [\n                    {\n                        \"id\": \"test-concept\",\n                        \"value\": 0\n                    },\n                    {\n                        \"id\": \"test-concept-1\",\n                        \"value\": 0\n                    }\n                ]\n            },\n            \"active_concept_count\": 2\n        }\n    ]\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/annotations/searches/metrics' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Content-Type: application/json' \\\n--data '{\n  \"ground_truth\": {\n    \"query\": {\n      \"ands\": [\n        {\n          \"annotation\": {\n            \"annotation_info\": {\n              \"set\": \"ground_truth\"\n            }\n          }\n        }\n      ]\n    }\n  },\n  \"search_to_eval\": {\n    \"query\": {\n      \"ands\": [\n        {\n          \"annotation\": {\n            \"annotation_info\": {\n              \"set\": \"set_to_eval\"\n            }\n          }\n        }\n      ]\n    }\n  },\n  \"data\": {\n    \"concepts\": [\n      {\n        \"id\": \"test-concept\"\n      },\n      {\n        \"id\": \"test-concept-1\"\n      }\n    ]\n  },\n  \"evaluation_type\": \"Detection\"\n}'"
          }
        ]
      },
      "get": {
        "tags": [
          "Search > Metrics"
        ],
        "summary": "List Annotation Search Metrics",
        "description": "This request is used to display all search metrics.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |",
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"cc7be6b686ca88b63d82f6fd5bfd5af4\"\n    },\n    \"annotation_search_metrics\": [\n        {\n            \"ground_truth\": {\n                \"query\": {\n                    \"ands\": [\n                        {\n                            \"annotation\": {\n                                \"annotation_info\": {\n                                    \"set\": \"ground_truth\"\n                                }\n                            }\n                        }\n                    ]\n                }\n            },\n            \"search_to_eval\": {\n                \"query\": {\n                    \"ands\": [\n                        {\n                            \"annotation\": {\n                                \"annotation_info\": {\n                                    \"set\": \"set_to_eval\"\n                                }\n                            }\n                        }\n                    ]\n                }\n            },\n            \"metrics\": {\n                \"status\": {\n                    \"code\": 43102,\n                    \"description\": \"Evaluated successfully\"\n                },\n                \"id\": \"b9262de40e5f469486716d46f03f4908\",\n                \"summary\": {\n                    \"macro_avg_roc_auc\": 0,\n                    \"macro_std_roc_auc\": 0,\n                    \"macro_avg_f1_score\": 0,\n                    \"macro_std_f1_score\": 0,\n                    \"macro_avg_precision\": 0,\n                    \"macro_avg_recall\": 0,\n                    \"mean_avg_precision_iou_50\": 0.40625,\n                    \"mean_avg_precision_iou_range\": 0.35\n                }\n            },\n            \"active_concept_count\": 2\n        }\n    ]\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/annotations/searches/metrics' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/annotations/searches/metrics/{annotation_search_metrics_id}": {
      "get": {
        "tags": [
          "Search > Metrics"
        ],
        "summary": "List Annotation Search Metrics By ID",
        "description": "This request is used to retrieve a specific annotation search metric by proving a unique ID.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `annotation_search_metrics_id` | **string** | **Stores the annotation metrics ID** |",
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "annotation_search_metrics_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"22349538c854e07b9d8496d9c8027f24\"\n    },\n    \"annotation_search_metrics\": [\n        {\n            \"ground_truth\": {\n                \"query\": {\n                    \"ands\": [\n                        {\n                            \"annotation\": {\n                                \"annotation_info\": {\n                                    \"set\": \"ground_truth\"\n                                }\n                            }\n                        }\n                    ]\n                }\n            },\n            \"search_to_eval\": {\n                \"query\": {\n                    \"ands\": [\n                        {\n                            \"annotation\": {\n                                \"annotation_info\": {\n                                    \"set\": \"set_to_eval\"\n                                }\n                            }\n                        }\n                    ]\n                }\n            },\n            \"metrics\": {\n                \"status\": {\n                    \"code\": 43102,\n                    \"description\": \"Evaluated successfully\"\n                },\n                \"id\": \"b9262de40e5f469486716d46f03f4908\",\n                \"summary\": {\n                    \"macro_avg_roc_auc\": 0,\n                    \"macro_std_roc_auc\": 0,\n                    \"macro_avg_f1_score\": 0,\n                    \"macro_std_f1_score\": 0,\n                    \"macro_avg_precision\": 0,\n                    \"macro_avg_recall\": 0,\n                    \"mean_avg_precision_iou_50\": 0.40625,\n                    \"mean_avg_precision_iou_range\": 0.35\n                },\n                \"confusion_matrix\": {},\n                \"cooccurrence_matrix\": {},\n                \"label_counts\": {},\n                \"test_set\": [\n                    {\n                        \"input\": {\n                            \"id\": \"1\",\n                            \"data\": {\n                                \"image\": {\n                                    \"url\": \"http://i.imgur.com/HEoT5xR.png\",\n                                    \"hosted\": {\n                                        \"prefix\": \"https://data-dev.clarifai.com\",\n                                        \"suffix\": \"users/a0btrubbaefn/apps/test-app-1701866015/inputs/image/89f4b090a85df4802d6cf5b767a66f37\",\n                                        \"sizes\": [\n                                            \"orig\",\n                                            \"tiny\",\n                                            \"small\",\n                                            \"large\"\n                                        ],\n                                        \"crossorigin\": \"use-credentials\"\n                                    },\n                                    \"image_info\": {\n                                        \"width\": 934,\n                                        \"height\": 587,\n                                        \"format\": \"PNG\",\n                                        \"color_mode\": \"RGB\"\n                                    }\n                                }\n                            },\n                            \"created_at\": \"2023-12-06T12:34:45.593192Z\",\n                            \"modified_at\": \"2023-12-06T12:34:50.627841Z\",\n                            \"status\": {\n                                \"code\": 30000,\n                                \"description\": \"Download complete\"\n                            }\n                        },\n                        \"predicted_concepts\": [\n                            {\n                                \"id\": \"test-concept\",\n                                \"name\": \"test-concept\",\n                                \"value\": 1,\n                                \"app_id\": \"test-app-1701866015\"\n                            }\n                        ],\n                        \"ground_truth_concepts\": [\n                            {\n                                \"id\": \"test-concept\",\n                                \"name\": \"test-concept\",\n                                \"value\": 1,\n                                \"app_id\": \"test-app-1701866015\"\n                            }\n                        ],\n                        \"annotation\": {\n                            \"data\": {\n                                \"regions\": [\n                                    {\n                                        \"id\": \"361d6a9253be9152968012660258a4bf\",\n                                        \"region_info\": {\n                                            \"bounding_box\": {\n                                                \"top_row\": 0,\n                                                \"left_col\": 0,\n                                                \"bottom_row\": 0.5,\n                                                \"right_col\": 0.5\n                                            }\n                                        }\n                                    }\n                                ]\n                            }\n                        }\n                    },\n                    {\n                        \"input\": {\n                            \"id\": \"2\",\n                            \"data\": {\n                                \"image\": {\n                                    \"url\": \"http://i.imgur.com/It5JRaj.jpg\",\n                                    \"hosted\": {\n                                        \"prefix\": \"https://data-dev.clarifai.com\",\n                                        \"suffix\": \"users/a0btrubbaefn/apps/test-app-1701866015/inputs/image/2ef6a4da17cdd348a5166fd6f30db3e2\",\n                                        \"sizes\": [\n                                            \"orig\",\n                                            \"tiny\",\n                                            \"small\",\n                                            \"large\"\n                                        ],\n                                        \"crossorigin\": \"use-credentials\"\n                                    },\n                                    \"image_info\": {\n                                        \"width\": 768,\n                                        \"height\": 1024,\n                                        \"format\": \"JPEG\",\n                                        \"color_mode\": \"YUV\"\n                                    }\n                                }\n                            },\n                            \"created_at\": \"2023-12-06T12:34:45.593192Z\",\n                            \"modified_at\": \"2023-12-06T12:34:50.627841Z\",\n                            \"status\": {\n                                \"code\": 30000,\n                                \"description\": \"Download complete\"\n                            }\n                        },\n                        \"predicted_concepts\": [\n                            {\n                                \"id\": \"test-concept\",\n                                \"name\": \"test-concept\",\n                                \"value\": 1,\n                                \"app_id\": \"test-app-1701866015\"\n                            }\n                        ],\n                        \"ground_truth_concepts\": [\n                            {\n                                \"id\": \"test-concept\",\n                                \"name\": \"test-concept\",\n                                \"value\": 1,\n                                \"app_id\": \"test-app-1701866015\"\n                            }\n                        ],\n                        \"annotation\": {\n                            \"data\": {\n                                \"regions\": [\n                                    {\n                                        \"id\": \"4eae01d6e5c036bcc3a66f97ae594ea7\",\n                                        \"region_info\": {\n                                            \"bounding_box\": {\n                                                \"top_row\": 0,\n                                                \"left_col\": 0,\n                                                \"bottom_row\": 0.4,\n                                                \"right_col\": 0.5\n                                            }\n                                        }\n                                    }\n                                ]\n                            }\n                        }\n                    },\n                    {\n                        \"input\": {\n                            \"id\": \"3\",\n                            \"data\": {\n                                \"image\": {\n                                    \"url\": \"http://i.imgur.com/9Knw6RS.jpg\",\n                                    \"hosted\": {\n                                        \"prefix\": \"https://data-dev.clarifai.com\",\n                                        \"suffix\": \"users/a0btrubbaefn/apps/test-app-1701866015/inputs/image/d9b6a576e9f5975db07556c882d32dc7\",\n                                        \"sizes\": [\n                                            \"orig\",\n                                            \"tiny\",\n                                            \"small\",\n                                            \"large\"\n                                        ],\n                                        \"crossorigin\": \"use-credentials\"\n                                    },\n                                    \"image_info\": {\n                                        \"width\": 1600,\n                                        \"height\": 1032,\n                                        \"format\": \"JPEG\",\n                                        \"color_mode\": \"YUV\"\n                                    }\n                                }\n                            },\n                            \"created_at\": \"2023-12-06T12:34:45.593192Z\",\n                            \"modified_at\": \"2023-12-06T12:34:50.627841Z\",\n                            \"status\": {\n                                \"code\": 30000,\n                                \"description\": \"Download complete\"\n                            }\n                        },\n                        \"predicted_concepts\": [\n                            {\n                                \"id\": \"test-concept-1\",\n                                \"name\": \"test-concept-1\",\n                                \"value\": 1,\n                                \"app_id\": \"test-app-1701866015\"\n                            }\n                        ],\n                        \"ground_truth_concepts\": [\n                            {\n                                \"id\": \"test-concept\",\n                                \"name\": \"test-concept\",\n                                \"value\": 1,\n                                \"app_id\": \"test-app-1701866015\"\n                            }\n                        ],\n                        \"annotation\": {\n                            \"data\": {\n                                \"regions\": [\n                                    {\n                                        \"id\": \"361d6a9253be9152968012660258a4bf\",\n                                        \"region_info\": {\n                                            \"bounding_box\": {\n                                                \"top_row\": 0,\n                                                \"left_col\": 0,\n                                                \"bottom_row\": 0.5,\n                                                \"right_col\": 0.5\n                                            }\n                                        }\n                                    }\n                                ]\n                            }\n                        }\n                    },\n                    {\n                        \"input\": {\n                            \"id\": \"4\",\n                            \"data\": {\n                                \"image\": {\n                                    \"url\": \"http://i.imgur.com/GeMQsiQ.jpg\",\n                                    \"hosted\": {\n                                        \"prefix\": \"https://data-dev.clarifai.com\",\n                                        \"suffix\": \"users/a0btrubbaefn/apps/test-app-1701866015/inputs/image/ce718f0ca1bdcac7f36bcc871fa68001\",\n                                        \"sizes\": [\n                                            \"orig\",\n                                            \"tiny\",\n                                            \"small\",\n                                            \"large\"\n                                        ],\n                                        \"crossorigin\": \"use-credentials\"\n                                    },\n                                    \"image_info\": {\n                                        \"width\": 4096,\n                                        \"height\": 3040,\n                                        \"format\": \"JPEG\",\n                                        \"color_mode\": \"YUV\"\n                                    }\n                                }\n                            },\n                            \"created_at\": \"2023-12-06T12:34:45.593192Z\",\n                            \"modified_at\": \"2023-12-06T12:34:50.627841Z\",\n                            \"status\": {\n                                \"code\": 30000,\n                                \"description\": \"Download complete\"\n                            }\n                        },\n                        \"predicted_concepts\": [\n                            {\n                                \"id\": \"test-concept-1\",\n                                \"name\": \"test-concept-1\",\n                                \"value\": 1,\n                                \"app_id\": \"test-app-1701866015\"\n                            }\n                        ],\n                        \"ground_truth_concepts\": [\n                            {\n                                \"id\": \"test-concept-1\",\n                                \"name\": \"test-concept-1\",\n                                \"value\": 1,\n                                \"app_id\": \"test-app-1701866015\"\n                            }\n                        ],\n                        \"annotation\": {\n                            \"data\": {\n                                \"regions\": [\n                                    {\n                                        \"id\": \"361d6a9253be9152968012660258a4bf\",\n                                        \"region_info\": {\n                                            \"bounding_box\": {\n                                                \"top_row\": 0,\n                                                \"left_col\": 0,\n                                                \"bottom_row\": 0.5,\n                                                \"right_col\": 0.5\n                                            }\n                                        }\n                                    }\n                                ]\n                            }\n                        }\n                    },\n                    {\n                        \"input\": {\n                            \"id\": \"6\",\n                            \"data\": {\n                                \"image\": {\n                                    \"url\": \"http://i.imgur.com/EnrVc0B.jpg\",\n                                    \"hosted\": {\n                                        \"prefix\": \"https://data-dev.clarifai.com\",\n                                        \"suffix\": \"users/a0btrubbaefn/apps/test-app-1701866015/inputs/image/5fcff2ae1739cd5964d75bd258cc4388\",\n                                        \"sizes\": [\n                                            \"orig\",\n                                            \"tiny\",\n                                            \"small\",\n                                            \"large\"\n                                        ],\n                                        \"crossorigin\": \"use-credentials\"\n                                    },\n                                    \"image_info\": {\n                                        \"width\": 1200,\n                                        \"height\": 900,\n                                        \"format\": \"JPEG\",\n                                        \"color_mode\": \"YUV\"\n                                    }\n                                }\n                            },\n                            \"created_at\": \"2023-12-06T12:34:45.593192Z\",\n                            \"modified_at\": \"2023-12-06T12:34:50.627841Z\",\n                            \"status\": {\n                                \"code\": 30000,\n                                \"description\": \"Download complete\"\n                            }\n                        },\n                        \"predicted_concepts\": [\n                            {\n                                \"id\": \"test-concept\",\n                                \"name\": \"test-concept\",\n                                \"value\": 1,\n                                \"app_id\": \"test-app-1701866015\"\n                            }\n                        ],\n                        \"ground_truth_concepts\": [\n                            {\n                                \"id\": \"test-concept\",\n                                \"name\": \"test-concept\",\n                                \"value\": 1,\n                                \"app_id\": \"test-app-1701866015\"\n                            }\n                        ],\n                        \"annotation\": {\n                            \"data\": {\n                                \"regions\": [\n                                    {\n                                        \"id\": \"361d6a9253be9152968012660258a4bf\",\n                                        \"region_info\": {\n                                            \"bounding_box\": {\n                                                \"top_row\": 0,\n                                                \"left_col\": 0,\n                                                \"bottom_row\": 0.5,\n                                                \"right_col\": 0.5\n                                            }\n                                        }\n                                    }\n                                ]\n                            }\n                        }\n                    },\n                    {\n                        \"input\": {\n                            \"id\": \"7\",\n                            \"data\": {\n                                \"image\": {\n                                    \"url\": \"http://s7d1.scene7.com/is/image/BedBathandBeyond/56879143899890p\",\n                                    \"hosted\": {\n                                        \"prefix\": \"https://data-dev.clarifai.com\",\n                                        \"suffix\": \"users/a0btrubbaefn/apps/test-app-1701866015/inputs/image/151ba0b95ded7054437f44a7595e3736\",\n                                        \"sizes\": [\n                                            \"orig\",\n                                            \"tiny\",\n                                            \"small\",\n                                            \"large\"\n                                        ],\n                                        \"crossorigin\": \"use-credentials\"\n                                    },\n                                    \"image_info\": {\n                                        \"width\": 400,\n                                        \"height\": 400,\n                                        \"format\": \"JPEG\",\n                                        \"color_mode\": \"YUV\"\n                                    }\n                                }\n                            },\n                            \"created_at\": \"2023-12-06T12:34:45.593192Z\",\n                            \"modified_at\": \"2023-12-06T12:34:50.627841Z\",\n                            \"status\": {\n                                \"code\": 30000,\n                                \"description\": \"Download complete\"\n                            }\n                        },\n                        \"predicted_concepts\": [\n                            {\n                                \"id\": \"test-concept\",\n                                \"name\": \"test-concept\",\n                                \"value\": 1,\n                                \"app_id\": \"test-app-1701866015\"\n                            }\n                        ],\n                        \"ground_truth_concepts\": [\n                            {\n                                \"id\": \"test-concept-1\",\n                                \"name\": \"test-concept-1\",\n                                \"value\": 1,\n                                \"app_id\": \"test-app-1701866015\"\n                            }\n                        ],\n                        \"annotation\": {\n                            \"data\": {\n                                \"regions\": [\n                                    {\n                                        \"id\": \"361d6a9253be9152968012660258a4bf\",\n                                        \"region_info\": {\n                                            \"bounding_box\": {\n                                                \"top_row\": 0,\n                                                \"left_col\": 0,\n                                                \"bottom_row\": 0.5,\n                                                \"right_col\": 0.5\n                                            }\n                                        }\n                                    }\n                                ]\n                            }\n                        }\n                    }\n                ],\n                \"metrics_by_area\": [\n                    {\n                        \"num_pos\": 6,\n                        \"num_neg\": 0,\n                        \"num_tot\": 6,\n                        \"roc_auc\": 0,\n                        \"f1\": 0,\n                        \"roc_curve\": {\n                            \"fpr\": [\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334\n                            ],\n                            \"tpr\": [\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"precision_recall_curve\": {\n                            \"recall\": [\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667\n                            ],\n                            \"precision\": [\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"avg_precision\": 0.40625,\n                        \"area_name\": \"all\",\n                        \"area_max\": 10000000000,\n                        \"iou\": 0.5\n                    },\n                    {\n                        \"num_pos\": 6,\n                        \"num_neg\": 0,\n                        \"num_tot\": 6,\n                        \"roc_auc\": 0,\n                        \"f1\": 0,\n                        \"roc_curve\": {\n                            \"fpr\": [\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334\n                            ],\n                            \"tpr\": [\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"precision_recall_curve\": {\n                            \"recall\": [\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667\n                            ],\n                            \"precision\": [\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"avg_precision\": 0.40625,\n                        \"area_name\": \"all\",\n                        \"area_max\": 10000000000,\n                        \"iou\": 0.6\n                    },\n                    {\n                        \"num_pos\": 6,\n                        \"num_neg\": 0,\n                        \"num_tot\": 6,\n                        \"roc_auc\": 0,\n                        \"f1\": 0,\n                        \"roc_curve\": {\n                            \"fpr\": [\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334\n                            ],\n                            \"tpr\": [\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"precision_recall_curve\": {\n                            \"recall\": [\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667\n                            ],\n                            \"precision\": [\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"avg_precision\": 0.40625,\n                        \"area_name\": \"all\",\n                        \"area_max\": 10000000000,\n                        \"iou\": 0.7\n                    },\n                    {\n                        \"num_pos\": 6,\n                        \"num_neg\": 0,\n                        \"num_tot\": 6,\n                        \"roc_auc\": 0,\n                        \"f1\": 0,\n                        \"roc_curve\": {\n                            \"fpr\": [\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334\n                            ],\n                            \"tpr\": [\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"precision_recall_curve\": {\n                            \"recall\": [\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667\n                            ],\n                            \"precision\": [\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"avg_precision\": 0.40625,\n                        \"area_name\": \"all\",\n                        \"area_max\": 10000000000,\n                        \"iou\": 0.8\n                    },\n                    {\n                        \"num_pos\": 6,\n                        \"num_neg\": 0,\n                        \"num_tot\": 6,\n                        \"roc_auc\": 0,\n                        \"f1\": 0,\n                        \"roc_curve\": {\n                            \"fpr\": [\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667\n                            ],\n                            \"tpr\": [\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"precision_recall_curve\": {\n                            \"recall\": [\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334\n                            ],\n                            \"precision\": [\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"avg_precision\": 0.125,\n                        \"area_name\": \"all\",\n                        \"area_max\": 10000000000,\n                        \"iou\": 0.9\n                    },\n                    {\n                        \"num_pos\": 6,\n                        \"num_neg\": 0,\n                        \"num_tot\": 6,\n                        \"roc_auc\": 0,\n                        \"f1\": 0,\n                        \"roc_curve\": {\n                            \"fpr\": [\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334\n                            ],\n                            \"tpr\": [\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"precision_recall_curve\": {\n                            \"recall\": [\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667\n                            ],\n                            \"precision\": [\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"avg_precision\": 0.40625,\n                        \"area_name\": \"large\",\n                        \"area_min\": 9216,\n                        \"area_max\": 10000000000,\n                        \"iou\": 0.5\n                    },\n                    {\n                        \"num_pos\": 6,\n                        \"num_neg\": 0,\n                        \"num_tot\": 6,\n                        \"roc_auc\": 0,\n                        \"f1\": 0,\n                        \"roc_curve\": {\n                            \"fpr\": [\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334\n                            ],\n                            \"tpr\": [\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"precision_recall_curve\": {\n                            \"recall\": [\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667\n                            ],\n                            \"precision\": [\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"avg_precision\": 0.40625,\n                        \"area_name\": \"large\",\n                        \"area_min\": 9216,\n                        \"area_max\": 10000000000,\n                        \"iou\": 0.6\n                    },\n                    {\n                        \"num_pos\": 6,\n                        \"num_neg\": 0,\n                        \"num_tot\": 6,\n                        \"roc_auc\": 0,\n                        \"f1\": 0,\n                        \"roc_curve\": {\n                            \"fpr\": [\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334\n                            ],\n                            \"tpr\": [\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"precision_recall_curve\": {\n                            \"recall\": [\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667\n                            ],\n                            \"precision\": [\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"avg_precision\": 0.40625,\n                        \"area_name\": \"large\",\n                        \"area_min\": 9216,\n                        \"area_max\": 10000000000,\n                        \"iou\": 0.7\n                    },\n                    {\n                        \"num_pos\": 6,\n                        \"num_neg\": 0,\n                        \"num_tot\": 6,\n                        \"roc_auc\": 0,\n                        \"f1\": 0,\n                        \"roc_curve\": {\n                            \"fpr\": [\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334\n                            ],\n                            \"tpr\": [\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"precision_recall_curve\": {\n                            \"recall\": [\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667\n                            ],\n                            \"precision\": [\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"avg_precision\": 0.40625,\n                        \"area_name\": \"large\",\n                        \"area_min\": 9216,\n                        \"area_max\": 10000000000,\n                        \"iou\": 0.8\n                    },\n                    {\n                        \"num_pos\": 6,\n                        \"num_neg\": 0,\n                        \"num_tot\": 6,\n                        \"roc_auc\": 0,\n                        \"f1\": 0,\n                        \"roc_curve\": {\n                            \"fpr\": [\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667,\n                                0.6666667\n                            ],\n                            \"tpr\": [\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"precision_recall_curve\": {\n                            \"recall\": [\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334\n                            ],\n                            \"precision\": [\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334,\n                                0.33333334\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"avg_precision\": 0.125,\n                        \"area_name\": \"large\",\n                        \"area_min\": 9216,\n                        \"area_max\": 10000000000,\n                        \"iou\": 0.9\n                    }\n                ],\n                \"metrics_by_class\": [\n                    {\n                        \"num_pos\": 4,\n                        \"num_neg\": 0,\n                        \"num_tot\": 4,\n                        \"roc_auc\": 0,\n                        \"f1\": 0,\n                        \"concept\": {\n                            \"id\": \"test-concept\",\n                            \"name\": \"test-concept\",\n                            \"value\": 1,\n                            \"app_id\": \"test-app-1701866015\"\n                        },\n                        \"roc_curve\": {\n                            \"fpr\": [\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25\n                            ],\n                            \"tpr\": [\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"precision_recall_curve\": {\n                            \"recall\": [\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75\n                            ],\n                            \"precision\": [\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"avg_precision\": 0.5625,\n                        \"area_name\": \"all\",\n                        \"area_max\": 10000000000,\n                        \"iou\": 0.5\n                    },\n                    {\n                        \"num_pos\": 2,\n                        \"num_neg\": 0,\n                        \"num_tot\": 2,\n                        \"roc_auc\": 0,\n                        \"f1\": 0,\n                        \"concept\": {\n                            \"id\": \"test-concept-1\",\n                            \"name\": \"test-concept-1\",\n                            \"value\": 1,\n                            \"app_id\": \"test-app-1701866015\"\n                        },\n                        \"roc_curve\": {\n                            \"fpr\": [\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5\n                            ],\n                            \"tpr\": [\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"precision_recall_curve\": {\n                            \"recall\": [\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5\n                            ],\n                            \"precision\": [\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"avg_precision\": 0.25,\n                        \"area_name\": \"all\",\n                        \"area_max\": 10000000000,\n                        \"iou\": 0.5\n                    },\n                    {\n                        \"num_pos\": 4,\n                        \"num_neg\": 0,\n                        \"num_tot\": 4,\n                        \"roc_auc\": 0,\n                        \"f1\": 0,\n                        \"concept\": {\n                            \"id\": \"test-concept\",\n                            \"name\": \"test-concept\",\n                            \"value\": 1,\n                            \"app_id\": \"test-app-1701866015\"\n                        },\n                        \"roc_curve\": {\n                            \"fpr\": [\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25\n                            ],\n                            \"tpr\": [\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"precision_recall_curve\": {\n                            \"recall\": [\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75\n                            ],\n                            \"precision\": [\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"avg_precision\": 0.5625,\n                        \"area_name\": \"all\",\n                        \"area_max\": 10000000000,\n                        \"iou\": 0.6\n                    },\n                    {\n                        \"num_pos\": 2,\n                        \"num_neg\": 0,\n                        \"num_tot\": 2,\n                        \"roc_auc\": 0,\n                        \"f1\": 0,\n                        \"concept\": {\n                            \"id\": \"test-concept-1\",\n                            \"name\": \"test-concept-1\",\n                            \"value\": 1,\n                            \"app_id\": \"test-app-1701866015\"\n                        },\n                        \"roc_curve\": {\n                            \"fpr\": [\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5\n                            ],\n                            \"tpr\": [\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"precision_recall_curve\": {\n                            \"recall\": [\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5\n                            ],\n                            \"precision\": [\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"avg_precision\": 0.25,\n                        \"area_name\": \"all\",\n                        \"area_max\": 10000000000,\n                        \"iou\": 0.6\n                    },\n                    {\n                        \"num_pos\": 4,\n                        \"num_neg\": 0,\n                        \"num_tot\": 4,\n                        \"roc_auc\": 0,\n                        \"f1\": 0,\n                        \"concept\": {\n                            \"id\": \"test-concept\",\n                            \"name\": \"test-concept\",\n                            \"value\": 1,\n                            \"app_id\": \"test-app-1701866015\"\n                        },\n                        \"roc_curve\": {\n                            \"fpr\": [\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25\n                            ],\n                            \"tpr\": [\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"precision_recall_curve\": {\n                            \"recall\": [\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75\n                            ],\n                            \"precision\": [\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"avg_precision\": 0.5625,\n                        \"area_name\": \"all\",\n                        \"area_max\": 10000000000,\n                        \"iou\": 0.7\n                    },\n                    {\n                        \"num_pos\": 2,\n                        \"num_neg\": 0,\n                        \"num_tot\": 2,\n                        \"roc_auc\": 0,\n                        \"f1\": 0,\n                        \"concept\": {\n                            \"id\": \"test-concept-1\",\n                            \"name\": \"test-concept-1\",\n                            \"value\": 1,\n                            \"app_id\": \"test-app-1701866015\"\n                        },\n                        \"roc_curve\": {\n                            \"fpr\": [\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5\n                            ],\n                            \"tpr\": [\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"precision_recall_curve\": {\n                            \"recall\": [\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5\n                            ],\n                            \"precision\": [\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"avg_precision\": 0.25,\n                        \"area_name\": \"all\",\n                        \"area_max\": 10000000000,\n                        \"iou\": 0.7\n                    },\n                    {\n                        \"num_pos\": 4,\n                        \"num_neg\": 0,\n                        \"num_tot\": 4,\n                        \"roc_auc\": 0,\n                        \"f1\": 0,\n                        \"concept\": {\n                            \"id\": \"test-concept\",\n                            \"name\": \"test-concept\",\n                            \"value\": 1,\n                            \"app_id\": \"test-app-1701866015\"\n                        },\n                        \"roc_curve\": {\n                            \"fpr\": [\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25\n                            ],\n                            \"tpr\": [\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"precision_recall_curve\": {\n                            \"recall\": [\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75\n                            ],\n                            \"precision\": [\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"avg_precision\": 0.5625,\n                        \"area_name\": \"all\",\n                        \"area_max\": 10000000000,\n                        \"iou\": 0.8\n                    },\n                    {\n                        \"num_pos\": 2,\n                        \"num_neg\": 0,\n                        \"num_tot\": 2,\n                        \"roc_auc\": 0,\n                        \"f1\": 0,\n                        \"concept\": {\n                            \"id\": \"test-concept-1\",\n                            \"name\": \"test-concept-1\",\n                            \"value\": 1,\n                            \"app_id\": \"test-app-1701866015\"\n                        },\n                        \"roc_curve\": {\n                            \"fpr\": [\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5\n                            ],\n                            \"tpr\": [\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"precision_recall_curve\": {\n                            \"recall\": [\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5\n                            ],\n                            \"precision\": [\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"avg_precision\": 0.25,\n                        \"area_name\": \"all\",\n                        \"area_max\": 10000000000,\n                        \"iou\": 0.8\n                    },\n                    {\n                        \"num_pos\": 4,\n                        \"num_neg\": 0,\n                        \"num_tot\": 4,\n                        \"roc_auc\": 0,\n                        \"f1\": 0,\n                        \"concept\": {\n                            \"id\": \"test-concept\",\n                            \"name\": \"test-concept\",\n                            \"value\": 1,\n                            \"app_id\": \"test-app-1701866015\"\n                        },\n                        \"roc_curve\": {\n                            \"fpr\": [\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5\n                            ],\n                            \"tpr\": [\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"precision_recall_curve\": {\n                            \"recall\": [\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5\n                            ],\n                            \"precision\": [\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"avg_precision\": 0.25,\n                        \"area_name\": \"all\",\n                        \"area_max\": 10000000000,\n                        \"iou\": 0.9\n                    },\n                    {\n                        \"num_pos\": 2,\n                        \"num_neg\": 0,\n                        \"num_tot\": 2,\n                        \"roc_auc\": 0,\n                        \"f1\": 0,\n                        \"concept\": {\n                            \"id\": \"test-concept-1\",\n                            \"name\": \"test-concept-1\",\n                            \"value\": 1,\n                            \"app_id\": \"test-app-1701866015\"\n                        },\n                        \"roc_curve\": {\n                            \"fpr\": [\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1\n                            ],\n                            \"tpr\": [\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"precision_recall_curve\": {\n                            \"recall\": [\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0\n                            ],\n                            \"precision\": [\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"area_name\": \"all\",\n                        \"area_max\": 10000000000,\n                        \"iou\": 0.9\n                    },\n                    {\n                        \"num_pos\": 4,\n                        \"num_neg\": 0,\n                        \"num_tot\": 4,\n                        \"roc_auc\": 0,\n                        \"f1\": 0,\n                        \"concept\": {\n                            \"id\": \"test-concept\",\n                            \"name\": \"test-concept\",\n                            \"value\": 1,\n                            \"app_id\": \"test-app-1701866015\"\n                        },\n                        \"roc_curve\": {\n                            \"fpr\": [\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25\n                            ],\n                            \"tpr\": [\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"precision_recall_curve\": {\n                            \"recall\": [\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75\n                            ],\n                            \"precision\": [\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"avg_precision\": 0.5625,\n                        \"area_name\": \"large\",\n                        \"area_min\": 9216,\n                        \"area_max\": 10000000000,\n                        \"iou\": 0.5\n                    },\n                    {\n                        \"num_pos\": 2,\n                        \"num_neg\": 0,\n                        \"num_tot\": 2,\n                        \"roc_auc\": 0,\n                        \"f1\": 0,\n                        \"concept\": {\n                            \"id\": \"test-concept-1\",\n                            \"name\": \"test-concept-1\",\n                            \"value\": 1,\n                            \"app_id\": \"test-app-1701866015\"\n                        },\n                        \"roc_curve\": {\n                            \"fpr\": [\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5\n                            ],\n                            \"tpr\": [\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"precision_recall_curve\": {\n                            \"recall\": [\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5\n                            ],\n                            \"precision\": [\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"avg_precision\": 0.25,\n                        \"area_name\": \"large\",\n                        \"area_min\": 9216,\n                        \"area_max\": 10000000000,\n                        \"iou\": 0.5\n                    },\n                    {\n                        \"num_pos\": 4,\n                        \"num_neg\": 0,\n                        \"num_tot\": 4,\n                        \"roc_auc\": 0,\n                        \"f1\": 0,\n                        \"concept\": {\n                            \"id\": \"test-concept\",\n                            \"name\": \"test-concept\",\n                            \"value\": 1,\n                            \"app_id\": \"test-app-1701866015\"\n                        },\n                        \"roc_curve\": {\n                            \"fpr\": [\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25\n                            ],\n                            \"tpr\": [\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"precision_recall_curve\": {\n                            \"recall\": [\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75\n                            ],\n                            \"precision\": [\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"avg_precision\": 0.5625,\n                        \"area_name\": \"large\",\n                        \"area_min\": 9216,\n                        \"area_max\": 10000000000,\n                        \"iou\": 0.6\n                    },\n                    {\n                        \"num_pos\": 2,\n                        \"num_neg\": 0,\n                        \"num_tot\": 2,\n                        \"roc_auc\": 0,\n                        \"f1\": 0,\n                        \"concept\": {\n                            \"id\": \"test-concept-1\",\n                            \"name\": \"test-concept-1\",\n                            \"value\": 1,\n                            \"app_id\": \"test-app-1701866015\"\n                        },\n                        \"roc_curve\": {\n                            \"fpr\": [\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5\n                            ],\n                            \"tpr\": [\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"precision_recall_curve\": {\n                            \"recall\": [\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5\n                            ],\n                            \"precision\": [\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"avg_precision\": 0.25,\n                        \"area_name\": \"large\",\n                        \"area_min\": 9216,\n                        \"area_max\": 10000000000,\n                        \"iou\": 0.6\n                    },\n                    {\n                        \"num_pos\": 4,\n                        \"num_neg\": 0,\n                        \"num_tot\": 4,\n                        \"roc_auc\": 0,\n                        \"f1\": 0,\n                        \"concept\": {\n                            \"id\": \"test-concept\",\n                            \"name\": \"test-concept\",\n                            \"value\": 1,\n                            \"app_id\": \"test-app-1701866015\"\n                        },\n                        \"roc_curve\": {\n                            \"fpr\": [\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25\n                            ],\n                            \"tpr\": [\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"precision_recall_curve\": {\n                            \"recall\": [\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75\n                            ],\n                            \"precision\": [\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"avg_precision\": 0.5625,\n                        \"area_name\": \"large\",\n                        \"area_min\": 9216,\n                        \"area_max\": 10000000000,\n                        \"iou\": 0.7\n                    },\n                    {\n                        \"num_pos\": 2,\n                        \"num_neg\": 0,\n                        \"num_tot\": 2,\n                        \"roc_auc\": 0,\n                        \"f1\": 0,\n                        \"concept\": {\n                            \"id\": \"test-concept-1\",\n                            \"name\": \"test-concept-1\",\n                            \"value\": 1,\n                            \"app_id\": \"test-app-1701866015\"\n                        },\n                        \"roc_curve\": {\n                            \"fpr\": [\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5\n                            ],\n                            \"tpr\": [\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"precision_recall_curve\": {\n                            \"recall\": [\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5\n                            ],\n                            \"precision\": [\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"avg_precision\": 0.25,\n                        \"area_name\": \"large\",\n                        \"area_min\": 9216,\n                        \"area_max\": 10000000000,\n                        \"iou\": 0.7\n                    },\n                    {\n                        \"num_pos\": 4,\n                        \"num_neg\": 0,\n                        \"num_tot\": 4,\n                        \"roc_auc\": 0,\n                        \"f1\": 0,\n                        \"concept\": {\n                            \"id\": \"test-concept\",\n                            \"name\": \"test-concept\",\n                            \"value\": 1,\n                            \"app_id\": \"test-app-1701866015\"\n                        },\n                        \"roc_curve\": {\n                            \"fpr\": [\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25,\n                                0.25\n                            ],\n                            \"tpr\": [\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"precision_recall_curve\": {\n                            \"recall\": [\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75\n                            ],\n                            \"precision\": [\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75,\n                                0.75\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"avg_precision\": 0.5625,\n                        \"area_name\": \"large\",\n                        \"area_min\": 9216,\n                        \"area_max\": 10000000000,\n                        \"iou\": 0.8\n                    },\n                    {\n                        \"num_pos\": 2,\n                        \"num_neg\": 0,\n                        \"num_tot\": 2,\n                        \"roc_auc\": 0,\n                        \"f1\": 0,\n                        \"concept\": {\n                            \"id\": \"test-concept-1\",\n                            \"name\": \"test-concept-1\",\n                            \"value\": 1,\n                            \"app_id\": \"test-app-1701866015\"\n                        },\n                        \"roc_curve\": {\n                            \"fpr\": [\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5\n                            ],\n                            \"tpr\": [\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"precision_recall_curve\": {\n                            \"recall\": [\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5\n                            ],\n                            \"precision\": [\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"avg_precision\": 0.25,\n                        \"area_name\": \"large\",\n                        \"area_min\": 9216,\n                        \"area_max\": 10000000000,\n                        \"iou\": 0.8\n                    },\n                    {\n                        \"num_pos\": 4,\n                        \"num_neg\": 0,\n                        \"num_tot\": 4,\n                        \"roc_auc\": 0,\n                        \"f1\": 0,\n                        \"concept\": {\n                            \"id\": \"test-concept\",\n                            \"name\": \"test-concept\",\n                            \"value\": 1,\n                            \"app_id\": \"test-app-1701866015\"\n                        },\n                        \"roc_curve\": {\n                            \"fpr\": [\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5\n                            ],\n                            \"tpr\": [\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"precision_recall_curve\": {\n                            \"recall\": [\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5\n                            ],\n                            \"precision\": [\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5,\n                                0.5\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"avg_precision\": 0.25,\n                        \"area_name\": \"large\",\n                        \"area_min\": 9216,\n                        \"area_max\": 10000000000,\n                        \"iou\": 0.9\n                    },\n                    {\n                        \"num_pos\": 2,\n                        \"num_neg\": 0,\n                        \"num_tot\": 2,\n                        \"roc_auc\": 0,\n                        \"f1\": 0,\n                        \"concept\": {\n                            \"id\": \"test-concept-1\",\n                            \"name\": \"test-concept-1\",\n                            \"value\": 1,\n                            \"app_id\": \"test-app-1701866015\"\n                        },\n                        \"roc_curve\": {\n                            \"fpr\": [\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1,\n                                1\n                            ],\n                            \"tpr\": [\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"precision_recall_curve\": {\n                            \"recall\": [\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0\n                            ],\n                            \"precision\": [\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0,\n                                0\n                            ],\n                            \"thresholds\": [\n                                0,\n                                0.01,\n                                0.02,\n                                0.03,\n                                0.04,\n                                0.05,\n                                0.06,\n                                0.07,\n                                0.08,\n                                0.09,\n                                0.1,\n                                0.11,\n                                0.12,\n                                0.13,\n                                0.14,\n                                0.15,\n                                0.16,\n                                0.17,\n                                0.18,\n                                0.19,\n                                0.2,\n                                0.21,\n                                0.22,\n                                0.23,\n                                0.24,\n                                0.25,\n                                0.26,\n                                0.27,\n                                0.28,\n                                0.29,\n                                0.3,\n                                0.31,\n                                0.32,\n                                0.33,\n                                0.34,\n                                0.35,\n                                0.36,\n                                0.37,\n                                0.38,\n                                0.39,\n                                0.4,\n                                0.41,\n                                0.42,\n                                0.43,\n                                0.44,\n                                0.45,\n                                0.46,\n                                0.47,\n                                0.48,\n                                0.49,\n                                0.5,\n                                0.51,\n                                0.52,\n                                0.53,\n                                0.54,\n                                0.55,\n                                0.56,\n                                0.57,\n                                0.58,\n                                0.59,\n                                0.6,\n                                0.61,\n                                0.62,\n                                0.63,\n                                0.64,\n                                0.65,\n                                0.66,\n                                0.67,\n                                0.68,\n                                0.69,\n                                0.7,\n                                0.71,\n                                0.72,\n                                0.73,\n                                0.74,\n                                0.75,\n                                0.76,\n                                0.77,\n                                0.78,\n                                0.79,\n                                0.8,\n                                0.81,\n                                0.82,\n                                0.83,\n                                0.84,\n                                0.85,\n                                0.86,\n                                0.87,\n                                0.88,\n                                0.89,\n                                0.9,\n                                0.91,\n                                0.92,\n                                0.93,\n                                0.94,\n                                0.95,\n                                0.96,\n                                0.97,\n                                0.98,\n                                0.99,\n                                1\n                            ]\n                        },\n                        \"area_name\": \"large\",\n                        \"area_min\": 9216,\n                        \"area_max\": 10000000000,\n                        \"iou\": 0.9\n                    }\n                ]\n            },\n            \"data\": {\n                \"concepts\": [\n                    {\n                        \"id\": \"test-concept\",\n                        \"value\": 0\n                    },\n                    {\n                        \"id\": \"test-concept-1\",\n                        \"value\": 0\n                    }\n                ]\n            },\n            \"active_concept_count\": 2\n        }\n    ]\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/annotations/searches/metrics/{annotation_search_metrics_id}' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      },
      "delete": {
        "tags": [
          "Search > Metrics"
        ],
        "summary": "Delete Annotation Search Metrics By ID",
        "description": "This request is used to remove a specific annotation search metric by proving a unique ID.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `annotation_search_metrics_id` | **string** | **Stores the annotation metrics ID** |",
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "annotation_search_metrics_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"c74340f575585712404ef0185832b8c7\"\n    }\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff --request DELETE 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/annotations/searches/metrics/{annotation_search_metrics_id}' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/workflows": {
      "get": {
        "tags": [
          "Workflows > Workflow Essentials"
        ],
        "summary": "List WorkFlows",
        "description": "The `List Workflows` allows the users to get all available workflows in an app.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application name** |",
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "examples": {
                  "example-0": {
                    "summary": "List WFs (Session Token)",
                    "value": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"6e34ac298cc2273fdf14442767469223\"\n    },\n    \"workflows\": [\n        {\n            \"id\": \"firehose3\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"created_at\": \"2023-11-27T13:48:37.837135Z\",\n            \"nodes\": [\n                {\n                    \"id\": \"Face-V3.0-Embedding\",\n                    \"model\": {\n                        \"id\": \"face-detection\",\n                        \"name\": \"Face\",\n                        \"app_id\": \"main\",\n                        \"model_version\": {\n                            \"id\": \"fe995da8cb73490f8556416ecf25cea3\"\n                        },\n                        \"display_name\": \"Face-visual-detector\",\n                        \"user_id\": \"clarifai\",\n                        \"model_type_id\": \"visual-detector\",\n                        \"toolkits\": [],\n                        \"use_cases\": [],\n                        \"languages\": [],\n                        \"languages_full\": [],\n                        \"check_consents\": [\n                            \"pii\"\n                        ]\n                    },\n                    \"output_info_override\": {}\n                },\n                {\n                    \"id\": \"Face-V3.0-Cluster\",\n                    \"model\": {\n                        \"id\": \"face-detection\",\n                        \"name\": \"Face\",\n                        \"app_id\": \"main\",\n                        \"model_version\": {\n                            \"id\": \"5e026c5fae004ed4a83263ebaabec49e\"\n                        },\n                        \"display_name\": \"Face-visual-detector\",\n                        \"user_id\": \"clarifai\",\n                        \"model_type_id\": \"visual-detector\",\n                        \"toolkits\": [],\n                        \"use_cases\": [],\n                        \"languages\": [],\n                        \"languages_full\": [],\n                        \"check_consents\": [\n                            \"pii\"\n                        ]\n                    },\n                    \"output_info_override\": {}\n                }\n            ],\n            \"metadata\": {},\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"modified_at\": \"2023-11-27T13:48:37.837135Z\",\n            \"version\": {\n                \"id\": \"96d9f8b3f36d460dbe5aef3d34ea7c6a\"\n            },\n            \"use_cases\": [],\n            \"check_consents\": [\n                \"pii\"\n            ]\n        },\n        {\n            \"id\": \"firehose\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"created_at\": \"2023-11-22T13:51:08.379477Z\",\n            \"nodes\": [\n                {\n                    \"id\": \"Face-V3.0-Embedding\",\n                    \"model\": {\n                        \"id\": \"face-detection\",\n                        \"name\": \"Face\",\n                        \"app_id\": \"main\",\n                        \"model_version\": {\n                            \"id\": \"fe995da8cb73490f8556416ecf25cea3\"\n                        },\n                        \"display_name\": \"Face-visual-detector\",\n                        \"user_id\": \"clarifai\",\n                        \"model_type_id\": \"visual-detector\",\n                        \"toolkits\": [],\n                        \"use_cases\": [],\n                        \"languages\": [],\n                        \"languages_full\": [],\n                        \"check_consents\": [\n                            \"pii\"\n                        ]\n                    },\n                    \"output_info_override\": {}\n                },\n                {\n                    \"id\": \"Face-V3.0-Cluster\",\n                    \"model\": {\n                        \"id\": \"face-detection\",\n                        \"name\": \"Face\",\n                        \"app_id\": \"main\",\n                        \"model_version\": {\n                            \"id\": \"5e026c5fae004ed4a83263ebaabec49e\"\n                        },\n                        \"display_name\": \"Face-visual-detector\",\n                        \"user_id\": \"clarifai\",\n                        \"model_type_id\": \"visual-detector\",\n                        \"toolkits\": [],\n                        \"use_cases\": [],\n                        \"languages\": [],\n                        \"languages_full\": [],\n                        \"check_consents\": [\n                            \"pii\"\n                        ]\n                    },\n                    \"output_info_override\": {}\n                }\n            ],\n            \"metadata\": {},\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"modified_at\": \"2023-11-27T13:44:59.084118Z\",\n            \"version\": {\n                \"id\": \"cca4ff16c8f747dda03bbe48a07f30d9\"\n            },\n            \"use_cases\": [],\n            \"check_consents\": [\n                \"pii\"\n            ]\n        },\n        {\n            \"id\": \"firehose2\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"created_at\": \"2023-11-27T13:43:38.033989Z\",\n            \"nodes\": [\n                {\n                    \"id\": \"general-concept\",\n                    \"model\": {\n                        \"id\": \"general-image-recognition\",\n                        \"name\": \"Image Recognition\",\n                        \"app_id\": \"main\",\n                        \"model_version\": {\n                            \"id\": \"aa7f35c01e0642fda5cf400f543e7c40\"\n                        },\n                        \"display_name\": \"general-visual-classifier\",\n                        \"user_id\": \"clarifai\",\n                        \"model_type_id\": \"visual-classifier\",\n                        \"toolkits\": [],\n                        \"use_cases\": [],\n                        \"languages\": [],\n                        \"languages_full\": [],\n                        \"check_consents\": []\n                    },\n                    \"output_info_override\": {}\n                }\n            ],\n            \"metadata\": {},\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"modified_at\": \"2023-11-27T13:43:38.033989Z\",\n            \"version\": {\n                \"id\": \"8b434732b93742688ecd419b8577df4a\"\n            },\n            \"use_cases\": [],\n            \"check_consents\": []\n        },\n        {\n            \"id\": \"workflow-e67492\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"created_at\": \"2023-11-27T10:22:38.456279Z\",\n            \"nodes\": [\n                {\n                    \"id\": \"general-concept\",\n                    \"model\": {\n                        \"id\": \"general-image-recognition\",\n                        \"name\": \"Image Recognition\",\n                        \"app_id\": \"main\",\n                        \"model_version\": {\n                            \"id\": \"aa7f35c01e0642fda5cf400f543e7c40\"\n                        },\n                        \"display_name\": \"general-visual-classifier\",\n                        \"user_id\": \"clarifai\",\n                        \"model_type_id\": \"visual-classifier\",\n                        \"toolkits\": [],\n                        \"use_cases\": [],\n                        \"languages\": [],\n                        \"languages_full\": [],\n                        \"check_consents\": []\n                    },\n                    \"output_info_override\": {\n                        \"params\": {}\n                    }\n                }\n            ],\n            \"metadata\": {},\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"modified_at\": \"2023-11-27T10:22:38.456279Z\",\n            \"version\": {\n                \"id\": \"548aa1a3ed9d47c9a09f284fafb503bd\"\n            },\n            \"use_cases\": [],\n            \"check_consents\": []\n        },\n        {\n            \"id\": \"firehose1\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"created_at\": \"2023-11-27T10:18:06.645262Z\",\n            \"nodes\": [\n                {\n                    \"id\": \"general-concept\",\n                    \"model\": {\n                        \"id\": \"general-image-recognition\",\n                        \"name\": \"Image Recognition\",\n                        \"app_id\": \"main\",\n                        \"model_version\": {\n                            \"id\": \"aa7f35c01e0642fda5cf400f543e7c40\"\n                        },\n                        \"display_name\": \"general-visual-classifier\",\n                        \"user_id\": \"clarifai\",\n                        \"model_type_id\": \"visual-classifier\",\n                        \"toolkits\": [],\n                        \"use_cases\": [],\n                        \"languages\": [],\n                        \"languages_full\": [],\n                        \"check_consents\": []\n                    },\n                    \"output_info_override\": {}\n                }\n            ],\n            \"metadata\": {},\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"modified_at\": \"2023-11-27T10:18:06.645262Z\",\n            \"version\": {\n                \"id\": \"9d6948a5ba1f4753b450c1a80ff05b10\"\n            },\n            \"use_cases\": [],\n            \"check_consents\": []\n        },\n        {\n            \"id\": \"workflow-8875de\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"created_at\": \"2023-11-23T08:02:57.000421Z\",\n            \"nodes\": [\n                {\n                    \"id\": \"node-id-1\",\n                    \"model\": {\n                        \"id\": \"apparel-recognition\",\n                        \"name\": \"apparel\",\n                        \"app_id\": \"main\",\n                        \"model_version\": {\n                            \"id\": \"dc2cd6d9bff5425a80bfe0c4105583c1\"\n                        },\n                        \"display_name\": \"apparel-visual-classifier\",\n                        \"user_id\": \"clarifai\",\n                        \"model_type_id\": \"visual-classifier\",\n                        \"toolkits\": [],\n                        \"use_cases\": [],\n                        \"languages\": [],\n                        \"languages_full\": [],\n                        \"check_consents\": []\n                    },\n                    \"output_info_override\": {}\n                }\n            ],\n            \"metadata\": {},\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"modified_at\": \"2023-11-23T08:03:21.309440Z\",\n            \"version\": {\n                \"id\": \"993473e875e74d539f1a498a2fa48f19\"\n            },\n            \"use_cases\": [],\n            \"check_consents\": []\n        },\n        {\n            \"id\": \"apparel\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"created_at\": \"2023-11-23T08:02:37.384778Z\",\n            \"nodes\": [\n                {\n                    \"id\": \"node-id-1\",\n                    \"model\": {\n                        \"id\": \"apparel-recognition\",\n                        \"name\": \"apparel\",\n                        \"app_id\": \"main\",\n                        \"model_version\": {\n                            \"id\": \"dc2cd6d9bff5425a80bfe0c4105583c1\"\n                        },\n                        \"display_name\": \"apparel-visual-classifier\",\n                        \"user_id\": \"clarifai\",\n                        \"model_type_id\": \"visual-classifier\",\n                        \"toolkits\": [],\n                        \"use_cases\": [],\n                        \"languages\": [],\n                        \"languages_full\": [],\n                        \"check_consents\": []\n                    },\n                    \"output_info_override\": {}\n                }\n            ],\n            \"metadata\": {},\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"modified_at\": \"2023-11-23T08:02:41.288891Z\",\n            \"version\": {\n                \"id\": \"ee5849474a7646b3999425bb0c484864\"\n            },\n            \"use_cases\": [],\n            \"check_consents\": []\n        },\n        {\n            \"id\": \"General\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"created_at\": \"2023-11-23T07:21:41.046596Z\",\n            \"nodes\": [\n                {\n                    \"id\": \"general-v1.5-concept\",\n                    \"model\": {\n                        \"id\": \"general-image-recognition\",\n                        \"name\": \"Image Recognition\",\n                        \"app_id\": \"main\",\n                        \"model_version\": {\n                            \"id\": \"aa7f35c01e0642fda5cf400f543e7c40\"\n                        },\n                        \"display_name\": \"general-visual-classifier\",\n                        \"user_id\": \"clarifai\",\n                        \"model_type_id\": \"visual-classifier\",\n                        \"toolkits\": [],\n                        \"use_cases\": [],\n                        \"languages\": [],\n                        \"languages_full\": [],\n                        \"check_consents\": []\n                    },\n                    \"output_info_override\": {}\n                },\n                {\n                    \"id\": \"general-v1.5-embed\",\n                    \"model\": {\n                        \"id\": \"general-image-embedding\",\n                        \"name\": \"general\",\n                        \"app_id\": \"main\",\n                        \"model_version\": {\n                            \"id\": \"bb186755eda04f9cbb6fe32e816be104\"\n                        },\n                        \"display_name\": \"general-visual-embedder\",\n                        \"user_id\": \"clarifai\",\n                        \"model_type_id\": \"visual-embedder\",\n                        \"toolkits\": [],\n                        \"use_cases\": [],\n                        \"languages\": [],\n                        \"languages_full\": [],\n                        \"check_consents\": []\n                    },\n                    \"output_info_override\": {}\n                },\n                {\n                    \"id\": \"general-v1.5-cluster\",\n                    \"model\": {\n                        \"id\": \"general-clusterering\",\n                        \"name\": \"general\",\n                        \"app_id\": \"main\",\n                        \"model_version\": {\n                            \"id\": \"cc2074cff6dc4c02b6f4e1b8606dcb54\"\n                        },\n                        \"display_name\": \"general-clusterer\",\n                        \"user_id\": \"clarifai\",\n                        \"model_type_id\": \"clusterer\",\n                        \"toolkits\": [],\n                        \"use_cases\": [],\n                        \"languages\": [],\n                        \"languages_full\": [],\n                        \"check_consents\": []\n                    },\n                    \"node_inputs\": [\n                        {\n                            \"node_id\": \"general-v1.5-embed\"\n                        }\n                    ],\n                    \"output_info_override\": {}\n                }\n            ],\n            \"metadata\": {},\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"modified_at\": \"2023-11-23T07:21:41.046596Z\",\n            \"version\": {\n                \"id\": \"704ea3fd35c04d44b3084d921d8dde34\"\n            },\n            \"use_cases\": [],\n            \"check_consents\": []\n        },\n        {\n            \"id\": \"some\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"created_at\": \"2023-11-23T07:10:11.770527Z\",\n            \"nodes\": [\n                {\n                    \"id\": \"node-id-1\",\n                    \"model\": {\n                        \"id\": \"general-image-detection\",\n                        \"name\": \"Image Detection\",\n                        \"app_id\": \"main\",\n                        \"model_version\": {\n                            \"id\": \"e68c9e00b9db49e2b5ba13934dc4a5ec\"\n                        },\n                        \"user_id\": \"clarifai\",\n                        \"model_type_id\": \"visual-detector\",\n                        \"toolkits\": [],\n                        \"use_cases\": [],\n                        \"languages\": [],\n                        \"languages_full\": [],\n                        \"check_consents\": []\n                    },\n                    \"output_info_override\": {}\n                }\n            ],\n            \"metadata\": {},\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"modified_at\": \"2023-11-23T07:10:36.254917Z\",\n            \"version\": {\n                \"id\": \"b41b3b3dc9c2445791c86e651d539bc2\"\n            },\n            \"use_cases\": [],\n            \"check_consents\": []\n        },\n        {\n            \"id\": \"Empty\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"created_at\": \"2023-11-22T07:36:15.533238Z\",\n            \"metadata\": {},\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"modified_at\": \"2023-11-22T07:36:15.533238Z\",\n            \"version\": {\n                \"id\": \"9c6ae53705374722b7ff8ac78130b97b\"\n            },\n            \"use_cases\": [],\n            \"check_consents\": []\n        }\n    ]\n}"
                  },
                  "example-1": {
                    "summary": "Get All Base Workflows",
                    "value": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"7ea76b4c60e7940405312fd4411079c9\"\n    },\n    \"workflows\": [\n        {\n            \"id\": \"firehose3\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"created_at\": \"2023-11-27T13:48:37.837135Z\",\n            \"nodes\": [\n                {\n                    \"id\": \"Face-V3.0-Embedding\",\n                    \"model\": {\n                        \"id\": \"face-detection\",\n                        \"name\": \"Face\",\n                        \"app_id\": \"main\",\n                        \"model_version\": {\n                            \"id\": \"fe995da8cb73490f8556416ecf25cea3\"\n                        },\n                        \"display_name\": \"Face-visual-detector\",\n                        \"user_id\": \"clarifai\",\n                        \"model_type_id\": \"visual-detector\",\n                        \"toolkits\": [],\n                        \"use_cases\": [],\n                        \"languages\": [],\n                        \"languages_full\": [],\n                        \"check_consents\": [\n                            \"pii\"\n                        ]\n                    },\n                    \"output_info_override\": {}\n                },\n                {\n                    \"id\": \"Face-V3.0-Cluster\",\n                    \"model\": {\n                        \"id\": \"face-detection\",\n                        \"name\": \"Face\",\n                        \"app_id\": \"main\",\n                        \"model_version\": {\n                            \"id\": \"5e026c5fae004ed4a83263ebaabec49e\"\n                        },\n                        \"display_name\": \"Face-visual-detector\",\n                        \"user_id\": \"clarifai\",\n                        \"model_type_id\": \"visual-detector\",\n                        \"toolkits\": [],\n                        \"use_cases\": [],\n                        \"languages\": [],\n                        \"languages_full\": [],\n                        \"check_consents\": [\n                            \"pii\"\n                        ]\n                    },\n                    \"output_info_override\": {}\n                }\n            ],\n            \"metadata\": {},\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"modified_at\": \"2023-11-27T13:48:37.837135Z\",\n            \"version\": {\n                \"id\": \"96d9f8b3f36d460dbe5aef3d34ea7c6a\"\n            },\n            \"use_cases\": [],\n            \"check_consents\": [\n                \"pii\"\n            ]\n        },\n        {\n            \"id\": \"firehose\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"created_at\": \"2023-11-22T13:51:08.379477Z\",\n            \"nodes\": [\n                {\n                    \"id\": \"Face-V3.0-Embedding\",\n                    \"model\": {\n                        \"id\": \"face-detection\",\n                        \"name\": \"Face\",\n                        \"app_id\": \"main\",\n                        \"model_version\": {\n                            \"id\": \"fe995da8cb73490f8556416ecf25cea3\"\n                        },\n                        \"display_name\": \"Face-visual-detector\",\n                        \"user_id\": \"clarifai\",\n                        \"model_type_id\": \"visual-detector\",\n                        \"toolkits\": [],\n                        \"use_cases\": [],\n                        \"languages\": [],\n                        \"languages_full\": [],\n                        \"check_consents\": [\n                            \"pii\"\n                        ]\n                    },\n                    \"output_info_override\": {}\n                },\n                {\n                    \"id\": \"Face-V3.0-Cluster\",\n                    \"model\": {\n                        \"id\": \"face-detection\",\n                        \"name\": \"Face\",\n                        \"app_id\": \"main\",\n                        \"model_version\": {\n                            \"id\": \"5e026c5fae004ed4a83263ebaabec49e\"\n                        },\n                        \"display_name\": \"Face-visual-detector\",\n                        \"user_id\": \"clarifai\",\n                        \"model_type_id\": \"visual-detector\",\n                        \"toolkits\": [],\n                        \"use_cases\": [],\n                        \"languages\": [],\n                        \"languages_full\": [],\n                        \"check_consents\": [\n                            \"pii\"\n                        ]\n                    },\n                    \"output_info_override\": {}\n                }\n            ],\n            \"metadata\": {},\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"modified_at\": \"2023-11-27T13:44:59.084118Z\",\n            \"version\": {\n                \"id\": \"cca4ff16c8f747dda03bbe48a07f30d9\"\n            },\n            \"use_cases\": [],\n            \"check_consents\": [\n                \"pii\"\n            ]\n        },\n        {\n            \"id\": \"firehose2\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"created_at\": \"2023-11-27T13:43:38.033989Z\",\n            \"nodes\": [\n                {\n                    \"id\": \"general-concept\",\n                    \"model\": {\n                        \"id\": \"general-image-recognition\",\n                        \"name\": \"Image Recognition\",\n                        \"app_id\": \"main\",\n                        \"model_version\": {\n                            \"id\": \"aa7f35c01e0642fda5cf400f543e7c40\"\n                        },\n                        \"display_name\": \"general-visual-classifier\",\n                        \"user_id\": \"clarifai\",\n                        \"model_type_id\": \"visual-classifier\",\n                        \"toolkits\": [],\n                        \"use_cases\": [],\n                        \"languages\": [],\n                        \"languages_full\": [],\n                        \"check_consents\": []\n                    },\n                    \"output_info_override\": {}\n                }\n            ],\n            \"metadata\": {},\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"modified_at\": \"2023-11-27T13:43:38.033989Z\",\n            \"version\": {\n                \"id\": \"8b434732b93742688ecd419b8577df4a\"\n            },\n            \"use_cases\": [],\n            \"check_consents\": []\n        },\n        {\n            \"id\": \"workflow-e67492\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"created_at\": \"2023-11-27T10:22:38.456279Z\",\n            \"nodes\": [\n                {\n                    \"id\": \"general-concept\",\n                    \"model\": {\n                        \"id\": \"general-image-recognition\",\n                        \"name\": \"Image Recognition\",\n                        \"app_id\": \"main\",\n                        \"model_version\": {\n                            \"id\": \"aa7f35c01e0642fda5cf400f543e7c40\"\n                        },\n                        \"display_name\": \"general-visual-classifier\",\n                        \"user_id\": \"clarifai\",\n                        \"model_type_id\": \"visual-classifier\",\n                        \"toolkits\": [],\n                        \"use_cases\": [],\n                        \"languages\": [],\n                        \"languages_full\": [],\n                        \"check_consents\": []\n                    },\n                    \"output_info_override\": {\n                        \"params\": {}\n                    }\n                }\n            ],\n            \"metadata\": {},\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"modified_at\": \"2023-11-27T10:22:38.456279Z\",\n            \"version\": {\n                \"id\": \"548aa1a3ed9d47c9a09f284fafb503bd\"\n            },\n            \"use_cases\": [],\n            \"check_consents\": []\n        },\n        {\n            \"id\": \"firehose1\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"created_at\": \"2023-11-27T10:18:06.645262Z\",\n            \"nodes\": [\n                {\n                    \"id\": \"general-concept\",\n                    \"model\": {\n                        \"id\": \"general-image-recognition\",\n                        \"name\": \"Image Recognition\",\n                        \"app_id\": \"main\",\n                        \"model_version\": {\n                            \"id\": \"aa7f35c01e0642fda5cf400f543e7c40\"\n                        },\n                        \"display_name\": \"general-visual-classifier\",\n                        \"user_id\": \"clarifai\",\n                        \"model_type_id\": \"visual-classifier\",\n                        \"toolkits\": [],\n                        \"use_cases\": [],\n                        \"languages\": [],\n                        \"languages_full\": [],\n                        \"check_consents\": []\n                    },\n                    \"output_info_override\": {}\n                }\n            ],\n            \"metadata\": {},\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"modified_at\": \"2023-11-27T10:18:06.645262Z\",\n            \"version\": {\n                \"id\": \"9d6948a5ba1f4753b450c1a80ff05b10\"\n            },\n            \"use_cases\": [],\n            \"check_consents\": []\n        },\n        {\n            \"id\": \"workflow-8875de\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"created_at\": \"2023-11-23T08:02:57.000421Z\",\n            \"nodes\": [\n                {\n                    \"id\": \"node-id-1\",\n                    \"model\": {\n                        \"id\": \"apparel-recognition\",\n                        \"name\": \"apparel\",\n                        \"app_id\": \"main\",\n                        \"model_version\": {\n                            \"id\": \"dc2cd6d9bff5425a80bfe0c4105583c1\"\n                        },\n                        \"display_name\": \"apparel-visual-classifier\",\n                        \"user_id\": \"clarifai\",\n                        \"model_type_id\": \"visual-classifier\",\n                        \"toolkits\": [],\n                        \"use_cases\": [],\n                        \"languages\": [],\n                        \"languages_full\": [],\n                        \"check_consents\": []\n                    },\n                    \"output_info_override\": {}\n                }\n            ],\n            \"metadata\": {},\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"modified_at\": \"2023-11-23T08:03:21.309440Z\",\n            \"version\": {\n                \"id\": \"993473e875e74d539f1a498a2fa48f19\"\n            },\n            \"use_cases\": [],\n            \"check_consents\": []\n        },\n        {\n            \"id\": \"apparel\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"created_at\": \"2023-11-23T08:02:37.384778Z\",\n            \"nodes\": [\n                {\n                    \"id\": \"node-id-1\",\n                    \"model\": {\n                        \"id\": \"apparel-recognition\",\n                        \"name\": \"apparel\",\n                        \"app_id\": \"main\",\n                        \"model_version\": {\n                            \"id\": \"dc2cd6d9bff5425a80bfe0c4105583c1\"\n                        },\n                        \"display_name\": \"apparel-visual-classifier\",\n                        \"user_id\": \"clarifai\",\n                        \"model_type_id\": \"visual-classifier\",\n                        \"toolkits\": [],\n                        \"use_cases\": [],\n                        \"languages\": [],\n                        \"languages_full\": [],\n                        \"check_consents\": []\n                    },\n                    \"output_info_override\": {}\n                }\n            ],\n            \"metadata\": {},\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"modified_at\": \"2023-11-23T08:02:41.288891Z\",\n            \"version\": {\n                \"id\": \"ee5849474a7646b3999425bb0c484864\"\n            },\n            \"use_cases\": [],\n            \"check_consents\": []\n        },\n        {\n            \"id\": \"General\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"created_at\": \"2023-11-23T07:21:41.046596Z\",\n            \"nodes\": [\n                {\n                    \"id\": \"general-v1.5-concept\",\n                    \"model\": {\n                        \"id\": \"general-image-recognition\",\n                        \"name\": \"Image Recognition\",\n                        \"app_id\": \"main\",\n                        \"model_version\": {\n                            \"id\": \"aa7f35c01e0642fda5cf400f543e7c40\"\n                        },\n                        \"display_name\": \"general-visual-classifier\",\n                        \"user_id\": \"clarifai\",\n                        \"model_type_id\": \"visual-classifier\",\n                        \"toolkits\": [],\n                        \"use_cases\": [],\n                        \"languages\": [],\n                        \"languages_full\": [],\n                        \"check_consents\": []\n                    },\n                    \"output_info_override\": {}\n                },\n                {\n                    \"id\": \"general-v1.5-embed\",\n                    \"model\": {\n                        \"id\": \"general-image-embedding\",\n                        \"name\": \"general\",\n                        \"app_id\": \"main\",\n                        \"model_version\": {\n                            \"id\": \"bb186755eda04f9cbb6fe32e816be104\"\n                        },\n                        \"display_name\": \"general-visual-embedder\",\n                        \"user_id\": \"clarifai\",\n                        \"model_type_id\": \"visual-embedder\",\n                        \"toolkits\": [],\n                        \"use_cases\": [],\n                        \"languages\": [],\n                        \"languages_full\": [],\n                        \"check_consents\": []\n                    },\n                    \"output_info_override\": {}\n                },\n                {\n                    \"id\": \"general-v1.5-cluster\",\n                    \"model\": {\n                        \"id\": \"general-clusterering\",\n                        \"name\": \"general\",\n                        \"app_id\": \"main\",\n                        \"model_version\": {\n                            \"id\": \"cc2074cff6dc4c02b6f4e1b8606dcb54\"\n                        },\n                        \"display_name\": \"general-clusterer\",\n                        \"user_id\": \"clarifai\",\n                        \"model_type_id\": \"clusterer\",\n                        \"toolkits\": [],\n                        \"use_cases\": [],\n                        \"languages\": [],\n                        \"languages_full\": [],\n                        \"check_consents\": []\n                    },\n                    \"node_inputs\": [\n                        {\n                            \"node_id\": \"general-v1.5-embed\"\n                        }\n                    ],\n                    \"output_info_override\": {}\n                }\n            ],\n            \"metadata\": {},\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"modified_at\": \"2023-11-23T07:21:41.046596Z\",\n            \"version\": {\n                \"id\": \"704ea3fd35c04d44b3084d921d8dde34\"\n            },\n            \"use_cases\": [],\n            \"check_consents\": []\n        },\n        {\n            \"id\": \"some\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"created_at\": \"2023-11-23T07:10:11.770527Z\",\n            \"nodes\": [\n                {\n                    \"id\": \"node-id-1\",\n                    \"model\": {\n                        \"id\": \"general-image-detection\",\n                        \"name\": \"Image Detection\",\n                        \"app_id\": \"main\",\n                        \"model_version\": {\n                            \"id\": \"e68c9e00b9db49e2b5ba13934dc4a5ec\"\n                        },\n                        \"user_id\": \"clarifai\",\n                        \"model_type_id\": \"visual-detector\",\n                        \"toolkits\": [],\n                        \"use_cases\": [],\n                        \"languages\": [],\n                        \"languages_full\": [],\n                        \"check_consents\": []\n                    },\n                    \"output_info_override\": {}\n                }\n            ],\n            \"metadata\": {},\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"modified_at\": \"2023-11-23T07:10:36.254917Z\",\n            \"version\": {\n                \"id\": \"b41b3b3dc9c2445791c86e651d539bc2\"\n            },\n            \"use_cases\": [],\n            \"check_consents\": []\n        },\n        {\n            \"id\": \"Empty\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"created_at\": \"2023-11-22T07:36:15.533238Z\",\n            \"metadata\": {},\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"modified_at\": \"2023-11-22T07:36:15.533238Z\",\n            \"version\": {\n                \"id\": \"9c6ae53705374722b7ff8ac78130b97b\"\n            },\n            \"use_cases\": [],\n            \"check_consents\": []\n        }\n    ]\n}"
                  },
                  "example-2": {
                    "summary": "Get All (Public) Workflows",
                    "value": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"902125a93c5ba4e2544d4c61aeabba99\"\n    },\n    \"workflows\": [\n        {\n            \"id\": \"firehose3\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"created_at\": \"2023-11-27T13:48:37.837135Z\",\n            \"nodes\": [\n                {\n                    \"id\": \"Face-V3.0-Embedding\",\n                    \"model\": {\n                        \"id\": \"face-detection\",\n                        \"name\": \"Face\",\n                        \"app_id\": \"main\",\n                        \"model_version\": {\n                            \"id\": \"fe995da8cb73490f8556416ecf25cea3\"\n                        },\n                        \"display_name\": \"Face-visual-detector\",\n                        \"user_id\": \"clarifai\",\n                        \"model_type_id\": \"visual-detector\",\n                        \"toolkits\": [],\n                        \"use_cases\": [],\n                        \"languages\": [],\n                        \"languages_full\": [],\n                        \"check_consents\": [\n                            \"pii\"\n                        ]\n                    },\n                    \"output_info_override\": {}\n                },\n                {\n                    \"id\": \"Face-V3.0-Cluster\",\n                    \"model\": {\n                        \"id\": \"face-detection\",\n                        \"name\": \"Face\",\n                        \"app_id\": \"main\",\n                        \"model_version\": {\n                            \"id\": \"5e026c5fae004ed4a83263ebaabec49e\"\n                        },\n                        \"display_name\": \"Face-visual-detector\",\n                        \"user_id\": \"clarifai\",\n                        \"model_type_id\": \"visual-detector\",\n                        \"toolkits\": [],\n                        \"use_cases\": [],\n                        \"languages\": [],\n                        \"languages_full\": [],\n                        \"check_consents\": [\n                            \"pii\"\n                        ]\n                    },\n                    \"output_info_override\": {}\n                }\n            ],\n            \"metadata\": {},\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"modified_at\": \"2023-11-27T13:48:37.837135Z\",\n            \"version\": {\n                \"id\": \"96d9f8b3f36d460dbe5aef3d34ea7c6a\"\n            },\n            \"use_cases\": [],\n            \"check_consents\": [\n                \"pii\"\n            ]\n        },\n        {\n            \"id\": \"firehose\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"created_at\": \"2023-11-22T13:51:08.379477Z\",\n            \"nodes\": [\n                {\n                    \"id\": \"Face-V3.0-Embedding\",\n                    \"model\": {\n                        \"id\": \"face-detection\",\n                        \"name\": \"Face\",\n                        \"app_id\": \"main\",\n                        \"model_version\": {\n                            \"id\": \"fe995da8cb73490f8556416ecf25cea3\"\n                        },\n                        \"display_name\": \"Face-visual-detector\",\n                        \"user_id\": \"clarifai\",\n                        \"model_type_id\": \"visual-detector\",\n                        \"toolkits\": [],\n                        \"use_cases\": [],\n                        \"languages\": [],\n                        \"languages_full\": [],\n                        \"check_consents\": [\n                            \"pii\"\n                        ]\n                    },\n                    \"output_info_override\": {}\n                },\n                {\n                    \"id\": \"Face-V3.0-Cluster\",\n                    \"model\": {\n                        \"id\": \"face-detection\",\n                        \"name\": \"Face\",\n                        \"app_id\": \"main\",\n                        \"model_version\": {\n                            \"id\": \"5e026c5fae004ed4a83263ebaabec49e\"\n                        },\n                        \"display_name\": \"Face-visual-detector\",\n                        \"user_id\": \"clarifai\",\n                        \"model_type_id\": \"visual-detector\",\n                        \"toolkits\": [],\n                        \"use_cases\": [],\n                        \"languages\": [],\n                        \"languages_full\": [],\n                        \"check_consents\": [\n                            \"pii\"\n                        ]\n                    },\n                    \"output_info_override\": {}\n                }\n            ],\n            \"metadata\": {},\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"modified_at\": \"2023-11-27T13:44:59.084118Z\",\n            \"version\": {\n                \"id\": \"cca4ff16c8f747dda03bbe48a07f30d9\"\n            },\n            \"use_cases\": [],\n            \"check_consents\": [\n                \"pii\"\n            ]\n        },\n        {\n            \"id\": \"firehose2\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"created_at\": \"2023-11-27T13:43:38.033989Z\",\n            \"nodes\": [\n                {\n                    \"id\": \"general-concept\",\n                    \"model\": {\n                        \"id\": \"general-image-recognition\",\n                        \"name\": \"Image Recognition\",\n                        \"app_id\": \"main\",\n                        \"model_version\": {\n                            \"id\": \"aa7f35c01e0642fda5cf400f543e7c40\"\n                        },\n                        \"display_name\": \"general-visual-classifier\",\n                        \"user_id\": \"clarifai\",\n                        \"model_type_id\": \"visual-classifier\",\n                        \"toolkits\": [],\n                        \"use_cases\": [],\n                        \"languages\": [],\n                        \"languages_full\": [],\n                        \"check_consents\": []\n                    },\n                    \"output_info_override\": {}\n                }\n            ],\n            \"metadata\": {},\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"modified_at\": \"2023-11-27T13:43:38.033989Z\",\n            \"version\": {\n                \"id\": \"8b434732b93742688ecd419b8577df4a\"\n            },\n            \"use_cases\": [],\n            \"check_consents\": []\n        },\n        {\n            \"id\": \"workflow-e67492\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"created_at\": \"2023-11-27T10:22:38.456279Z\",\n            \"nodes\": [\n                {\n                    \"id\": \"general-concept\",\n                    \"model\": {\n                        \"id\": \"general-image-recognition\",\n                        \"name\": \"Image Recognition\",\n                        \"app_id\": \"main\",\n                        \"model_version\": {\n                            \"id\": \"aa7f35c01e0642fda5cf400f543e7c40\"\n                        },\n                        \"display_name\": \"general-visual-classifier\",\n                        \"user_id\": \"clarifai\",\n                        \"model_type_id\": \"visual-classifier\",\n                        \"toolkits\": [],\n                        \"use_cases\": [],\n                        \"languages\": [],\n                        \"languages_full\": [],\n                        \"check_consents\": []\n                    },\n                    \"output_info_override\": {\n                        \"params\": {}\n                    }\n                }\n            ],\n            \"metadata\": {},\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"modified_at\": \"2023-11-27T10:22:38.456279Z\",\n            \"version\": {\n                \"id\": \"548aa1a3ed9d47c9a09f284fafb503bd\"\n            },\n            \"use_cases\": [],\n            \"check_consents\": []\n        },\n        {\n            \"id\": \"firehose1\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"created_at\": \"2023-11-27T10:18:06.645262Z\",\n            \"nodes\": [\n                {\n                    \"id\": \"general-concept\",\n                    \"model\": {\n                        \"id\": \"general-image-recognition\",\n                        \"name\": \"Image Recognition\",\n                        \"app_id\": \"main\",\n                        \"model_version\": {\n                            \"id\": \"aa7f35c01e0642fda5cf400f543e7c40\"\n                        },\n                        \"display_name\": \"general-visual-classifier\",\n                        \"user_id\": \"clarifai\",\n                        \"model_type_id\": \"visual-classifier\",\n                        \"toolkits\": [],\n                        \"use_cases\": [],\n                        \"languages\": [],\n                        \"languages_full\": [],\n                        \"check_consents\": []\n                    },\n                    \"output_info_override\": {}\n                }\n            ],\n            \"metadata\": {},\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"modified_at\": \"2023-11-27T10:18:06.645262Z\",\n            \"version\": {\n                \"id\": \"9d6948a5ba1f4753b450c1a80ff05b10\"\n            },\n            \"use_cases\": [],\n            \"check_consents\": []\n        },\n        {\n            \"id\": \"workflow-8875de\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"created_at\": \"2023-11-23T08:02:57.000421Z\",\n            \"nodes\": [\n                {\n                    \"id\": \"node-id-1\",\n                    \"model\": {\n                        \"id\": \"apparel-recognition\",\n                        \"name\": \"apparel\",\n                        \"app_id\": \"main\",\n                        \"model_version\": {\n                            \"id\": \"dc2cd6d9bff5425a80bfe0c4105583c1\"\n                        },\n                        \"display_name\": \"apparel-visual-classifier\",\n                        \"user_id\": \"clarifai\",\n                        \"model_type_id\": \"visual-classifier\",\n                        \"toolkits\": [],\n                        \"use_cases\": [],\n                        \"languages\": [],\n                        \"languages_full\": [],\n                        \"check_consents\": []\n                    },\n                    \"output_info_override\": {}\n                }\n            ],\n            \"metadata\": {},\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"modified_at\": \"2023-11-23T08:03:21.309440Z\",\n            \"version\": {\n                \"id\": \"993473e875e74d539f1a498a2fa48f19\"\n            },\n            \"use_cases\": [],\n            \"check_consents\": []\n        },\n        {\n            \"id\": \"apparel\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"created_at\": \"2023-11-23T08:02:37.384778Z\",\n            \"nodes\": [\n                {\n                    \"id\": \"node-id-1\",\n                    \"model\": {\n                        \"id\": \"apparel-recognition\",\n                        \"name\": \"apparel\",\n                        \"app_id\": \"main\",\n                        \"model_version\": {\n                            \"id\": \"dc2cd6d9bff5425a80bfe0c4105583c1\"\n                        },\n                        \"display_name\": \"apparel-visual-classifier\",\n                        \"user_id\": \"clarifai\",\n                        \"model_type_id\": \"visual-classifier\",\n                        \"toolkits\": [],\n                        \"use_cases\": [],\n                        \"languages\": [],\n                        \"languages_full\": [],\n                        \"check_consents\": []\n                    },\n                    \"output_info_override\": {}\n                }\n            ],\n            \"metadata\": {},\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"modified_at\": \"2023-11-23T08:02:41.288891Z\",\n            \"version\": {\n                \"id\": \"ee5849474a7646b3999425bb0c484864\"\n            },\n            \"use_cases\": [],\n            \"check_consents\": []\n        },\n        {\n            \"id\": \"General\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"created_at\": \"2023-11-23T07:21:41.046596Z\",\n            \"nodes\": [\n                {\n                    \"id\": \"general-v1.5-concept\",\n                    \"model\": {\n                        \"id\": \"general-image-recognition\",\n                        \"name\": \"Image Recognition\",\n                        \"app_id\": \"main\",\n                        \"model_version\": {\n                            \"id\": \"aa7f35c01e0642fda5cf400f543e7c40\"\n                        },\n                        \"display_name\": \"general-visual-classifier\",\n                        \"user_id\": \"clarifai\",\n                        \"model_type_id\": \"visual-classifier\",\n                        \"toolkits\": [],\n                        \"use_cases\": [],\n                        \"languages\": [],\n                        \"languages_full\": [],\n                        \"check_consents\": []\n                    },\n                    \"output_info_override\": {}\n                },\n                {\n                    \"id\": \"general-v1.5-embed\",\n                    \"model\": {\n                        \"id\": \"general-image-embedding\",\n                        \"name\": \"general\",\n                        \"app_id\": \"main\",\n                        \"model_version\": {\n                            \"id\": \"bb186755eda04f9cbb6fe32e816be104\"\n                        },\n                        \"display_name\": \"general-visual-embedder\",\n                        \"user_id\": \"clarifai\",\n                        \"model_type_id\": \"visual-embedder\",\n                        \"toolkits\": [],\n                        \"use_cases\": [],\n                        \"languages\": [],\n                        \"languages_full\": [],\n                        \"check_consents\": []\n                    },\n                    \"output_info_override\": {}\n                },\n                {\n                    \"id\": \"general-v1.5-cluster\",\n                    \"model\": {\n                        \"id\": \"general-clusterering\",\n                        \"name\": \"general\",\n                        \"app_id\": \"main\",\n                        \"model_version\": {\n                            \"id\": \"cc2074cff6dc4c02b6f4e1b8606dcb54\"\n                        },\n                        \"display_name\": \"general-clusterer\",\n                        \"user_id\": \"clarifai\",\n                        \"model_type_id\": \"clusterer\",\n                        \"toolkits\": [],\n                        \"use_cases\": [],\n                        \"languages\": [],\n                        \"languages_full\": [],\n                        \"check_consents\": []\n                    },\n                    \"node_inputs\": [\n                        {\n                            \"node_id\": \"general-v1.5-embed\"\n                        }\n                    ],\n                    \"output_info_override\": {}\n                }\n            ],\n            \"metadata\": {},\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"modified_at\": \"2023-11-23T07:21:41.046596Z\",\n            \"version\": {\n                \"id\": \"704ea3fd35c04d44b3084d921d8dde34\"\n            },\n            \"use_cases\": [],\n            \"check_consents\": []\n        },\n        {\n            \"id\": \"some\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"created_at\": \"2023-11-23T07:10:11.770527Z\",\n            \"nodes\": [\n                {\n                    \"id\": \"node-id-1\",\n                    \"model\": {\n                        \"id\": \"general-image-detection\",\n                        \"name\": \"Image Detection\",\n                        \"app_id\": \"main\",\n                        \"model_version\": {\n                            \"id\": \"e68c9e00b9db49e2b5ba13934dc4a5ec\"\n                        },\n                        \"user_id\": \"clarifai\",\n                        \"model_type_id\": \"visual-detector\",\n                        \"toolkits\": [],\n                        \"use_cases\": [],\n                        \"languages\": [],\n                        \"languages_full\": [],\n                        \"check_consents\": []\n                    },\n                    \"output_info_override\": {}\n                }\n            ],\n            \"metadata\": {},\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"modified_at\": \"2023-11-23T07:10:36.254917Z\",\n            \"version\": {\n                \"id\": \"b41b3b3dc9c2445791c86e651d539bc2\"\n            },\n            \"use_cases\": [],\n            \"check_consents\": []\n        },\n        {\n            \"id\": \"Empty\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"created_at\": \"2023-11-22T07:36:15.533238Z\",\n            \"metadata\": {},\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"modified_at\": \"2023-11-22T07:36:15.533238Z\",\n            \"version\": {\n                \"id\": \"9c6ae53705374722b7ff8ac78130b97b\"\n            },\n            \"use_cases\": [],\n            \"check_consents\": []\n        }\n    ]\n}"
                  }
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/workflows' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      },
      "post": {
        "tags": [
          "Walkthroughs > RAG"
        ],
        "summary": "Create RAG Workflow",
        "description": "Using this request you can create a RAG workflow by specifying the required nodes.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application ID** |\n| `workflow_id` | **string** | **Stores the Workflow ID** |\n| `nodes-id` | string | **Stores the Node ID** |\n| `model-id` | string | **Stores the Model ID** |\n| `model_version` | string | **Stores the Model version.** |",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "example": {
                  "workflows": [
                    {
                      "id": "rag-workflow",
                      "nodes": [
                        {
                          "id": "rag_prompter",
                          "model": {
                            "id": "{{model_id}}",
                            "model_version": {
                              "id": "{{version_id}}"
                            }
                          }
                        },
                        {
                          "id": "llm",
                          "node_inputs": [
                            {
                              "node_id": "rag_prompter"
                            }
                          ],
                          "model": {
                            "id": "mistral-7B-Instruct",
                            "user_id": "mistralai",
                            "app_id": "completion",
                            "model_version": {
                              "id": "2d48077b457e4a6d899ca48a89fa91d3"
                            }
                          }
                        }
                      ]
                    }
                  ]
                }
              }
            }
          }
        },
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"8c11523d25b6050ee413fa34fd3ecf68\"\n    },\n    \"workflows\": [\n        {\n            \"id\": \"rag-workflow\",\n            \"app_id\": \"rag_app\",\n            \"created_at\": \"2024-04-29T12:53:24.361459443Z\",\n            \"nodes\": [\n                {\n                    \"id\": \"rag_prompter\",\n                    \"model\": {\n                        \"id\": \"rag_prompter\",\n                        \"name\": \"rag_prompter\",\n                        \"app_id\": \"rag_app\",\n                        \"model_version\": {\n                            \"id\": \"b94181fa354d43ac8215f3568bb9c741\"\n                        },\n                        \"user_id\": \"8tzpjy1a841y\",\n                        \"model_type_id\": \"rag-prompter\",\n                        \"toolkits\": [],\n                        \"use_cases\": [],\n                        \"languages\": [],\n                        \"languages_full\": [],\n                        \"check_consents\": []\n                    },\n                    \"output_info_override\": {}\n                },\n                {\n                    \"id\": \"llm\",\n                    \"model\": {\n                        \"id\": \"mistral-7B-Instruct\",\n                        \"name\": \"mistral-7B-Instruct\",\n                        \"app_id\": \"completion\",\n                        \"model_version\": {\n                            \"id\": \"2d48077b457e4a6d899ca48a89fa91d3\"\n                        },\n                        \"user_id\": \"mistralai\",\n                        \"model_type_id\": \"text-to-text\",\n                        \"toolkits\": [],\n                        \"use_cases\": [],\n                        \"languages\": [],\n                        \"languages_full\": [],\n                        \"check_consents\": []\n                    },\n                    \"node_inputs\": [\n                        {\n                            \"node_id\": \"rag_prompter\"\n                        }\n                    ],\n                    \"output_info_override\": {}\n                }\n            ],\n            \"metadata\": {},\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"8tzpjy1a841y\",\n            \"modified_at\": \"2024-04-29T12:53:24.361459443Z\",\n            \"version\": {\n                \"id\": \"a133ce20024847428d2f5cfd6fe78fdb\"\n            },\n            \"use_cases\": [],\n            \"check_consents\": []\n        }\n    ]\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/workflows' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Content-Type: application/json' \\\n--data '{\n  \"workflows\": [\n    {\n      \"id\": \"rag-workflow\",\n      \"nodes\": [\n        {\n          \"id\": \"rag_prompter\",\n          \"model\": {\n            \"id\": \"{{model_id}}\",\n            \"model_version\": {\n              \"id\": \"{{version_id}}\"\n            }\n          }\n        },\n        {\n          \"id\": \"llm\",\n          \"node_inputs\": [\n            {\n              \"node_id\": \"rag_prompter\"\n            }\n          ],\n          \"model\": {\n            \"id\": \"mistral-7B-Instruct\",\n            \"user_id\": \"mistralai\",\n            \"app_id\": \"completion\",\n            \"model_version\": {\n              \"id\": \"2d48077b457e4a6d899ca48a89fa91d3\"\n            }\n          }\n        }\n      ]\n    }\n  ]\n}'"
          }
        ]
      },
      "patch": {
        "tags": [
          "Workflows > Workflow Essentials"
        ],
        "summary": "Patch Workflows",
        "description": "The `Patch Workflows` endpoint allows users to update existing workflows within an app.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application name** |\n| `workflow_id` | **string** | **Stores the Workflow ID** |\n| `nodes-id` | string | **Stores the Node ID** |\n| `model-id` | string | **Stores the Model ID** |\n| `model_version` | string | **Stores the Model version.** |",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "example": {
                  "workflows": [
                    {
                      "id": "firehose",
                      "nodes": [
                        {
                          "id": "Face-V3.0-Embedding",
                          "model": {
                            "id": "face-detection",
                            "model_version": {
                              "id": "fe995da8cb73490f8556416ecf25cea3"
                            }
                          }
                        },
                        {
                          "id": "Face-V3.0-Cluster",
                          "model": {
                            "id": "face-detection",
                            "model_version": {
                              "id": "5e026c5fae004ed4a83263ebaabec49e"
                            }
                          }
                        }
                      ]
                    }
                  ],
                  "action": "overwrite"
                }
              }
            }
          }
        },
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"7fad18cd312f980f65f7ebd7a0dc6f61\"\n    },\n    \"workflows\": [\n        {\n            \"id\": \"firehose\",\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"created_at\": \"2023-11-22T13:51:08.379477Z\",\n            \"nodes\": [\n                {\n                    \"id\": \"Face-V3.0-Embedding\",\n                    \"model\": {\n                        \"id\": \"face-detection\",\n                        \"name\": \"Face\",\n                        \"app_id\": \"main\",\n                        \"model_version\": {\n                            \"id\": \"fe995da8cb73490f8556416ecf25cea3\"\n                        },\n                        \"display_name\": \"Face-visual-detector\",\n                        \"user_id\": \"clarifai\",\n                        \"model_type_id\": \"visual-detector\",\n                        \"toolkits\": [],\n                        \"use_cases\": [],\n                        \"languages\": [],\n                        \"languages_full\": [],\n                        \"check_consents\": [\n                            \"pii\"\n                        ]\n                    },\n                    \"output_info_override\": {}\n                },\n                {\n                    \"id\": \"Face-V3.0-Cluster\",\n                    \"model\": {\n                        \"id\": \"face-detection\",\n                        \"name\": \"Face\",\n                        \"app_id\": \"main\",\n                        \"model_version\": {\n                            \"id\": \"5e026c5fae004ed4a83263ebaabec49e\"\n                        },\n                        \"display_name\": \"Face-visual-detector\",\n                        \"user_id\": \"clarifai\",\n                        \"model_type_id\": \"visual-detector\",\n                        \"toolkits\": [],\n                        \"use_cases\": [],\n                        \"languages\": [],\n                        \"languages_full\": [],\n                        \"check_consents\": [\n                            \"pii\"\n                        ]\n                    },\n                    \"output_info_override\": {}\n                }\n            ],\n            \"metadata\": {},\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"user_id\": \"a0btrubbaefn\",\n            \"modified_at\": \"2023-11-27T13:44:59.084118862Z\",\n            \"version\": {\n                \"id\": \"cca4ff16c8f747dda03bbe48a07f30d9\"\n            },\n            \"use_cases\": [],\n            \"check_consents\": [\n                \"pii\"\n            ]\n        }\n    ]\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff --request PATCH 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/workflows' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Content-Type: application/json' \\\n--data '{\n  \"workflows\": [\n    {\n      \"id\": \"firehose\",\n      \"nodes\": [\n        {\n          \"id\": \"Face-V3.0-Embedding\",\n          \"model\": {\n            \"id\": \"face-detection\",\n            \"model_version\": {\n              \"id\": \"fe995da8cb73490f8556416ecf25cea3\"\n            }\n          }\n        },\n        {\n          \"id\": \"Face-V3.0-Cluster\",\n          \"model\": {\n            \"id\": \"face-detection\",\n            \"model_version\": {\n              \"id\": \"5e026c5fae004ed4a83263ebaabec49e\"\n            }\n          }\n        }\n      ]\n    }\n  ],\n  \"action\": \"overwrite\"\n}'"
          }
        ]
      },
      "delete": {
        "tags": [
          "Workflows > Workflow Essentials"
        ],
        "summary": "Delete All Workflows (async)",
        "description": "This endpoint removes all the workflows in an app.",
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"1ec06e22805e173e3e734551d2e92cdd\"\n    }\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff --request DELETE 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/workflows' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/workflows/{workflow_id}": {
      "get": {
        "tags": [
          "Workflows > Workflow Essentials"
        ],
        "summary": "Get Workflow by ID",
        "description": "The `Get Workflow` request allows users to retrieve information about a particular workflow by specifying the workflow ID.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application name** |\n| `workflow_id` | **string** | **Stores the Workflow ID** |",
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "workflow_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"9db76dd4b1381e4a3b07392f56729048\"\n    },\n    \"workflow\": {\n        \"id\": \"firehose3\",\n        \"app_id\": \"test-app-1700638575-empty\",\n        \"created_at\": \"2023-11-27T13:48:37.837135Z\",\n        \"nodes\": [\n            {\n                \"id\": \"Face-V3.0-Embedding\",\n                \"model\": {\n                    \"id\": \"face-detection\",\n                    \"name\": \"Face\",\n                    \"app_id\": \"main\",\n                    \"model_version\": {\n                        \"id\": \"fe995da8cb73490f8556416ecf25cea3\"\n                    },\n                    \"display_name\": \"Face-visual-detector\",\n                    \"user_id\": \"clarifai\",\n                    \"model_type_id\": \"visual-detector\",\n                    \"toolkits\": [],\n                    \"use_cases\": [],\n                    \"languages\": [],\n                    \"languages_full\": [],\n                    \"check_consents\": [\n                        \"pii\"\n                    ]\n                },\n                \"output_info_override\": {}\n            },\n            {\n                \"id\": \"Face-V3.0-Cluster\",\n                \"model\": {\n                    \"id\": \"face-detection\",\n                    \"name\": \"Face\",\n                    \"app_id\": \"main\",\n                    \"model_version\": {\n                        \"id\": \"5e026c5fae004ed4a83263ebaabec49e\"\n                    },\n                    \"display_name\": \"Face-visual-detector\",\n                    \"user_id\": \"clarifai\",\n                    \"model_type_id\": \"visual-detector\",\n                    \"toolkits\": [],\n                    \"use_cases\": [],\n                    \"languages\": [],\n                    \"languages_full\": [],\n                    \"check_consents\": [\n                        \"pii\"\n                    ]\n                },\n                \"output_info_override\": {}\n            }\n        ],\n        \"metadata\": {},\n        \"visibility\": {\n            \"gettable\": 10\n        },\n        \"user_id\": \"a0btrubbaefn\",\n        \"modified_at\": \"2023-11-27T13:48:37.837135Z\",\n        \"version\": {\n            \"id\": \"96d9f8b3f36d460dbe5aef3d34ea7c6a\"\n        },\n        \"use_cases\": [],\n        \"check_consents\": [\n            \"pii\"\n        ]\n    }\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/workflows/{workflow_id}' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      },
      "delete": {
        "tags": [
          "Workflows > Workflow Essentials"
        ],
        "summary": "Delete Workflow by ID",
        "description": "This request will remove a workflow with a specific ID.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application name** |\n| `workflow_id` | **string** | **Stores the Workflow ID** |",
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "workflow_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"fd402535f0a7c87edb371fcc41eed255\"\n    }\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff --request DELETE 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/workflows/{workflow_id}' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/workflows/{workflow_id}/results": {
      "post": {
        "tags": [
          "Walkthroughs > RAG"
        ],
        "summary": "RAG Workflow Predict",
        "description": "This endpoint allows users to perform prediction on an input using a workflow.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application name** |\n| `workflow_id` | **string** | **Stores the Workflow ID** |\n| `text-raw` | string | **Stores the text you want to predict** |",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "example": {
                  "inputs": [
                    {
                      "data": {
                        "text": {
                          "raw": "Summarize this PDF in less than 100 words"
                        }
                      }
                    }
                  ]
                }
              }
            }
          }
        },
        "parameters": [
          {
            "name": "Authorization",
            "in": "header",
            "schema": {
              "type": "string"
            },
            "example": "Key {{key}}"
          },
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "workflow_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"2356948cdbd6a63af9dae932964a2522\"\n    },\n    \"workflow\": {\n        \"id\": \"rag-workflow\",\n        \"app_id\": \"rag_app\",\n        \"created_at\": \"2024-04-29T12:53:24.361459Z\",\n        \"metadata\": {},\n        \"visibility\": {\n            \"gettable\": 10\n        },\n        \"user_id\": \"8tzpjy1a841y\",\n        \"modified_at\": \"2024-04-29T12:53:24.361459Z\",\n        \"version\": {\n            \"id\": \"a133ce20024847428d2f5cfd6fe78fdb\"\n        },\n        \"use_cases\": [],\n        \"check_consents\": []\n    },\n    \"results\": [\n        {\n            \"status\": {\n                \"code\": 10000,\n                \"description\": \"Ok\"\n            },\n            \"input\": {\n                \"id\": \"c899c0fe2aa44459934896bc3b4888f1\",\n                \"data\": {\n                    \"text\": {\n                        \"raw\": \"Summarize this PDF in less than 100 words\",\n                        \"url\": \"https://samples.clarifai.com/placeholder.gif\"\n                    }\n                }\n            },\n            \"outputs\": [\n                {\n                    \"id\": \"7f0528ce337c40e8a269ef2b43a5a823\",\n                    \"status\": {\n                        \"code\": 10000,\n                        \"description\": \"Ok\"\n                    },\n                    \"created_at\": \"2024-04-29T13:04:48.709413948Z\",\n                    \"model\": {\n                        \"id\": \"rag_prompter\",\n                        \"name\": \"rag_prompter\",\n                        \"created_at\": \"2024-04-29T12:36:30.474807Z\",\n                        \"modified_at\": \"2024-04-29T12:36:30.474807Z\",\n                        \"app_id\": \"rag_app\",\n                        \"model_version\": {\n                            \"id\": \"b94181fa354d43ac8215f3568bb9c741\",\n                            \"created_at\": \"2024-04-29T12:41:12.502239Z\",\n                            \"status\": {\n                                \"code\": 21100,\n                                \"description\": \"Model is trained and ready for deployment\"\n                            },\n                            \"visibility\": {\n                                \"gettable\": 10\n                            },\n                            \"app_id\": \"rag_app\",\n                            \"user_id\": \"8tzpjy1a841y\",\n                            \"metadata\": {}\n                        },\n                        \"user_id\": \"8tzpjy1a841y\",\n                        \"model_type_id\": \"rag-prompter\",\n                        \"visibility\": {\n                            \"gettable\": 10\n                        },\n                        \"toolkits\": [],\n                        \"use_cases\": [],\n                        \"languages\": [],\n                        \"languages_full\": [],\n                        \"check_consents\": [],\n                        \"workflow_recommended\": false\n                    },\n                    \"data\": {\n                        \"text\": {\n                            \"raw\": \"Context information is below:\\nMM1: Methods, Analysis & Insights from Multimodal LLM Pre-training 25\\nAppendix\\nA Dataset Details ................................................ 25\\nA.1 Interleaved Image-Text Data................................ 25\\nA.2 Text-Only Data ........................................... 25\\nA.3 Visual Instruction Tuning Data ............................. 26\\nB Training Details ............................................... 27\\nB.1 Pre-training .............................................. 27\\nB.2 Supervised Fine-tuning (SFT) .............................. 29\\nC Evaluation Details ............................................. 30\\nC.1 Pre-training Evaluation .................................... 30\\nC.2 SFT Evaluation Benchmarks................................ 30\\nC.3 SFT Evaluation Meta-Average .............................. 30\\nC.4 Additional SFT Ablations .................................. 31\\nC.5 Implementation Details for Few-shot MM1-30B-Chat .......... 32\\nD Qualitative Examples........................................... 33\\nE Author Contributions and Acknowledgements ..................... 40\\nA Dataset Details\\nA.1 Interleaved Image-Text Data\\nFollowing a process similar to OBELICS [58], we construct a dataset of 500M\\ninterleaved image-text documents, containing 1B images and 500B text tokens.\\nThese 500M documents are built from a collection of 3B HTML files described\\nin Sec. A.2. From each of the HTML files, we extract the text body layer and\\nall the <img>tags. We remove documents that have no images or more than 30\\nimages. We then download the images and insert them at their original positions\\nin the text. Finally, we perform image filtering andimage de-duplication\\nto remove low-quality and repetitive images.\\nDuring image filtering, we remove images that have corrupted bytes and/or\\nheader, aspect ratio less than 1/2 or greater than 2, are too small (less than\\n100px) or too large (larger than 10,000px), or if their URL contains logo,button,\\nicon,pluginorwidget. During image de-duplication, we remove images whose\\nURL or MD5 hash have appeared more than 10 times in the dataset. Addition-\\nally, when an image appears multiple times on a single page, we only retain its\\nfirst appearance.\\nA.2 Text-Only Data\\nFrom an initial Web corpus of 150B English HTML files, we perform boilerplate\\nremoval to arrive at the HTML representing the main content. We then follow\\nsimilar processes as GPT-3 [10] and CCNet [118] to filter out documents that\\nare too short, contain profanity, or are otherwise considered low-quality doc-\\numents. We de-duplicate the data using exact-hash matching and LSH-based\\nnear-duplicate detection. Using these methods, we arrive at 3B HTML files.\\n26 B. McKinzie et al.\\nDatasets Size Prompting Strategy\\nText-only SFT 13k–\\nLLaVA-Conv [76] 57k\\nLLaVA-Complex [76] 77k–\\nShareGPT-4V [15] 102k\\nVQAv2 [38] 83k\\n“Answer the question using a single word or\\nphrase.”GQA [46] 72k\\nOKVQA [82] 9k\\nOCRVQA [86] 80k\\nDVQA [51] 200k\\nChartQA [83] 18k\\nAI2D [52] 3k\\nDocVQA [85] 39k\\nInfoVQA [84] 24k\\nA-OKVQA [98] 66k“Answer with the option’s letter from the given\\nchoices directly.”\\nCOCO Captions [18] 83kSample from a pre-generated prompt list, e.g.,\\n“Provide a brief description of the given image.” TextCaps [103] 22k\\nSynthDog-EN [53] 500kSample from a pre-generated prompt list, e.g.,\\n“Please transcribe all the text in the picture.”\\nTotal 1.45M–\\nTable 5: List of datasets used for supervised fine-tuning.\\nA.3 Visual Instruction Tuning Data\\nOur final SFT data mixture contains a variety of datasets, mostly follow LLaVA-\\n1.5 [74] and LLaVA-NeXT [75]. Specifically,\\n–To encourage the model to provide long-form detailed responses and perform\\nconversations, we follow previous work, use the existing GPT-4 generated\\ndata (LLaVA-Conv and LLaVA-Complex [76]) and the existing GPT-4V gen-\\nerated data (ShareGPT-4V [15]) for model training. We also experimented\\nwith LAION-GPT4V, but did not observe further performance improvement,\\nthus not included in the final mixture.\\n–To enhance the model with better vision-language (VL) understanding capa-\\nbility, we use a variety of academic task oriented VL datasets. These datasets\\nare either in the form of image captioning, or in the form of VQA with short\\nanswers. Specifically,\\n•For natural images: VQAv2 [38], GQA [46], OKVQA [82], A-OKVQA [98],\\nand COCO Captions [18];\\n•For text-rich images: OCRVQA [86], and TextCaps [103];\\n•Fordocumentandchartunderstanding:DVQA[51],ChartQA[83],AI2D[52],\\nDocVQA [85], InfoVQA [84], and SynthDog-En [53];\\n–To enhance the model’s text-only instruction following capability, we also\\nblend in a small amount of text-only SFT data.\\n24 B. McKinzie et al.\\n129. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., Choi, Y.: Hellaswag: Can a ma-\\nchine really finish your sentence? (2019)\\n130. Zhang, H., Li, H., Li, F., Ren, T., Zou, X., Liu, S., Huang, S., Gao, J., Zhang,\\nL., Li, C., et al.: Llava-grounding: Grounded visual chat with large multimodal\\nmodels. arXiv preprint arXiv:2312.02949 (2023)\\n131. Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C.,\\nDiab, M., Li, X., Lin, X.V., et al.: Opt: Open pre-trained transformer language\\nmodels. arXiv preprint arXiv:2205.01068 (2022)\\n132. Zhao, B., Wu, B., Huang, T.: Svit: Scaling up visual instruction tuning. arXiv\\npreprint arXiv:2307.04087 (2023)\\n133. Zhou, B., Hu, Y., Weng, X., Jia, J., Luo, J., Liu, X., Wu, J., Huang, L.:\\nTinyllava: A framework of small-scale large multimodal models. arXiv preprint\\narXiv:2402.14289 (2024)\\n134. Zhu, D., Chen, J., Shen, X., Li, X., Elhoseiny, M.: Minigpt-4: Enhancing vision-\\nlanguage understanding with advanced large language models. arXiv preprint\\narXiv:2304.10592 (2023)\\n135. Zhu, Y., Zhu, M., Liu, N., Ou, Z., Mou, X., Tang, J.: Llava-phi: Efficient multi-\\nmodal assistant with small language model. arXiv preprint arXiv:2401.02330\\n(2024)\\n136. Zoph, B., Bello, I., Kumar, S., Du, N., Huang, Y., Dean, J., Shazeer, N., Fedus,\\nW.: St-moe: Designing stable and transferable sparse expert models (2022)\\n:mplug-docowl:Modularizedmultimodallargelanguagemodelfordocument\\nunderstanding. arXiv preprint arXiv:2307.02499 (2023)\\n124. Ye, Q., Xu, H., Xu, G., Ye, J., Yan, M., Zhou, Y., Wang, J., Hu, A., Shi, P.,\\nShi, Y., et al.: mplug-owl: Modularization empowers large language models with\\nmultimodality. arXiv preprint arXiv:2304.14178 (2023)\\n125. Ye, Q., Xu, H., Ye, J., Yan, M., Liu, H., Qian, Q., Zhang, J., Huang, F., Zhou,\\nJ.: mplug-owl2: Revolutionizing multi-modal large language model with modality\\ncollaboration. arXiv preprint arXiv:2311.04257 (2023)\\n126. You, H., Zhang, H., Gan, Z., Du, X., Zhang, B., Wang, Z., Cao, L., Chang, S.F.,\\nYang, Y.: Ferret: Refer and ground anything anywhere at any granularity. In:\\nICLR (2024)\\n127. Yu, W., Yang, Z., Li, L., Wang, J., Lin, K., Liu, Z., Wang, X., Wang, L.: Mm-vet:\\nEvaluating large multimodal models for integrated capabilities. arXiv preprint\\narXiv:2308.02490 (2023)\\n128. Yue, X., Ni, Y., Zhang, K., Zheng, T., Liu, R., Zhang, G., Stevens, S.,\\nJiang, D., Ren, W., Sun, Y., et al.: Mmmu: A massive multi-discipline multi-\\nmodal understanding and reasoning benchmark for expert agi. arXiv preprint\\narXiv:2311.16502 (2023)\\n: Mixtral of experts (2024)\\n50. Joshi, M., Choi, E., Weld, D.S., Zettlemoyer, L.: Triviaqa: A large scale dis-\\ntantly supervised challenge dataset for reading comprehension. arXiv preprint\\narXiv:1705.03551 (2017)\\n51. Kafle, K., Price, B., Cohen, S., Kanan, C.: Dvqa: Understanding data visualiza-\\ntions via question answering. In: CVPR (2018)\\n52. Kembhavi, A., Salvato, M., Kolve, E., Seo, M., Hajishirzi, H., Farhadi, A.: A\\ndiagram is worth a dozen images. In: ECCV (2016)\\n53. Kim, G., Hong, T., Yim, M., Nam, J., Park, J., Yim, J., Hwang, W., Yun, S., Han,\\nD., Park, S.: Ocr-free document understanding transformer. In: ECCV (2022)\\n54. Koh, J.Y., Fried, D., Salakhutdinov, R.: Generating images with multimodal lan-\\nguage models. arXiv preprint arXiv:2305.17216 (2023)\\n55. Komatsuzaki, A., Puigcerver, J., Lee-Thorp, J., Ruiz, C.R., Mustafa, B., Ainslie,\\nJ., Tay, Y., Dehghani, M., Houlsby, N.: Sparse upcycling: Training mixture-of-\\nexperts from dense checkpoints. In: ICLR (2023)\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: Summarize this PDF in less than 100 words\\nAnswer: \",\n                            \"text_info\": {\n                                \"encoding\": \"UnknownTextEnc\"\n                            }\n                        }\n                    }\n                },\n                {\n                    \"id\": \"b0f171ebf3334025bd8ff449e9060b40\",\n                    \"status\": {\n                        \"code\": 10000,\n                        \"description\": \"Ok\"\n                    },\n                    \"created_at\": \"2024-04-29T13:04:48.711852113Z\",\n                    \"model\": {\n                        \"id\": \"mistral-7B-Instruct\",\n                        \"name\": \"mistral-7B-Instruct\",\n                        \"created_at\": \"2023-09-28T16:31:37.932586Z\",\n                        \"modified_at\": \"2024-02-13T08:28:17.266611Z\",\n                        \"app_id\": \"completion\",\n                        \"model_version\": {\n                            \"id\": \"2d48077b457e4a6d899ca48a89fa91d3\",\n                            \"created_at\": \"2024-02-20T11:57:12.747884Z\",\n                            \"status\": {\n                                \"code\": 21100,\n                                \"description\": \"Model is trained and ready\"\n                            },\n                            \"completed_at\": \"2024-02-20T13:04:33.226582Z\",\n                            \"visibility\": {\n                                \"gettable\": 50\n                            },\n                            \"app_id\": \"completion\",\n                            \"user_id\": \"mistralai\",\n                            \"metadata\": {}\n                        },\n                        \"user_id\": \"mistralai\",\n                        \"model_type_id\": \"text-to-text\",\n                        \"visibility\": {\n                            \"gettable\": 50\n                        },\n                        \"toolkits\": [],\n                        \"use_cases\": [],\n                        \"languages\": [],\n                        \"languages_full\": [],\n                        \"check_consents\": [],\n                        \"workflow_recommended\": false,\n                        \"image\": {\n                            \"url\": \"https://data.clarifai.com/large/users/mistralai/apps/completion/inputs/image/aa67589e41714fe7ebe0703173736116\",\n                            \"hosted\": {\n                                \"prefix\": \"https://data.clarifai.com\",\n                                \"suffix\": \"users/mistralai/apps/completion/inputs/image/aa67589e41714fe7ebe0703173736116\",\n                                \"sizes\": [\n                                    \"small\",\n                                    \"large\"\n                                ],\n                                \"crossorigin\": \"use-credentials\"\n                            }\n                        }\n                    },\n                    \"data\": {\n                        \"text\": {\n                            \"raw\": \"1. This paper proposes a method called mplug-docowl, which is a modularized multimodal large language model for document understanding. 2. The model is designed to collaborate with other models to improve its performance. 3. It is trained on a large dataset of interleaved image-text documents. 4. The model is evaluated on several downstream tasks and achieves state-of-the-art results. 5. The authors also discuss the potential of their method for multimodal large language models.\\nB Training Details\\nB.1 Pre-training\\nWe use a version of the mplug-docowl model with 136B parameters, which is a\\nscaled-up version of the original mplug-docowl model. We perform pre-training\\non a dataset of 500M interleaved image-text documents, which is constructed\\nfrom a collection of 3B HTML files. We perform image filtering and image\\nde-duplication to remove low-quality and repetitive images. We use a batch\\nsize of 256 and a learning rate of 1e-4. We perform pre-training for 100k steps,\\nor until convergence.\\nB.2 Supervised Fine-tuning\\nWe use a version of the mplug-docowl model with 136B parameters for supervised\\nfine-tuning. We use a dataset mixture of 1.45M text-only and 500k visual in-\\nstruction tuning documents. We perform fine-tuning with a batch size of 256\\nand a learning rate of 1e-5. We perform fine-tuning for 10k steps, or until con-\\nvergence.\\nC Evaluation Details\\nC.1 Pre-training Evaluation\\nWe evaluate the pre-trained model on several downstream tasks. Specifically,\\nwe evaluate on the following tasks: TextCaps, SynthDog-EN, and ChartQA.\\nTextCaps: We evaluate the model on the TextCaps dataset, which consists of\\nimage-text pairs and requires the model to generate a caption for each image.\\nSynthDog-EN: We evaluate the model on the SynthDog-EN dataset, which\\nconsists of synthetic images and requires the\",\n                            \"text_info\": {\n                                \"encoding\": \"UnknownTextEnc\"\n                            }\n                        }\n                    }\n                }\n            ]\n        }\n    ]\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/workflows/{workflow_id}/results' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Authorization: Key {{key}}' \\\n--header 'Content-Type: application/json' \\\n--data '{\n  \"inputs\": [\n    {\n      \"data\": {\n        \"text\": {\n          \"raw\": \"Summarize this PDF in less than 100 words\"\n        }\n      }\n    }\n  ]\n}'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/workflows/celebrity-face-recognition-workflow-1twch/results/similarity": {
      "post": {
        "tags": [
          "Workflows > Workflow Essentials"
        ],
        "summary": "PostWorkflowResultsSimilarity - Embedding comparisons",
        "description": "This endpoint allows users to compare the results from workflows.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application name** |\n| `workflow_id` | **string** | **Stores the Workflow ID** |\n| `model_version` | string | **Stores the Model version.** |\n| `probe_input` | string | **Stores value for input 1** |\n| `pool_input` | string | **Stores value for input 2** |",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "example": {
                  "model_version_id": "0676ebddd5d6413ebdaa101570295a39",
                  "probe_inputs": [
                    {
                      "data": {
                        "image": {
                          "url": "https://samples.clarifai.com/brangelina_just_brad.jpg"
                        }
                      }
                    }
                  ],
                  "pool_inputs": [
                    {
                      "data": {
                        "video": {
                          "url": "https://samples.clarifai.com/brangelina_video.gif"
                        }
                      }
                    }
                  ]
                }
              }
            }
          }
        },
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"b9aeae084be1b322937fd868d34313dd\"\n    },\n    \"results\": [\n        {\n            \"probe_input\": {\n                \"id\": \"6e577eeefae349db8356c79817873d37\",\n                \"data\": {\n                    \"image\": {\n                        \"url\": \"https://samples.clarifai.com/brangelina_just_brad.jpg\"\n                    }\n                },\n                \"status\": {\n                    \"code\": 10000,\n                    \"description\": \"Ok\"\n                }\n            },\n            \"pool_results\": [\n                {\n                    \"score\": -1,\n                    \"input\": {\n                        \"id\": \"4b48934ae67e4c5eb58be9b8f9ca8d83\",\n                        \"data\": {\n                            \"video\": {\n                                \"url\": \"https://samples.clarifai.com/brangelina_video.gif\"\n                            },\n                            \"frames\": [\n                                {\n                                    \"frame_info\": {\n                                        \"index\": 0,\n                                        \"time\": 500\n                                    },\n                                    \"id\": \"faa9f3d5c8569123d8bea365ea478031\"\n                                }\n                            ]\n                        },\n                        \"status\": {\n                            \"code\": 39999,\n                            \"description\": \"No embeddings found for this input\"\n                        }\n                    }\n                },\n                {\n                    \"score\": -1,\n                    \"input\": {\n                        \"id\": \"4b48934ae67e4c5eb58be9b8f9ca8d83\",\n                        \"data\": {\n                            \"video\": {\n                                \"url\": \"https://samples.clarifai.com/brangelina_video.gif\"\n                            },\n                            \"frames\": [\n                                {\n                                    \"frame_info\": {\n                                        \"index\": 1,\n                                        \"time\": 1500\n                                    },\n                                    \"id\": \"8e5f8672bdda2f2682d59ccc019d48c0\"\n                                }\n                            ]\n                        },\n                        \"status\": {\n                            \"code\": 39999,\n                            \"description\": \"No embeddings found for this input\"\n                        }\n                    }\n                },\n                {\n                    \"score\": -1,\n                    \"input\": {\n                        \"id\": \"4b48934ae67e4c5eb58be9b8f9ca8d83\",\n                        \"data\": {\n                            \"video\": {\n                                \"url\": \"https://samples.clarifai.com/brangelina_video.gif\"\n                            },\n                            \"frames\": [\n                                {\n                                    \"frame_info\": {\n                                        \"index\": 2,\n                                        \"time\": 2500\n                                    },\n                                    \"id\": \"3f4cd8b6cbe2361de2d3a3f84906723c\"\n                                }\n                            ]\n                        },\n                        \"status\": {\n                            \"code\": 39999,\n                            \"description\": \"No embeddings found for this input\"\n                        }\n                    }\n                },\n                {\n                    \"score\": -1,\n                    \"input\": {\n                        \"id\": \"4b48934ae67e4c5eb58be9b8f9ca8d83\",\n                        \"data\": {\n                            \"video\": {\n                                \"url\": \"https://samples.clarifai.com/brangelina_video.gif\"\n                            },\n                            \"frames\": [\n                                {\n                                    \"frame_info\": {\n                                        \"index\": 3,\n                                        \"time\": 3500\n                                    },\n                                    \"id\": \"049f7331f17764126fa433ccc7eb27a6\"\n                                }\n                            ]\n                        },\n                        \"status\": {\n                            \"code\": 39999,\n                            \"description\": \"No embeddings found for this input\"\n                        }\n                    }\n                }\n            ]\n        }\n    ]\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/workflows/celebrity-face-recognition-workflow-1twch/results/similarity' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Content-Type: application/json' \\\n--data '{\n  \"model_version_id\": \"0676ebddd5d6413ebdaa101570295a39\",\n  \"probe_inputs\": [\n    {\n      \"data\": {\n        \"image\": {\n          \"url\": \"https://samples.clarifai.com/brangelina_just_brad.jpg\"\n        }\n      }\n    }\n  ],\n  \"pool_inputs\": [\n    {\n      \"data\": {\n        \"video\": {\n          \"url\": \"https://samples.clarifai.com/brangelina_video.gif\"\n        }\n      }\n    }\n  ]\n}'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/workflows/{workflow_id}/metrics": {
      "get": {
        "tags": [
          "Workflows > Workflow Metrics"
        ],
        "summary": "Get Workflow Metrics",
        "description": "This endpoint allows users to retrieve the metrics for a specific workflow.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application name** |\n| `workflow_id` | **string** | **Stores the Workflow ID** |",
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "workflow_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"2202a18532ed9006b9a7f391168b3110\"\n    },\n    \"workflow_metrics\": []\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/workflows/{workflow_id}/metrics' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      },
      "post": {
        "tags": [
          "Workflows > Workflow Metrics"
        ],
        "summary": "Add Workflow Metrics With Ground Truth Search",
        "description": "Using this request you can add workflow metrics along with ground truth which provides more insights from the predictions.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application name** |\n| `workflow_id` | **string** | **Stores the Workflow ID** |\n| `workflow_metrics_id` | **string** | **Stores the Metrics ID** |\n| `concept_id` | **string** | **Stores the Concept ID** |\n| `ground_truth` | **string** | **Stores the ground truth value** |",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "example": {
                  "workflow_metrics": [
                    {
                      "id": "workflow-metrics-{{$timestamp}}",
                      "data": {
                        "concepts": [
                          {
                            "id": "{{concept_id}}"
                          }
                        ]
                      },
                      "ground_truth": {
                        "query": {
                          "ands": [
                            {
                              "annotation": {
                                "data": {
                                  "concepts": [
                                    {
                                      "id": "{{concept_id}}"
                                    }
                                  ]
                                }
                              }
                            }
                          ]
                        }
                      }
                    }
                  ]
                }
              }
            }
          }
        },
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "workflow_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"adb6d2400c00c218c2d05e8ae09e6aed\"\n    },\n    \"workflow_metrics\": [\n        {\n            \"id\": \"workflow-metrics-1701093534\",\n            \"workflow_id\": \"firehose3\",\n            \"created_at\": \"2023-11-27T13:58:53.873005565Z\",\n            \"modified_at\": \"2023-11-27T13:58:53.873005565Z\",\n            \"status\": {\n                \"code\": 43100,\n                \"description\": \"Evaluation queued\"\n            },\n            \"data\": {\n                \"concepts\": [\n                    {\n                        \"id\": \"foo1\",\n                        \"value\": 1\n                    }\n                ]\n            },\n            \"node_metrics\": {\n                \"general-concept\": {\n                    \"status\": {\n                        \"code\": 43106,\n                        \"description\": \"Evaluation pending\"\n                    }\n                }\n            },\n            \"ground_truth\": {\n                \"query\": {\n                    \"ands\": [\n                        {\n                            \"annotation\": {\n                                \"data\": {\n                                    \"concepts\": [\n                                        {\n                                            \"id\": \"foo1\",\n                                            \"value\": 1\n                                        }\n                                    ]\n                                }\n                            }\n                        }\n                    ]\n                }\n            },\n            \"visibility\": {\n                \"gettable\": 10\n            }\n        }\n    ]\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/workflows/{workflow_id}/metrics' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Content-Type: application/json' \\\n--data '{\n  \"workflow_metrics\": [\n    {\n      \"id\": \"workflow-metrics-{{$timestamp}}\",\n      \"data\": {\n        \"concepts\": [\n          {\n            \"id\": \"{{concept_id}}\"\n          }\n        ]\n      },\n      \"ground_truth\": {\n        \"query\": {\n          \"ands\": [\n            {\n              \"annotation\": {\n                \"data\": {\n                  \"concepts\": [\n                    {\n                      \"id\": \"{{concept_id}}\"\n                    }\n                  ]\n                }\n              }\n            }\n          ]\n        }\n      }\n    }\n  ]\n}'"
          }
        ]
      },
      "delete": {
        "tags": [
          "Workflows > Workflow Metrics"
        ],
        "summary": "Delete Workflow Metrics",
        "description": "The endpoint allows users to remove a workflow metric by providing a specific ID.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application name** |\n| `workflow_id` | **string** | **Stores the Workflow ID** |\n| `workflow_metrics_id` | **string** | **Stores the Metrics ID** |",
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "workflow_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"details\": \"Workflow metrics 'workflow-metrics-1701093534' deleted\",\n        \"req_id\": \"071c1fc21c2170235b02eaf2fc1e569f\"\n    }\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff --request DELETE 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/workflows/{workflow_id}/metrics' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/workflows/{workflow_id}/metrics/{workflow_metrics_id}/nodes/general-concept": {
      "get": {
        "tags": [
          "Workflows > Workflow Metrics"
        ],
        "summary": "Get Workflow Node Metrics",
        "description": "This endpoint allows users to retrieve the metrics for a specific workflow node.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application name** |\n| `workflow_id` | **string** | **Stores the Workflow ID** |\n| `workflow_metrics_id` | string | **Stores the Metrics ID** |",
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "workflow_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "workflow_metrics_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"3809c464871e83ff06b578904efc87ca\"\n    },\n    \"node_metrics\": {\n        \"status\": {\n            \"code\": 43106,\n            \"description\": \"Evaluation pending\"\n        },\n        \"id\": \"7190d795d3f04696bab1cdb04277baeb\"\n    }\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/workflows/{workflow_id}/metrics/{workflow_metrics_id}/nodes/general-concept' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/workflows/{workflow_id}/versions": {
      "get": {
        "tags": [
          "Workflows > Workflow Versions"
        ],
        "summary": "List Workflow Versions",
        "description": "This endpoint allows you to list all versions of a workflow.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application name** |\n| `workflow_id` | **string** | **Stores the Workflow ID** |",
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "workflow_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"9988a637a50a841d59d64f37533845a6\"\n    },\n    \"workflow_versions\": [\n        {\n            \"id\": \"f81c94796fdf4661aaa7d4bffb3e614a\",\n            \"workflow_id\": \"firehose3\",\n            \"created_at\": \"2023-11-27T13:58:42.098144Z\",\n            \"modified_at\": \"2023-11-27T13:58:42.098144Z\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"nodes\": [\n                {\n                    \"id\": \"general-concept\",\n                    \"model\": {\n                        \"id\": \"general-image-recognition\",\n                        \"name\": \"Image Recognition\",\n                        \"app_id\": \"main\",\n                        \"model_version\": {\n                            \"id\": \"aa7f35c01e0642fda5cf400f543e7c40\"\n                        },\n                        \"display_name\": \"general-visual-classifier\",\n                        \"user_id\": \"clarifai\",\n                        \"model_type_id\": \"visual-classifier\",\n                        \"toolkits\": [],\n                        \"use_cases\": [],\n                        \"languages\": [],\n                        \"languages_full\": [],\n                        \"check_consents\": []\n                    },\n                    \"output_info_override\": {}\n                }\n            ],\n            \"metadata\": {},\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"user_id\": \"a0btrubbaefn\"\n        }\n    ]\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/workflows/{workflow_id}/versions' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/workflows/{workflow_id}/versions/{workflow_version_id}": {
      "get": {
        "tags": [
          "Workflows > Workflow Versions"
        ],
        "summary": "Get Workflow Version",
        "description": "This endpoint allows you to retrieve a specific version of a workflow.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application name** |\n| `workflow_id` | **string** | **Stores the Workflow ID** |\n| `workflow_version_id` | **string** | **Stores the workflow version ID** |",
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "workflow_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "workflow_version_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"1648220e1524bac8fec5282a28d5b0c7\"\n    },\n    \"workflow_version\": {\n        \"id\": \"f81c94796fdf4661aaa7d4bffb3e614a\",\n        \"workflow_id\": \"firehose3\",\n        \"created_at\": \"2023-11-27T13:58:42.098144Z\",\n        \"modified_at\": \"2023-11-27T13:58:42.098144Z\",\n        \"visibility\": {\n            \"gettable\": 10\n        },\n        \"nodes\": [\n            {\n                \"id\": \"general-concept\",\n                \"model\": {\n                    \"id\": \"general-image-recognition\",\n                    \"name\": \"Image Recognition\",\n                    \"app_id\": \"main\",\n                    \"model_version\": {\n                        \"id\": \"aa7f35c01e0642fda5cf400f543e7c40\"\n                    },\n                    \"display_name\": \"general-visual-classifier\",\n                    \"user_id\": \"clarifai\",\n                    \"model_type_id\": \"visual-classifier\",\n                    \"toolkits\": [],\n                    \"use_cases\": [],\n                    \"languages\": [],\n                    \"languages_full\": [],\n                    \"check_consents\": []\n                },\n                \"output_info_override\": {}\n            }\n        ],\n        \"metadata\": {},\n        \"app_id\": \"test-app-1700638575-empty\",\n        \"user_id\": \"a0btrubbaefn\"\n    }\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/workflows/{workflow_id}/versions/{workflow_version_id}' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      }
    },
    "/v2/users/{user_id}/apps/{app_id}/workflows/{workflow_id}/versions/": {
      "patch": {
        "tags": [
          "Workflows > Workflow Versions"
        ],
        "summary": "Patch Workflow Version",
        "description": "This endpoint allows users to modify a specific workflow version.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application name** |\n| `workflow_id` | **string** | **Stores the Workflow ID** |\n| `id` | **string** | **Stores the workflow version ID** |",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "example": {
                  "workflow_versions": [
                    {
                      "id": "{{workflow_version_id}}",
                      "metadata": {
                        "foo": "bar"
                      }
                    }
                  ],
                  "action": "merge"
                }
              }
            }
          }
        },
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "workflow_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"req_id\": \"abc43a04a7fa82f621090443a6ad4763\"\n    },\n    \"workflow_versions\": [\n        {\n            \"id\": \"f81c94796fdf4661aaa7d4bffb3e614a\",\n            \"workflow_id\": \"firehose3\",\n            \"created_at\": \"2023-11-27T13:58:42.098144Z\",\n            \"modified_at\": \"2023-11-27T14:01:23.129516622Z\",\n            \"visibility\": {\n                \"gettable\": 10\n            },\n            \"nodes\": [\n                {\n                    \"id\": \"general-concept\",\n                    \"model\": {\n                        \"id\": \"general-image-recognition\",\n                        \"name\": \"Image Recognition\",\n                        \"app_id\": \"main\",\n                        \"model_version\": {\n                            \"id\": \"aa7f35c01e0642fda5cf400f543e7c40\"\n                        },\n                        \"display_name\": \"general-visual-classifier\",\n                        \"user_id\": \"clarifai\",\n                        \"model_type_id\": \"visual-classifier\",\n                        \"toolkits\": [],\n                        \"use_cases\": [],\n                        \"languages\": [],\n                        \"languages_full\": [],\n                        \"check_consents\": []\n                    },\n                    \"output_info_override\": {}\n                }\n            ],\n            \"metadata\": {\n                \"foo\": \"bar\"\n            },\n            \"app_id\": \"test-app-1700638575-empty\",\n            \"user_id\": \"a0btrubbaefn\"\n        }\n    ]\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff --request PATCH 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/workflows/{workflow_id}/versions' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json' \\\n--header 'Content-Type: application/json' \\\n--data '{\n  \"workflow_versions\": [\n    {\n      \"id\": \"{{workflow_version_id}}\",\n      \"metadata\": {\n        \"foo\": \"bar\"\n      }\n    }\n  ],\n  \"action\": \"merge\"\n}'"
          }
        ]
      },
      "delete": {
        "tags": [
          "Workflows > Workflow Versions"
        ],
        "summary": "Delete Workflow Version",
        "description": "This endpoint allows users to delete a specific workflow version.\n\n## **Parameters**\n\n| **Name** | **Type** | **Description** |\n| --- | --- | --- |\n| **`user_id`** | **string** | **Stores the User ID** |\n| **`app_id`** | **string** | **Stores the Application name** |\n| `workflow_id` | **string** | **Stores the Workflow ID** |\n| `workflow_version_id` | **string** | **Stores the workflow version ID** |",
        "parameters": [
          {
            "name": "user_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "app_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          },
          {
            "name": "workflow_id",
            "in": "path",
            "schema": {
              "type": "string"
            },
            "required": true
          }
        ],
        "responses": {
          "default": {
            "description": "default",
            "content": {
              "text/plain": {
                "schema": {
                  "type": "string"
                },
                "example": "{\n    \"status\": {\n        \"code\": 10000,\n        \"description\": \"Ok\",\n        \"details\": \"Workflow version '399e86b1e0a84fa481143a4217d1bea0' deleted\",\n        \"req_id\": \"84baf3850aca89d5ae7499c64f40bb09\"\n    }\n}"
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "cURL",
            "label": "CURL",
            "source": "curl --location --globoff --request DELETE 'https://api.clarifai.com/v2/users/{user_id}/apps/{app_id}/workflows/{workflow_id}/versions' \\\n--header 'Authorization: Bearer YOUR_PERSONAL_ACCESS_TOKEN' \\\n--header 'Accept: application/json'"
          }
        ]
      }
    }
  }
}
# This is the sample config file for the llama model.

model:
  id: "llama-3-8b-instruct"
  user_id: "user_id"
  app_id: "app_id"
  model_type_id: "text-to-text"

build_info:
  python_version: "3.11"

inference_compute_info:
  cpu_limit: "1"
  cpu_memory: "8Gi"
  num_accelerators: 1
  accelerator_type: ["NVIDIA-T4", "NVIDIA-A10G","NVIDIA-L4","NVIDIA-L40S","NVIDIA-A100","NVIDIA-H100"]
  accelerator_memory: "12Gi"

checkpoints:
  type: "huggingface"
  repo_id: "casperhansen/llama-3-8b-instruct-awq"
  hf_token: "hf_token"
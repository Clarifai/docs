---
description: Evaluate model performance.
---

# Evaluating Models

**Learn about model evaluation tools**
<hr />

Now that you've successfully trained the model, you may want to test its performance before using it in a production environment. 
The Model Evaluation tool allows you to perform a cross validation on a specified model version. 
Once the evaluation is complete, you can view the various metrics that inform the model’s performance.

## How It Works

Model Evaluation performs a K-split cross validation on data you used to train your custom model.

![cross validation](/img/cross_validation.jpg)

In the cross validation process, it will: 

1. Set aside a random 1/K subset of the training data and designate as a test set;
2. Train a new model with the remaining training data;
3. Pass the test set data through this new model to make predictions;
4. Compare the predictions against the test set’s actual labels;
5. Repeat steps 1\) through 4\) across K splits to average out the evaluation results.

## Requirements

To run the evaluation on your custom model, it should meet the following criteria:

* It should be a custom trained model version with:
  1. At least 2 concepts.
  2. At least 10 training inputs per concept \(at least 50 inputs per concept is recommended\).

:::important Note

The evaluation may result in an error if the model version doesn’t satisfy the requirements above.

:::

## Running Evaluation

You can run the evaluation on a specific version of your custom model in the [Portal](https://portal.clarifai.com/). 

Go to your Application, click on your model of interest, and select the Versions tab. 
And to start the evaluation, simply click the Evaluate button for the specific model version

![](/img/previewui-versions-new.png)

![](/img/preview-evaluate-new.png)

The evaluation may take up to 30 minutes. Once it is complete, the Evaluate button will become a View button. 

Click the View button to see the evaluation results.

![](/img/preview-view-new.png)

For more information on how to interpret the evaluation results, check the next [Interpreting Evaluations](../evaluate/interpreting-evaluations/) section. 
 


---
description: Learn about model evaluation tools
pagination_prev: null
---

# Evaluating Models

**Evaluate a model's performance**
<hr />

Now that you've successfully trained the model, you may want to test its performance before using it in a production environment. 
The Model Evaluation tool allows you to perform a cross validation on a specified model version. Once the evaluation is complete, you can view the various metrics that inform the model’s performance.

## How It Works

Model Evaluation performs a K-split cross validation on data you used to train your custom model.

![cross validation](/img/cross_validation.jpg)

In the cross validation process, it will: 
1. Set aside a random 1/K subset of the training data and designate as a test set; 
2. Train a new model with the remaining training data; 
3. Pass the test set data through this new model to make predictions; 
4. Compare the predictions against the test set’s actual labels; and,
5. Repeat steps 1\) through 4\) across K splits to average out the evaluation results.

## Requirements

To run the evaluation on your custom model, it should meet the following criteria:

* It should be a custom trained model version with:
  1. At least 2 concepts.
  2. At least 10 training inputs per concept \(at least 50 inputs per concept is recommended\).

:::caution

The evaluation may result in an error if the model version doesn’t satisfy the requirements above.

:::

:::info

The initialization code used in the following examples is outlined in detail on the [client installation page.](https://docs.clarifai.com/api-guide/api-overview/api-clients/#client-installation-instructions)

:::

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import CodeBlock from "@theme/CodeBlock";

import PythonEvaluateModel from "!!raw-loader!../../../code_snippets/api-guide/evaluate/evaluate_model.py";
import JSEvaluateModel from "!!raw-loader!../../../code_snippets/api-guide/evaluate/evaluate_model.html";
import NodeEvaluateModel from "!!raw-loader!../../../code_snippets/api-guide/evaluate/evaluate_model.js";
import JavaEvaluateModel from "!!raw-loader!../../../code_snippets/api-guide/evaluate/evaluate_model.java";
import PHPEvaluateModel from "!!raw-loader!../../../code_snippets/api-guide/evaluate/evaluate_model.php";
import CurlEvaluateModel from "!!raw-loader!../../../code_snippets/api-guide/evaluate/evaluate_model.sh";

import PythonEvaluateModelVersion from "!!raw-loader!../../../code_snippets/api-guide/evaluate/evaluate_model_version.py";
import JSEvaluateModelVersion from "!!raw-loader!../../../code_snippets/api-guide/evaluate/evaluate_model_version.html";
import NodeEvaluateModelVersion from "!!raw-loader!../../../code_snippets/api-guide/evaluate/evaluate_model_version.js";
import JavaEvaluateModelVersion from "!!raw-loader!../../../code_snippets/api-guide/evaluate/evaluate_model_version.java";
import PHPEvaluateModelVersion from "!!raw-loader!../../../code_snippets/api-guide/evaluate/evaluate_model_version.php";
import CurlEvaluateModelVersion from "!!raw-loader!../../../code_snippets/api-guide/evaluate/evaluate_model_version.sh";

## Running Evaluation

:::tip

If evaluating an `embedding-classifier` model type, you need to set `use_kfold` to `false` in the `eval_info.params` of the evaluation request. Here is an example:
`params.update({"dataset_id": DATASET_ID, "use_kfold": False})`

:::

### PostModelVersionEvaluations

Below is an example of how you would use the `PostModelVersionEvaluations` method to run an evaluation on a specific version of a custom model.

<Tabs>

<TabItem value="python" label="Python">
    <CodeBlock className="language-python">{PythonEvaluateModelVersion}</CodeBlock>
</TabItem>

<TabItem value="js_rest" label="JavaScript (REST)">
 <CodeBlock className="language-javascript">{JSEvaluateModelVersion}</CodeBlock>
</TabItem>

<TabItem value="nodejs" label="NodeJS">
 <CodeBlock className="language-javascript">{NodeEvaluateModelVersion}</CodeBlock>
</TabItem>

<TabItem value="java" label="Java">
 <CodeBlock className="language-java">{JavaEvaluateModelVersion}</CodeBlock>
</TabItem>

<TabItem value="php" label="PHP">
    <CodeBlock className="language-php">{PHPEvaluateModelVersion}</CodeBlock>
</TabItem>

<TabItem value="curl" label="cURL">
    <CodeBlock className="language-bash">{CurlEvaluateModelVersion}</CodeBlock>
</TabItem>

</Tabs>

### PostEvaluations

Below is an example of how you would use the `PostEvaluations` method to run an evaluation on a specific version of a custom model. The method allows you to choose models and datasets from different apps that you have access to.

<Tabs>

<TabItem value="python" label="Python">
    <CodeBlock className="language-python">{PythonEvaluateModel}</CodeBlock>
</TabItem>

<TabItem value="js_rest" label="JavaScript (REST)">
 <CodeBlock className="language-javascript">{JSEvaluateModel}</CodeBlock>
</TabItem>

<TabItem value="nodejs" label="NodeJS">
 <CodeBlock className="language-javascript">{NodeEvaluateModel}</CodeBlock>
</TabItem>

<TabItem value="java" label="Java">
 <CodeBlock className="language-java">{JavaEvaluateModel}</CodeBlock>
</TabItem>

<TabItem value="php" label="PHP">
    <CodeBlock className="language-php">{PHPEvaluateModel}</CodeBlock>
</TabItem>

<TabItem value="curl" label="cURL">
    <CodeBlock className="language-bash">{CurlEvaluateModel}</CodeBlock>
</TabItem>

</Tabs>

Once the evaluation is complete, you can retrieve the results and analyze the performance of your custom model.

We'll talk about how to interpret a model's evaluation results in the next section. 

:::tip

You can also learn how to perform evaluation on the Portal [here](https://docs.clarifai.com/portal-guide/evaluate/). 

:::

import DocCardList from '@theme/DocCardList';
import {useCurrentSidebarCategory} from '@docusaurus/theme-common';

<DocCardList items={useCurrentSidebarCategory().items}/>
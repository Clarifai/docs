---
description: Learn about model evaluation tools.
---

# Evaluating Models

**Evaluate a model's performance**
<hr />

Now that you've successfully trained the model, you may want to test its performance before using it in a production environment. 
The Model Evaluation tool allows you to perform a cross validation on a specified model version. Once the evaluation is complete, you can view the various metrics that inform the model’s performance.

## How It Works

Model Evaluation performs a K-split cross validation on data you used to train your custom model.

![cross validation](/img/cross_validation.jpg)

In the cross validation process, it will: 
1. Set aside a random 1/K subset of the training data and designate as a test set; 
2. Train a new model with the remaining training data; 
3. Pass the test set data through this new model to make predictions; 
4. Compare the predictions against the test set’s actual labels; and,
5. Repeat steps 1\) through 4\) across K splits to average out the evaluation results.

## Requirements

To run the evaluation on your custom model, it should meet the following criteria:

* It should be a custom trained model version with:
  1. At least 2 concepts.
  2. At least 10 training inputs per concept \(at least 50 inputs per concept is recommended\).

:::caution

The evaluation may result in an error if the model version doesn’t satisfy the requirements above.

:::

## Running Evaluation

Below is an example of how you would run an evaluation on a specific version of a custom model. 

Note that the initialization code used here is outlined in detail on the [client installation page.](https://docs.clarifai.com/api-guide/api-overview/api-clients/#client-installation-instructions)

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import CodeBlock from "@theme/CodeBlock";

import PythonEvaluateModel from "!!raw-loader!../../../code_snippets/api-guide/evaluate/evaluate_model.py";
import JSEvaluateModel from "!!raw-loader!../../../code_snippets/api-guide/evaluate/evaluate_model.html";
import NodeEvaluateModel from "!!raw-loader!../../../code_snippets/api-guide/evaluate/evaluate_model.js";
import JavaEvaluateModel from "!!raw-loader!../../../code_snippets/api-guide/evaluate/evaluate_model.java";
import CurlEvaluateModel from "!!raw-loader!../../../code_snippets/api-guide/evaluate/evaluate_model.sh";

<Tabs>

<TabItem value="python" label="Python">
    <CodeBlock className="language-python">{PythonEvaluateModel}</CodeBlock>
</TabItem>

<TabItem value="js_rest" label="JavaScript (REST)">
 <CodeBlock className="language-javascript">{JSEvaluateModel}</CodeBlock>
</TabItem>

<TabItem value="nodejs" label="NodeJS">
 <CodeBlock className="language-javascript">{NodeEvaluateModel}</CodeBlock>
</TabItem>

<TabItem value="java" label="Java">
 <CodeBlock className="language-java">{JavaEvaluateModel}</CodeBlock>
</TabItem>

<TabItem value="curl" label="cURL">
    <CodeBlock className="language-bash">{CurlEvaluateModel}</CodeBlock>
</TabItem>

</Tabs>

Once the evaluation is complete, you can retrieve the results and analyze the performance of your custom model.

We'll talk about how to interpret a model's evaluation results in the next section. 

:::tip

You can also learn how to perform evaluation on the Portal [here](https://docs.clarifai.com/portal-guide/evaluate/). 

:::

